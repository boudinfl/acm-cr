@article{10.5555/2051237.2051248,
author = {Kurland, Oren and Krikon, Eyal},
title = {The Opposite of Smoothing: A Language Model Approach to Ranking Query-Specific Document Clusters},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Exploiting information induced from (query-specific) clustering of top-retrieved documents has long been proposed as a means for improving precision at the very top ranks of the returned results. We present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. While most previous cluster ranking approaches focus on the cluster as a whole, our model utilizes also information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identifying clusters containing a high relevant-document percentage. Furthermore, using the model to produce document ranking yields precision-at-top-ranks performance that is consistently better than that of the initial ranking upon which clustering is performed. The performance also favorably compares with that of a state-of-the-art pseudo-feedback-based retrieval method.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {367–395},
numpages = {29}
}

@article{10.5555/2051237.2051247,
author = {Siddiqi, Sajjad and Huang, Jinbo},
title = {Sequential Diagnosis by Abstraction},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {When a system behaves abnormally, sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified, and the goal is to reduce the diagnostic cost, defined here as the number of measurements. To propose measurement points, previous work employs a heuristic based on reducing the entropy over a computed set of diagnoses. This approach generally has good performance in terms of diagnostic cost, but can fail to diagnose large systems when the set of diagnoses is too large. Focusing on a smaller set of probable diagnoses scales the approach but generally leads to increased average diagnostic costs. In this paper, we propose a new diagnostic framework employing four new techniques, which scales to much larger systems with good performance in terms of diagnostic cost. First, we propose a new heuristic for measurement point selection that can be computed efficiently, without requiring the set of diagnoses, once the system is modeled as a Bayesian network and compiled into a logical form known as d-DNNF. Second, we extend hierarchical diagnosis, a technique based on system abstraction from our previous work, to handle probabilities so that it can be applied to sequential diagnosis to allow larger systems to be diagnosed. Third, for the largest systems where even hierarchical diagnosis fails, we propose a novel method that converts the system into one that has a smaller abstraction and whose diagnoses form a superset of those of the original system; the new system can then be diagnosed and the result mapped back to the original system. Finally, we propose a novel cost estimation function which can be used to choose an abstraction of the system that is more likely to provide optimal average cost. Experiments with ISCAS-85 benchmark circuits indicate that our approach scales to all circuits in the suite except one that has a flat structure not susceptible to useful abstraction.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {329–365},
numpages = {37}
}

@article{10.5555/2051237.2051246,
author = {Korzhyk, Dmytro and Yin, Zhengyu and Kiekintveld, Christopher and Conitzer, Vincent and Tambe, Milind},
title = {Stackelberg vs. Nash in Security Games: An Extended Investigation of Interchangeability, Equivalence, and Uniqueness},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {There has been significant recent interest in game-theoretic approaches to security, with much of the recent research focused on utilizing the leader-follower Stackelberg game model. Among the major applications are the ARMOR program deployed at LAX Airport and the IRIS program in use by the US Federal Air Marshals (FAMS). The foundational assumption for using Stackelberg games is that security forces (leaders), acting first, commit to a randomized strategy; while their adversaries (followers) choose their best response after surveillance of this randomized strategy. Yet, in many situations, a leader may face uncertainty about the follower's surveillance capability. Previous work fails to address how a leader should compute her strategy given such uncertainty.We provide five contributions in the context of a general class of security games. First, we show that the Nash equilibria in security games are interchangeable, thus alleviating the equilibrium selection problem. Second, under a natural restriction on security games, any Stackelberg strategy is also a Nash equilibrium strategy; and furthermore, the solution is unique in a class of security games of which ARMOR is a key exemplar. Third, when faced with a follower that can attack multiple targets, many of these properties no longer hold. Fourth, we show experimentally that in most (but not all) games where the restriction does not hold, the Stackelberg strategy is still a Nash equilibrium strategy, but this is no longer true when the attacker can attack multiple targets. Finally, as a possible direction for future research, we propose an extensive-form game model that makes the defender's uncertainty about the attacker's ability to observe explicit.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {297–327},
numpages = {31}
}

@article{10.5555/2051237.2051245,
author = {Kozorovitsky, Anna Khudyak and Kurland, Oren},
title = {From "Identical" to "Similar": Fusing Retrieved Lists Based on Inter-Document Similarities},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Methods for fusing document lists that were retrieved in response to a query often utilize the retrieval scores and/or ranks of documents in the lists. We present a novel fusion approach that is based on using, in addition, information induced from inter-document similarities. Specifically, our methods let similar documents from different lists provide relevance-status support to each other. We use a graph-based method to model relevance-status propagation between documents. The propagation is governed by inter-document-similarities and by retrieval scores of documents in the lists. Empirical evaluation demonstrates the effectiveness of our methods in fusing TREC runs. The performance of our most effective methods transcends that of effective fusion methods that utilize only retrieval scores or ranks.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {267–296},
numpages = {30}
}

@article{10.5555/2051237.2051244,
author = {Joshi, Saket and Khardon, Roni},
title = {Probabilistic Relational Planning with First Order Decision Diagrams},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision diagrams, to capture domain dynamics and value functions. Work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes. Recent work introduced a first order variant of decision diagrams (FODD) and developed a value iteration algorithm for this representation. This paper develops several improvements to the FODD algorithm that make the approach practical. These include, new reduction operators that decrease the size of the representation, several speedup techniques, and techniques for value approximation. Incorporating these, the paper presents a planning system, FODD-PLANNER, for solving relational stochastic planning problems. The system is evaluated on several domains, including problems from the recent international planning competition, and shows competitive performance with top ranking systems. This is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {231–266},
numpages = {36}
}

@article{10.5555/2051237.2051243,
author = {Hoffmann, J\"{o}rg},
title = {Analyzing Search Topology without Running Any Search: On the Connection between Causal Graphs and h<sup>+</sup>},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {The ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning. In earlier work, it was observed that the optimal relaxation heuristic h+ has amazing qualities in many classical planning benchmarks, in particular pertaining to the complete absence of local minima. The proofs of this are hand-made, raising the question whether such proofs can be lead automatically by domain analysis techniques. In contrast to earlier disappointing results - the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains - we herein answer this question in the afirmative. We establish connections between causal graph structure and h+ topology. This results in low-order polynomial time analysis methods, implemented in a tool we call TorchLight. Of the 12 domains where the absence of local minima has been proved, TorchLight gives strong success guarantees in 8 domains. Empirically, its analysis exhibits strong performance in a further 2 of these domains, plus in 4 more domains where local minima may exist but are rare. In this way, TorchLight can distinguish "easy" domains from "hard" ones. By summarizing structural reasons for analysis failure, TorchLight also provides diagnostic output indicating domain aspects that may cause local minima.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {155–229},
numpages = {75}
}

@article{10.5555/2051237.2051242,
author = {Gujar, Sujit and Narahari, Y.},
title = {Redistribution Mechanisms for Assignment of Heterogeneous Objects},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {There are p heterogeneous objects to be assigned to n competing agents (n &gt; p) each with unit demand. It is required to design a Groves mechanism for this assignment problem satisfying weak budget balance, individual rationality, and minimizing the budget imbalance. This calls for designing an appropriate rebate function. When the objects are identical, this problem has been solved which we refer as WCO mechanism. We measure the performance of such mechanisms by the redistribution index. We first prove an impossibility theorem which rules out linear rebate functions with non-zero redistribution index in heterogeneous object assignment. Motivated by this theorem, we explore two approaches to get around this impossibility. In the first approach, we show that linear rebate functions with non-zero redistribution index are possible when the valuations for the objects have a certain type of relationship and we design a mechanism with linear rebate function that is worst case optimal. In the second approach, we show that rebate functions with non-zero efficiency are possible if linearity is relaxed. We extend the rebate functions of the WCO mechanism to heterogeneous objects assignment and conjecture them to be worst case optimal.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {131–154},
numpages = {24}
}

@article{10.5555/2051237.2051241,
author = {Hebrard, Emmanuel and Marx, D\'{a}niel and O'Sullivan, Barry and Razgon, Igor},
title = {Soft Constraints of Difference and Equality},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {In many combinatorial problems one may need to model the diversity or similarity of sets of assignments. For example, one may wish to maximise or minimise the number of distinct values in a solution. To formulate problems of this type we can use soft variants of the well known ALLDIFFERENT and ALLEQUAL constraints. We present a taxonomy of six soft global constraints, generated by combining the two latter ones and the two standard cost functions, which are either maximised or minimised. We characterise the complexity of achieving arc and bounds consistency on these constraints, resolving those cases for which NP-hardness was neither proven nor disproven. In particular, we explore in depth the constraint ensuring that at least k pairs of variables have a common value. We show that achieving arc consistency is NP-hard, however bounds consistency can be achieved in polynomial time through dynamic programming. Moreover, we show that the maximum number of pairs of equal variables can be approximated by a factor of 1/2 with a linear time greedy algorithm. Finally, we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {97–130},
numpages = {34}
}

@article{10.5555/2051237.2051240,
author = {Bilgic, Mustafa and Getoor, Lise},
title = {Value of Information Lattice: Exploiting Probabilistic Independence for Effective Feature Subset Acquisition},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. We describe the Value of Information Lattice (VOILA), an optimal and eficient feature subset acquisition framework. Unlike the common practice, which is to acquire features greedily, VOILA can reason with subsets of features. VOILA eficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {69–95},
numpages = {27}
}

@article{10.5555/2051237.2051239,
author = {Xia, Lirong and Conitzer, Vincent},
title = {Determining Possible and Necessary Winners under Common Voting Rules given Partial Orders},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Usually a voting rule requires agents to give their preferences as linear orders. However, in some cases it is impractical for an agent to give a linear order over all the alternatives. It has been suggested to let agents submit partial orders instead. Then, given a voting rule, a profile of partial orders, and an alternative (candidate) c, two important questions arise: first, is it still possible for c to win, and second, is c guaranteed to win? These are the possible winner and necessary winner problems, respectively. Each of these two problems is further divided into two sub-problems: determining whether c is a unique winner (that is, c is the only winner), or determining whether c is a co-winner (that is, c is in the set of winners).We consider the setting where the number of alternatives is unbounded and the votes are unweighted. We completely characterize the complexity of possible/necessary winner problems for the following common voting rules: a class of positional scoring rules (including Borda), Copeland, maximin, Bucklin, ranked pairs, voting trees, and plurality with runoff.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {25–67},
numpages = {43}
}

@article{10.5555/2051237.2051238,
author = {Cseke, Botond and Heskes, Tom},
title = {Properties of Bethe Free Energies and Message Passing in Gaussian Models},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We address the problem of computing approximate marginals in Gaussian probabilistic models by using mean field and fractional Bethe approximations. We define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate marginals, derive a lower and an upper bound on the fractional Bethe free energy and establish a necessary condition for the lower bound to be bounded from below. It turns out that the condition is identical to the pairwise normalizability condition, which is known to be a sufficient condition for the convergence of the message passing algorithm. We show that stable fixed points of the Gaussian message passing algorithm are local minima of the Gaussian Bethe free energy. By a counterexample, we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–24},
numpages = {24}
}

@article{10.5555/2016945.2016967,
author = {Papadopoulos, Harris and Vovk, Vladimir and Gammerman, Alex},
title = {Regression Conformal Prediction with Nearest Neighbours},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours Regression (k-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {815–840},
numpages = {26}
}

@article{10.5555/2016945.2016966,
author = {de la Rosa, Tom\'{a}s and Jim\'{e}nez, Sergio and Fuentetaja, Raquel and Borrajo, Daniel},
title = {Scaling up Heuristic Planning with Relational Decision Trees},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {767–813},
numpages = {47}
}

@article{10.5555/2016945.2016965,
author = {Li, Wei and Poupart, Pascal and van Beek, Peter},
title = {Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations-- two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisy-OR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {729–765},
numpages = {37}
}

@article{10.5555/2016945.2016964,
author = {Cimatti, Alessandro and Griggio, Alberto and Sebastiani, Roberto},
title = {Computing Small Unsatisfiable Cores in Satisfiability modulo Theories},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {The problem of finding small unsatisfiable cores for SAT formulas has recently received a lot of interest, mostly for its applications in formal verification. However, propositional logic is often not expressive enough for representing many interesting verification problems, which can be more naturally addressed in the framework of Satisfiability Modulo Theories, SMT. Surprisingly, the problem of finding unsatisfiable cores in SMT has received very little attention in the literature.In this paper we present a novel approach to this problem, called the Lemma-Lifting approach. The main idea is to combine an SMT solver with an external propositional core extractor. The SMT solver produces the theory lemmas found during the search, dynamically lifting the suitable amount of theory information to the Boolean level. The core extractor is then called on the Boolean abstraction of the original SMT problem and of the theory lemmas. This results in an unsatisfiable core for the original SMT problem, once the remaining theory lemmas are removed.The approach is conceptually interesting, and has several advantages in practice. In fact, it is extremely simple to implement and to update, and it can be interfaced with every propositional core extractor in a plug-and-play manner, so as to benefit for free of all unsat-core reduction techniques which have been or will be made available.We have evaluated our algorithm with a very extensive empirical test on SMT-LIB benchmarks, which confirms the validity and potential of this approach.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {701–728},
numpages = {28}
}

@article{10.5555/2016945.2016963,
author = {Wu, Fei and Madhavan, Jayant and Halevy, Alon},
title = {Identifying Aspects for Web-Search Queries},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Many web-search queries serve as the beginning of an exploration of an unknown space of information, rather than looking for a specific web page. To answer such queries effectively, the search engine should attempt to organize the space of relevant information in a way that facilitates exploration.We describe the ASPECTOR system that computes aspects for a given query. Each aspect is a set of search queries that together represent a distinct information need relevant to the original search query. To serve as an effective means to explore the space, ASPECTOR computes aspects that are orthogonal to each other and to have high combined coverage.ASPECTOR combines two sources of information to compute aspects. We discover candidate aspects by analyzing query logs, and cluster them to eliminate redundancies. We then use a mass-collaboration knowledge base (e.g., Wikipedia) to compute candidate aspects for queries that occur less frequently and to group together aspects that are likely to be "semantically" related. We present a user study that indicates that the aspects we compute are rated favorably against three competing alternatives - related searches proposed by Google, cluster labels assigned by the Clusty search engine, and navigational searches proposed by Bing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {677–700},
numpages = {24}
}

@article{10.5555/2016945.2016962,
author = {Bordeaux, Lucas and Katsirelos, George and Narodytska, Nina and Vardi, Moshe Y.},
title = {The Complexity of Integer Bound Propagation},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Bound propagation is an important Artificial Intelligence technique used in Constraint Programming tools to deal with numerical constraints. It is typically embedded within a search procedure ("branch and prune") and used at every node of the search tree to narrow down the search space, so it is critical that it be fast. The procedure invokes constraint propagators until a common fixpoint is reached, but the known algorithms for this have a pseudo-polynomial worst-case time complexity: they are fast indeed when the variables have a small numerical range, but they have the well-known problem of being prohibitively slow when these ranges are large. An important question is therefore whether strongly-polynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints. This paper answers this question. In particular we show that this fixpoint computation is in fact NP-complete, even when restricted to binary linear constraints.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {657–676},
numpages = {20}
}

@article{10.5555/2016945.2016961,
author = {Aravantinos, Vincent and Caferra, Ricardo and Peltier, Nicolas},
title = {Decidability and Undecidability Results for Propositional Schemata},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {We define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions (e.g., pi) and iterated connectives ∨ or ∧ ranging over intervals parameterized by arithmetic variables (e.g., ∧i-1n pi, where n is a parameter). The satisfiability problem is shown to be undecidable for this new logic, but we introduce a very general class of schemata, called bound-linear, for which this problem becomes decidable. This result is obtained by reduction to a particular class of schemata called regular, for which we provide a sound and complete terminating proof procedure. This schemata calculus (called STAB) allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic. We also show that the satisfiability problem becomes again undecidable for slight extensions of this class, thus demonstrating that bound-linear schemata represent a good compromise between expressivity and decidability},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {599–656},
numpages = {58}
}

@article{10.5555/2016945.2016960,
author = {Kash, Ian A. and Friedman, Eric J. and Halpern, Joseph Y.},
title = {Multiagent Learning in Large Anonymous Games},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {In large systems, it is important for agents to learn to act effectively, but sophisticated multi-agent learning algorithms generally do not scale. An alternative approach is to find restricted classes of games where simple, efficient algorithms converge. It is shown that stage learning efficiently converges to Nash equilibria in large anonymous games if bestreply dynamics converge. Two features are identified that improve convergence. First, rather than making learning more difficult, more agents are actually beneficial in many settings. Second, providing agents with statistical information about the behavior of others can significantly reduce the number of observations needed.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {571–598},
numpages = {28}
}

@article{10.5555/2016945.2016959,
author = {He, Ruijie and Brunskill, Emma and Roy, Nicholas},
title = {Efficient Planning under Uncertainty with Macro-Actions},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Deciding how to act in partially observable environments remains an active area of research. Identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states. Towards addressing this challenge, we present an online, forward-search algorithm called the Posterior Belief Distribution (PBD). PBD leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken, given the set of observation sequences that could be received during this process. This method allows us to efficiently evaluate the expected reward of a sequence of primitive actions, which we refer to as macro-actions. We present a formal analysis of our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment, which suggests that our approach has practical potential for planning in real-world, large partially observable domains where a multistep lookahead is required to achieve good performance.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523–570},
numpages = {48}
}

@article{10.5555/2016945.2016958,
author = {Rahman, Altaf and Ng, Vincent},
title = {Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Traditional learning-based coreference resolvers operate by training the mention-pair model for determining whether two mentions are coreferent or not. Though conceptually simple and easy to understand, the mention-pair model is linguistically rather unappealing and lags far behind the heuristic-based coreference models proposed in the prestatistical NLP era in terms of sophistication. Two independent lines of recent research have attempted to improve the mention-pair model, one by acquiring the mention-ranking model to rank preceding mentions for a given anaphor, and the other by training the entity-mention model to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution, which combines the strengths of the mention-ranking model and the entity-mention model, and is therefore theoretically more appealing than both of these models. In addition, we seek to improve cluster rankers via two extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution. Experimental results on the ACE data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {469–521},
numpages = {53}
}

@article{10.5555/2016945.2016957,
author = {Ruml, Wheeler and Do, Minh Binh and Zhou, Rong and Fromherz, Markus P. J.},
title = {On-Line Planning and Scheduling: An Application to Controlling Modular Printers},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {We present a case study of artificial intelligence techniques applied to the control of production printing equipment. Like many other real-world applications, this complex domain requires high-speed autonomous decision-making and robust continual operation. To our knowledge, this work represents the first successful industrial application of embedded domain-independent temporal planning. Our system handles execution failures and multi-objective preferences. At its heart is an on-line algorithm that combines techniques from state-space planning and partial-order scheduling. We suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual, on-line settings. Our system has been used to drive several commercial prototypes and has enabled a new product architecture for our industrial partner. When compared with state-of-the-art off-line planners, our system is hundreds of times faster and often finds better plans. Our experience demonstrates that domain-independent AI planning based on heuristic search can flexibly handle time, resources, replanning, and multiple objectives in a high-speed practical application without requiring hand-coded control knowledge.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {415–468},
numpages = {54}
}

@article{10.5555/2016945.2016956,
author = {Tannier, Xavier and Muller, Philippe},
title = {Evaluating Temporal Graphs Built from Texts via Transitive Reduction},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Temporal information has been the focus of recent attention in information extraction, leading to some standardization effort, in particular for the task of relating events in a text. This task raises the problem of comparing two annotations of a given text, because relations between events in a story are intrinsically interdependent and cannot be evaluated separately. A proper evaluation measure is also crucial in the context of a machine learning approach to the problem. Finding a common comparison referent at the text level is not obvious, and we argue here in favor of a shift from eventbased measures to measures on a unique textual object, a minimal underlying temporal graph, or more formally the transitive reduction of the graph of relations between event boundaries. We support it by an investigation of its properties on synthetic data and on a well-know temporal corpus.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {375–413},
numpages = {39}
}

@article{10.5555/2016945.2016955,
author = {Atserias, Albert and Fichte, Johannes Klaus and Thurley, Marc},
title = {Clause-Learning Algorithms with Many Restarts and Bounded-Width Resolution},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {We offer a new understanding of some aspects of practical SAT-solvers that are based on DPLL with unit-clause propagation, clause-learning, and restarts. We do so by analyzing a concrete algorithm which we claim is faithful to what practical solvers do. In particular, before making any new decision or restart, the solver repeatedly applies the unit-resolution rule until saturation, and leaves no component to the mercy of non-determinism except for some internal randomness. We prove the perhaps surprising fact that, although the solver is not explicitly designed for it, with high probability it ends up behaving as width-k resolution after no more than O(n2k+2) conflicts and restarts, where n is the number of variables. In other words, width-k resolution can be thought of as O(n2k+2) restarts of the unit-resolution rule with learning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {353–373},
numpages = {21}
}

@article{10.5555/2016945.2016954,
author = {Faliszewski, Piotr and Hemaspaandra, Edith and Hemaspaandra, Lane A.},
title = {Multimode Control Attacks on Elections},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on elections-- attempts to improve the election outcome by such actions as adding/deleting candidates or voters. That work has led to many results on how algorithms can be used to find attacks on elections and how complexity-theoretic hardness results can be used as shields against attacks. However, all the work in this line has assumed that the attacker employs just a single type of attack. In this paper, we model and study the case in which the attacker launches a multipronged (i.e., multimode) attack. We do so to more realistically capture the richness of real-life settings. For example, an attacker might simultaneously try to suppress some voters, attract new voters into the election, and introduce a spoiler candidate. Our model provides a unified framework for such varied attacks. By constructing polynomialtime multiprong attack algorithms we prove that for various election systems even such concerted, flexible attacks can be perfectly planned in deterministic polynomial time.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {305–351},
numpages = {47}
}

@article{10.5555/2016945.2016953,
author = {Hunter, Aaron and Delgrande, James P.},
title = {Iterated Belief Change Due to Actions and Observations},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {In action domains where agents may have erroneous beliefs, reasoning about the effects of actions involves reasoning about belief change. In this paper, we use a transition system approach to reason about the evolution of an agent's beliefs as actions are executed. Some actions cause an agent to perform belief revision while others cause an agent to perform belief update, but the interaction between revision and update can be nonelementary. We present a set of rationality properties describing the interaction between revision and update, and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates. Our belief change operators can be characterized in terms of a natural shifting operation on total pre-orderings over interpretations. We compare our approach with related work on iterated belief change due to action, and we conclude with some directions for future research.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {269–304},
numpages = {36}
}

@article{10.5555/2016945.2016952,
author = {Wang, Yonghong and Hang, Chung-Wei and Singh, Munindar P.},
title = {A Probabilistic Approach for Maintaining Trust Based on Evidence},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Leading agent-based trust models address two important needs. First, they show how an agent may estimate the trustworthiness of another agent based on prior interactions. Second, they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others. However, in real-life settings, information relevant to trust is usually obtained piecemeal, not all at once. Unfortunately, the problem of maintaining trust has drawn little attention. Existing approaches handle trust updates in a heuristic, not a principled, manner.This paper builds on a formal model that considers probability and certainty as two dimensions of trust. It proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis. This paper shows via simulation that the proposed approach (a) provides accurate estimates of the trustworthiness of agents that change behavior frequently; and (b) captures the dynamic behavior of the agents. This paper includes an evaluation based on a real dataset drawn from Amazon Marketplace, a leading e-commerce site.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {221–267},
numpages = {47}
}

@article{10.5555/2016945.2016951,
author = {Lecoutre, Christophe and Cardon, St\'{e}phane and Vion, Julien},
title = {Second-Order Consistencies},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we propose a comprehensive study of second-order consistencies (i.e., consistencies identifying inconsistent pairs of values) for constraint satisfaction. We build a full picture of the relationships existing between four basic second-order consistencies, namely path consistency (PC), 3-consistency (3C), dual consistency (DC) and 2-singleton arc consistency (2SAC), as well as their conservative and strong variants. Interestingly, dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency (GAC), which makes it rather easy to obtain since constraint solvers typically maintain GAC during search. On binary constraint networks, DC is equivalent to PC, but its restriction to existing constraints, called conservative dual consistency (CDC), is strictly stronger than traditional conservative consistencies derived from path consistency, namely partial path consistency (PPC) and conservative path consistency (CPC). After introducing a general algorithm to enforce strong (C)DC, we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of (conservative) dual consistency. In particular, we show that enforcing (C)DC before search clearly improves the performance of MAC (the algorithm that maintains GAC during search) on several binary and non-binary structured problems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {175–219},
numpages = {45}
}

@article{10.5555/2016945.2016950,
author = {Geist, Christian and Endriss, Ulle},
title = {Automated Search for Impossibility Theorems in Social Choice Theory: Ranking Sets of Objects},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {We present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects. The key question in this area, which has important applications in social choice theory and decision making under uncertainty, is how to extend an agent's preferences over a number of objects to a preference relation over nonempty sets of such objects. Certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies, which has led to a number of important impossibility theorems. We first prove a general result that shows that for a wide range of such principles, characterised by their syntactic form when expressed in a many-sorted first-order logic, any impossibility exhibited at a fixed (small) domain size will necessarily extend to the general case. We then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic, which in turn enables us to automatically search for (general) impossibility theorems using a SAT solver. When applied to a space of 20 principles for preference extension familiar from the literature, this method yields a total of 84 impossibility theorems, including both known and nontrivial new results.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {143–174},
numpages = {32}
}

@article{10.5555/2016945.2016949,
author = {Veness, Joel and Ng, Kee Siong and Hutter, Marcus and Uther, William and Silver, David},
title = {A Monte-Carlo AIXI Approximation},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. Our approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a new Monte-Carlo Tree Search algorithm along with an agent-specific extension to the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a variety of stochastic and partially observable domains. We conclude by proposing a number of directions for future research.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {95–142},
numpages = {48}
}

@article{10.5555/2016945.2016948,
author = {Aziz, Haris and Bachrach, Yoram and Elkind, Edith and Paterson, Mike},
title = {False-Name Manipulations in Weighted Voting Games},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Weighted voting is a classic model of cooperation among agents in decision-making domains. In such games, each player has a weight, and a coalition of players wins the game if its total weight meets or exceeds a given quota. A player's power in such games is usually not directly proportional to his weight, and is measured by a power index, the most prominent among which are the Shapley-Shubik index and the Banzhaf index.In this paper, we investigate by how much a player can change his power, as measured by the Shapley-Shubik index or the Banzhaf index, by means of a false-name manipulation, i.e., splitting his weight among two or more identities. For both indices, we provide upper and lower bounds on the effect of weight-splitting. We then show that checking whether a beneficial split exists is NP-hard, and discuss efficient algorithms for restricted cases of this problem, as well as randomized algorithms for the general case. We also provide an experimental evaluation of these algorithms.Finally, we examine related forms of manipulative behavior, such as annexation, where a player subsumes other players, or merging, where several players unite into one. We characterize the computational complexity of such manipulations and provide limits on their effects. For the Banzhaf index, we describe a new paradox, which we term the Annexation Non-monotonicity Paradox.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {57–93},
numpages = {37}
}

@article{10.5555/2016945.2016947,
author = {Zhou, Yi and Zhang, Yan},
title = {A Logical Study of Partial Entailment},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {We introduce a novel logical notion-partial entailment-to propositional logic. In contrast with classical entailment, that a formula P partially entails another formula Q with respect to a background formula set Γ intuitively means that under the circumstance of Γ, if P is true then some "part" of Q will also be true. We distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant. We study their semantic properties, which show that, surprisingly, partial entailments fail for many simple inference rules. Then, we study the related computational properties, which indicate that partial entailments are relatively difficult to be computed. Finally, we consider a potential application of partial entailments in reasoning about rational agents.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {25–56},
numpages = {32}
}

@article{10.5555/2016945.2016946,
author = {Fard, Mahdi Milani and Pineau, Joelle},
title = {Non-Deterministic Policies in Markovian Decision Processes},
year = {2011},
issue_date = {January 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {40},
number = {1},
issn = {1076-9757},
abstract = {Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the user's input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems.This paper introduces the new concept of non-deterministic policies to allow more flexibility in the user's decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–24},
numpages = {24}
}

@article{10.5555/1946417.1946434,
author = {Xu, Jing and Shelton, Christian R.},
title = {Intrusion Detection Using Continuous Time Bayesian Networks},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Intrusion detection systems (IDSs) fall into two high-level categories: network-based systems (NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls. In this work, we present a general technique for both systems. We use anomaly detection, which identifies patterns not conforming to a historic norm. In both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid specifying a fixed update interval common to discrete-time models. We build generative models from the normal training data, and abnormal behaviors are flagged based on their likelihood under this norm. For NIDS, we construct a hierarchical CTBN model for the network packet traces and use Rao-Blackwellized particle filtering to learn the parameters. We illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. We demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {745–774},
numpages = {30}
}

@article{10.5555/1946417.1946433,
author = {Burns, Ethan and Lemons, Sofia and Ruml, Wheeler and Zhou, Rong},
title = {Best-First Heuristic Search for Multicore Machines},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. PBNF allows speculative expansions when necessary to keep threads busy. We identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. Our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than improved versions of previous parallel search proposals.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {689–743},
numpages = {55}
}

@article{10.5555/1946417.1946432,
author = {J\"{a}ger, Gerold and Zhang, Weixiong},
title = {An Effective Algorithm for and Phase Transitions of the Directed Hamiltonian Cycle Problem},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {The Hamiltonian cycle problem (HCP) is an important combinatorial problem with applications in many areas. It is among the first problems used for studying intrinsic properties, including phase transitions, of combinatorial problems. While thorough theoretical and experimental analyses have been made on the HCP in undirected graphs, a limited amount of work has been done for the HCP in directed graphs (DHCP). The main contribution of this work is an effective algorithm for the DHCP. Our algorithm explores and exploits the close relationship between the DHCP and the Assignment Problem (AP) and utilizes a technique based on Boolean satisfiability (SAT). By combining effective algorithms for the AP and SAT, our algorithm significantly outperforms previous exact DHCP algorithms, including an algorithm based on the award-winning Concorde TSP algorithm. The second result of the current study is an experimental analysis of phase transitions of the DHCP, verifying and refining a known phase transition of the DHCP.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {663–687},
numpages = {25}
}

@article{10.5555/1946417.1946431,
author = {Krause, Andreas and Horvitz, Eric},
title = {A Utility-Theoretic Approach to Privacy in Online Services},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Online offerings such as web search, news portals, and e-commerce applications face the challenge of providing high-quality service to a large, heterogeneous user base. Recent efforts have highlighted the potential to improve performance by introducing methods to personalize services based on special knowledge about users and their context. For example, a user's demographics, location, and past search and browsing may be useful in enhancing the results offered in response to web search queries. However, reasonable concerns about privacy by both users, providers, and government agencies acting on behalf of citizens, may limit access by services to such information. We introduce and explore an economics of privacy in personalization, where people can opt to share personal information, in a standing or on-demand manner, in return for expected enhancements in the quality of an online service. We focus on the example of web search and formulate realistic objective functions for search efficacy and privacy. We demonstrate how we can find a provably near-optimal optimization of the utility-privacy tradeoff in an efficient manner. We evaluate our methodology on data drawn from a log of the search activity of volunteer participants. We separately assess users preferences about privacy and utility via a large-scale survey, aimed at eliciting preferences about peoples willingness to trade the sharing of personal data in returns for gains in search efficiency. We show that a significant level of personalization can be achieved using a relatively small amount of information about users.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {633–662},
numpages = {30}
}

@article{10.5555/1946417.1946430,
author = {Dasgupta, Sajib and Ng, Vincent},
title = {Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {While traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the author's mood, gender, age, or sentiment. Without knowing the user's intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the user's intention or having a human construct a feature space in an interactive manner during the clustering process. With the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. We demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {581–632},
numpages = {52}
}

@article{10.5555/1946417.1946429,
author = {Daniel, Kenny and Nash, Alex and Koenig, Sven and Felner, Ariel},
title = {Theta*: Any-Angle Path Planning on Grids},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Grids with blocked and unblocked cells are often used to represent terrain in robotics and video games. However, paths formed by grid edges can be longer than true shortest paths in the terrain since their headings are artificially constrained. We present two new correct and complete anyangle path-planning algorithms that avoid this shortcoming. Basic Theta* and Angle-Propagation Theta* are both variants of A* that propagate information along grid edges without constraining paths to grid edges. Basic Theta* is simple to understand and implement, fast and finds short paths. However, it is not guaranteed to find true shortest paths. Angle-Propagation Theta* achieves a better worst-case complexity per vertex expansion than Basic Theta* by propagating angle ranges when it expands vertices, but is more complex, not as fast and finds slightly longer paths. We refer to Basic Theta* and Angle-Propagation Theta* collectively as Theta*. Theta* has unique properties, which we analyze in detail. We show experimentally that it finds shorter paths than both A* with post-smoothed paths and Field D* (the only other version of A* we know of that propagates information along grid edges without constraining paths to grid edges) with a runtime comparable to that of A* on grids. Finally, we extend Theta* to grids that contain unblocked cells with non-uniformtraversal costs and introduce variants of Theta* which provide different tradeoffs between path length and runtime.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {533–579},
numpages = {47}
}

@article{10.5555/1946417.1946428,
author = {Geist, Matthieu and Pietquin, Olivier},
title = {Kalman Temporal Differences},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {483–532},
numpages = {50}
}

@article{10.5555/1946417.1946427,
author = {Rudolph, Sebastian and Glimm, Birte},
title = {Nominals, Inverses, Counting, and Conjunctive Queries or: Why Infinity is Your Friend!},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Description Logics are knowledge representation formalisms that provide, for example, the logical underpinning of the W3C OWL standards. Conjunctive queries, the standard query language in databases, have recently gained significant attention as an expressive formalism for querying Description Logic knowledge bases. Several different techniques for deciding conjunctive query entailment are available for a wide range of DLs. Nevertheless, the combination of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL causes unsolvable problems for the techniques hitherto available. We tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the DL ALCHOIQb that contains all three problematic constructors simultaneously. Provided that queries contain only simple roles, our result also shows decidability of entailment of (unions of) conjunctive queries in the logic that underpins OWL 1 DL and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the Description Logics underlying the OWL standards.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {429–481},
numpages = {53}
}

@article{10.5555/1946417.1946426,
author = {Banerjee, Bonny and Chandrasekaran, B.},
title = {A Constraint Satisfaction Framework for Executing Perceptions and Actions in Diagrammatic Reasoning},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported in this paper is a contribution to building a general purpose DR system as an extension to a soar-like problem solving architecture. The work is in a framework in which DR is modeled as a process where subtasks are solved, as appropriate, either by inference from symbolic representations or by interaction with a diagram, i.e., perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs. The perceptions and actions in most DR systems built so far are hand-coded for the specific application, even when the rest of the system is built using the general architecture. The absence of a general framework for executing perceptions/ actions poses as a major hindrance to using them opportunistically - the essence of open-ended search in problem solving.Our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasks/domains without human intervention. We observe that the domain/task-specific visual perceptions/actions can be transformed into domain/taskindependent spatial problems. We specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an open-ended vocabulary of properties, relations and actions involving three kinds of diagrammatic objects - points, curves, regions. Solving a spatial problem from this specification requires computing the equivalent simplified quantifier-free expression, the complexity of which is inherently doubly exponential. We represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems. We show that, if the symbolic solution to a subproblem can be expressed concisely, quantifiers can be eliminated from spatial problems in low-order polynomial time using similar previously solved subproblems. This requires determining the similarity of two problems, the existence of a mapping between them computable in polynomial time, and designing a memory for storing previously solved problems so as to facilitate search. The efficacy of the idea is shown by time complexity analysis. We demonstrate the proposed approach by executing perceptions and actions involved in DR tasks in two army applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {373–427},
numpages = {55}
}

@article{10.5555/1946417.1946425,
author = {Bidyuk, Bozhena and Dechter, Rina and Rollon, Emma},
title = {Active Tuples-Based Scheme for Bounding Posterior Beliefs},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {The paper presents a scheme for computing lower and upper bounds on the posterior marginals in Bayesian networks with discrete variables. Its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. The scheme uses the cutset conditioning principle to tighten existing bounding schemes and to facilitate anytime behavior, utilizing a fixed number of cutset tuples. The accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. We demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {335–371},
numpages = {37}
}

@article{10.5555/1946417.1946424,
author = {Feldman, Alexander and Provan, Gregory and van Gemund, Arjan},
title = {A Model-Based Active Testing Approach to Sequential Diagnosis},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses. The set of diagnoses can be reduced by taking into account extra observations (passive monitoring), measuring additional variables (probing) or executing additional tests (sequential diagnosis/test sequencing). In this paper we combine the above approaches with techniques from Automated Test Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework called Fractal (FRamework for ACtive Testing ALgorithms). Apart from the inputs and outputs that connect a system to its environment, in active testing we consider additional input variables to which a sequence of test vectors can be supplied. We address the computationally hard problem of computing optimal control assignments (as defined in Fractal) in terms of a greedy approximation algorithm called FractalG. We compare the decrease in the number of remaining minimal cardinality diagnoses of FractalG to that of two more Fractal algorithms: FractalATPG and FractalP. FractalATPG is based on ATPG and sequential diagnosis while FractalP is based on probing and, although not an active testing algorithm, provides a baseline for comparing the lower bound on the number of reachable diagnoses for the Fractal algorithms. We empirically evaluate the trade-offs of the three Fractal algorithms by performing extensive experimentation on the ISCAS85/74XXX benchmark of combinational circuits.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {301–334},
numpages = {34}
}

@article{10.5555/1946417.1946423,
author = {Bulitko, Vadim and Bj\"{o}rnsson, Yngvi and Lawrence, Ramon},
title = {Case-Based Subgoaling in Real-Time Heuristic Search for Video Game Pathfinding},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action, independent of problem size. As a result, they scale up well as problems become larger. This property would make them well suited for video games where Artificial Intelligence controlled agents must react quickly to user commands and to other agents. actions. On the downside, real-time search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by re-visiting the same problem states repeatedly. The situation changed recently with a new algorithm, D LRTA*, which attempted to eliminate learning by automatically selecting subgoals. D LRTA* is well poised for video games, except it has a complex and memory-demanding pre-computation phase during which it builds a database of subgoals. In this paper, we propose a simpler and more memory-efficient way of pre-computering subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games. The new algorithm solves a number of randomly chosen problems off-line, compresses the solutions into a series of subgoals and stores them in a database. When presented with a novel problem on-line, it queries the database for the most similar previously solved case and uses its subgoals to solve the problem. In the domain of pathfinding on four large video game maps, the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14% less pre-computation time.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {269–300},
numpages = {32}
}

@article{10.5555/1946417.1946422,
author = {Riedl, Mark O. and Young, R. Michael},
title = {Narrative Planning: Balancing Plot and Character},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Narrative, and in particular storytelling, is an important part of the human experience. Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. One of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. There are many factors - logical and aesthetic - that contribute to the success of a narrative artifact. Central to this success is its understandability. We argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability. Character believability is the perception by the audience that the actions performed by characters do not negatively impact the audience's suspension of disbelief. Specifically, characters must be perceived by the audience to be intentional agents. In this article, we explore the use of refinement search as a technique for solving the narrative generation problem - to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. We describe a novel refinement search planning algorithm - the Intent-based Partial Order Causal Link (IPOCL) planner - that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. We present the results of an empirical evaluation that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {217–268},
numpages = {52}
}

@article{10.5555/1946417.1946421,
author = {Chalkiadakis, Georgios and Elkind, Edith and Markakis, Evangelos and Polukarov, Maria and Jennings, Nicholas R.},
title = {Cooperative Games with Overlapping Coalitions},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {In the usual models of cooperative game theory, the outcome of a coalition formation process is either the grand coalition or a coalition structure that consists of disjoint coalitions. However, in many domains where coalitions are associated with tasks, an agent may be involved in executing more than one task, and thus may distribute his resources among several coalitions. To tackle such scenarios, we introduce a model for cooperative games with overlapping coalitions-or overlapping coalition formation (OCF) games. We then explore the issue of stability in this setting. In particular, we introduce a notion of the core, which generalizes the corresponding notion in the traditional (non-overlapping) scenario. Then, under some quite general conditions, we characterize the elements of the core, and show that any element of the core maximizes the social welfare. We also introduce a concept of balancedness for overlapping coalitional games, and use it to characterize coalition structures that can be extended to elements of the core. Finally, we generalize the notion of convexity to our setting, and show that under some natural assumptions convex games have a non-empty core. Moreover, we introduce two alternative notions of stability in OCF that allow a wider range of deviations, and explore the relationships among the corresponding definitions of the core, as well as the classic (non-overlapping) core and the Aubin core. We illustrate the general properties of the three cores, and also study them from a computational perspective, thus obtaining additional insights into their fundamental structure.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {179–216},
numpages = {38}
}

@article{10.5555/1946417.1946420,
author = {Richter, Silvia and Westphal, Matthias},
title = {The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {LAMA is a classical planning system based on heuristic forward search. Its core feature is the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true in every solution of a planning task. LAMA builds on the Fast Downward planning system, using finite-domain rather than binary state variables and multi-heuristic search. The latter is employed to combine the landmark heuristic with a variant of the well-known FF heuristic. Both heuristics are cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost. A weighted A* search is used with iteratively decreasing weights, so that the planner continues to search for plans of better quality until the search is terminated.LAMA showed best performance among all planners in the sequential satisficing track of the International Planning Competition 2008. In this paper we present the system in detail and investigate which features of LAMA are crucial for its performance. We present individual results for some of the domains used at the competition, demonstrating good and bad cases for the techniques implemented in LAMA. Overall, we find that using landmarks improves performance, whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial. We show that in some domains a search that ignores cost solves far more problems, raising the question of how to deal with action costs more effectively in the future. The iterated weighted A* search greatly improves results, and shows synergy effects with the use of landmarks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {127–177},
numpages = {51}
}

@article{10.5555/1946417.1946419,
author = {Katz, Michael and Domshlak, Carmel},
title = {Implicit Abstraction Heuristics},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {State-space search with explicit abstraction heuristics is at the state of the art of cost-optimal planning. These heuristics are inherently limited, nonetheless, because the size of the abstract space must be bounded by some, even if a very large, constant. Targeting this shortcoming, we introduce the notion of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning. We then introduce a concrete setting of this framework, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. The induced admissible heuristics are then studied formally and empirically. This study testifies for the accuracy of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them. Indeed, some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offine and then determining h(s) for each evaluated state s by a very fast lookup in a "database." By contrast, while fork-decomposition heuristics can be calculated in polynomial time, computing them is far from being fast. To address this problem, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. We demonstrate that an equivalent of the explicit abstraction notion of a "database" exists for the fork-decomposition abstractions as well, despite their exponential-size abstract spaces. We then verify empirically that heuristic search with the "databased" fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {51–126},
numpages = {76}
}

@article{10.5555/1946417.1946418,
author = {Lang, Tobias and Toussaint, Marc},
title = {Planning with Noisy Probabilistic Relational Rules},
year = {2010},
issue_date = {September 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {39},
number = {1},
issn = {1076-9757},
abstract = {Noisy probabilistic relational rules are a promising world model representation for several reasons. They are compact and generalize over world instantiations. They are usually interpretable and they can be learned effectively from the action experiences in complex worlds. We investigate reasoning with such rules in grounded relational domains. Our algorithms exploit the compactness of rules for efficient and fexible decision-theoretic planning. As a first approach, we combine these rules with the Upper Confidence Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second approach converts these rules into a structured dynamic Bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states. We evaluate the effectiveness of our approaches for planning in a simulated complex 3D robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition. Empirical results show that our methods can solve problems where existing methods fail.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–49},
numpages = {49}
}

@article{10.5555/1892211.1892228,
author = {Wu, Jia-Hong and Givan, Robert},
title = {Automatic Induction of Bellman-Error Features for Probabilistic Planning},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Domain-specific features are important in representing problem structure throughout machine learning and decision-theoretic planning. In planning, once state features are provided, domain-independent algorithms such as approximate value iteration can learn weighted combinations of those features that often perform well as heuristic estimates of state value (e.g., distance to the goal). Successful applications in real-world domains often require features crafted by human experts. Here, we propose automatic processes for learning useful domain-specific feature sets with little or no human intervention. Our methods select and add features that describe state-space regions of high inconsistency in the Bellman equation (statewise Bellman error) during approximate value iteration. Our method can be applied using any real-valued-feature hypothesis space and corresponding learning method for selecting features from training sets of state-value pairs. We evaluate the method with hypothesis spaces defined by both relational and propositional feature languages, using nine probabilistic planning domains. We show that approximate value iteration using a relational feature space performs at the state-of-the-art in domain-independent stochastic relational planning. Our method provides the first domain-independent approach that plays Tetris successfully (without human-engineered features).},
journal = {J. Artif. Int. Res.},
month = may,
pages = {687–755},
numpages = {69}
}

@article{10.5555/1892211.1892227,
author = {Greco, Gianluigi and Malizia, Enrico and Palopoli, Luigi and Scarcello, Francesco},
title = {Non-Transferable Utility Coalitional Games via Mixed-Integer Linear Constraints},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Coalitional games serve the purpose of modeling payoff distribution problems in scenarios where agents can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. In the classical Transferable Utility (TU) setting, coalition worths can be freely distributed amongst agents. However, in several application scenarios, this is not the case and the Non-Transferable Utility setting (NTU) must be considered, where additional application-oriented constraints are imposed on the possible worth distributions.In this paper, an approach to define NTU games is proposed which is based on describing allowed distributions via a set of mixed-integer linear constraints applied to an underlying TU game. It is shown that such games allow non-transferable conditions on worth distributions to be specified in a natural and succinct way. The properties and the relationships among the most prominent solution concepts for NTU games that hold when they are applied on (mixed-integer) constrained games are investigated. Finally, a thorough analysis is carried out to assess the impact of issuing constraints on the computational complexity of some of these solution concepts.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {633–685},
numpages = {53}
}

@article{10.5555/1892211.1892226,
author = {Abedin, Muhammad Arshad Ul and Ng, Vincent and Khan, Latifur},
title = {Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloff's Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {569–631},
numpages = {63}
}

@article{10.5555/1892211.1892225,
author = {de Bruijn, Jos and Heymans, Stijn},
title = {Logical Foundations of RDF(S) with Datatypes},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {The Resource Description Framework (RDF) is a Semantic Web standard that provides a data language, simply called RDF, as well as a lightweight ontology language, called RDF Schema. We investigate embeddings of RDF in logic and show how standard logic programming and description logic technology can be used for reasoning with RDF. We subsequently consider extensions of RDF with datatype support, considering D entailment, defined in the RDF semantics specification, and D* entailment, a semantic weakening of D entailment, introduced by ter Horst. We use the embeddings and properties of the logics to establish novel upper bounds for the complexity of deciding entailment. We subsequently establish two novel lower bounds, establishing that RDFS entailment is PTime-complete and that simple-D entailment is coNP-hard, when considering arbitrary datatypes, both in the size of the entailing graph. The results indicate that RDFS may not be as lightweight as one may expect.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {535–568},
numpages = {34}
}

@article{10.5555/1892211.1892224,
author = {Benisch, Michael and Davis, George B. and Sandholm, Tuomas},
title = {Algorithms for Closed under Rational Behavior (CURB) Sets},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {We provide a series of algorithms demonstrating that solutions according to the fundamental game-theoretic solution concept of closed under rational behavior (CURB) sets in two-player, normal-form games can be computed in polynomial time (we also discuss extensions to n-player games). First, we describe an algorithm that identifies all of a player's best responses conditioned on the belief that the other player will play from within a given subset of its strategy space. This algorithm serves as a subroutine in a series of polynomial-time algorithms for finding all minimal CURB sets, one minimal CURB set, and the smallest minimal CURB set in a game. We then show that the complexity of finding a Nash equilibrium can be exponential only in the size of a game's smallest CURB set. Related to this, we show that the smallest CURB set can be an arbitrarily small portion of the game, but it can also be arbitrarily larger than the supports of its only enclosed Nash equilibrium. We test our algorithms empirically and find that most commonly studied academic games tend to have either very large or very small minimal CURB sets.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {513–534},
numpages = {22}
}

@article{10.5555/1892211.1892223,
author = {Ortega, Pedro A. and Braun, Daniel A.},
title = {A Minimum Relative Entropy Principle for Learning and Acting},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {475–511},
numpages = {37}
}

@article{10.5555/1892211.1892222,
author = {Wu, Jianhui and Durfee, Edmund H.},
title = {Resource-Driven Mission-Phasing Techniques for Constrained Agents in Stochastic Environments},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Because an agent's resources dictate what actions it can possibly take, it should plan which resources it holds over time carefully, considering its inherent limitations (such as power or payload restrictions), the competing needs of other agents for the same resources, and the stochastic nature of the environment. Such agents can, in general, achieve more of their objectives if they can use -- and even create -- opportunities to change which resources they hold at various times. Driven by resource constraints, the agents could break their overall missions into an optimal series of phases, optimally reconfiguring their resources at each phase, and optimally using their assigned resources in each phase, given their knowledge of the stochastic environment.In this paper, we formally define and analyze this constrained, sequential optimization problem in both the single-agent and multi-agent contexts. We present a family of mixed integer linear programming (MILP) formulations of this problem that can optimally create phases (when phases are not predefined) accounting for costs and limitations in phase creation. Because our formulations simultaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase, they exploit structure across these coupled problems. This allows them to find solutions significantly faster (orders of magnitude faster in larger problems) than alternative solution techniques, as we demonstrate empirically.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {415–473},
numpages = {59}
}

@article{10.5555/1892211.1892221,
author = {Feldman, Alexander and Provan, Gregory and van Gemund, Arjan},
title = {Approximate Model-Based Diagnosis Using Greedy Stochastic Search},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which trades off guarantees of computing minimal diagnoses for computational efficiency. We empirically demonstrate, using the 74XXX and ISCAS85 suites of benchmark combinatorial circuits, that SAFARI achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA* and HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of multiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). We discuss the optimality of SAFARI in a class of strong-fault circuit models with stuck-at failure modes. By modeling the algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. SAFARI also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {371–413},
numpages = {43}
}

@article{10.5555/1892211.1892220,
author = {Babaioff, Moshe and Feldman, Michal and Nisan, Noam},
title = {Mixed Strategies in Combinatorial Agency},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {In many multiagent domains a set of agents exert effort towards a joint outcome, yet the individual effort levels cannot be easily observed. A typical example for such a scenario is routing in communication networks, where the sender can only observe whether the packet reached its destination, but often has no information about the actions of the intermediate routers, which influences the final outcome. We study a setting where a principal needs to motivate a team of agents whose combination of hidden efforts stochastically determines an outcome. In a companion paper we devise and study a basic "combinatorial agency" model for this setting, where the principal is restricted to inducing a pure Nash equilibrium. Here we study various implications of this restriction. First, we show that, in contrast to the case of observable efforts, inducing a mixed-strategies equilibrium may be beneficial for the principal. Second, we present a sufficient condition for technologies for which no gain can be generated. Third, we bound the principal's gain for various families of technologies. Finally, we study the robustness of mixed equilibria to coalitional deviations and the computational hardness of the optimal mixed equilibria.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {339–369},
numpages = {31}
}

@article{10.5555/1892211.1892219,
author = {Gange, Graeme and Stuckey, Peter J. and Lagoon, Vitaly},
title = {Fast Set Bounds Propagation Using a BDD-SAT Hybrid},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Binary Decision Diagram (BDD) based set bounds propagation is a powerful approach to solving set-constraint satisfaction problems. However, prior BDD based techniques incur the significant overhead of constructing and manipulating graphs during search. We present a set-constraint solver which combines BDD-based set-bounds propagators with the learning abilities of a modern SAT solver. Together with a number of improvements beyond the basic algorithm, this solver is highly competitive with existing propagation based set constraint solvers.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {307–338},
numpages = {32}
}

@article{10.5555/1892211.1892218,
author = {Lesaint, David and Mehta, Deepak and O'Sullivan, Barry and Quesada, Luis and Wilson, Nic},
title = {Developing Approaches for Solving a Telecommunications Feature Subscription Problem},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Call control features (e.g., call-divert, voice-mail) are primitive options to which users can subscribe off-line to personalise their service. The configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to constraints that prevent undesirable feature interactions at run-time. When the subscription requested by a user is inconsistent, one problem is to find an optimal relaxation, which is a generalisation of the feedback vertex set problem on directed graphs, and thus it is an NP-hard task. We present several constraint programming formulations of the problem. We also present formulations using partial weighted maximum Boolean satisfiability and mixed integer linear programming. We study all these formulations by experimentally comparing them on a variety of randomly generated instances of the feature subscription problem.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {271–305},
numpages = {35}
}

@article{10.5555/1892211.1892217,
author = {Wittocx, Johan and Mari\"{e}n, Maarten and Denecker, Marc},
title = {Grounding FO and FO(ID) with Bounds},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning.Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder's output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems.We first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {223–269},
numpages = {47}
}

@article{10.5555/1892211.1892216,
author = {Michelson, Matthew and Knoblock, Craig A.},
title = {Constructing Reference Sets from Unstructured, Ungrammatical Text},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text "posts." Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called "reference sets," by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {189–221},
numpages = {33}
}

@article{10.5555/1892211.1892215,
author = {Androutsopoulos, Ion and Malakasiotis, Prodromos},
title = {A Survey of Paraphrasing and Textual Entailment Methods},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural language expressions that convey almost the same information. Textual entailment methods, on the other hand, recognize, generate, or extract pairs of natural language expressions, such that a human who reads (and trusts) the first element of a pair would most likely infer that the other element is also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of natural language processing applications, including question answering, summarization, text generation, and machine translation. We summarize key ideas from the two areas by considering in turn recognition, generation, and extraction methods, also pointing to prominent articles and resources.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {135–187},
numpages = {53}
}

@article{10.5555/1892211.1892214,
author = {Yeoh, William and Felner, Ariel and Koenig, Sven},
title = {BnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {Distributed constraint optimization (DCOP) problems are a popular way of formulating and solving agent-coordination problems. A DCOP problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal. It is often desirable to solve DCOP problems with memory-bounded and asynchronous algorithms. We introduce Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search algorithm that uses the message-passing and communication framework of ADOPT (Modi, Shen, Tambe, &amp; Yokoo, 2005), a well known memory-bounded asynchronous DCOP search algorithm, but changes the search strategy of ADOPT from best-first search to depth-first branch-and-bound search. Our experimental results show that BnB-ADOPT finds cost-minimal solutions up to one order of magnitude faster than ADOPT for a variety of large DCOP problems and is as fast as NCBB, a memory-bounded synchronous DCOP search algorithm, for most of these DCOP problems. Additionally, it is often desirable to find bounded-error solutions for DCOP problems within a reasonable amount of time since finding cost-minimal solutions is NP-hard. The existing bounded-error approximation mechanism allows users only to specify an absolute error bound on the solution cost but a relative error bound is often more intuitive. Thus, we present two new bounded-error approximation mechanisms that allow for relative error bounds and implement them on top of BnB-ADOPT.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {85–133},
numpages = {49}
}

@article{10.5555/1892211.1892213,
author = {Cayrol, Claudette and de Saint-Cyr, Florence Dupin and Lagasquie-Schiex, Marie-Christine},
title = {Change in Abstract Argumentation Frameworks: Adding an Argument},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we address the problem of change in an abstract argumentation system. We focus on a particular change: the addition of a new argument which interacts with previous arguments. We study the impact of such an addition on the outcome of the argumentation system, more particularly on the set of its extensions. Several properties for this change operation are defined by comparing the new set of extensions to the initial one, these properties are called "structural" when the comparisons are based on set-cardinality or set-inclusion relations. Several other properties are proposed where comparisons are based on the status of some particular arguments: the accepted arguments; these properties refer to the "evolution of this status" during the change, e.g., Monotony and Priority to Recency. All these properties may be more or less desirable according to specific applications. They are studied under two particular semantics: the grounded and preferred semantics.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {49–84},
numpages = {36}
}

@article{10.5555/1892211.1892212,
author = {Katrenko, Sophia and Adriaans, Pieter and van Someren, Maarten},
title = {Using Local Alignments for Relation Recognition},
year = {2010},
issue_date = {May 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {38},
number = {1},
issn = {1076-9757},
abstract = {This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–48},
numpages = {48}
}

@article{10.5555/1861751.1861763,
author = {Engel, Yagil and Wellman, Michael P.},
title = {Multiattribute Auctions Based on Generalized Additive Independence},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {We develop multiattribute auctions that accommodate generalized additive independent (GAI) preferences. We propose an iterative auction mechanism that maintains prices on potentially overlapping GAI clusters of attributes, thus decreases elicitation and computational burden, and creates an open competition among suppliers over a multidimensional domain. Most significantly, the auction is guaranteed to achieve surplus which approximates optimal welfare up to a small additive factor, under reasonable equilibrium strategies of traders. The main departure of GAI auctions from previous literature is to accommodate non-additive trader preferences, hence allowing traders to condition their evaluation of specific attributes on the value of other attributes. At the same time, the GAI structure supports a compact representation of prices, enabling a tractable auction process. We perform a simulation study, demonstrating and quantifying the significant efficiency advantage of more expressive preference modeling. We draw random GAI-structured utility functions with various internal structures, generate additive functions that approximate the GAI utility, and compare the performance of the auctions using the two representations. We find that allowing traders to express existing dependencies among attributes improves the economic efficiency of multiattribute auctions.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {479–526},
numpages = {48}
}

@article{10.5555/1861751.1861762,
author = {van der Hoek, Wiebe and Walther, Dirk and Wooldridge, Michael},
title = {Reasoning about the Transfer of Control},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents - the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of CL-PC are of the form 'coalition C can cooperate to bring about ϕ'. DCL-PC extends CL-PC with dynamic logic modalities in which atomic programs are of the form 'agent i gives control of variable p to agent j'; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a 'direct' semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the logic. We investigate the computational complexity of model checking and satisfiability for DCL-PC, and show that both problems are PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally, we investigate the characterisation of control in DCL-PC. We distinguish between first-order control - the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition - and second-order control - the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. We give a logical characterisation of second-order control.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {437–478},
numpages = {42}
}

@article{10.5555/1861751.1861761,
author = {Chen, David L. and Kim, Joohyun and Mooney, Raymond J.},
title = {Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {397–436},
numpages = {40}
}

@article{10.5555/1861751.1861760,
author = {Aras, Raghav and Dutech, Alain},
title = {An Investigation into Mathematical Programming for Finite Horizon Decentralized POMDPs},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {Decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach, mainly through the framework of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS are a general and powerful modeling tool, solving them is a task with an overwhelming complexity that can be doubly exponential. In this paper, we study an alternate formulation of DEC-POMDPs relying on a sequence-form representation of policies. From this formulation, we show how to derive Mixed Integer Linear Programming (MILP) problems that, once solved, give exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be derived either by using some combinatorial characteristics of the optimal solutions of the DEC-POMDPs or by using concepts borrowed from game theory. Through an experimental validation on classical test problems from the DEC-POMDP literature, we compare our approach to existing algorithms. Results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except for some particular problems.The main contributions of this work are the use of mathematical programming for DEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions. Besides, we argue that our alternate representation of DEC-POMDPs could be helpful for designing novel algorithms looking for approximate solutions to DEC-POMDPs.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {329–396},
numpages = {68}
}

@article{10.5555/1861751.1861759,
author = {Mateescu, Robert and Kask, Kalev and Gogate, Vibhav and Dechter, Rina},
title = {Join-Graph Propagation Algorithms},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {The paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by Pearl's belief propagation algorithm (BP). We start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other state-of-the-art algorithms on several classes of networks. We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {279–328},
numpages = {50}
}

@article{10.5555/1861751.1861758,
author = {Qu, Shaolin and Chai, Joyce Y.},
title = {Context-Based Word Acquisition for Situated Dialogue in a Virtual World},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {To tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring speech and eye gaze during interaction to automatically acquire new words. Although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. First, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. While these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge base about the domain and vocabulary. While the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. Automated recognition of closely-coupled speech-gaze streams based on conversation context is important. To address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {247–278},
numpages = {32}
}

@article{10.5555/1861751.1861757,
author = {Varzinczak, Ivan Jos\'{e}},
title = {On Action Theory Change},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {As historically acknowledged in the Reasoning about Actions and Change community, intuitiveness of a logical domain description cannot be fully automated. Moreover, like any other logical theory, action theories may also evolve, and thus knowledge engineers need revision methods to help in accommodating new incoming information about the behavior of actions in an adequate manner. The present work is about changing action domain descriptions in multimodal logic. Its contribution is threefold: first we revisit the semantics of action theory contraction proposed in previous work, giving more robust operators that express minimal change based on a notion of distance between Kripke-models. Second we give algorithms for syntactical action theory contraction and establish their correctness with respect to our semantics for those action theories that satisfy a principle of modularity investigated in previous work. Since modularity can be ensured for every action theory and, as we show here, needs to be computed at most once during the evolution of a domain description, it does not represent a limitation at all to the method here studied. Finally we state AGM-like postulates for action theory contraction and assess the behavior of our operators with respect to them. Moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {189–246},
numpages = {58}
}

@article{10.5555/1861751.1861756,
author = {Turney, Peter D. and Pantel, Patrick},
title = {From Frequency to Meaning: Vector Space Models of Semantics},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {141–188},
numpages = {48}
}

@article{10.5555/1861751.1861755,
author = {Andersen, Henrik Reif and Hadzic, Tarik and Pisinger, David},
title = {Interactive Cost Configuration over Decision Diagrams},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {In many AI domains such as product configuration, a user should interactively specify a solution that must satisfy a set of constraints. In such scenarios, offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free user interaction online. In particular, binary decision diagrams (BDDs) have been successfully used as a compilation target for product and service configuration. In this paper we discuss how to extend BDD-based configuration to scenarios involving cost functions which express user preferences.We first show that an efficient, robust and easy to implement extension is possible if the cost function is additive, and feasible solutions are represented using multi-valued decision diagrams (MDDs). We also discuss the effect on MDD size if the cost function is non-additive or if it is encoded explicitly into MDD. We then discuss interactive configuration in the presence of multiple cost functions. We prove that even in its simplest form, multiple-cost configuration is NP-hard in the input MDD. However, for solving two-cost configuration we develop a pseudo-polynomial scheme and a fully polynomial approximation scheme. The applicability of our approach is demonstrated through experiments over real-world configuration models and product-catalogue datasets. Response times are generally within a fraction of a second even for very large instances.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {99–140},
numpages = {42}
}

@article{10.5555/1861751.1861754,
author = {Dobzinski, Shahar and Nisan, Noam},
title = {Mechanisms for Multi-Unit Auctions},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {We present an incentive-compatible polynomial-time approximation scheme for multiunit auctions with general k-minded player valuations. The mechanism fully optimizes over an appropriately chosen sub-range of possible allocations and then uses VCG payments over this sub-range. We show that obtaining a fully polynomial-time incentive-compatible approximation scheme, at least using VCG payments, is NP-hard. For the case of valuations given by black boxes, we give a polynomial-time incentive-compatible 2-approximation mechanism and show that no better is possible, at least using VCG payments.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {85–98},
numpages = {14}
}

@article{10.5555/1861751.1861753,
author = {Zahavi, Uzi and Felner, Ariel and Burch, Neil and Holte, Robert C.},
title = {Predicting the Performance of IDA* Using Conditional Distributions},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes IDA* will expand on a single iteration for a given consistent heuristic, and experimentally demonstrated that it could make very accurate predictions. In this paper we show that, in addition to requiring the heuristic to be consistent, their formula's predictions are accurate only at levels of the brute-force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula. We then propose a new formula that works well without these requirements, i.e., it can make accurate predictions of IDA*'s performance for inconsistent heuristics and if the heuristic values in any level do not obey the unconditional distribution. In order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution. We also provide extensions of our formula that handle individual start states and the augmentation of IDA* with bidirectional pathmax (BPMX), a tech nique for propagating heuristic values when inconsistent heuristics are used. Experimental results demonstrate the accuracy of our new method and all its variations.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {41–84},
numpages = {44}
}

@article{10.5555/1861751.1861752,
author = {Tsatsaronis, George and Varlamis, Iraklis and Vazirgiannis, Michalis},
title = {Text Relatedness Based on a Word Thesaurus},
year = {2010},
issue_date = {January 2010},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {37},
number = {1},
issn = {1076-9757},
abstract = {The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–40},
numpages = {40}
}

@article{10.5555/1734953.1734966,
author = {Keyder, Emil and Geffner, Hector},
title = {Soft Goals Can Be Compiled Away},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {Soft goals extend the classical model of planning with a simple model of preferences. The best plans are then not the ones with least cost but the ones with maximum utility, where the utility of a plan is the sum of the utilities of the soft goals achieved minus the plan cost. Finding plans with high utility appears to involve two linked problems: choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them. New search algorithms and heuristics have been developed for planning with soft goals, and a new track has been introduced in the International Planning Competition (IPC) to test their performance. In this note, we show however that these extensions are not needed: soft goals do not increase the expressive power of the basic model of planning with action costs, as they can easily be compiled away. We apply this compilation to the problems of the net-benefit track of the most recent IPC, and show that optimal and satisficing cost-based planners do better on the compiled problems than optimal and satisficing netbenefit planners on the original problems with explicit soft goals. Furthermore, we show that penalties, or negative preferences expressing conditions to avoid, can also be compiled away using a similar idea.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {547–556},
numpages = {10}
}

@article{10.5555/1734953.1734965,
author = {Greenwald, Amy and Lee, Seong Jae and Naroditskiy, Victor},
title = {RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged victorious in the travel division of the 2006 Trading Agent Competition in a photo finish. At a high level, the design of many successful trading agents can be summarized as follows: (i) price prediction: build a model of market prices; and (ii) optimization: solve for an approximately optimal set of bids, given this model. To predict, RoxyBot builds a stochastic model of market prices by simulating simultaneous ascending auctions. To optimize, RoxyBot relies on the sample average approximation method, a stochastic optimization technique.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {513–546},
numpages = {34}
}

@article{10.5555/1734953.1734964,
author = {Jonsson, Anders},
title = {The Role of Macros in Tractable Planning},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {This paper presents several new tractability results for planning based on macros. We describe an algorithm that optimally solves planning problems in a class that we call inverted tree reducible, and is provably tractable for several subclasses of this class. By using macros to store partial plans that recur frequently in the solution, the algorithm is polynomial in time and space even for exponentially long plans. We generalize the inverted tree reducible class in several ways and describe modifications of the algorithm to deal with these new classes. Theoretical results are validated in experiments.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {471–511},
numpages = {41}
}

@article{10.5555/1734953.1734963,
author = {Domshlak, Carmel and Hoffmann, J\"{o}rg and Sabharwal, Ashish},
title = {Friends or Foes? On Planning as Satisfiability and Abstract CNF Encodings},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {Planning as satisfiability, as implemented in, for instance, the SATPLAN tool, is a highly competitive method for finding parallel step-optimal plans. A bottleneck in this approach is to prove the absence of plans of a certain length. Specifically, if the optimal plan has n steps, then it is typically very costly to prove that there is no plan of length n-1. We pursue the idea of leading this proof within solution length preserving abstractions (over-approximations) of the original planning task. This is promising because the abstraction may have a much smaller state space; related methods are highly successful in model checking. In particular, we design a novel abstraction technique based on which one can, in several widely used planning benchmarks, construct abstractions that have exponentially smaller state spaces while preserving the length of an optimal plan.Surprisingly, the idea turns out to appear quite hopeless in the context of planning as satisfiability. Evaluating our idea empirically, we run experiments on almost all benchmarks of the international planning competitions up to IPC 2004, and find that even handmade abstractions do not tend to improve the performance of SATPLAN. Exploring these findings from a theoretical point of view, we identify an interesting phenomenon that may cause this behavior. We compare various planning-graph based CNF encodings Φ of the original planning task with the CNF encodings Φσ of the abstracted planning task. We prove that, in many cases, the shortest resolution refutation for Φσ can never be shorter than that for Φ. This suggests a fundamental weakness of the approach, and motivates further investigation of the interplay between declarative transition-systems, over-approximating abstractions, and SAT encodings.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {415–469},
numpages = {55}
}

@article{10.5555/1734953.1734962,
author = {Feldman, Michal and Tamir, Tami},
title = {Approximate Strong Equilibrium in Job Scheduling Games},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {A Nash Equilibrium (NE) is a strategy profile resilient to unilateral deviations, and is predominantly used in the analysis of multiagent systems. A downside of NE is that it is not necessarily stable against deviations by coalitions. Yet, as we show in this paper, in some cases, NE does exhibit stability against coalitional deviations, in that the benefits from a joint deviation are bounded. In this sense, NE approximates strong equilibrium.Coalition formation is a key issue in multiagent systems. We provide a framework for quantifying the stability and the performance of various assignment policies and solution concepts in the face of coalitional deviations. Within this framework we evaluate a given configuration according to three measures: (i) IRmin: the maximal number α, such that there exists a coalition in which the minimal improvement ratio among the coalition members is α, (ii) IRmax: the maximal number α, such that there exists a coalition in which the maximal improvement ratio among the coalition members is α, and (iii) DRmax: the maximal possible damage ratio of an agent outside the coalition.We analyze these measures in job scheduling games on identical machines. In particular, we provide upper and lower bounds for the above three measures for both NE and the well-known assignment rule Longest Processing Time (LPT).Our results indicate that LPT performs better than a general NE. However, LPT is not the best possible approximation. In particular, we present a polynomial time approximation scheme (PTAS) for the makespan minimization problem which provides a schedule with IRmin of 1 + undefined for any given undefined. With respect to computational complexity, we show that given an NE on m ≤ 3 identical machines or m ≤ 2 unrelated machines, it is NP-hard to determine whether a given coalition can deviate such that every member decreases its cost.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {387–414},
numpages = {28}
}

@article{10.5555/1734953.1734961,
author = {Naseem, Tahira and Snyder, Benjamin and Eisenstein, Jacob and Barzilay, Regina},
title = {Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {341–385},
numpages = {45}
}

@article{10.5555/1734953.1734960,
author = {Pad\'{o}, Sebastian and Lapata, Mirella},
title = {Cross-Lingual Annotation Projection of Semantic Roles},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {This article considers the task of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We propose a general framework that is based on annotation projection, phrased as a graph optimization problem. It is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources. Within this framework, we present projection models that exploit lexical and syntactic information. We provide an experimental evaluation on an English-German parallel corpus which demonstrates the feasibility of inducing high-precision German semantic role annotation both for manually and automatically annotated English data.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {307–340},
numpages = {34}
}

@article{10.5555/1734953.1734959,
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin and St\"{u}tzle, Thomas},
title = {ParamILS: An Automatic Algorithm Configuration Framework},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {267–306},
numpages = {40}
}

@article{10.5555/1734953.1734958,
author = {Chieu, Hai Leong and Lee, Wee Sun},
title = {Relaxed Survey Propagation for the Weighted Maximum Satisfiability Problem},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {The survey propagation (SP) algorithm has been shown to work well on large instances of the random 3-SAT problem near its phase transition. It was shown that SP estimates marginals over covers that represent clusters of solutions. The SP-y algorithm generalizes SP to work on the maximum satisfiability (Max-SAT) problem, but the cover interpretation of SP does not generalize to SP-y. In this paper, we formulate the relaxed survey propagation (RSP) algorithm, which extends the SP algorithm to apply to the weighted Max-SAT problem. We show that RSP has an interpretation of estimating marginals over covers violating a set of clauses with minimal weight. This naturally generalizes the cover interpretation of SP. Empirically, we show that RSP outperforms SP-y and other state-of-the-art Max-SAT solvers on random Max-SAT instances. RSP also outperforms state-of-the-art weighted Max-SAT solvers on random weighted Max-SAT instances.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {229–266},
numpages = {38}
}

@article{10.5555/1734953.1734957,
author = {Motik, Boris and Shearer, Rob and Horrocks, Ian},
title = {Hypertableau Reasoning for Description Logics},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {We present a novel reasoning calculus for the description logic SHOIQ+--a knowledge representation formalism with applications in areas such as the SemanticWeb. Unnecessary nondeterminism and the construction of large models are two primary sources of inefficiency in the tableau-based reasoning calculi used in state-of-the-art reasoners. In order to reduce nondeterminism, we base our calculus on hypertableau and hyperresolution calculi, which we extend with a blocking condition to ensure termination. In order to reduce the size of the constructed models, we introduce anywhere pairwise blocking. We also present an improved nominal introduction rule that ensures termination in the presence of nominals, inverse roles, and number restrictions--a combination of DL constructs that has proven notoriously difficult to handle. Our implementation shows significant performance improvements over state-of-the-art reasoners on several well-known ontologies.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {165–228},
numpages = {64}
}

@article{10.5555/1734953.1734956,
author = {Chen, Harr and Branavan, S. R. K. and Barzilay, Regina and Karger, David R.},
title = {Content Modeling Using Latent Permutations},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {129–163},
numpages = {35}
}

@article{10.5555/1734953.1734955,
author = {Bienvenu, Meghyn},
title = {Prime Implicates and Prime Implicants: From Propositional to Modal Logic},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {Prime implicates and prime implicants have proven relevant to a number of areas of artificial intelligence, most notably abductive reasoning and knowledge compilation. The purpose of this paper is to examine how these notions might be appropriately extended from propositional logic to the modal logic κ. We begin the paper by considering a number of potential definitions of clauses and terms for κ. The different definitions are evaluated with respect to a set of syntactic, semantic, and complexity-theoretic properties characteristic of the propositional definition. We then compare the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. While there is no definition that perfectly generalizes the propositional notions, we show that there does exist one definition which satisfies many of the desirable properties of the propositional case. In the second half of the paper, we consider the computational properties of the selected definition. To this end, we provide sound and complete algorithms for generating and recognizing prime implicates, and we show the prime implicate recognition task to be Pspace-complete. We also prove upper and lower bounds on the size and number of prime implicates. While the paper focuses on the logic κ, all of our results hold equally well for multi-modal κ and for concept expressions in the description logic ALC.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {71–128},
numpages = {58}
}

@article{10.5555/1734953.1734954,
author = {Artale, Alessandro and Calvanese, Diego and Kontchakov, Roman and Zakharyaschev, Michael},
title = {The DL-Lite Family and Relations},
year = {2009},
issue_date = {September 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {36},
number = {1},
issn = {1076-9757},
abstract = {The recently introduced series of description logics under the common moniker 'DL-Lite' has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent conceptual modeling formalisms, on the other. The main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original DL-Lite logics along five axes: by (i) adding the Boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity constraints, and (v) adopting or dropping the unique name assumption. We analyze the combined complexity of satisfiability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries. Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable first-order logic, which provides useful insights into their properties and, in particular, computational behavior.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–69},
numpages = {69}
}

@article{10.5555/1641503.1641522,
author = {Janhunen, Tomi and Oikarinen, Emilia and Tompits, Hans and Woltran, Stefan},
title = {Modularity Aspects of Disjunctive Stable Models},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Practically all programming languages allow the programmer to split a program into several modules which brings along several advantages in software development. In this paper, we are interested in the area of answer-set programming where fully declarative and nonmonotonic languages are applied. In this context, obtaining a modular structure for programs is by no means straightforward since the output of an entire program cannot in general be composed from the output of its components. To better understand the effects of disjunctive information on modularity we restrict the scope of analysis to the case of disjunctive logic programs (DLPs) subject to stable-model semantics. We define the notion of a DLP-function, where a well-defined input/output interface is provided, and establish a novel module theorem which indicates the compositionality of stable-model semantics for DLP-functions. The module theorem extends the well-known splitting-set theorem and enables the decomposition of DLP-functions given their strongly connected components based on positive dependencies induced by rules. In this setting, it is also possible to split shared disjunctive rules among components using a generalized shifting technique. The concept of modular equivalence is introduced for the mutual comparison of DLP-functions using a generalization of a translation-based verification method.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {813–857},
numpages = {45}
}

@article{10.5555/1641503.1641521,
author = {Saquete, Estela and Vicedo, Jose L. and Mart\'{\i}nez-Barco, Patricio and Mu\~{n}oz, Rafael and Llorens, Hector},
title = {Enhancing QA Systems with Complex Temporal Question Processing Capabilities},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a multilayered architecture that enhances the capabilities of current QA systems and allows different types of complex questions or queries to be processed. The answers to these questions need to be gathered from factual information scattered throughout different documents. Specifically, we designed a specialized layer to process the different types of temporal questions. Complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. In the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. A novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. In this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. The temporal layer was first performed for English, then evaluated and compared with: a) a general purpose QA system (F-measure 65.47% for QA plus English temporal layer vs. 38.01% for the general QA system), and b) a well-known QA system. Much better results were obtained for temporal questions with the multilayered system. This system was therefore extended to Spanish and very good results were again obtained in the evaluation (F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general QA system).},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {755–811},
numpages = {57}
}

@article{10.5555/1641503.1641520,
author = {Bonatti, Piero A. and Lutz, Carsten and Wolter, Frank},
title = {The Complexity of Circumscription in Description Logic},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {As fragments of first-order logic, Description logics (DLs) do not provide nonmonotonic features such as defeasible inheritance and default rules. Since many applications would benefit from the availability of such features, several families of nonmonotonic DLs have been developed that are mostly based on default logic and autoepistemic logic. In this paper, we consider circumscription as an interesting alternative approach to nonmonotonic DLs that, in particular, supports defeasible inheritance in a natural way. We study DLs extended with circumscription under different language restrictions and under different constraints on the sets of minimized, fixed, and varying predicates, and pinpoint the exact computational complexity of reasoning for DLs ranging from ALC to ALCIO and ALCQO. When the minimized and fixed predicates include only concept names but no role names, then reasoning is complete for NEXPNP. It becomes complete for NPNEXP when the number of minimized and fixed predicates is bounded by a constant. If roles can be minimized or fixed, then complexity ranges from NEXPNP to undecidability.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {717–773},
numpages = {57}
}

@article{10.5555/1641503.1641519,
author = {Su, Kaile and Sattar, Abdul and Lv, Guanfeng and Zhang, Yan},
title = {Variable Forgetting in Reasoning about Knowledge},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we investigate knowledge reasoning within a simple framework called knowledge structure. We use variable forgetting as a basic operation for one agent to reason about its own or other agents' knowledge. In our framework, two notions namely agents' observable variables and the weakest sufficient condition play important roles in knowledge reasoning. Given a background knowledge base Γ and a set of observable variables Oi for each agent i, we show that the notion of agent i knowing a formula ϕ can be defined as a weakest sufficient condition of ϕ over Oi under Γ Moreover, we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition. Also, we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure. Further, we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure. In the general case, this problem is PSPACE-hard; however, for some interesting subcases, it can be reduced to co-NP. Finally, we discuss possible applications of our framework in some interesting domains such as the automated analysis of the well-known muddy children puzzle and the verification of the revised Needham-Schroeder protocol. We believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure. What makes it valuable compared with the corresponding multi-agent S5 Kripke structure is that it can be much more succinct.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {677–716},
numpages = {40}
}

@article{10.5555/1641503.1641518,
author = {Palacios, Hector and Geffner, Hector},
title = {Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Conformant planning is the problem of finding a sequence of actions for achieving a goal in the presence of uncertainty in the initial state or action effects. The problem has been approached as a path-finding problem in belief space where good belief representations and heuristics are critical for scaling up. In this work, a different formulation is introduced for conformant problems with deterministic actions where they are automatically converted into classical ones and solved by an off-the-shelf classical planner. The translation maps literals L and sets of assumptions t about the initial situation, into new literals KL/t that represent that L must be true if t is initially true. We lay out a general translation scheme that is sound and establish the conditions under which the translation is also complete. We show that the complexity of the complete translation is exponential in a parameter of the problem called the conformant width, which for most benchmarks is bounded. The planner based on this translation exhibits good performance in comparison with existing planners, and is the basis for T0, the best performing planner in the Conformant Track of the 2006 International Planning Competition.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {623–675},
numpages = {53}
}

@article{10.5555/1641503.1641517,
author = {Zytnicki, Matthias and Gaspin, Christine and de Givry, Simon and Schiex, Thomas},
title = {Bounds Arc Consistency for Weighted CSPs},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {The Weighted Constraint Satisfaction Problem (WCSP) framework allows representing and solving problems involving both hard constraints and cost functions. It has been applied to various problems, including resource allocation, bioinformatics, scheduling, etc. To solve such problems, solvers usually rely on branch-and-bound algorithms equipped with local consistency filtering, mostly soft arc consistency. However, these techniques are not well suited to solve problems with very large domains. Motivated by the resolution of an RNA gene localization problem inside large genomic sequences, and in the spirit of bounds consistency for large domains in crisp CSPs, we introduce soft bounds arc consistency, a new weighted local consistency specifically designed for WCSP with very large domains. Compared to soft arc consistency, BAC provides significantly improved time and space asymptotic complexity. In this paper, we show how the semantics of cost functions can be exploited to further improve the time complexity of BAC. We also compare both in theory and in practice the efficiency of BAC on a WCSP with bounds consistency enforced on a crisp CSP using cost variables. On two different real problems modeled as WCSP, including our RNA gene localization problem, we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {593–621},
numpages = {29}
}

@article{10.5555/1641503.1641516,
author = {Krause, Andreas and Guestrin, Carlos},
title = {Optimal Value of Information in Graphical Models},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan.Furthermore we prove a surprising result: In most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as HMMs, this procedure can be generalized to poly-tree graphical models. We prove that the optimizing value of information is NPPP-hard even for polytrees. It also follows from our results that just computing decision theoretic value of information objective functions, which are commonly used in practice, is a #P-complete problem even on Naive Bayes models (a simple special case of polytrees).In addition, we consider several extensions, such as using our algorithms for scheduling observation selection for multiple sensors. We demonstrate the effectiveness of our approach on several real-world datasets, including a prototype sensor network deployment for energy conservation in buildings.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {557–591},
numpages = {35}
}

@article{10.5555/1641503.1641515,
author = {Gallardo, Jos\'{e} E. and Cotta, Carlos and Fern\'{a}ndez, Antonio J.},
title = {Solving Weighted Constraint Satisfaction Problems with Memetic/Exact Hybrid Algorithms},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {A weighted constraint satisfaction problem (WCSP) is a constraint satisfaction problem in which preferences among solutions can be expressed. Bucket elimination is a complete technique commonly used to solve this kind of constraint satisfaction problem. When the memory required to apply bucket elimination is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. Nevertheless, the curse of dimensionality makes these techniques impractical on large scale problems. In response to this situation, we present a memetic algorithm for WCSPs in which bucket elimination is used as a mechanism for recombining solutions, providing the best possible child from the parental set. Subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. As a case study, we have applied these algorithms to the resolution of the maximum density still life problem, a hard constraint optimization problem based on Conway's game of life. The resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. Moreover, it is shown that this proposal provides new best known solutions for very large instances.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {533–555},
numpages = {23}
}

@article{10.5555/1641503.1641514,
author = {Faliszewski, Piotr and Hemaspaandra, Edith and Hemaspaandra, Lane A.},
title = {How Hard is Bribery in Elections?},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {We study the complexity of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by paying certain voters to change their preferences a specified candidate can be made the election's winner? We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k- approval,and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {485–532},
numpages = {48}
}

@article{10.5555/1641503.1641513,
author = {Bromberg, Facundo and Margaritis, Dimitris and Honavar, Vasant},
title = {Efficient Markov Network Structure Discovery Using Independence Tests},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {We present two algorithms for learning the structure of a Markov network from data: GSMN* and GSIMN. Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests. Until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood. The independence-based approach does not require the computation of the likelihood, and thus both GSMN* and GSIMN can compute the structure efficiently (as shown in our experiments). GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearl's well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle theorem, also introduced in this work, which is a simplified version of the set of Markov axioms. Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN*, while generating a Markov network with comparable or in some cases improved quality. We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH, that produces all possible conditional independences resulting from repeatedly applying Pearl's theorems on the known conditional independence tests. The results of this comparison show that GSIMN, by the sole use of the Triangle theorem, is nearly optimal in terms of the set of independences tests that it infers.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {449–484},
numpages = {36}
}

@article{10.5555/1641503.1641512,
author = {Daly, R\'{o}n\'{a}n and Shen, Qiang},
title = {Learning Bayesian Network Equivalence Classes with Ant Colony Optimization},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of moves are allowed. Secondly, moves can be given in terms of indices that are not based on construction graph nodes. The results of testing show that ACO-E performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {391–447},
numpages = {57}
}

@article{10.5555/1641503.1641511,
author = {Sebastiani, Roberto and Vescovi, Michele},
title = {Automated Reasoning in Modal and Description Logics via SAT Encoding: The Case Study of K<sub>m</sub>ALC-Satisfiability},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {In the last two decades, modal and description logics have been applied to numerous areas of computer science, including knowledge representation, formal verification, database theory, distributed computing and, more recently, semantic web and ontologies. For this reason, the problem of automated reasoning in modal and description logics has been thoroughly investigated. In particular, many approaches have been proposed for efficiently handling the satisfiability of the core normal modal logic Km, and of its notational variant, the description logic ALC. Although simple in structure, Km/ALC is computationally very hard to reason on, its satisfiability being PSpace-complete.In this paper we start exploring the idea of performing automated reasoning tasks in modal and description logics by encoding them into SAT, so that to be handled by state-of-the-art SAT tools; as with most previous approaches, we begin our investigation from the satisfiability in Km. We propose an efficient encoding, and we test it on an extensive set of benchmarks, comparing the approach with the main state-of-the-art tools available. Although the encoding is necessarily worst-case exponential, from our experiments we notice that, in practice, this approach can handle most or all the problems which are at the reach of the other approaches, with performances which are comparable with, or even better than, those of the current state-of-the-art tools.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {343–389},
numpages = {47}
}

@article{10.5555/1641503.1641510,
author = {Faliszewski, Piotr and Hemaspaandra, Edith and Hemaspaandra, Lane A. and Rothe, J\"{o}rg},
title = {Llull and Copeland Voting Computationally Resist Bribery and Constructive Control},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Constructive control of elections refers to attempts by an agent to, via such actions as addition/deletion/partition of candidates or voters, ensure that a given candidate wins. Destructive control refers to attempts by an agent to, via the same actions, preclude a given candidate's victory. An election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control. An election system in which an agent can sometimes affect the result, yet in which it is NP-hard to recognize the inputs on which the agent can succeed, is said to be resistant to the given type of control.Aside from election systems with an NP-hard winner problem, the only systems previously known to be resistant to all the standard control types were highly artificial election systems created by hybridization. This paper studies a parameterized version of Copeland voting, denoted by Copelandα, where the parameter α is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates. In every previously studied constructive or destructive control scenario, we determine which of resistance or vulnerability holds for Copelandα for each rational α, 0 ≤ α ≤ 1. In particular, we prove that Copeland0.5, the system commonly referred to as "Copeland voting," provides full resistance to constructive control, and we prove the same for Copelandα, for all rational α, 0 &lt; α &lt; 1. Among systems with a polynomial-time winner problem, Copeland voting is the first natural election system proven to have full resistance to constructive control. In addition, we prove that both Copeland0 and Copeland1 (interestingly, Copeland1 is an election system developed by the thirteenth-century mystic Llull) are resistant to all standard types of constructive control other than one variant of addition of candidates. Moreover,we show that for each rational α, 0 ≤ α ≤ 1, Copelandα voting is fully resistant to bribery attacks, and we establish fixed-parameter tractability of bounded-case control for Copelandα.We also study Copelandα elections under more flexible models such as microbribery and extended control, we integrate the potential irrationality of voter preferences into many of our results, and we prove our results in both the unique-winner model and the nonunique-winner model. Our vulnerability results for microbribery are proven via a novel technique involving min-cost network flow.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {275–341},
numpages = {67}
}

@article{10.5555/1641503.1641509,
author = {Petrik, Marek and Zilberstein, Shlomo},
title = {A Bilinear Programming Approach for Multiagent Planning},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Multiagent planning and coordination problems are common and known to be computationally hard. We show that a wide range of two-agent problems can be formulated as bilinear programs. We present a successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state-of-the-art method for this class of multiagent problems. Because the algorithm is formulated for bilinear programs, it is more general and simpler to implement. The new algorithm can be terminated at any time and-unlike the coverage set algorithm-it facilitates the derivation of a useful online performance bound. It is also much more efficient, on average reducing the computation time of the optimal solution by about four orders of magnitude. Finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm, extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {235–274},
numpages = {40}
}

@article{10.5555/1641503.1641508,
author = {El-Yaniv, Ran and Pechyony, Dmitry},
title = {Transductive Rademacher Complexity and Its Applications},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {We develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive Rademacher complexity. Our technique is based on a novel general error bound for transduction in terms of transductive Rademacher complexity, together with a novel bounding technique for Rademacher averages for particular algorithms, in terms of their "unlabeled-labeled" representation. This technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. Finally, we present a new PAC-Bayesian bound for mixtures of transductive algorithms based on our Rademacher bounds.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {193–234},
numpages = {42}
}

@article{10.5555/1641503.1641507,
author = {Conitzer, Vincent},
title = {Eliciting Single-Peaked Preferences Using Comparison Queries},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. Rather, the agents' preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference elicitation in voting has focused on the case of unrestricted preferences. It has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. In contrast, in this paper, we focus on single-peaked preferences. We show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agent's complete preferences are known. We show that using a sublinear number of queries does not suffice. We also consider the case of cardinally single-peaked preferences. For this case, we show that if the alternatives' cardinal positions are known, then an agent's preferences can be elicited using only a logarithmic number of queries; however, we also show that if the cardinal positions are not known, then a sublinear number of queries does not suffice. We present experimental results for all elicitation algorithms. We also consider the problem of only eliciting enough information to determine the aggregate ranking, and show that even for this more modest objective, a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions. Finally, we discuss whether and how these techniques can be applied when preferences are almost single-peaked.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {161–191},
numpages = {31}
}

@article{10.5555/1641503.1641506,
author = {Ramchurn, Sarvapali D. and Mezzetti, Claudio and Giovannucci, Andrea and Rodriguez-Aguilar, Juan A. and Dash, Rajdeep K. and Jennings, Nicholas R.},
title = {Trust-Based Mechanisms for Robust and Efficient Task Allocation in the Presence of Execution Uncertainty},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Vickrey-Clarke-Groves (VCG) mechanisms are often used to allocate tasks to selfish and rational agents. VCG mechanisms are incentive compatible, direct mechanisms that are efficient (i.e., maximise social utility) and individually rational (i.e., agents prefer to join rather than opt out). However, an important assumption of these mechanisms is that the agents will always successfully complete their allocated tasks. Clearly, this assumption is unrealistic in many real-world applications, where agents can, and often do, fail in their endeavours. Moreover, whether an agent is deemed to have failed may be perceived differently by different agents. Such subjective perceptions about an agent's probability of succeeding at a given task are often captured and reasoned about using the notion of trust. Given this background, in this paper we investigate the design of novel mechanisms that take into account the trust between agents when allocating tasks.Specifically, we develop a new class of mechanisms, called trust-based mechanisms, that can take into account multiple subjective measures of the probability of an agent succeeding at a given task and produce allocations that maximise social utility, whilst ensuring that no agent obtains a negative utility. We then show that such mechanisms pose a challenging new combinatorial optimisation problem (that is NP-complete), devise a novel representation for solving the problem, and develop an effective integer programming solution (that can solve instances with about 2\texttimes{}105 possible allocations in 40 seconds).},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {119–159},
numpages = {41}
}

@article{10.5555/1641503.1641505,
author = {Hoffmann, J\"{o}rg and Bertoli, Piergiorgio and Helmert, Malte and Pistore, Marco},
title = {Message-Based Web Service Composition, Integrity Constraints, and Planning under Uncertainty: A New Connection},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Thanks to recent advances, AI Planning has become the underlying technique for several applications. Figuring prominently among these is automated Web Service Composition (WSC) at the "capability" level, where services are described in terms of preconditions and effects over ontological concepts. A key issue in addressing WSC as planning is that ontologies are not only formal vocabularies; they also axiomatize the possible relationships between concepts. Such axioms correspond to what has been termed "integrity constraints" in the actions and change literature, and applying a web service is essentially a belief update operation. The reasoning required for belief update is known to be harder than reasoning in the ontology itself. The support for belief update is severely limited in current planning tools.Our first contribution consists in identifying an interesting special case of WSC which is both significant and more tractable. The special case, which we term forward effects, is characterized by the fact that every ramification of a web service application involves at least one new constant generated as output by the web service. We show that, in this setting, the reasoning required for belief update simplifies to standard reasoning in the ontology itself. This relates to, and extends, current notions of "message-based" WSC, where the need for belief update is removed by a strong (often implicit or informal) assumption of "locality" of the individual messages. We clarify the computational properties of the forward effects case, and point out a strong relation to standard notions of planning under uncertainty, suggesting that effective tools for the latter can be successfully adapted to address the former.Furthermore, we identify a significant sub-case, named strictly forward effects, where an actual compilation into planning under uncertainty exists. This enables us to exploit off-the-shelf planning tools to solve message-based WSC in a general form that involves powerful ontologies, and requires reasoning about partial matches between concepts. We provide empirical evidence that this approach may be quite effective, using Conformant-FF as the underlying planner.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {49–117},
numpages = {69}
}

@article{10.5555/1641503.1641504,
author = {Chali, Yllias and Joty, Shafiq R. and Hasan, Sadid A.},
title = {Complex Question Answering: Unsupervised Learning Approaches and Experiments},
year = {2009},
issue_date = {May 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {35},
number = {1},
issn = {1076-9757},
abstract = {Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–47},
numpages = {47}
}

@article{10.5555/1622716.1622736,
author = {Zaffalon, Marco and Miranda, Enrique},
title = {Conservative Inference Rule for Uncertain Reasoning under Incompleteness},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {In this paper we formulate the problem of inference under incomplete information in very general terms. This includes modelling the process responsible for the incompleteness, which we call the incompleteness process. We allow the process' behaviour to be partly unknown. Then we use Walley's theory of coherent lower previsions, a generalisation of the Bayesian theory to imprecision, to derive the rule to update beliefs under incompleteness that logically follows from our assumptions, and that we call conservative inference rule. This rule has some remarkable properties: it is an abstract rule to update beliefs that can be applied in any situation or domain; it gives us the opportunity to be neither too optimistic nor too pessimistic about the incompleteness process, which is a necessary condition to draw reliable while strong enough conclusions; and it is a coherent rule, in the sense that it cannot lead to inconsistencies. We give examples to show how the new rule can be applied in expert systems, in parametric statistical inference, and in pattern classification, and discuss more generally the view of incompleteness processes defended here as well as some of its consequences.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {757–821},
numpages = {65}
}

@article{10.5555/1622716.1622735,
author = {Singh, Amarjeet and Krause, Andreas and Guestrin, Carlos and Kaiser, William J.},
title = {Efficient Informative Sensing Using Multiple Robots},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {The need for efficient monitoring of spatio-temporal dynamics in large environmental applications, such as the water quality monitoring in rivers and lakes, motivates the use of robotic sensors in order to achieve sufficient spatial coverage. Typically, these robots have bounded resources, such as limited battery or limited amounts of time to obtain measurements. Thus, careful coordination of their paths is required in order to maximize the amount of information collected, while respecting the resource constraints. In this paper, we present an efficient approach for near-optimally solving the NP-hard optimization problem of planning such informative paths. In particular, we first develop eSIP (efficient Single-robot Informative Path planning), an approximation algorithm for optimizing the path of a single robot. Hereby, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to quantify the amount of information collected. We prove that the mutual information collected using paths obtained by using eSIP is close to the information obtained by an optimal solution. We then provide a general technique, sequential allocation, which can be used to extend any single robot planning algorithm, such as eSIP, for the multi-robot problem. This procedure approximately generalizes any guarantees for the single-robot problem to the multi-robot case. We extensively evaluate the effectiveness of our approach on several experiments performed infield for two important environmental sensing applications, lake and river monitoring, and simulation experiments performed using several real world sensor network data sets.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {707–755},
numpages = {49}
}

@article{10.5555/1622716.1622734,
author = {Gim\'{e}nez, Omer and Jonsson, Anders},
title = {Planning over Chain Causal Graphs for Variables with Domains of Size 5 Is NP-Hard},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Recently, considerable focus has been given to the problem of determining the boundary between tractable and intractable planning problems. In this paper, we study the complexity of planning in the class Cn of planning problems, characterized by unary operators and directed path causal graphs. Although this is one of the simplest forms of causal graphs a planning problem can have, we show that planning is intractable for Cn (unless P = NP), even if the domains of state variables have bounded size. In particular, we show that plan existence for Ckn is NP-hard for k ≥ 5 by reduction from CNF-SAT. Here, k denotes the upper bound on the size of the state variable domains. Our result reduces the complexity gap for the class Ckn to cases k = 3 and k = 4 only, since C2n is known to be tractable.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {675–706},
numpages = {32}
}

@article{10.5555/1622716.1622733,
author = {Cohn, Trevor and Lapata, Mirella},
title = {Sentence Compression as Tree Transduction},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {637–674},
numpages = {38}
}

@article{10.5555/1622716.1622732,
author = {S\'{a}nchez-Mart\'{\i}nez, Felipe and Forcada, Mikel L.},
title = {Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules. The experiments conducted using three difierent language pairs in the free/open-source MT platform Apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {605–635},
numpages = {31}
}

@article{10.5555/1622716.1622731,
author = {Branavan, S. R. K. and Chen, Harr and Eisenstein, Jacob and Barzilay, Regina},
title = {Learning Document-Level Semantic Properties from Free-Text Annotations},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as "a real bargain" or "good value." These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {569–603},
numpages = {35}
}

@article{10.5555/1622716.1622730,
author = {Rahwan, Talal and Ramchurn, Sarvapali D. and Jennings, Nicholas R. and Giovannucci, Andrea},
title = {An Anytime Algorithm for Optimal Coalition Structure Generation},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. Forming effective coalitions is a major research challenge in the field of multi-agent systems. Central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal. This usually requires calculating a value for every possible coalition, known as the coalition value, which indicates how beneficial that coalition would be if it was formed. Once these values are calculated, the agents usually need to find a combination of coalitions, in which every agent belongs to exactly one coalition, and by which the overall outcome of the system is maximized. However, this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined, which grows exponentially with the number of agents involved. To date, therefore, many algorithms have been proposed to solve this problem using different techniques -- ranging from dynamic programming, to integer programming, to stochastic search -- all of which suffer from major limitations relating to execution time, solution quality, and memory requirements.With this in mind, we develop an anytime algorithm to solve the coalition structure generation problem. Specifically, the algorithm uses a novel representation of the search space, which partitions the space of possible solutions into sub-spaces such that it is possible to compute upper and lower bounds on the values of the best coalition structures in them. These bounds are then used to identify the sub-spaces that have no potential of containing the optimal solution so that they can be pruned. The algorithm, then, searches through the remaining sub-spaces very efficiently using a branch-and-bound technique to avoid examining all the solutions within the searched subspace(s). In this setting, we prove that our algorithm enumerates all coalition structures efficiently by avoiding redundant and invalid solutions automatically. Moreover, in order to effectively test our algorithm we develop a new type of input distribution which allows us to generate more reliable benchmarks compared to the input distributions previously used in the field. Given this new distribution, we show that for 27 agents our algorithm is able to find solutions that are optimal in 0.175% of the time required by the fastest available algorithm in the literature. The algorithm is anytime, and if interrupted before it would have normally terminated, it can still provide a solution that is guaranteed to be within a bound from the optimal one. Moreover, the guarantees we provide on the quality of the solution are significantly better than those provided by the previous state of the art algorithms designed for this purpose. For example, for the worst case distribution given 25 agents, our algorithm is able to find a 90% efficient solution in around 10% of time it takes to find the optimal solution.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {521–567},
numpages = {47}
}

@article{10.5555/1622716.1622729,
author = {de Angulo, Vicente Ruiz and Torras, Carme},
title = {Exploiting Single-Cycle Symmetries in Continuous Constraint Problems},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Symmetries in discrete constraint satisfaction problems have been explored and exploited in the last years, but symmetries in continuous constraint problems have not received the same attention. Here we focus on permutations of the variables consisting of one single cycle. We propose a procedure that takes advantage of these symmetries by interacting with a continuous constraint solver without interfering with it. A key concept in this procedure are the classes of symmetric boxes formed by bisecting a n-dimensional cube at the same point in all dimensions at the same time. We analyze these classes and quantify them as a function of the cube dimensionality. Moreover, we propose a simple algorithm to generate the representatives of all these classes for any number of variables at very high rates. A problem example from the chemical field and the cyclic n-roots problem are used to show the performance of the approach in practice.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {499–520},
numpages = {22}
}

@article{10.5555/1622716.1622728,
author = {Gabrilovich, Evgeniy and Markovitch, Shaul},
title = {Wikipedia-Based Semantic Interpretation for Natural Language Processing},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {443–498},
numpages = {56}
}

@article{10.5555/1622716.1622727,
author = {Bacchus, Fahiem and Dalmao, Shannon and Pitassi, Toniann},
title = {Solving #SAT and Bayesian Inference with Backtracking Search},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Inference in Bayes Nets (BAYES) is an important problem with numerous applications in probabilistic reasoning. Counting the number of satisfying assignments of a propositional formula (#SAT) is a closely related problem of fundamental theoretical importance. Both these problems, and others, are members of the class of sum-of-products (SUMPROD) problems. In this paper we show that standard backtracking search when augmented with a simple memoization scheme (caching) can solve any sum-of-products problem with time complexity that is at least as good any other state-of-the-art exact algorithm, and that it can also achieve the best known time-space tradeoff. Furthermore, backtracking's ability to utilize more flexible variable orderings allows us to prove that it can achieve an exponential speedup over other standard algorithms for SUMPROD on some instances.The ideas presented here have been utilized in a number of solvers that have been applied to various types of sum-of-product problem's. These system's have exploited the fact that backtracking can naturally exploit more of the problem's structure to achieve improved performance on a range of problem instances. Empirical evidence of this performance gain has appeared in published works describing these solvers, and we provide references to these works.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {391–442},
numpages = {52}
}

@article{10.5555/1622716.1622726,
author = {Li, Yifan and Musilek, Petr and Reformat, Marek and Wyard-Scott, Loren},
title = {Identification of Pleonastic It Using the Web},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it : the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {339–389},
numpages = {51}
}

@article{10.5555/1622716.1622725,
author = {Doshi, Prashant and Gmytrasiewicz, Piotr J.},
title = {Monte Carlo Sampling Methods for Approximating Interactive POMDPs},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Partially observable Markov decision processes (POMDPs) provide a principled framework for sequential planning in uncertain single agent settings. An extension of POMDPs to multiagent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent's belief about the physical world, about beliefs of other agents, and about their beliefs about others' beliefs. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a general method for obtaining approximate solutions of I-POMDPs based on particle filtering (PF). We introduce the interactive PF, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to mitigate the belief space complexity, but it does not address the policy space complexity. To mitigate the policy space complexity - sometimes also called the curse of history - we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. While this approach does not completely address the curse of history, it beats back the curse's impact substantially. We provide experimental results and chart future work.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {297–337},
numpages = {41}
}

@article{10.5555/1622716.1622724,
author = {Yates, Alexander and Etzioni, Oren},
title = {Unsupervised Methods for Determining Object and Relation Synonyms on the Web},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of RESOLVER's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic RESOLVER system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {255–296},
numpages = {42}
}

@article{10.5555/1622716.1622723,
author = {Jurca, Radu and Faltings, Boi},
title = {Mechanisms for Making Crowds Truthful},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {We consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational, self-interested agents. One example are online feedback mechanisms, where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect. However, (i) providing such feedback is costly, and (ii) there are many motivations for providing incorrect feedback.Both problems can be addressed by reward schemes which (i) cover the cost of obtaining and reporting feedback, and (ii) maximize the expected reward of a rational agent who reports truthfully. We address the design of such incentive-compatible rewards for feedback generated in environments with pure adverse selection. Here, the correlation between the true knowledge of an agent and her beliefs regarding the likelihoods of reports of other agents can be exploited to make honest reporting a Nash equilibrium.In this paper we extend existing methods for designing incentive-compatible rewards by also considering collusion. We analyze different scenarios, where, for example, some or all of the agents collude. For each scenario we investigate whether a collusion-resistant, incentive-compatible reward scheme exists, and use automated mechanism design to specify an algorithm for deriving an efficient reward mechanism.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {209–253},
numpages = {45}
}

@article{10.5555/1622716.1622722,
author = {Wallace, Scott},
title = {Behavior Bounding: An Efficient Method for High-Level Behavior Comparison},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we explore methods for comparing agent behavior with human behavior to assist with validation. Our exploration begins by considering a simple method of behavior comparison. Motivated by shortcomings in this initial approach, we introduce behavior bounding, an automated model-based approach for comparing behavior that is inspired, in part, by Mitchell's Version Spaces. We show that behavior bounding can be used to compactly represent both human and agent behavior. We argue that relatively low amounts of human effort are required to build, maintain, and use the data structures that underlie behavior bounding, and we provide a theoretical basis for these arguments using notions of PAC Learnability. Next, we show empirical results indicating that this approach is efiective at identifying differences in certain types of behaviors and that it performs well when compared against our initial benchmark methods. Finally, we demonstrate that behavior bounding can produce information that allows developers to identify and fix problems in an agent's behavior much more efficiently than standard debugging techniques.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {165–208},
numpages = {44}
}

@article{10.5555/1622716.1622721,
author = {Binshtok, Maxim and Brafman, Ronen I. and Domshlak, Carmel and Shimony, Solomon E.},
title = {Generic Preferences over Subsets of Structured Objects},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Various tasks in decision making and decision support systems require selecting a preferred subset of a given set of items. Here we focus on problems where the individual items are described using a set of characterizing attributes, and a generic preference specification is required, that is, a specification that can work with an arbitrary set of items. For example, preferences over the content of an online newspaper should have this form: At each viewing, the newspaper contains a subset of the set of articles currently available. Our preference specification over this subset should be provided offline, but we should be able to use it to select a subset of any currently available set of articles, e.g., based on their tags. We present a general approach for lifting formalisms for specifying preferences over objects with multiple attributes into ones that specify preferences over subsets of such objects. We also show how we can compute an optimal subset given such a specification in a relatively efficient manner. We provide an empirical evaluation of the approach as well as some worst-case complexity results.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {133–164},
numpages = {32}
}

@article{10.5555/1622716.1622720,
author = {Bernstein, Daniel S. and Amato, Christopher and Hansen, Eric A. and Zilberstein, Shlomo},
title = {Policy Iteration for Decentralized Control of Markov Decision Processes},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. As a formal framework for such problems, we use the decentralized partially observable Markov decision process (DECPOMDP). Though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem, optimal algorithms for the multiagent case have been elusive. The main contribution of this paper is an optimal policy iteration algorithm for solving DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent policies. The solution can include a correlation device, which allows agents to correlate their actions without communicating. This approach alternates between expanding the controller and performing value-preserving transformations, which modify the controller without sacrificing value. We present two efficient value-preserving transformations: one can reduce the size of the controller and the other can improve its value while keeping the size fixed. Empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm, which sacrifices convergence to optimality. This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents' actions are known. While this assumption may not hold in general, it helps produce higher quality solutions in our test problems.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {89–132},
numpages = {44}
}

@article{10.5555/1622716.1622719,
author = {Gershman, Amir and Meisels, Amnon and Zivan, Roie},
title = {Asynchronous Forward Bounding for Distributed COPs},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {A new search algorithm for solving distributed constraint optimization problems (DisCOPs) is presented. Agents assign variables sequentially and compute bounds on partial assignments asynchronously. The asynchronous bounds computation is based on the propagation of partial assignments. The asynchronous forward-bounding algorithm (AFB) is a distributed optimization search algorithm that keeps one consistent partial assignment at all times. The algorithm is described in detail and its correctness proven. Experimental evaluation shows that AFB outperforms synchronous branch and bound by many orders of magnitude, and produces a phase transition as the tightness of the problem increases. This is an analogous effect to the phase transition that has been observed when local consistency maintenance is applied to MaxCSPs. The AFB algorithm is further enhanced by the addition of a backjumping mechanism, resulting in the AFB-BJ algorithm. Distributed backjumping is based on accumulated information on bounds of all values and on processing concurrently a queue of candidate goals for the next move back. The AFB-BJ algorithm is compared experimentally to other DisCOP algorithms (ADOPT, DPOP, OptAPO) and is shown to be a very efficient algorithm for DisCOPs.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {61–88},
numpages = {28}
}

@article{10.5555/1622716.1622718,
author = {Meuleau, Nicolas and Benazera, Emmanuel and Brafman, Ronen I. and Hansen, Eric A. and Mausam},
title = {A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of optimal planning in stochastic domains with resource constraints, where the resources are continuous and the choice of action at each step depends on resource availability. We introduce the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables, where the continuous variables represent monotonic resources. Like other heuristic search algorithms, HAO* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy. We show that this approach is especially effective when resource constraints limit how much of the state space is reachable. Experimental results demonstrate its effectiveness in the domain that motivates our research: automated planning for planetary exploration rovers.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {27–59},
numpages = {33}
}

@article{10.5555/1622716.1622717,
author = {Chernova, Sonia and Veloso, Manuela},
title = {Interactive Policy Learning through Confidence-Based Autonomy},
year = {2009},
issue_date = {January 2009},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {34},
number = {1},
issn = {1076-9757},
abstract = {We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration. The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. CBA and its individual components are compared and evaluated in a complex simulated driving domain. The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–25},
numpages = {25}
}

@article{10.5555/1622698.1622715,
author = {Turney, Peter D.},
title = {The Latent Relation Mapping Engine: Algorithm and Experiments},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {615–655},
numpages = {41}
}

@article{10.5555/1622698.1622714,
author = {Ashlagi, Itai and Monderer, Dov and Tennenholtz, Moshe},
title = {On the Value of Correlation},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Correlated equilibrium generalizes Nash equilibrium to allow correlation devices. Correlated equilibrium captures the idea that in many systems there exists a trusted administrator who can recommend behavior to a set of agents, but can not enforce such behavior. This makes this solution concept most appropriate to the study of multi-agent systems in AI. Aumann showed an example of a game, and of a correlated equilibrium in this game in which the agents' welfare (expected sum of players' utilities) is greater than their welfare in all mixed-strategy equilibria. Following the idea initiated by the price of anarchy literature this suggests the study of two major measures for the value of correlation in a game with nonnegative payoffs: 1. The ratio between the maximal welfare obtained in a correlated equilibrium to the maximal welfare obtained in a mixed-strategy equilibrium. We refer to this ratio as the mediation value. 2. The ratio between the maximal welfare to the maximal welfare obtained in a correlated equilibrium. We refer to this ratio as the enforcement value. In this work we initiate the study of the mediation and enforcement values, providing several general results on the value of correlation as captured by these concepts. We also present a set of results for the more specialized case of congestion games, a class of games that received a lot of attention in the recent literature.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {575–613},
numpages = {39}
}

@article{10.5555/1622698.1622713,
author = {de Jong, Steven and Uyttendaele, Simon and Tuyls, Karl},
title = {Learning to Reach Agreement in a Continuous Ultimatum Game},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {It is well-known that acting in an individually rational manner, according to the principles of classical game theory, may lead to sub-optimal solutions in a class of problems named social dilemmas. In contrast, humans generally do not have much difficulty with social dilemmas, as they are able to balance personal benefit and group benefit. As agents in multi-agent systems are regularly confronted with social dilemmas, for instance in tasks such as resource allocation, these agents may benefit from the inclusion of mechanisms thought to facilitate human fairness. Although many of such mechanisms have already been implemented in a multi-agent systems context, their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies (usually two). Given that many real-world examples of social dilemmas are actually continuous in nature, we extend this previous work to more general dilemmas, in which agents operate in a continuous strategy space. The social dilemma under study here is the well-known Ultimatum Game, in which an optimal solution is achieved if agents agree on a common strategy. We investigate whether a scale-free interaction network facilitates agents to reach agreement, especially in the presence of fixed-strategy agents that represent a desired (e.g. human) outcome. Moreover, we study the influence of rewiring in the interaction network. The agents are equipped with continuous-action learning automata and play a large number of random pairwise games in order to establish a common strategy. From our experiments, we may conclude that results obtained in discrete-strategy games can be generalized to continuous-strategy games to a certain extent: a scale-free interaction network structure allows agents to achieve agreement on a common strategy, and rewiring in the interaction network greatly enhances the agents' ability to reach agreement. However, it also becomes clear that some alternative mechanisms, such as reputation and volunteering, have many subtleties involved and do not have convincing beneficial effects in the continuous case.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {551–574},
numpages = {24}
}

@article{10.5555/1622698.1622712,
author = {Abdallah, Sherief and Lesser, Victor},
title = {A Multiagent Reinforcement Learning Algorithm with Non-Linear Dynamics},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents' decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received.We introduce a new MARL algorithm called theWeighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley's game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently.An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL's convergence is difficult, because of the non-linear nature of WPL's dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL's dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {521–549},
numpages = {29}
}

@article{10.5555/1622698.1622711,
author = {Mateescu, Robert and Dechter, Rina and Marinescu, Radu},
title = {AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Inspired by the recently introduced framework of AND/OR search spaces for graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD) with AND nodes, in order to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). We present the AND/OR Multi-Valued Decision Diagram (AOMDD) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of graphical models) queries. We provide two algorithms for compiling the AOMDD of a graphical model. The first is search-based, and works by applying reduction rules to the trace of the memory intensive AND/OR search algorithm. The second is inference-based and uses a Bucket Elimination schedule to combine the AOMDDs of the input functions via the the APPLY operator. For both algorithms, the compilation time and the size of the AOMDD are, in the worst case, exponential in the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision diagrams (OBDDs). We introduce the concept of semantic treewidth, which helps explain why the size of a decision diagram is often much smaller than the worst case bound. We provide an experimental evaluation that demonstrates the potential of AOMDDs.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {465–519},
numpages = {55}
}

@article{10.5555/1622698.1622710,
author = {Zhang, Dongmo and Zhang, Yan},
title = {An Ordinal Bargaining Solution with Fixed-Point Property},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Shapley's impossibility result indicates that the two-person bargaining problem has no non-trivial ordinal solution with the traditional game-theoretic bargaining model. Although the result is no longer true for bargaining problems with more than two agents, none of the well known bargaining solutions are ordinal. Searching for meaningful ordinal solutions, especially for the bilateral bargaining problem, has been a challenging issue in bargaining theory for more than three decades. This paper proposes a logic-based ordinal solution to the bilateral bargaining problem. We argue that if a bargaining problem is modeled in terms of the logical relation of players' physical negotiation items, a meaningful bargaining solution can be constructed based on the ordinal structure of bargainers' preferences. We represent bargainers' demands in propositional logic and bargainers' preferences over their demands in total preorder. We show that the solution satisfies most desirable logical properties, such as individual rationality (logical version), consistency, collective rationality as well as a few typical game-theoretic properties, such as weak Pareto optimality and contraction invariance. In addition, if all players' demand sets are logically closed, the solution satisfies a fixed-point condition, which says that the outcome of a negotiation is the result of mutual belief revision. Finally, we define various decision problems in relation to our bargaining model and study their computational complexity.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {433–464},
numpages = {32}
}

@article{10.5555/1622698.1622709,
author = {Goldsmith, Judy and Lang, J\'{e}r\^{o}me and Truszczynski, Miroslaw and Wilson, Nic},
title = {The Computational Complexity of Dominance and Consistency in CP-Nets},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {We investigate the computational complexity of testing dominance and consistency in CP-nets. Previously, the complexity of dominance has been determined for restricted classes in which the dependency graph of the CP-net is acyclic. However, there are preferences of interest that define cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show here that both dominance and consistency for general CP-nets are PSPACE-complete. We then consider the concept of strong dominance, dominance equivalence and dominance incomparability, and several notions of optimality, and identify the complexity of the corresponding decision problems. The reductions used in the proofs are from STRIPS planning, and thus reinforce the earlier established connections between both areas.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {403–432},
numpages = {30}
}

@article{10.5555/1622698.1622708,
author = {Amir, Eyal and Chang, Allen},
title = {Learning Partially Observable Deterministic Action Models},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {We present exact algorithms for identifying deterministic-actions' effects and preconditions in dynamic partially observable domains. They apply when one does not know the action model (the way actions affect the world) of a domain and must learn it from partial observations over time. Such scenarios are common in real world applications. They are challenging for AI tasks because traditional domain structures that underly tractability (e.g., conditional independence) fail there (e.g., world features become correlated). Our work departs from traditional assumptions about partial observations and action models. In particular, it focuses on problems in which actions are deterministic of simple logical structure and observation models have all features observed with some frequency. We yield tractable algorithms for the modified problem for such domains.Our algorithms take sequences of partial observations over time as input, and output deterministic action models that could have lead to those observations. The algorithms output all or one of those models (depending on our choice), and are exact in that no model is misclassified given the observations. Our algorithms take polynomial time in the number of time steps and state features for some traditional action classes examined in the AI-planning literature, e.g., STRIPS actions. In contrast, traditional approaches for HMMs and Reinforcement Learning are inexact and exponentially intractable for such domains. Our experiments verify the theoretical tractability guarantees, and show that we identify action models exactly. Several applications in planning, autonomous exploration, and adventure-game playing already use these results. They are also promising for probabilistic settings, partially observable reinforcement learning, and diagnosis.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {349–402},
numpages = {54}
}

@article{10.5555/1622698.1622707,
author = {Kakas, Antonis and Mancarella, Paolo and Sadri, Fariba and Stathis, Kostas and Toni, Francesca},
title = {Computational Logic Foundations of KGP Agents},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan) model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent's state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {285–348},
numpages = {64}
}

@article{10.5555/1622698.1622706,
author = {Rezek, Iead and Leslie, David S. and Reece, Steven and Roberts, Stephen J. and Rogers, Alex and Dash, Rajdeep K. and Jennings, Nicholas R.},
title = {On Similarities between Inference in Game Theory and Machine Learning},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we elucidate the equivalence between inference in game theory and machine learning. Our aim in so doing is to establish an equivalent vocabulary between the two domains so as to facilitate developments at the intersection of both fields, and as proof of the usefulness of this approach, we use recent developments in each field to make useful improvements to the other. More specifically, we consider the analogies between smooth best responses in fictitious play and Bayesian inference methods. Initially, we use these insights to develop and demonstrate an improved algorithm for learning in games based on probabilistic moderation. That is, by integrating over the distribution of opponent strategies (a Bayesian approach within machine learning) rather than taking a simple empirical average (the approach used in standard fictitious play) we derive a novel moderated fictitious play algorithm and show that it is more likely than standard fictitious play to converge to a payoff-dominant but risk-dominated Nash equilibrium in a simple coordination game. Furthermore we consider the converse case, and show how insights from game theory can be used to derive two improved mean field variational learning algorithms. We first show that the standard update rule of mean field variational learning is analogous to a Cournot adjustment within game theory. By analogy with fictitious play, we then suggest an improved update rule, and show that this results in fictitious variational play, an improved mean field variational learning algorithm that exhibits better convergence in highly or strongly connected graphical models. Second, we use a recent advance in fictitious play, namely dynamic fictitious play, to derive a derivative action variational learning algorithm, that exhibits superior convergence properties on a canonical machine learning problem (clustering a mixture distribution).},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {259–283},
numpages = {25}
}

@article{10.5555/1622698.1622705,
author = {Grinshpoun, Tal and Meisels, Amnon},
title = {Completeness and Performance of the APO Algorithm},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Asynchronous Partial Overlay (APO) is a search algorithm that uses cooperative mediation to solve Distributed Constraint Satisfaction Problems (DisCSPs). The algorithm partitions the search into different subproblems of the DisCSP. The original proof of completeness of the APO algorithm is based on the growth of the size of the subproblems. The present paper demonstrates that this expected growth of subproblems does not occur in some situations, leading to a termination problem of the algorithm. The problematic parts in the APO algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given. The resulting version of the algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensures its completeness. Formal proofs for the soundness and completeness of CompAPO are given. A detailed performance evaluation of CompAPO comparing it to other DisCSP algorithms is presented, along with an extensive experimental evaluation of the algorithm's unique behavior. Additionally, an optimization version of the algorithm, CompOptAPO, is presented, discussed, and evaluated.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {223–258},
numpages = {36}
}

@article{10.5555/1622698.1622704,
author = {De Laet, Tinne and De Schutter, Joris and Bruyninckx, Herman},
title = {A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {This paper proposes and experimentally validates a Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in a transparent and intuitive model. With respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in the state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters.Furthermore, the RBBM is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. This sample-based approximation enables handling dynamic environments and capturing multimodality, which occurs even in simple static environments.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {179–222},
numpages = {44}
}

@article{10.5555/1622698.1622703,
author = {Meir, Reshef and Procaccia, Ariel D. and Rosenschein, Jeffrey S. and Zohar, Aviv},
title = {Complexity of Strategic Behavior in Multi-Winner Elections},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Although recent years have seen a surge of interest in the computational aspects of social choice, no specific attention has previously been devoted to elections with multiple winners, e.g., elections of an assembly or committee. In this paper, we characterize the worst-case complexity of manipulation and control in the context of four prominent multiwinner voting systems, under different formulations of the strategic agent's goal.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {149–178},
numpages = {30}
}

@article{10.5555/1622698.1622702,
author = {Gal, Ya'akov and Pfeffer, Avi},
title = {Networks of Influence Diagrams: A Formalism for Representing Agents' Beliefs and Decision-Making Processes},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {This paper presents Networks of Influence Diagrams (NID), a compact, natural and highly expressive language for reasoning about agents' beliefs and decision-making processes. NIDs are graphical structures in which agents' mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance. NIDs are equivalent in representation to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents' optimal strategies, and how they actually behave in reality.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {109–147},
numpages = {39}
}

@article{10.5555/1622698.1622701,
author = {Martinez, David and de Lacalle, Oier Lopez and Agirre, Eneko},
title = {On the Use of Automatically Acquired Examples for All-Nouns Word Sense Disambiguation},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {This article focuses on Word Sense Disambiguation (WSD), which is a Natural Language Processing task that is thought to be important for many Language Technology applications, such as Information Retrieval, Information Extraction, or Machine Translation. One of the main issues preventing the deployment of WSD technology is the lack of training examples for Machine Learning systems, also known as the Knowledge Acquisition Bottleneck. A method which has been shown to work for small samples of words is the automatic acquisition of examples. We have previously shown that one of the most promising example acquisition methods scales up and produces a freely available database of 150 million examples from Web snippets for all polysemous nouns in WordNet. This paper focuses on the issues that arise when using those examples, all alone or in addition to manually tagged examples, to train a supervised WSD system for all nouns. The extensive evaluation on both lexical-sample and all-words Senseval benchmarks shows that we are able to improve over commonly used baselines and to achieve top-rank performance. The good use of the prior distributions from the senses proved to be a crucial factor.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {79–107},
numpages = {29}
}

@article{10.5555/1622698.1622700,
author = {Lubin, Benjamin and Juda, Adam I. and Cavallo, Ruggiero and Lahaie, S\'{e}bastien and Shneidman, Jeffrey and Parkes, David C.},
title = {ICE: An Expressive Iterative Combinatorial Exchange},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {We present the design and analysis of the first fully expressive, iterative combinatorial exchange (ICE). The exchange incorporates a tree-based bidding language (TBBL) that is concise and expressive for CEs. Bidders specify lower and upper bounds in TBBL on their value for different trades and refine these bounds across rounds. These bounds allow price discovery and useful preference elicitation in early rounds, and allow termination with an efficient trade despite partial information on bidder valuations. All computation in the exchange is carefully optimized to exploit the structure of the bid-trees and to avoid enumerating trades. A proxied interpretation of a revealed-preference activity rule, coupled with simple linear prices, ensures progress across rounds. The exchange is fully implemented, and we give results demonstrating several aspects of its scalability and economic properties with simulated bidding strategies.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {33–77},
numpages = {45}
}

@article{10.5555/1622698.1622699,
author = {Esmeir, Saher and Markovitch, Shaul},
title = {Anytime Induction of Low-Cost, Low-Error Classifiers: A Sampling-Based Approach},
year = {2008},
issue_date = {September 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {33},
number = {1},
issn = {1076-9757},
abstract = {Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–31},
numpages = {31}
}

@article{10.5555/1622673.1622697,
author = {Gerding, Enrico H. and Dash, Rajdeep K. and Byde, Andrew and Jennings, Nicholas R.},
title = {Optimal Strategies for Bidding Agents Participating in Simultaneous Vickrey Auctions with Perfect Substitutes},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {We derive optimal strategies for a bidding agent that participates in multiple, simultaneous second-price auctions with perfect substitutes. We prove that, if everyone else bids locally in a single auction, the global bidder should always place non-zero bids in all available auctions, provided there are no budget constraints. With a budget, however, the optimal strategy is to bid locally if this budget is equal or less than the valuation. Furthermore, for a wide range of valuation distributions, we prove that the problem of finding the optimal bids reduces to two dimensions if all auctions are identical. Finally, we address markets with both sequential and simultaneous auctions, non-identical auctions, and the allocative efficiency of the market.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {939–982},
numpages = {44}
}

@article{10.5555/1622673.1622696,
author = {Moore, Neil C. A. and Prosser, Patrick},
title = {The Ultrametric Constraint and Its Application to Phylogenetics},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {A phylogenetic tree shows the evolutionary relationships among species. Internal nodes of the tree represent speciation events and leaf nodes correspond to species. A goal of phylogenetics is to combine such trees into larger trees, called supertrees, whilst respecting the relationships in the original trees. A rooted tree exhibits an ultrametric property; that is, for any three leaves of the tree it must be that one pair has a deeper most recent common ancestor than the other pairs, or that all three have the same most recent common ancestor. This inspires a constraint programming encoding for rooted trees. We present an efficient constraint that enforces the ultrametric property over a symmetric array of constrained integer variables, with the inevitable property that the lower bounds of any three variables are mutually supportive. We show that this allows an efficient constraint-based solution to the supertree construction problem. We demonstrate that the versatility of constraint programming can be exploited to allow solutions to variants of the supertree construction problem.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {901–938},
numpages = {38}
}

@article{10.5555/1622673.1622695,
author = {Wang, Yi and Zhang, Nevin L. and Chen, Tao},
title = {Latent Tree Models and Approximate Inference in Bayesian Networks},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {879–900},
numpages = {22}
}

@article{10.5555/1622673.1622694,
author = {Coghill, George M. and Srinivasan, Ashwin and King, Ross D.},
title = {Qualitative System Identification from Imperfect Data},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Experience in the physical sciences suggests that the only realistic means of understanding complex systems is through the use of mathematical models. Typically, this has come to mean the identification of quantitative models expressed as differential equations. Quantitative modelling works best when the structure of the model (i.e., the form of the equations) is known; and the primary concern is one of estimating the values of the parameters in the model. For complex biological systems, the model-structure is rarely known and the modeler has to deal with both model-identification and parameter-estimation. In this paper we are concerned with providing automated assistance to the first of these problems. Specifically, we examine the identification by machine of the structural relationships between experimentally observed variables. These relationship will be expressed in the form of qualitative abstractions of a quantitative model. Such qualitative models may not only provide clues to the precise quantitative model, but also assist in understanding the essence of that model. Our position in this paper is that background knowledge incorporating system modelling principles can be used to constrain effectively the set of good qualitative models. Utilising the model-identification framework provided by Inductive Logic Programming (ILP) we present empirical support for this position using a series of increasingly complex artificial datasets. The results are obtained with qualitative and quantitative data subject to varying amounts of noise and different degrees of sparsity. The results also point to the presence of a set of qualitative states, which we term kernel subsets, that may be necessary for a qualitative model-learner to learn correct models. We demonstrate scalability of the method to biological system modelling by identification of the glycolysis metabolic pathway from data.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {825–877},
numpages = {53}
}

@article{10.5555/1622673.1622693,
author = {Miclet, Laurent and Bayoudh, Sabri and Delhay, Arnaud},
title = {Analogical Dissimilarity},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {This paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {793–824},
numpages = {32}
}

@article{10.5555/1622673.1622692,
author = {Delgrande, James and Jin, Yi and Pelletier, Francis Jeffry},
title = {Compositional Belief Update},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In this paper we explore a class of belief update operators, in which the definition of the operator is compositional with respect to the sentence to be added. The goal is to provide an update operator that is intuitive, in that its definition is based on a recursive decomposition of the update sentence's structure, and that may be reasonably implemented. In addressing update, we first provide a definition phrased in terms of the models of a knowledge base. While this operator satisfies a core group of the benchmark Katsuno-Mendelzon update postulates, not all of the postulates are satisfied. Other Katsuno-Mendelzon postulates can be obtained by suitably restricting the syntactic form of the sentence for update, as we show. In restricting the syntactic form of the sentence for update, we also obtain a hierarchy of update operators withWinslett's standard semantics as the most basic interesting approach captured. We subsequently give an algorithm which captures this approach; in the general case the algorithm is exponential, but with some not-unreasonable assumptions we obtain an algorithm that is linear in the size of the knowledge base. Hence the resulting approach has much better complexity characteristics than other operators in some situations. We also explore other compositional belief change operators: erasure is developed as a dual operator to update; we show that a forget operator is definable in terms of update; and we give a definition of the compositional revision operator. We obtain that compositional revision, under the most natural definition, yields the Satoh revision operator.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {757–791},
numpages = {35}
}

@article{10.5555/1622673.1622691,
author = {Petcu, Adrian and Faltings, Boi and Parkes, David C.},
title = {M-DPOP: Faithful Distributed Implementation of Efficient Social Choice Problems},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In the efficient social choice problem, the goal is to assign values, subject to side constraints, to a set of variables to maximize the total utility across a population of agents, where each agent has private information about its utility function. In this paper we model the social choice problem as a distributed constraint optimization problem (DCOP), in which each agent can communicate with other agents that share an interest in one or more variables. Whereas existing DCOP algorithms can be easily manipulated by an agent, either by misreporting private information or deviating from the algorithm, we introduce M-DPOP, the first DCOP algorithm that provides a faithful distributed implementation for efficient social choice. This provides a concrete example of how the methods of mechanism design can be unified with those of distributed optimization. Faithfulness ensures that no agent can benefit by unilaterally deviating from any aspect of the protocol, neither information-revelation, computation, nor communication, and whatever the private information of other agents. We allow for payments by agents to a central bank, which is the only central authority that we require. To achieve faithfulness, we carefully integrate the Vickrey-Clarke-Groves (VCG) mechanism with the DPOP algorithm, such that each agent is only asked to perform computation, report information, and send messages that is in its own best interest. Determining agent i's payment requires solving the social choice problem without agent i. Here, we present a method to reuse computation performed in solving the main problem in a way that is robust against manipulation by the excluded agent. Experimental results on structured problems show that as much as 87% of the computation required for solving the marginal problems can be avoided by re-use, providing very good scalability in the number of agents. On unstructured problems, we observe a sensitivity of M-DPOP to the density of the problem, and we show that reusability decreases from almost 100% for very sparse problems to around 20% for highly connected problems. We close with a discussion of the features of DCOP that enable faithful implementations in this problem, the challenge of reusing computation from the main problem to marginal problems in other algorithms such as ADOPT and OptAPO, and the prospect of methods to avoid the welfare loss that can occur because of the transfer of payments to the bank.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {705–755},
numpages = {51}
}

@article{10.5555/1622673.1622690,
author = {Ross, St\'{e}phane and Pineau, Joelle and Paquet, S\'{e}bastien and Chaib-draa, Brahim},
title = {Online Planning Algorithms for POMDPs},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Partially Observable Markov Decision Processes (POMDPs) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. However, solving a POMDP is often intractable except for small problems due to their complexity. Here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. Our objectives here are to survey the various existing online POMDP methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results indicate that state-of-the-art online heuristic search methods can handle large POMDP domains efficiently.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {663–704},
numpages = {42}
}

@article{10.5555/1622673.1622689,
author = {Yang, Fan and Culberson, Joseph and Holte, Robert and Zahavi, Uzi and Felner, Ariel},
title = {A General Theory of Additive State Space Abstractions},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Informally, a set of abstractions of a state space S is additive if the distance between any two states in S is always greater than or equal to the sum of the corresponding distances in the abstract spaces. The first known additive abstractions, called disjoint pattern databases, were experimentally demonstrated to produce state of the art performance on certain state spaces. However, previous applications were restricted to state spaces with special properties, which precludes disjoint pattern databases from being defined for several commonly used testbeds, such as Rubik's Cube, TopSpin and the Pancake puzzle. In this paper we give a general definition of additive abstractions that can be applied to any state space and prove that heuristics based on additive abstractions are consistent as well as admissible. We use this new definition to create additive abstractions for these testbeds and show experimentally that well chosen additive abstractions can reduce search time substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude over state of the art methods for the 17-Pancake puzzle. We also derive a way of testing if the heuristic value returned by additive abstractions is provably too low and show that the use of this test can reduce search time for the 15-puzzle and TopSpin by roughly a factor of two.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {631–662},
numpages = {32}
}

@article{10.5555/1622673.1622688,
author = {Bordeaux, Lucas and Cadoli, Marco and Mancini, Toni},
title = {A Unifying Framework for Structural Properties of CSPs: Definitions, Complexity, Tractabilit},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Literature on Constraint Satisfaction exhibits the definition of several "structural" properties that can be possessed by CSPs, like (in)consistency, substitutability or interchangeability. Current tools for constraint solving typically detect such properties efficiently by means of incomplete yet effective algorithms, and use them to reduce the search space and boost search.In this paper, we provide a unifying framework encompassing most of the properties known so far, both in CSP and other fields' literature, and shed light on the semantical relationships among them. This gives a unified and comprehensive view of the topic, allows new, unknown, properties to emerge, and clarifies the computational complexity of the various detection problems.In particular, among the others, two new concepts, fixability and removability emerge, that come out to be the ideal characterisations of values that may be safely assigned or removed from a variable's domain, while preserving problem satisfiability. These two notions subsume a large number of known properties, including inconsistency, substitutability and others.Because of the computational intractability of all the property-detection problems, by following the CSP approach we then determine a number of relaxations which provide sufficient conditions for their tractability. In particular, we exploit forms of language restrictions and local reasoning.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {607–629},
numpages = {23}
}

@article{10.5555/1622673.1622687,
author = {Xu, Lin and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
title = {SATzilla: Portfolio-Based Algorithm Selection for SAT},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {It has been widely observed that there is no single "dominant" SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe SATzilla, an automated approach for constructing per-instance algorithm portfolios for SAT that use so-called empirical hardness models to choose among their constituent solvers. This approach takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of instances solved, or score in a competition). The excellent performance of SATzilla was independently verified in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one silver and one bronze medal. In this article, we go well beyond SATzilla07 by making the portfolio construction scalable and completely automated, and improving it by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances. We demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent SAT competition.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {565–606},
numpages = {42}
}

@article{10.5555/1622673.1622686,
author = {Bouveret, Sylvain and Lang, J\'{e}r\^{o}me},
title = {Efficiency and Envy-Freeness in Fair Division of Indivisible Goods: Logical Representation and Complexity},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of allocating fairly a set of indivisible goods among agents from the point of view of compact representation and computational complexity. We start by assuming that agents have dichotomous preferences expressed by propositional formulae. We express efficiency and envy-freeness in a logical setting, which reveals unexpected connections to nonmonotonic reasoning. Then we identify the complexity of determining whether there exists an efficient and envy-free allocation, for several notions of efficiency, when preferences are represented in a succinct way (as well as restrictions of this problem). We first study the problem under the assumption that preferences are dichotomous, and then in the general case.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {525–564},
numpages = {40}
}

@article{10.5555/1622673.1622685,
author = {Stulp, Freek and Beetz, Michael},
title = {Refining the Execution of Abstract Actions with Learned Action Models},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Robots reason about abstract actions, such as go to position 'l', in order to decide what to do or to generate plans for their intended course of action. The use of abstract actions enables robots to employ small action libraries, which reduces the search space for decision making. When executing the actions, however, the robot must tailor the abstract actions to the specific task and situation context at hand.In this article we propose a novel robot action execution system that learns success and performance models for possible specializations of abstract actions. At execution time, the robot uses these models to optimize the execution of abstract actions to the respective task contexts. The robot can so use abstract actions for efficient reasoning, without compromising the performance of action execution. We show the impact of our action execution model in three robotic domains and on two kinds of action execution problems: (1) the instantiation of free action parameters to optimize the expected performance of action sequences; (2) the automatic introduction of additional subgoals to make action sequences more reliable.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {487–523},
numpages = {37}
}

@article{10.5555/1622673.1622684,
author = {Cs\'{a}ji, Bal\'{a}zs Csan\'{a}d and Monostori, L\'{a}szl\'{o}},
title = {Adaptive Stochastic Resource Control: A Machine Learning Approach},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, ν-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {453–486},
numpages = {34}
}

@article{10.5555/1622673.1622683,
author = {Bulitko, Vadim and Lu\v{s}trek, Mitja and Schaeffer, Jonathan and Bj\"{o}rnsson, Yngvi and Sigmundarson, Sverrir},
title = {Dynamic Control in Real-Time Heuristic Search},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Real-time heuristic search is a challenging type of agent-centered search because the agent's planning time per action is bounded by a constant independent of problem size. A common problem that imposes such restrictions is pathfinding in modern computer games where a large number of units must plan their paths simultaneously over large maps. Common search algorithms (e.g., A*, IDA*, D*, ARA*, AD*) are inherently not real-time and may lose completeness when a constant bound is imposed on per-action planning time. Real-time search algorithms retain completeness but frequently produce unacceptably suboptimal solutions. In this paper, we extend classic and modern real-time search algorithms with an automated mechanism for dynamic depth and subgoal selection. The new algorithms remain real-time and complete. On large computer game maps, they find paths within 7% of optimal while on average expanding roughly a single state per action. This is nearly a three-fold improvement in suboptimality over the existing state-of-the-art algorithms and, at the same time, a 15-fold improvement in the amount of planning per action.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {419–452},
numpages = {34}
}

@article{10.5555/1622673.1622682,
author = {Dubois, Didier and Fargier, H\'{e}l\`{e}ne and Bonnefon, Jean-Fran\c{c}ois},
title = {On the Qualitative Comparison of Decisions Having Positive and Negative Features},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Making a decision is often a matter of listing and comparing positive and negative arguments. In such cases, the evaluation scale for decisions should be considered bipolar, that is, negative and positive values should be explicitly distinguished. That is what is done, for example, in Cumulative Prospect Theory. However, contrary to the latter framework that presupposes genuine numerical assessments, human agents often decide on the basis of an ordinal ranking of the pros and the cons, and by focusing on the most salient arguments. In other terms, the decision process is qualitative as well as bipolar. In this article, based on a bipolar extension of possibility theory, we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting. The simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case, and consequently suffer from poor decisive power. More decisive rules that refine the former are also proposed. These refinements agree both with principles of efficiency and with the spirit of order-of-magnitude reasoning, that prevails in qualitative decision theory. The most refined decision rule uses leximin rankings of the pros and the cons, and the ideas of counting arguments of equal strength and cancelling pros by cons. It is shown to come down to a special case of Cumulative Prospect Theory, and to subsume the "Take the Best" heuristic studied by cognitive psychologists.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {385–417},
numpages = {33}
}

@article{10.5555/1622673.1622681,
author = {Liu, Fei Tony and Ting, Kai Ming and Yu, Yang and Zhou, Zhi-Hua},
title = {Spectrum of Variable-Random Trees},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we show that a continuous spectrum of randomisation exists, in which most existing tree randomisations are only operating around the two ends of the spectrum. That leaves a huge part of the spectrum largely unexplored. We propose a base learner VR-Tree which generates trees with variable-randomness. VR-Trees are able to span from the conventional deterministic trees to the complete-random trees using a probabilistic parameter. Using VR-Trees as the base models, we explore the entire spectrum of randomised ensembles, together with Bagging and Random Subspace. We discover that the two halves of the spectrum have their distinct characteristics; and the understanding of which allows us to propose a new approach in building better decision tree ensembles. We name this approach Coalescence, which coalesces a number of points in the random-half of the spectrum. Coalescence acts as a committee of "experts" to cater for unforeseeable conditions presented in training data. Coalescence is found to perform better than any single operating point in the spectrum, without the need to tune to a specific level of randomness. In our empirical study, Coalescence ranks top among the benchmarking ensemble methods including Random Forests, Random Subspace and C5 Boosting; and only Coalescence is significantly better than Bagging and Max-Diverse Ensemble among all the methods in the comparison. Although Coalescence is not significantly better than Random Forests, we have identified conditions under which one will perform better than the other.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {355–384},
numpages = {30}
}

@article{10.5555/1622673.1622680,
author = {Oliehoek, Frans A. and Spaan, Matthijs T. J. and Vlassis, Nikos},
title = {Optimal and Approximate Q-Value Functions for Decentralized POMDPs},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value functions: an optimal Q-value function Q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from Q*. In this paper we study whether similar Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs), and how policies can be extracted from such value functions. We define two forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. This computation, however, is infeasible for all but the smallest problems. Therefore, we analyze various approximate Q-value functions that allow for efficient computation. We describe how they relate, and we prove that they all provide an upper bound to the optimal Q-value function Q*. Finally, unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {289–353},
numpages = {65}
}

@article{10.5555/1622673.1622679,
author = {Katz, Michael and Domshlak, Carmel},
title = {New Islands of Tractability of Cost-Optimal Planning},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {We study the complexity of cost-optimal classical planning over propositional state variables and unary-effect actions. We discover novel problem fragments for which such optimization is tractable, and identify certain conditions that differentiate between tractable and intractable problems. These results are based on exploiting both structural and syntactic characteristics of planning problems. Specifically, following Brafman and Domshlak (2003), we relate the complexity of planning and the topology of the causal graph. The main results correspond to tractability of cost-optimal planning for propositional problems with polytree causal graphs that either have O(1)-bounded in-degree, or are induced by actions having at most one prevail condition each. Almost all our tractability results are based on a constructive proof technique that connects between certain tools from planning and tractable constraint optimization, and we believe this technique is of interest on its own due to a clear evidence for its robustness.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {203–288},
numpages = {86}
}

@article{10.5555/1622673.1622678,
author = {Goldman, Claudia V. and Zilberstein, Shlomo},
title = {Communication-Based Decomposition Mechanisms for Decentralized MDPs},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Multi-agent planning in stochastic environments can be framed formally as a decentralized Markov decision problem. Many real-life distributed problems that arise in manufacturing, multi-robot coordination and information gathering scenarios can be formalized using this framework. However, finding the optimal solution in the general case is hard, limiting the applicability of recently developed algorithms. This paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible, but costly. We develop the notion of communication-based mechanism that allows us to decompose a decentralized MDP into multiple single-agent problems. In this framework, referred to as decentralized semi-Markov decision process with direct communication (Dec-SMDP-Com), agents operate separately between communications. We show that finding an optimal mechanism is equivalent to solving optimally a Dec-SMDP-Com. We also provide a heuristic search algorithm that converges on the optimal decomposition. Restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning. In particular, we present a polynomial-time algorithm for the case in which individual agents perform goal-oriented behaviors between communications. The paper concludes with an additional tractable algorithm that enables the introduction of human knowledge, thereby reducing the overall problem to finding the best time to communicate. Empirical results show that these approaches provide good approximate solutions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {169–202},
numpages = {34}
}

@article{10.5555/1622673.1622677,
author = {Terekhov, Daria and Beck, J. Christopher},
title = {A Constraint Programming Approach for Solving a Queueing Control Problem},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In a facility with front room and back room operations, it is useful to switch workers between the rooms in order to cope with changing customer demand. Assuming stochastic customer arrival and service times, we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work. Three novel constraint programming models and several shaving procedures for these models are presented. Experimental results show that a model based on closed-form expressions together with a combination of shaving procedures is the most efficient. This model is able to find and prove optimal solutions for many problem instances within a reasonable run-time. Previously, the only available approach was a heuristic algorithm. Furthermore, a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time, while achieving the same performance in terms of proving optimality as the pure constraint programming model. This is the first work of which we are aware that solves such queueing-based problems with constraint programming.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {123–167},
numpages = {45}
}

@article{10.5555/1622673.1622676,
author = {van den Broek, Bart and Wiegerinck, Wim and Kappen, Bert},
title = {Graphical Model Inference in Optimal Control of Stochastic Multi-Agent Systems},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In this article we consider the issue of optimal control in collaborative multi-agent systems with stochastic dynamics. The agents have a joint task in which they have to reach a number of target states. The dynamics of the agents contains additive control and additive noise, and the autonomous part factorizes over the agents. Full observation of the global state is assumed. The goal is to minimize the accumulated joint cost, which consists of integrated instantaneous costs and a joint end cost. The joint end cost expresses the joint task of the agents. The instantaneous costs are quadratic in the control and factorize over the agents. The optimal control is given as a weighted linear combination of single-agent to single-target controls. The single-agent to single-target controls are expressed in terms of diffusion processes. These controls, when not closed form expressions, are formulated in terms of path integrals, which are calculated approximately by Metropolis-Hastings sampling. The weights in the control are interpreted as marginals of a joint distribution over agent to target assignments. The structure of the latter is represented by a graphical model, and the marginals are obtained by graphical model inference. Exact inference of the graphical model will break down in large systems, and so approximate inference methods are needed. We use naive mean field approximation and belief propagation to approximate the optimal control in systems with linear dynamics. We compare the approximate inference methods with the exact solution, and we show that they can accurately compute the optimal control. Finally, we demonstrate the control method in multi-agent systems with nonlinear dynamics consisting of up to 80 agents that have to reach an equal number of target states.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {95–122},
numpages = {28}
}

@article{10.5555/1622673.1622675,
author = {Analyti, Anastasia and Antoniou, Grigoris and Dam\'{a}sio, Carlos Viegas and Wagner, Gerd},
title = {Extended RDF as a Semantic Foundation of Rule Markup Languages},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {Ontologies and automated reasoning are the building blocks of the Semantic Web initiative. Derivation rules can be included in an ontology to define derived concepts, based on base concepts. For example, rules allow to define the extension of a class or property, based on a complex relation between the extensions of the same or other classes and properties. On the other hand, the inclusion of negative information both in the form of negation-as-failure and explicit negative information is also needed to enable various forms of reasoning. In this paper, we extend RDF graphs with weak and strong negation, as well as derivation rules. The ERDF stable model semantics of the extended framework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive feature of our theory, which is based on Partial Logic, is that both truth and falsity extensions of properties and classes are considered, allowing for truth value gaps. Our framework supports both closed-world and open-world reasoning through the explicit representation of the particular closed-world assumptions and the ERDF ontological categories of total properties and total classes.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {37–94},
numpages = {58}
}

@article{10.5555/1622673.1622674,
author = {Manisterski, Efrat and Sarne, David and Kraus, Sarit},
title = {Enhancing Cooperative Search with Concurrent Interactions},
year = {2008},
issue_date = {May 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {32},
number = {1},
issn = {1076-9757},
abstract = {In this paper we show how taking advantage of autonomous agents' capability to maintain parallel interactions with others, and incorporating it into the cooperative economic search model results in a new search strategy which outperforms current strategies in use. As a framework for our analysis we use the electronic marketplace, where buyer agents have the incentive to search cooperatively. The new search technique is quite intuitive, however its analysis and the process of extracting the optimal search strategy are associated with several significant complexities. These difficulties are derived mainly from the unbounded search space and simultaneous dual affects of decisions taken along the search. We provide a comprehensive analysis of the model, highlighting, demonstrating and proving important characteristics of the optimal search strategy. Consequently, we manage to come up with an efficient modular algorithm for extracting the optimal cooperative search strategy for any given environment. A computational based comparative illustration of the system performance using the new search technique versus the traditional methods is given, emphasizing the main differences in the optimal strategy's structure and the advantage of using the proposed model.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–36},
numpages = {36}
}

@article{10.5555/1622655.1622672,
author = {Dresner, Kurt and Stone, Peter},
title = {A Multiagent Approach to Autonomous Intersection Management},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Artificial intelligence research is ushering in a new era of sophisticated, mass-market transportation technology. While computers can already fly a passenger jet better than a trained human pilot, people are still faced with the dangerous yet tedious task of driving automobiles. Intelligent Transportation Systems (ITS) is the field that focuses on integrating information technology with vehicles and transportation infrastructure to make transportation safer, cheaper, and more efficient. Recent advances in ITS point to a future in which vehicles themselves handle the vast majority of the driving task. Once autonomous vehicles become popular, autonomous interactions amongst multiple vehicles will be possible. Current methods of vehicle coordination, which are all designed to work with human drivers, will be outdated. The bottleneck for roadway efficiency will no longer be the drivers, but rather the mechanism by which those drivers' actions are coordinated. While open-road driving is a well-studied and more-or-less-solved problem, urban traffic scenarios, especially intersections, are much more challenging.We believe current methods for controlling traffic, specifically at intersections, will not be able to take advantage of the increased sensitivity and precision of autonomous vehicles as compared to human drivers. In this article, we suggest an alternative mechanism for coordinating the movement of autonomous vehicles through intersections. Drivers and intersections in this mechanism are treated as autonomous agents in a multiagent system. In this multiagent system, intersections use a new reservation-based approach built around a detailed communication protocol, which we also present. We demonstrate in simulation that our new mechanism has the potential to significantly outperform current intersection control technology--traffic lights and stop signs. Because our mechanism can emulate a traffic light or stop sign, it subsumes the most popular current methods of intersection control. This article also presents two extensions to the mechanism. The first extension allows the system to control human-driven vehicles in addition to autonomous vehicles. The second gives priority to emergency vehicles without significant cost to civilian vehicles. The mechanism, including both extensions, is implemented and tested in simulation, and we present experimental results that strongly attest to the efficacy of this approach.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {591–656},
numpages = {66}
}

@article{10.5555/1622655.1622671,
author = {Michelson, Matthew and Knoblock, Craig A.},
title = {Creating Relational Data from Unstructured and Ungrammatical Data Sources},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {In order for agents to act on behalf of users, they will have to retrieve and integrate vast amounts of textual data on the World Wide Web. However, much of the useful data on the Web is neither grammatical nor formally structured, making querying dificult. Examples of these types of data sources are online classifieds like Craigslist1 and auction item listings like eBay.2 We call this unstructured, ungrammatical data "posts." The unstructured nature of posts makes query and integration dificult because the attributes are embedded within the text. Also, these attributes do not conform to standardized values, which prevents queries based on a common attribute value. The schema is unknown and the values may vary dramatically making accurate search dificult. Creating relational data for easy querying requires that we define a schema for the embedded attributes and extract values from the posts while standardizing these values. Traditional information extraction (IE) is inadequate to perform this task because it relies on clues from the data, such as structure or natural language, neither of which are found in posts. Furthermore, traditional information extraction does not incorporate data cleaning, which is necessary to accurately query and integrate the source. The two-step approach described in this paper creates relational data sets from unstructured and ungrammatical text by addressing both issues. To do this, we require a set of known entities called a "reference set." The first step aligns each post to each member of each reference set. This allows our algorithm to define a schema over the post and include standard values for the attributes defined by this schema. The second step performs information extraction for the attributes, including attributes not easily represented by reference sets, such as a price. In this manner we create a relational structure over previously unstructured data, supporting deep and accurate queries over the data as well as standard values for integration. Our experimental results show that our technique matches the posts to the reference set accurately and eficiently and outperforms state-of-the-art extraction systems on the extraction task from posts.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {543–590},
numpages = {48}
}

@article{10.5555/1622655.1622670,
author = {Ryan, Malcolm R. K.},
title = {Exploiting Subgraph Structure in Multi-Robot Path Planning},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Multi-robot path planning is dificult due to the combinatorial explosion of the search space with every new robot added. Complete search of the combined state-space soon becomes intractable. In this paper we present a novel form of abstraction that allows us to plan much more eficiently. The key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly. Planning then becomes a search in the much smaller space of subgraph configurations. Once an abstract plan is found, it can be quickly resolved into a correct (but possibly sub-optimal) concrete plan without the need for further search. We prove that this technique is sound and complete and demonstrate its practical effiectiveness on a real map.A contending solution, prioritised planning, is also evaluated and shown to have similar performance albeit at the cost of completeness. The two approaches are not necessarily conflicting; we demonstrate how they can be combined into a single algorithm which out-performs either approach alone.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {497–542},
numpages = {46}
}

@article{10.5555/1622655.1622669,
author = {Altman, Alon and Tennenholtz, Moshe},
title = {Axiomatic Foundations for Ranking Systems},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. In this paper we present an extensive axiomatic study of ranking systems. In particular, we consider two fundamental axioms: Transitivity, and Ranked Independence of Irrelevant Alternatives. Surprisingly, we find that there is no general social ranking rule that satisfies both requirements. Furthermore, we show that our impossibility result holds under various restrictions on the class of ranking problems considered. However, when transitivity is weakened, an interesting possibility result is obtained. In addition, we show a complete axiomatization of approval voting using ranked IIA.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {473–495},
numpages = {23}
}

@article{10.5555/1622655.1622668,
author = {Wang, Chenggang and Joshi, Saket and Khardon, Roni},
title = {First Order Decision Diagrams for Relational MDPs},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Markov decision processes capture sequential decision making under uncertainty, where an agent must choose actions so as to optimize long term reward. The paper studies efficient reasoning mechanisms for Relational Markov Decision Processes (RMDP) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them. Two contributions are presented. First, the paper develops First Order Decision Diagrams (FODD), a new compact representation for functions over relational structures, together with a set of operators to combine FODDs, and novel reduction techniques to keep the representation small. Second, the paper shows how FODDs can be used to develop solutions for RMDPs, where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size (number of objects) or instantiation. In particular, a variant of the value iteration algorithm is developed by using special operations over FODDs, and the algorithm is shown to converge to the optimal policy.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {431–472},
numpages = {42}
}

@article{10.5555/1622655.1622667,
author = {Clarke, James and Lapata, Mirella},
title = {Global Inference for Sentence Compression an Integer Linear Programming Approach},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {399–429},
numpages = {31}
}

@article{10.5555/1622655.1622666,
author = {Eisenstein, Jacob and Barzilay, Regina and Davis, Randall},
title = {Gesture Salience as a Hidden Variable for Coreference Resolution and Keyframe Extraction},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Gesture is a non-verbal modality that can contribute crucial information to the understanding of natural language. But not all gestures are informative, and noncommunicative hand motions may confuse natural language processing (NLP) and impede learning. People have little difficulty ignoring irrelevant hand movements and focusing on meaningful gestures, suggesting that an automatic system could also be trained to perform this task. However, the informativeness of a gesture is context-dependent and labeling enough data to cover all cases would be expensive. We present conditional modality fusion, a conditional hidden-variable model that learns to predict which gestures are salient for coreference resolution, the task of determining whether two noun phrases refer to the same semantic entity. Moreover, our approach uses only coreference annotations, and not annotations of gesture salience itself. We show that gesture features improve performance on coreference resolution, and that by attending only to gestures that are salient, our method achieves further significant gains. In addition, we show that the model of gesture salience learned in the context of coreference accords with human intuition, by demonstrating that gestures judged to be salient by our model can be used successfully to create multimedia keyframe summaries of video. These summaries are similar to those created by human raters, and significantly outperform summaries produced by baselines from the literature.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {353–398},
numpages = {46}
}

@article{10.5555/1622655.1622665,
author = {Gim\'{e}nez, Omer and Jonsson, Anders},
title = {The Complexity of Planning Problems with Simple Causal Graphs},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {We present three new complexity results for classes of planning problems with simple causal graphs. First, we describe a polynomial-time algorithm that uses macros to generate plans for the class 3S of planning problems with binary state variables and acyclic causal graphs. This implies that plan generation may be tractable even when a planning problem has an exponentially long minimal solution. We also prove that the problem of plan existence for planning problems with multi-valued variables and chain causal graphs is NP-hard. Finally, we show that plan existence for planning problems with binary state variables and polytree causal graphs is NP-complete.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {319–351},
numpages = {33}
}

@article{10.5555/1622655.1622664,
author = {Grau, Bernardo Cuenca and Horrocks, Ian and Kazakov, Yevgeny and Sattler, Ulrike},
title = {Modular Reuse of Ontologies: Theory and Practice},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we propose a set of tasks that are relevant for the modular reuse of ontologies. In order to formalize these tasks as reasoning problems, we introduce the notions of conservative extension, safety and module for a very general class of logic-based ontology languages. We investigate the general properties of and relationships between these notions and study the relationships between the relevant reasoning problems we have previously identified. To study the computability of these problems, we consider, in particular, Description Logics (DLs), which provide the formal underpinning of the W3C Web Ontology Language (OWL), and show that all the problems we consider are undecidable or algorithmically unsolvable for the description logic underlying OWL DL. In order to achieve a practical solution, we identify conditions sufficient for an ontology to reuse a set of symbols "safely"--that is, without changing their meaning. We provide the notion of a safety class, which characterizes any sufficient condition for safety, and identify a family of safety classes-called locality--which enjoys a collection of desirable properties. We use the notion of a safety class to extract modules from ontologies, and we provide various modularization algorithms that are appropriate to the properties of the particular safety class in use. Finally, we show practical benefits of our safety checking and module extraction algorithms.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {273–318},
numpages = {46}
}

@article{10.5555/1622655.1622663,
author = {Liu, Yongmei and Lakemeyer, Gerhard},
title = {On the Expressiveness of Levesque's Normal Form},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Levesque proposed a generalization of a database called a proper knowledge base (KB), which is equivalent to a possibly infinite consistent set of ground literals. In contrast to databases, proper KBs do not make the closed-world assumption and hence the entailment problem becomes undecidable. Levesque then proposed a limited but efficient inference method V for proper KBs, which is sound and, when the query is in a certain normal form, also logically complete. He conjectured that for every first-order query there is an equivalent one in normal form. In this note, we show that this conjecture is false. In fact, we show that any class of formulas for which V is complete must be strictly less expressive than full first-order logic. Moreover, in the propositional case it is very unlikely that a formula always has a polynomial-size normal form.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {259–272},
numpages = {14}
}

@article{10.5555/1622655.1622662,
author = {van den Briel, Menkes H. L. and Vossen, Thomas and Kambhampati, Subbarao},
title = {Loosely Coupled Formulations for Automated Planning: An Integer Programming Perspective},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {We represent planning as a set of loosely coupled network flow problems, where each network corresponds to one of the state variables in the planning domain. The network nodes correspond to the state variable values and the network arcs correspond to the value transitions. The planning problem is to find a path (a sequence of actions) in each network such that, when merged, they constitute a feasible plan. In this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility. Since merging may introduce exponentially many ordering constraints we implement a so-called branch-and-cut algorithm, in which these constraints are dynamically generated and added to the formulation when needed. Our results are very promising, they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {217–257},
numpages = {41}
}

@article{10.5555/1622655.1622661,
author = {Wong, Ka-Shu},
title = {Sound and Complete Inference Rules for SE-Consequence},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {The notion of strong equivalence on logic programs with answer set semantics gives rise to a consequence relation on logic program rules, called SE-consequence. We present a sound and complete set of inference rules for SE-consequence on disjunctive logic programs.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {205–216},
numpages = {12}
}

@article{10.5555/1622655.1622660,
author = {Glimm, Birte and Horrocks, Ian and Lutz, Carsten and Sattler, Ulrike},
title = {Conjunctive Query Answering for the Description Logic SHIQ},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, conjunctive query answering over DL knowledge bases is only poorly understood if transitive roles are admitted in the query. In this paper, we consider unions of conjunctive queries over knowledge bases formulated in the prominent DL SHIQ and allow transitive roles in both the query and the knowledge base. We show decidability of query answering in this setting and establish two tight complexity bounds: regarding combined complexity, we prove that there is a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query, which is optimal. Regarding data complexity, we prove containment in co-NP.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {157–204},
numpages = {48}
}

@article{10.5555/1622655.1622659,
author = {Zhang, Yan and Ding, Yulin},
title = {CTL Model Update for System Modifications},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Model checking is a promising technology, which has been applied for verification of many hardware and software systems. In this paper, we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions. We define primitive update operations on the models of Computation Tree Logic (CTL) and formalize the principle of minimal change for CTL model update. These primitive update operations, together with the underlying minimal change principle, serve as the foundation for CTL model update. Essential semantic and computational characterizations are provided for our CTL model update approach. We then describe a formal algorithm that implements this approach. We also illustrate two case studies of CTL model updates for the well-known microwave oven example and the Andrew File System 1, from which we further propose a method to optimize the update results in complex system modifications.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {113–155},
numpages = {43}
}

@article{10.5555/1622655.1622658,
author = {Engel, Yagil and Wellman, Michael P.},
title = {CUI Networks: A Graphical Representation for Conditional Utility Independence},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {We introduce CUI networks, a compact graphical representation of utility functions over multiple attributes. CUI networks model multiattribute utility functions using the well-studied and widely applicable utility independence concept. We show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes can be used to calculate joint utility. We discuss aspects of elicitation, network construction, and optimization, and contrast our new representation with previous graphical preference modeling.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {83–112},
numpages = {30}
}

@article{10.5555/1622655.1622657,
author = {Mausam and Weld, Daniel S.},
title = {Planning with Durative Actions in Stochastic Domains},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {Probabilistic planning problems are typically modeled as a Markov Decision Process (MDP). MDPs, while an otherwise expressive model, allow only for sequential, non-durative actions. This poses severe restrictions in modeling and solving a real world planning problem. We extend the MDP model to incorporate-1) simultaneous action execution, 2) durative actions, and 3) stochastic durations. We develop several algorithms to combat the computational explosion introduced by these features. The key theoretical ideas used in building these algorithms are -- modeling a complex problem as an MDP in extended state/action space, pruning of irrelevant actions, sampling of relevant actions, using informed heuristics to guide the search, hybridizing different planners to achieve benefits of both, approximating the problem and replanning. Our empirical evaluation illuminates the different merits in using various algorithms, viz., optimality, empirical closeness to optimality, theoretical error bounds, and speed.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {33–82},
numpages = {50}
}

@article{10.5555/1622655.1622656,
author = {Heras, Federico and Larrosa, Javier and Oliveras, Albert},
title = {MINIMAXSAT: An Efficient Weighted Max-SAT Solver},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
abstract = {In this paper we introduce MINIMAXSAT, a new Max-SAT solver that is built on top of MIN-ISAT+.It incorporates the best current SAT and Max-SAT techniques. It can handle hard clauses (clauses of mandatory satisfaction as in SAT), soft clauses (clauses whose falsification is penalized by a cost as in Max-SAT) as well as pseudo-boolean objective functions and constraints. Its main features are: learning and backjumping on hard clauses; resolution-based and substraction-based lower bounding; and lazy propagation with the two-watched literal scheme. Our empirical evaluation comparing a wide set of solving alternatives on a broad set of optimization benchmarks indicates that the performance of MINIMAXSAT is usually close to the best specialized alternative and, in some cases, even better.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–32},
numpages = {32}
}

@article{10.5555/1622637.1622654,
author = {Szita, Istv\'{a}n and L\~{o}rincz, Andr\'{a}s},
title = {Learning to Play Using Low-Complexity Rule-Based Policies: Illustrations through Ms. Pac-Man},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {In this article we propose a method that can deal with certain combinatorial reinforcement learning tasks. We demonstrate the approach in the popular Ms. Pac-Man game. We define a set of high-level observation and action modules, from which rule-based policies are constructed automatically. In these policies, actions are temporally extended, and may work concurrently. The policy of the agent is encoded by a compact decision list. The components of the list are selected from a large pool of rules, which can be either handcrafted or generated automatically. A suitable selection of rules is learnt by the cross-entropy method, a recent global optimization algorithm that fits our framework smoothly. Cross-entropy-optimized policies perform better than our hand-crafted policy, and reach the score of average human players. We argue that learning is successful mainly because (i) policies may apply concurrent actions and thus the policy space is sufficiently rich, (ii) the search is biased towards low-complexity policies and therefore, solutions with a compact description can be found quickly if they exist.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {659–684},
numpages = {26}
}

@article{10.5555/1622637.1622653,
author = {Bhattacharya, Indrajit and Getoor, Lise},
title = {Query-Time Entity Resolution},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Entity resolution is the problem of reconciling database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of query-time entity resolution: quick and accurate resolution for answering queries over such 'unclean' databases at query-time. Since collective entity resolution approaches -- where related references are resolved jointly -- have been shown to be more accurate than independent attribute-based resolution for off-line entity resolution, we focus on developing new algorithms for collective resolution for answering entity resolution queries at query-time. For this purpose, we first formally show that, for collective resolution, precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered. Unfolding this progression leads naturally to a two stage 'expand and resolve' query processing strategy. In this strategy, we first extract the related records for a query using two novel expansion operators, and then resolve the extracted records collectively. We then show how the same strategy can be adapted for query-time entity resolution by identifying and resolving only those database references that are the most helpful for processing the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution. In addition to experiments on real datasets, we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {621–657},
numpages = {37}
}

@article{10.5555/1622637.1622652,
author = {Domshlak, Carmel and Hoffmann, J\"{o}rg},
title = {Probabilistic Planning via Heuristic Forward Search and Weighted Model Counting},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {We present a new algorithm for probabilistic planning with no observability. Our algorithm, called Probabilistic-FF, extends the heuristic forward-searchmachinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects. Specifically, Probabilistic-FF combines Conformant-FF's techniques with a powerful machinery for weighted model counting in (weighted) CNFs, serving to elegantly define both the search space and the heuristic function. Our evaluation of Probabilistic-FF shows its fine scalability in a range of probabilistic domains, constituting a several orders of magnitude improvement over previous results in this area. We use a problematic case to point out the main open issue to be addressed by further research.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {565–620},
numpages = {56}
}

@article{10.5555/1622637.1622651,
author = {Hill, Simon I. and Doucet, Arnaud},
title = {A Framework for Kernel-Based Multi-Category Classification},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {A geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood. The structure enables parsimonious optimisation, through a direct extension of the binary methodology. The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms. An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm. Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {525–564},
numpages = {40}
}

@article{10.5555/1622637.1622650,
author = {Greco, Sergio and Trubitsyna, Irina and Zumpano, Ester},
title = {On the Semantics of Logic Programs with Preferences},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {This work is a contribution to prioritized reasoning in logic programming in the presence of preference relations involving atoms. The technique, providing a new interpretation for prioritized logic programs, is inspired by the semantics of Prioritized Logic Programming and enriched with the use of structural information of preference of Answer Set Optimization Programming. Specifically, the analysis of the logic program is carried out together with the analysis of preferences in order to determine the choice order and the sets of comparable models. The new semantics is compared with other approaches known in the literature and complexity analysis is also performed, showing that, with respect to other similar approaches previously proposed, the complexity of computing preferred stable models does not increase.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {501–523},
numpages = {23}
}

@article{10.5555/1622637.1622649,
author = {Mairesse, Fran\c{c}ois and Walker, Marilyn A. and Mehl, Matthias R. and Moore, Roger K.},
title = {Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content. One such type of information consists of cues to the speaker's personality traits, the most fundamental dimension of variation between humans. Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality affects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications. However, to date, there is little work on the automatic recognition of personality traits. This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality. While other work reports classification results, we experiment with classification, regression and ranking models. For each model, we analyse the effect of different feature sets on accuracy. Results show that for some traits, any type of statistical model performs significantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classifiers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using self-reports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models confirms previous findings linking language and personality, while revealing many new linguistic markers.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {457–500},
numpages = {44}
}

@article{10.5555/1622637.1622648,
author = {Walker, Marilyn and Stent, Amanda and Mairesse, Fran\c{c}ois and Prasad, Rashmi},
title = {Individual and Domain Adaptation in Sentence Planning for Dialogue},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {One of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module. This challenge arises from the need for the generator to adapt to many features of the dialogue domain, user population, and dialogue context. A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system. We show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a template-based generator tuned to this domain. We also show that our method easily supports adapting the sentence planner to individuals, and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals. Previous work has documented and utilized individual preferences for content selection, but to our knowledge, these results provide the first demonstration of individual preferences for sentence planning operations, affecting the content order, discourse structure and sentence structure of system responses. Finally, we evaluate the contribution of different feature sets, and show that, in our application, n-gram features often do as well as features based on higher-level linguistic representations.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {413–456},
numpages = {44}
}

@article{10.5555/1622637.1622647,
author = {Bell, John},
title = {Natural Events},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {This paper develops an inductive theory of predictive common sense reasoning. The theory provides the basis for an integrated solution to the three traditional problems of reasoning about change; the frame, qualification, and ramification problems. The theory is also capable of representing non-deterministic events, and it provides a means for stating defeasible preferences over the outcomes of conflicting simultaneous events.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {361–412},
numpages = {52}
}

@article{10.5555/1622637.1622646,
author = {Li, Chu Min and Many\`{a}, Felip and Planes, Jordi},
title = {New Inference Rules for Max-SAT},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Exact Max-SAT solvers, compared with SAT solvers, apply little inference at each node of the proof tree. Commonly used SAT inference rules like unit propagation produce a simplified formula that preserves satisfiability but, unfortunately, solving the Max-SAT problem for the simplified formula is not equivalent to solving it for the original formula. In this paper, we define a number of original inference rules that, besides being applied efficiently, transform Max-SAT instances into equivalent Max-SAT instances which are easier to solve. The soundness of the rules, that can be seen as refinements of unit resolution adapted to Max-SAT, are proved in a novel and simple way via an integer programming transformation. With the aim of finding out how powerful the inference rules are in practice, we have developed a new Max-SAT solver, called MaxSatz, which incorporates those rules, and performed an experimental investigation. The results provide empirical evidence that MaxSatz is very competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph 3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation 2006.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {321–359},
numpages = {39}
}

@article{10.5555/1622637.1622645,
author = {Stoilos, Giorgos and Stamou, Giorgos and Pan, Jeff Z. and Tzouvaras, Vassilis and Horrocks, Ian},
title = {Reasoning with Very Expressive Fuzzy Description Logics},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {It is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledge-based applications. Description Logics (DLs) are a family of knowledge representation languages that have gained considerable attention the last decade, mainly due to their decidability and the existence of empirically high performance of reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms (S), inverse roles (I), role hierarchies (H) and number restrictions (N). We illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly. Then we extend these results by adding role hierarchies and finally number restrictions. The main contributions of the paper are the decidability proof of the fuzzy DL languages fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base satisfiability problem of the fuzzy-SI and fuzzy-SHIN.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {273–320},
numpages = {48}
}

@article{10.5555/1622637.1622644,
author = {McCallum, Andrew and Wang, Xuerui and Corrada-Emmanuel, Andr\'{e}s},
title = {Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the attributes such as language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the direction-sensitive messages sent between entities. The model builds on Latent Dirichlet Allocation (LDA) and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient--steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher's email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people's roles and gives lower perplexity on previously unseen messages. We also present the Role-Author-Recipient-Topic (RART) model, an extension to ART that explicitly represents people's roles.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {249–272},
numpages = {24}
}

@article{10.5555/1622637.1622643,
author = {Felner, Ariel and Korf, Richard E. and Meshulam, Ram and Holte, Robert},
title = {Compressed Pattern Databases},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {A pattern database (PDB) is a heuristic function implemented as a lookup table that stores the lengths of optimal solutions for subproblem instances. Standard PDBs have a distinct entry in the table for each subproblem instance. In this paper we investigate compressing PDBs by merging several entries into one, thereby allowing the use of PDBs that exceed available memory in their uncompressed form. We introduce a number of methods for determining which entries to merge and discuss their relative merits. These vary from domain-independent approaches that allow any set of entries in the PDB to be merged, to more intelligent methods that take into account the structure of the problem. The choice of the best compression method is based on domain-dependent attributes. We present experimental results on a number of combinatorial problems, including the four-peg Towers of Hanoi problem, the sliding-tile puzzles, and the Top-Spin puzzle. For the Towers of Hanoi, we show that the search time can be reduced by up to three orders of magnitude by using compressed PDBs compared to uncompressed PDBs of the same size. More modest improvements were observed for the other domains.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {213–247},
numpages = {35}
}

@article{10.5555/1622637.1622642,
author = {Ponzetto, Simone Paolo and Strube, Michael},
title = {Knowledge Derived from Wikipedia for Computing Semantic Relatedness},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than WordNet. We present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet on some datasets. We also address the question whether and how Wikipedia can be integrated into NLP applications as a knowledge base. Including Wikipedia improves the performance of a machine learning based coreference resolution system, indicating that it represents a valuable resource for NLP applications. Finally, we show that our method can be easily used for languages other than English by computing semantic relatedness for a German dataset.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {181–212},
numpages = {32}
}

@article{10.5555/1622637.1622641,
author = {Bredin, Jonathan and Parkes, David C. and Duong, Quang},
title = {Chain: A Dynamic Double Auction Framework for Matching Patient Agents},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {In this paper we present and evaluate a general framework for the design of truthful auctions for matching agents in a dynamic, two-sided market. A single commodity, such as a resource or a task, is bought and sold by multiple buyers and sellers that arrive and depart over time. Our algorithm, CHAIN, provides the first framework that allows a truthful dynamic double auction (DA) to be constructed from a truthful, single-period (i.e. static) double-auction rule. The pricing and matching method of the CHAIN construction is unique amongst dynamic-auction rules that adopt the same building block. We examine experimentally the allocative efficiency of CHAIN when instantiated on various single-period rules, including the canonical McAfee double-auction rule. For a baseline we also consider non-truthful double auctions populated with "zero-intelligence plus"-style learning agents. CHAIN-based auctions perform well in comparison with other schemes, especially as arrival intensity falls and agent valuations become more volatile.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {133–179},
numpages = {47}
}

@article{10.5555/1622637.1622640,
author = {Pistore, Marco and Vardi, Moshe Y.},
title = {The Planning Spectrum: One, Two, Three, Infinity},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Linear Temporal Logic (LTL) is widely used for defining conditions on the execution paths of dynamic systems. In the case of dynamic systems that allow for nondeterministic evolutions, one has to specify, along with an LTL formula ϕ, which are the paths that are required to satisfy the formula. Two extreme cases are the universal interpretation A.ϕ, which requires that the formula be satisfied for all execution paths, and the existential interpretation Ε.ϕ, which requires that the formula be satisfied for some execution path.When LTL is applied to the definition of goals in planning problems on nondeterministic domains, these two extreme cases are too restrictive. It is often impossible to develop plans that achieve the goal in all the nondeterministic evolutions of a system, and it is too weak to require that the goal is satisfied by some execution.In this paper we explore alternative interpretations of an LTL formula that are between these extreme cases. We define a new language that permits an arbitrary combination of the A and Ε quantifiers, thus allowing, for instance, to require that each finite execution can be extended to an execution satisfying an LTL formula (AΕ.ϕ), or that there is some finite execution whose extensions all satisfy an LTL formula (ΕA.ϕ). We show that only eight of these combinations of path quantifiers are relevant, corresponding to an alternation of the quantifiers of length one (A and Ε), two (AΕ and ΕA), three (AΕA and ΕAΕ), and infinity ((AΕ)ω and (ΕA)ω). We also present a planning algorithm for the new language that is based on an automata-theoretic approach, and study its complexity.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {101–132},
numpages = {32}
}

@article{10.5555/1622637.1622639,
author = {Bulitko, Vadim and Sturtevant, Nathan and Lu, Jieshan and Yau, Timothy},
title = {Graph Abstraction in Real-Time Heuristic Search},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {Real-time heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size. Such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time. We extend a wide class of real-time search algorithms with automatically-built state abstraction and prove completeness and convergence of the resulting family of algorithms. We then analyze the impact of abstraction in an extensive empirical study in real-time pathfinding. Abstraction is found to improve efficiency by providing better trading offs between planning time, learning speed and other negatively correlated performance measures.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {51–100},
numpages = {50},
keywords = {state abstraction, goal-directed navigation, learning real-time heuristic search}
}

@article{10.5555/1622637.1622638,
author = {Carman, Mark James and Knoblock, Craig A.},
title = {Learning Semantic Definitions of Online Information Sources},
year = {2007},
issue_date = {September 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {30},
number = {1},
issn = {1076-9757},
abstract = {The Internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information. These sources can be accessed via Web-forms, Web Services, RSS feeds and so on. In order to make automated use of these sources, we need to model them semantically, but writing semantic descriptions for Web Services is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions of Web sources. In order to learn these definitions, our system actively invokes the sources and compares the data they produce with that of known sources of information. It then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. In this paper we perform an empirical evaluation of the system using real-world Web sources. The evaluation demonstrates the effectiveness of the approach, showing that we can automatically learn complex models for real sources in reasonable time. We also compare our system with a complex schema matching system, showing that our approach can handle the kinds of problems tackled by the latter.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–50},
numpages = {50}
}

@article{10.5555/1622606.1622619,
author = {Pralet, C\'{e}dric and Verfaillie, G\'{e}rard and Schiex, Thomas},
title = {An Algebraic Graphical Model for Decision with Uncertainties, Feasibilities, and Utilities},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {Numerous formalisms and dedicated algorithms have been designed in the last decades to model and solve decision making problems. Some formalisms, such as constraint networks, can express "simple" decision problems, while others are designed to take into account uncertainties, unfeasible decisions, and utilities. Even in a single formalism, several variants are often proposed to model different types of uncertainty (probability, possibility...) or utility (additive or not). In this article, we introduce an algebraic graphical model that encompasses a large number of such formalisms: (1) we first adapt previous structures from Friedman, Chu and Halpern for representing uncertainty, utility, and expected utility in order to deal with generic forms of sequential decision making; (2) on these structures, we then introduce composite graphical models that express information via variables linked by "local" functions, thanks to conditional independence; (3) on these graphical models, we finally define a simple class of queries which can represent various scenarios in terms of observabilities and controllabilities. A natural decision-tree semantics for such queries is completed by an equivalent operational semantics, which induces generic algorithms. The proposed framework, called the Plausibility-Feasibility-Utility (PFU) framework, not only provides a better understanding of the links between existing formalisms, but it also covers yet unpublished frameworks (such as possibilistic influence diagrams) and unifies formalisms such as quantified boolean formulas and influence diagrams. Our backtrack and variable elimination generic algorithms are a first step towards unified algorithms.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {421–489},
numpages = {69}
}

@article{10.5555/1622606.1622618,
author = {Jurca, Radu and Faltings, Boi},
title = {Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {Reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard. Future clients guide their business decisions by considering the feedback from past transactions; if truthfully exposed, cheating behavior is sanctioned and thus becomes irrational.It therefore becomes important to ensure that rational clients have the right incentives to report honestly. As an alternative to side-payment schemes that explicitly reward truthful reports, we show that honesty can emerge as a rational behavior when clients have a repeated presence in the market. To this end we describe a mechanism that supports an equilibrium where truthful feedback is obtained. Then we characterize the set of pareto-optimal equilibria of the mechanism, and derive an upper bound on the percentage of false reports that can be recorded by the mechanism. An important role in the existence of this bound is played by the fact that rational clients can establish a reputation for reporting honestly.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {391–419},
numpages = {29}
}

@article{10.5555/1622606.1622617,
author = {Son, Tran Cao and Pontelli, Enrico and Tu, Phan Huy},
title = {Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs.The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations.The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms.We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms.We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {353–389},
numpages = {37}
}

@article{10.5555/1622606.1622616,
author = {Pasula, Hanna M. and Zettlemoyer, Luke S. and Kaelbling, Leslie Pack},
title = {Learning Symbolic Models of Stochastic Domains},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {In this article, we work towards the goal of developing agents that can learn to act in complex worlds. We develop a probabilistic, relational planning rule representation that compactly models noisy, nondeterministic action effects, and show how such rules can be effectively learned. Through experiments in simple planning domains and a 3D simulated blocks world with realistic physics, we demonstrate that this learning algorithm allows agents to effectively model world dynamics.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {309–352},
numpages = {44}
}

@article{10.5555/1622606.1622615,
author = {Di Noia, Tommaso and Di Sciascio, Eugenio and Donini, Francesco M.},
title = {Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {Matchmaking arises when supply and demand meet in an electronic marketplace, or when agents search for a web service to perform some task, or even when recruiting agencies match curricula and job profiles. In such open environments, the objective of a matchmaking process is to discover best available offers to a given request.We address the problem of matchmaking from a knowledge representation perspective, with a formalization based on Description Logics. We devise Concept Abduction and Concept Contraction as non-monotonic inferences in Description Logics suitable for modeling matchmaking in a logical framework, and prove some related complexity results. We also present reasonable algorithms for semantic matchmaking based on the devised inferences, and prove that they obey to some commonsense properties.Finally, we report on the implementation of the proposed matchmaking framework, which has been used both as a mediator in e-marketplaces and for semantic web services discovery.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {269–307},
numpages = {39}
}

@article{10.5555/1622606.1622614,
author = {Vieira, Renata and Moreira, \'{A}lvaro and Wooldridge, Michael and Bordini, Rafael H.},
title = {On the Formal Semantics of Speech-Act Based Communication in an Agent-Oriented Programming Language},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the "mental states" of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. In addition, agent programming languages have only had their semantics formalised for abstract, stand-alone versions, neglecting aspects such as communication primitives. With respect to communication, implemented agent programming languages have tended to be rather ad hoc. This paper addresses both of these problems, by giving semantics to speech-act based messages received by an AgentSpeak agent. AgentSpeak is a logic-based agent programming language which incorporates the main features of the PRS model of reactive planning systems. The paper builds upon a structural operational semantics to AgentSpeak that we developed in previous work. The main contributions of this paper are as follows: an extension of our earlier work on the theoretical foundations of AgentSpeak interpreters; a computationally grounded semantics for (the core) performatives used in speech-act based agent communication languages; and a well-defined extension of AgentSpeak that supports agent communication.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {221–267},
numpages = {47}
}

@article{10.5555/1622606.1622613,
author = {Huang, Jinbo and Darwiche, Adnan},
title = {The Language of Search},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {This paper is concerned with a class of algorithms that perform exhaustive search on propositional knowledge bases. We show that each of these algorithms defines and generates a propositional language. Specifically, we show that the trace of a search can be interpreted as a combinational circuit, and a search algorithm then defines a propositional language consisting of circuits that are generated across all possible executions of the algorithm. In particular, we show that several versions of exhaustive DPLL search correspond to such well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF. By thus mapping search algorithms to propositional languages, we provide a uniform and practical framework in which successful search techniques can be harnessed for compilation of knowledge into various languages of interest, and a new methodology whereby the power and limitations of search algorithms can be understood by looking up the tractability and succinctness of the corresponding propositional languages.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {191–219},
numpages = {29}
}

@article{10.5555/1622606.1622612,
author = {Felzenszwalb, Pedro F. and McAllester, David},
title = {The Generalized A* Architecture},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of computing a lightest derivation of a global structure using a set of weighted rules. A large variety of inference problems in AI can be formulated in this framework. We generalize A* search and heuristics derived from abstractions to a broad class of lightest derivation problems. We also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions. Our generalization of A* gives a new algorithm for searching AND/OR graphs in a bottom-up fashion.We discuss how the algorithms described here provide a general architecture for addressing the pipeline problem -- the problem of passing information back and forth between various stages of processing in a perceptual system. We consider examples in computer vision and natural language processing. We apply the hierarchical search algorithm to the problem of estimating the boundaries of convex objects in grayscale images and compare it to other search methods. A second set of experiments demonstrate the use of a new compositional model for finding salient curves in images.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {153–190},
numpages = {38}
}

@article{10.5555/1622606.1622611,
author = {Surdeanu, Mihai and M\`{a}rquez, Llu\'{\i}s and Carreras, Xavier and Comas, Pere R.},
title = {Combination Strategies for Semantic Role Labeling},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces and analyzes a battery of inference models for the problem of semantic role labeling: one based on constraint satisfaction, and several strategies that model the inference as a meta-learning problem using discriminative classifiers. These classifiers are developed with a rich set of novel features that encode proposition and sentence-level information. To our knowledge, this is the first work that: (a) performs a thorough analysis of learning-based inference models for semantic role labeling, and (b) compares several inference strategies in this context. We evaluate the proposed inference strategies in the framework of the CoNLL-2005 shared task using only automatically-generated syntactic information. The extensive experimental evaluation and analysis indicates that all the proposed inference strategies are successful - they all outperform the current best results reported in the CoNLL-2005 evaluation exercise - but each of the proposed approaches has its advantages and disadvantages. Several important traits of a state-of-the-art SRL combination strategy emerge from this analysis: (i) individual models should be combined at the granularity of candidate arguments rather than at the granularity of complete solutions; (ii) the best combination strategy uses an inference model based in learning; and (iii) the learning-based inference benefits from max-margin classifiers and global feedback.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {105–151},
numpages = {47}
}

@article{10.5555/1622606.1622610,
author = {Orasan, Constantin and Evans, Richard},
title = {NP Animacy Identification for Anaphora Resolution},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {79–103},
numpages = {25}
}

@article{10.5555/1622606.1622609,
author = {Beck, J. Christopher},
title = {Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel constructive search technique that performs a series of resource-limited tree searches where each search begins either from an empty solution (as in randomized restart) or from a solution that has been encountered during the search. A small number of these "elite" solutions is maintained during the search. We introduce the technique and perform three sets of experiments on the job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS is carried out to evaluate the performance impact of various parameter settings. Second, we inquire into the diversity of the elite solution set, showing, contrary to expectations, that a less diverse set leads to stronger performance. Finally, we compare the best parameter setting of SGMPCS from the first two experiments to chronological backtracking, limited discrepancy search, randomized restart, and a sophisticated tabu search algorithm on a set of well-known benchmark problems. Results demonstrate that SGMPCS is significantly better than the other constructive techniques tested, though lags behind the tabu search.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {49–77},
numpages = {29}
}

@article{10.5555/1622606.1622608,
author = {Nisan, Noam and Ronen, Amir},
title = {Computationally Feasible VCG Mechanisms},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {A major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When applying this method to complex problems such as combinatorial auctions, a difficulty arises: VCG mechanisms are required to compute optimal outcomes and are, therefore, computationally infeasible. However, if the optimal outcome is replaced by the results of a sub-optimal algorithm, the resulting mechanism (termed VCG-based) is no longer necessarily truthful. The first part of this paper studies this phenomenon in depth and shows that it is near universal. Specifically, we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield non-truthful VCG-based mechanisms. We generalize these results for affine maximizers.The second part of this paper proposes a general method for circumventing the above problem. We introduce a modification of VCG-based mechanisms in which the agents are given a chance to improve the output of the underlying algorithm. When the agents behave truthfully, the welfare obtained by the mechanism is at least as good as the one obtained by the algorithm's output. We provide a strong rationale for truth-telling behavior. Our method satisfies individual rationality as well.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {19–47},
numpages = {29}
}

@article{10.5555/1622606.1622607,
author = {Creignou, Nadia and Daud\'{e}, Herv\'{e} and Egly, Uwe},
title = {Phase Transition for Random Quantified XOR-Formulas},
year = {2007},
issue_date = {May 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {29},
number = {1},
issn = {1076-9757},
abstract = {The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–18},
numpages = {18}
}

@article{10.5555/1622591.1622605,
author = {Gao, Yong and Culberson, Joseph},
title = {Consistency and Random Constraint Satisfaction Models},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we study the possibility of designing non-trivial random CSP models by exploiting the intrinsic connection between structures and typical-case hardness. We show that constraint consistency, a notion that has been developed to improve the efficiency of CSP algorithms, is in fact the key to the design of random CSP models that have interesting phase transition behavior and guaranteed exponential resolution complexity without putting much restriction on the parameter of constraint tightness or the domain size of the problem. We propose a very flexible framework for constructing problem instances with interesting behavior and develop a variety of concrete methods to construct specific random CSP models that enforce different levels of constraint consistency.A series of experimental studies with interesting observations are carried out to illustrate the effectiveness of introducing structural elements in random instances, to verify the robustness of our proposal, and to investigate features of some specific models based on our framework that are highly related to the behavior of backtracking search algorithms.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {517–557},
numpages = {41}
}

@article{10.5555/1622591.1622604,
author = {Clement, Bradley J. and Durfee, Edmund H. and Barrett, Anthony C.},
title = {Abstract Reasoning for Planning and Coordination},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {The judicious use of abstraction can help planning agents to identify key interactions between actions, and resolve them, without getting bogged down in details. However, ignoring the wrong details can lead agents into building plans that do not work, or into costly backtracking and replanning once overlooked interdependencies come to light. We claim that associating systematically-generated summary information with plans' abstract operators can ensure plan correctness, even for asynchronously-executed plans that must be coordinated across multiple agents, while still achieving valuable efficiency gains. In this paper, we formally characterize hierarchical plans whose actions have temporal extent, and describe a principled method for deriving summarized state and metric resource information for such actions. We provide sound and complete algorithms, along with heuristics, to exploit summary information during hierarchical refinement planning and plan coordination. Our analyses and experiments show that, under clearcut and reasonable conditions, using summary information can speed planning as much as doubly exponentially even for plans involving interacting subproblems.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {453–515},
numpages = {63}
}

@article{10.5555/1622591.1622603,
author = {Lin, Fangzhen and Chen, Yin},
title = {Discovering Classes of Strongly Equivalent Logic Programs},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {In this paper we apply computer-aided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics. Our discovered theorems capture new classes of strongly equivalent logic programs that can lead to new program simplification rules that preserve strong equivalence. Specifically, with the help of computers, we discovered exact conditions that capture the strong equivalence between a rule and the empty set, between two rules, between two rules and one of the two rules, between two rules and another rule, and between three rules and two of the three rules.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {431–451},
numpages = {21}
}

@article{10.5555/1622591.1622602,
author = {Fukunaga, Alex S. and Korf, Richard E.},
title = {Bin Completion Algorithms for Multicontainer Packing, Knapsack, and Covering Problems},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {Many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers. These problems can be used to model task and resource allocation problems in multi-agent systems and distributed systms, and can also be found as subproblems of scheduling problems. We propose bin completion, a branch-and-bound strategy for one-dimensional, multicontainer packing problems. Bin completion combines a bin-oriented search space with a powerful dominance criterion that enables us to prune much of the space. The performance of the basic bin completion framework can be enhanced by using a number of extensions, including nogood-based pruning techniques that allow further exploitation of the dominance criterion. Bin completion is applied to four problems: multiple knapsack, bin covering, min-cost covering, and bin packing. We show that our bin completion algorithms yield new, state-of-the-art results for the multiple knapsack, bin covering, and min-cost covering problems, outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard, random problem instances. For the bin packing problem, we demonstrate significant improvements compared to most previous results, but show that bin completion is not competitive with current state-of-the-art cutting-stock based approaches.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {393–429},
numpages = {37}
}

@article{10.5555/1622591.1622601,
author = {Jodogne, S\'{e}bastien and Piater, Justus H.},
title = {Closed-Loop Learning of Visual Control Policies},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {In this paper we present a general, flexible framework for learning mappings from images to actions by interacting with the environment. The basic idea is to introduce a feature-based image classifier in front of a reinforcement learning algorithm. The classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing. We also address the problem of fighting overfitting in such a greedy algorithm. Finally, we show how high-level visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states. This is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features. We demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical "Car on the Hill" control problem.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {349–391},
numpages = {43}
}

@article{10.5555/1622591.1622600,
author = {Bettini, Claudio and Mascetti, Sergio and Wang, X. Sean},
title = {Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal Periodic Sets},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {In the recent years several research efforts have focused on the concept of time granularity and its applications. A first stream of research investigated the mathematical models behind the notion of granularity and the algorithms to manage temporal data based on those models. A second stream of research investigated symbolic formalisms providing a set of algebraic operators to define granularities in a compact and compositional way. However, only very limited manipulation algorithms have been proposed to operate directly on the algebraic representation making it unsuitable to use the symbolic formalisms in applications that need manipulation of granularities.This paper aims at filling the gap between the results from these two streams of research, by providing an efficient conversion from the algebraic representation to the equivalent low-level representation based on the mathematical models. In addition, the conversion returns a minimal representation in terms of period length. Our results have a major practical impact: users can more easily define arbitrary granularities in terms of algebraic operators, and then access granularity reasoning and other services operating efficiently on the equivalent, minimal low-level representation. As an example, we illustrate the application to temporal constraint reasoning with multiple granularities.From a technical point of view, we propose an hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with the minimization of the period length. The algorithm returns set-based granularity representations having minimal period length, which is the most relevant parameter for the performance of the considered reasoning services. Extensive experimental work supports the techniques used in the algorithm, and shows the efficiency and effectiveness of the algorithm.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {299–348},
numpages = {50}
}

@article{10.5555/1622591.1622599,
author = {Hansen, Eric A. and Zhou, Rong},
title = {Anytime Heuristic Search},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {We describe how to convert the heuristic search algorithm A* into an anytime algorithm that finds a sequence of improved solutions and eventually converges to an optimal solution. The approach we adopt uses weighted heuristic search to find an approximate solution quickly, and then continues the weighted search to find improved solutions as well as to improve a bound on the suboptimality of the current solution. When the time available to solve a search problem is limited or uncertain, this creates an anytime heuristic search algorithm that allows a flexible tradeoff between search time and solution quality. We analyze the properties of the resulting Anytime A* algorithm, and consider its performance in three domains; sliding-tile puzzles, STRIPS planning, and multiple sequence alignment. To illustrate the generality of this approach, we also describe how to transform the memory-efficient search algorithm Recursive Best-First Search (RBFS) into an anytime algorithm.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {267–297},
numpages = {31}
}

@article{10.5555/1622591.1622598,
author = {Blumrosen, Liad and Nisan, Noam and Segal, Ilya},
title = {Auctions with Severely Bounded Communication},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {We study auctions with severe bounds on the communication allowed: each bidder may only transmit t bits of information to the auctioneer. We consider both welfare- and profit-maximizing auctions under this communication restriction. For both measures, we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild. We prove non-surprising properties of these kinds of auctions, e.g., that in optimal mechanisms bidders simply report the interval in which their valuation lies in, as well as some surprising properties, e.g., that asymmetric auctions are better than symmetric ones and that multi-round auctions reduce the communication complexity only by a linear factor.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {233–266},
numpages = {34}
}

@article{10.5555/1622591.1622597,
author = {Beck, J. Christopher and Wilson, Nic},
title = {Proactive Algorithms for Job Shop Scheduling with Probabilistic Durations},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {Most classical scheduling formulations assume a fixed and known duration for each activity. In this paper, we weaken this assumption, requiring instead that each duration can be represented by an independent random variable with a known mean and variance. The best solutions are ones which have a high probability of achieving a good makespan. We first create a theoretical framework, formally showing how Monte Carlo simulation can be combined with deterministic scheduling algorithms to solve this problem. We propose an associated deterministic scheduling problem whose solution is proved, under certain conditions, to be a lower bound for the probabilistic problem. We then propose and investigate a number of techniques for solving such problems based on combinations of Monte Carlo simulation, solutions to the associated deterministic problem, and either constraint programming or tabu search. Our empirical results demonstrate that a combination of the use of the associated deterministic problem and Monte Carlo simulation results in algorithms that scale best both in terms of problem size and uncertainty. Further experiments point to the correlation between the quality of the deterministic solution and the quality of the probabilistic solution as a major factor responsible for this success.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {183–232},
numpages = {50}
}

@article{10.5555/1622591.1622596,
author = {Procaccia, Ariel D. and Rosenschein, Jeffrey S.},
title = {Junta Distributions and the Average-Case Complexity of Manipulating Elections},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {Encouraging voters to truthfully reveal their preferences in an election has long been an important issue. Recently, computational complexity has been suggested as a means of precluding strategic behavior. Previous studies have shown that some voting protocols are hard to manipulate, but used NP-hardness as the complexity measure. Such a worst-case analysis may be an insufficient guarantee of resistance to manipulation.Indeed, we demonstrate that NP-hard manipulations may be tractable in the average-case. For this purpose, we augment the existing theory of average-case complexity with some new concepts. In particular, we consider elections distributed with respect to junta distributions, which concentrate on hard instances. We use our techniques to prove that scoring protocols are susceptible to manipulation by coalitions, when the number of candidates is constant.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {157–181},
numpages = {25}
}

@article{10.5555/1622591.1622595,
author = {Coles, Andrew and Smith, Amanda},
title = {Marvin: A Heuristic Search Planner with Online Macro-Action Learning},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {This paper describes Marvin, a planner that competed in the Fourth International Planning Competition (IPC 4). Marvin uses action-sequence-memoisation techniques to generate macro-actions, which are then used during search for a solution plan. We provide an overview of its architecture and search behaviour, detailing the algorithms used. We also empirically demonstrate the effectiveness of its features in various planning domains; in particular, the effects on performance due to the use of macro-actions, the novel features of its search behaviour, and the native support of ADL and Derived Predicates.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {119–156},
numpages = {38}
}

@article{10.5555/1622591.1622594,
author = {Jia, Haixia and Moore, Cristopher and Strain, Doug},
title = {Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {To test incomplete search algorithms for constraint satisfaction problems such as 3- SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the "hidden" assignment A. Last year, Achlioptas, Jia and Moore proposed a problem generator that cancels this effect by hiding both A and its complement A (Achlioptas, Jia, &amp; Moore, 2004). While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time.Here we propose a new method to cancel the attraction to A, by choosing a clause with t &gt; 0 literals satisfied by A with probability proportional to q&gt;t for some q &lt; 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to "deceptively" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {107–118},
numpages = {12}
}

@article{10.5555/1622591.1622593,
author = {Everaere, Patricia and Konieczny, S\'{e}bastien and Marquis, Pierre},
title = {The Strategy-Proofness Landscape of Merging},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {Merging operators aim at defining the beliefs/goals of a group of agents from the beliefs/goals of each member of the group. Whenever an agent of the group has preferences over the possible results of the merging process (i.e., the possible merged bases), she can try to rig the merging process by lying on her true beliefs/goals if this leads to a better merged base according to her point of view. Obviously, strategy-proof operators are highly desirable in order to guarantee equity among agents even when some of them are not sincere. In this paper, we draw the strategy-proof landscape for many merging operators from the literature, including model-based ones and formula-based ones. Both the general case and several restrictions on the merging process are considered.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {49–105},
numpages = {57}
}

@article{10.5555/1622591.1622592,
author = {Bidyuk, Bozhena and Dechter, Rina},
title = {Cutset Sampling for Bayesian Networks},
year = {2007},
issue_date = {January 2007},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {28},
number = {1},
issn = {1076-9757},
abstract = {The paper presents a new sampling methodology for Bayesian networks that samples only a subset of variables and applies exact inference to the rest. Cutset sampling is a network structure-exploiting application of the Rao-Blackwellisation principle to sampling in Bayesian networks. It improves convergence by exploiting memory-based inference algorithms. It can also be viewed as an anytime approximation of the exact cutset-conditioning algorithm developed by Pearl. Cutset sampling can be implemented efficiently when the sampled variables constitute a loop-cutset of the Bayesian network and, more generally, when the induced width of the network's graph conditioned on the observed sampled variables is bounded by a constant w. We demonstrate empirically the benefit of this scheme on a range of benchmarks.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–48},
numpages = {48}
}

@article{10.5555/1622572.1622590,
author = {Rossi, Francesca and Venable, Kristen Brent and Yorke-Smith, Neil},
title = {Uncertainty in Soft Temporal Constraint Problems: A General Framework and Controllability Algorithms for the Fuzzy Case},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {In real-life temporal scenarios, uncertainty and preferences are often essential and coexisting aspects. We present a formalism where quantitative temporal constraints with both preferences and uncertainty can be defined. We show how three classical notions of controllability (that is, strong, weak, and dynamic), which have been developed for uncertain temporal problems, can be generalized to handle preferences as well. After defining this general framework, we focus on problems where preferences follow the fuzzy approach, and with properties that assure tractability. For such problems, we propose algorithms to check the presence of the controllability properties. In particular, we show that in such a setting dealing simultaneously with preferences and uncertainty does not increase the complexity of controllability testing. We also develop a dynamic execution algorithm, of polynomial complexity, that produces temporal plans under uncertainty that are optimal with respect to fuzzy preferences.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {617–674},
numpages = {58}
}

@article{10.5555/1622572.1622589,
author = {Barbulescu, Laura and Howe, Adele E. and Whitley, L. Darrell and Roberts, Mark},
title = {Understanding Algorithm Performance on an Oversubscribed Scheduling Application},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {The best performing algorithms for a particular oversubscribed scheduling application, Air Force Satellite Control Network (AFSCN) scheduling, appear to have little in common. Yet, through careful experimentation and modeling of performance in real problem instances, we can relate characteristics of the best algorithms to characteristics of the application. In particular, we find that plateaus dominate the search spaces (thus favoring algorithms that make larger changes to solutions) and that some randomization in exploration is critical to good performance (due to the lack of gradient information on the plateaus). Based on our explanations of algorithm performance, we develop a new algorithm that combines characteristics of the best performers; the new algorithm's performance is better than the previous best. We show how hypothesis driven experimentation and search modeling can both explain algorithm performance and motivate the design of a new algorithm.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {577–615},
numpages = {39}
}

@article{10.5555/1622572.1622588,
author = {Yilmaz, \"{O}zg\"{u}r and Say, A. C. Cem},
title = {Causes of Ineradicable Spurious Predictions in Qualitative Simulation},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {It was recently proved that a sound and complete qualitative simulator does not exist, that is, as long as the input-output vocabulary of the state-of-the-art QSIM algorithm is used, there will always be input models which cause any simulator with a coverage guarantee to make spurious predictions in its output. In this paper, we examine whether a meaningfully expressive restriction of this vocabulary is possible so that one can build a simulator with both the soundness and completeness properties. We prove several negative results: All sound qualitative simulators, employing subsets of the QSIM representation which retain the operating region transition feature, and support at least the addition and constancy constraints, are shown to be inherently incomplete. Even when the simulations are restricted to run in a single operating region, a constraint vocabulary containing just the addition, constancy, derivative, and multiplication relations makes the construction of sound and complete qualitative simulators impossible.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {551–575},
numpages = {25}
}

@article{10.5555/1622572.1622587,
author = {Dolgov, Dmitri A. and Durfee, Edmund H.},
title = {Resource Allocation among Agents with MDP-Induced Preferences},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {Allocating scarce resources among agents to maximize global utility is, in general, computationally challenging. We focus on problems where resources enable agents to execute actions in stochastic environments, modeled as Markov decision processes (MDPs), such that the value of a resource bundle is defined as the expected value of the optimal MDP policy realizable given these resources. We present an algorithm that simultaneously solves the resource-allocation and the policy-optimization problems. This allows us to avoid explicitly representing utilities over exponentially many resource bundles, leading to drastic (often exponential) reductions in computational complexity. We then use this algorithm in the context of self-interested agents to design a combinatorial auction for allocating resources. We empirically demonstrate the effectiveness of our approach by showing that it can, in minutes, optimally solve problems for which a straightforward combinatorial resource-allocation technique would require the agents to enumerate up to 2100 resource bundles and the auctioneer to solve an NP-complete problem with an input of that size.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {505–549},
numpages = {45}
}

@article{10.5555/1622572.1622586,
author = {Viappiani, Paolo and Faltings, Boi and Pu, Pearl},
title = {Preference-Based Search Using Example-Critiquing with Suggestions},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {We consider interactive tools that help users search for their most preferred item in a large collection of options. In particular, we examine example-critiquing, a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them. We present novel techniques for improving the example-critiquing technology by adding suggestions to its displayed options. Such suggestions are calculated based on an analysis of users' current preference model and their potential hidden preferences. We evaluate the performance of our model-based suggestion techniques with both synthetic and real users. Results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78%.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {465–503},
numpages = {39}
}

@article{10.5555/1622572.1622585,
author = {Zhang, Yuanlin and Yap, Roland H. C.},
title = {Set Intersection and Consistency in Constraint Networks},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we show that there is a close relation between consistency in a constraint network and set intersection. A proof schema is provided as a generic way to obtain consistency properties from properties on set intersection. This approach not only simplifies the understanding of and unifies many existing consistency results, but also directs the study of consistency to that of set intersection properties in many situations, as demonstrated by the results on the convexity and tightness of constraints in this paper. Specifically, we identify a new class of tree convex constraints where local consistency ensures global consistency. This generalizes row convex constraints. Various consistency results are also obtained on constraint networks where only some, in contrast to all in the existing work, constraints are tight.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {441–464},
numpages = {24}
}

@article{10.5555/1622572.1622584,
author = {H\"{o}lldobler, Steffen and Karabaev, Eldar and Skvortsova, Olga},
title = {FLUCAP: A Heuristic Search Planner for First-Order MDPs},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {We present a heuristic search algorithm for solving first-order Markov Decision Processes (FOMDPs). Our approach combines first-order state abstraction that avoids evaluating states individually, and heuristic search that avoids evaluating all states. Firstly, in contrast to existing systems, which start with propositionalizing the FOMDP and then perform state abstraction on its propositionalized version we apply state abstraction directly on the FOMDP avoiding propositionalization. This kind of abstraction is referred to as first-order state abstraction. Secondly, guided by an admissible heuristic, the search is restricted to those states that are reachable from the initial state. We demonstrate the usefulness of the above techniques for solving FOMDPs with a system, referred to as FluCaP (formerly, FCPlanner), that entered the probabilistic track of the 2004 International Planning Competition (IPC'2004) and demonstrated an advantage over other planners on the problems represented in first-order terms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {419–439},
numpages = {21}
}

@article{10.5555/1622572.1622583,
author = {Fatima, Shaheen S. and Wooldridge, Michael and Jennings, Nicholas R.},
title = {Multi-Issue Negotiation with Deadlines},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {This paper studies bilateral multi-issue negotiation between self-interested autonomous agents. Now, there are a number of different procedures that can be used for this process; the three main ones being the package deal procedure in which all the issues are bundled and discussed together, the simultaneous procedure in which the issues are discussed simultaneously but independently of each other, and the sequential procedure in which the issues are discussed one after another. Since each of them yields a different outcome, a key problem is to decide which one to use in which circumstances. Specifically, we consider this question for a model in which the agents have time constraints (in the form of both deadlines and discount factors) and information uncertainty (in that the agents do not know the opponent's utility function). For this model, we consider issues that are both independent and those that are interdependent and determine equilibria for each case for each procedure. In so doing, we show that the package deal is in fact the optimal procedure for each party. We then go on to show that, although the package deal may be computationally more complex than the other two procedures, it generates Pareto optimal outcomes (unlike the other two), it has similar earliest and latest possible times of agreement to the simultaneous procedure (which is better than the sequential procedure), and that it (like the other two procedures) generates a unique outcome only under certain conditions (which we define).},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {381–417},
numpages = {37}
}

@article{10.5555/1622572.1622582,
author = {Pineau, Joelle and Gordon, Geoffrey and Thrun, Sebastian},
title = {Anytime Point-Based Approximations for Large POMDPs},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {The Partially Observable Markov Decision Process has long been recognized as a rich framework for real-world planning and control problems, especially in robotics. However exact solutions in this framework are typically computationally intractable for all but the smallest problems. A well-known technique for speeding up POMDP solving involves performing value backups at specific belief points, rather than over the entire belief simplex. The efficiency of this approach, however, depends greatly on the selection of points. This paper presents a set of novel techniques for selecting informative belief points which work well in practice. The point selection procedure is combined with point-based value backups to form an effective anytime POMDP algorithm called Point-Based Value Iteration (PBVI). The first aim of this paper is to introduce this algorithm and present a theoretical analysis justifying the choice of belief selection technique. The second aim of this paper is to provide a thorough empirical comparison between PBVI and other state-of-the-art POMDP methods, in particular the Perseus algorithm, in an effort to highlight their similarities and differences. Evaluation is performed using both standard POMDP domains and realistic robotic tasks.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {335–380},
numpages = {46}
}

@article{10.5555/1622572.1622581,
author = {Liu, Lengning and Truszczynski, Miros\law},
title = {Properties and Applications of Programs with Monotone and Convex Constraints},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {We study properties of programs with monotone and convex constraints. We extend to these formalisms concepts and results from normal logic programming. They include the notions of strong and uniform equivalence with their characterizations, tight programs and Fages Lemma, program completion and loop formulas. Our results provide an abstract account of properties of some recent extensions of logic programming with aggregates, especially the formalism of lparse programs. They imply a method to compute stable models of lparse programs by means of off-the-shelf solvers of pseudo-boolean constraints, which is often much faster than the smodels system.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {299–334},
numpages = {36}
}

@article{10.5555/1622572.1622580,
author = {Fox, Maria and Long, Derek},
title = {Modelling Mixed Discrete-Continuous Domains for Planning},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {In this paper we present PDDL+, a planning domain description language for modelling mixed discrete-continuous planning domains. We describe the syntax and modelling style of PDDL+, showing that the language makes convenient the modelling of complex time-dependent effects. We provide a formal semantics for PDDL+ by mapping planning instances into constructs of hybrid automata. Using the syntax of HAs as our semantic model we construct a semantic mapping to labelled transition systems to complete the formal interpretation of PDDL+ planning instances.An advantage of building a mapping from PDDL+ to HA theory is that it forms a bridge between the Planning and Real Time Systems research communities. One consequence is that we can expect to make use of some of the theoretical properties of HAs. For example, for a restricted class of HAs the Reachability problem (which is equivalent to Plan Existence) is decidable.PDDL+ provides an alternative to the continuous durative action model of PDDL2.1, adding a more flexible and robust model of time-dependent behaviour.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {235–297},
numpages = {63}
}

@article{10.5555/1622572.1622579,
author = {Muslea, Ion and Minton, Steven and Knoblock, Craig A.},
title = {Active Learning with Multiple Views},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain. We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept. In this paper we make several contributions. First, we introduce Co-Testing, which is the first approach to multi-view active learning. Second, we extend the multi-view learning framework by also exploiting weak views, which are adequate only for learning a concept that is more general/specific than the target concept. Finally, we empirically show that Co-Testing outperforms existing active learners on a variety of real world domains such as wrapper induction, Web page classification, advertisement removal, and discourse tree parsing.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {203–233},
numpages = {31}
}

@article{10.5555/1622572.1622578,
author = {Kveton, Branislav and Hauskrecht, Milos and Guestrin, Carlos},
title = {Solving Factored MDPs with Hybrid State and Action Variables},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {Efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems. In this paper, we describe a novel hybrid factored Markov decision process (MDP) model that allows for a compact representation of these problems, and a new hybrid approximate linear programming (HALP) framework that permits their efficient solutions. The central idea of HALP is to approximate the optimal value function by a linear combination of basis functions and optimize its weights by linear programming. We analyze both theoretical and computational aspects of this approach, and demonstrate its scale-up potential on several hybrid optimization problems.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {153–201},
numpages = {49}
}

@article{10.5555/1622572.1622577,
author = {Oh, Jong-Hoon and Choi, Key-Sun and Isahara, Hitoshi},
title = {A Comparison of Different Machine Transliteration Models},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {Machine transliteration is a method for automatically converting words in one language into phonetically equivalent ones in another language. Machine transliteration plays an important role in natural language applications such as information retrieval and machine translation, especially for handling proper nouns and technical terms. Four machine transliteration models - grapheme-based transliteration model, phoneme-based transliteration model, hybrid transliteration model, and correspondence-based transliteration model - have been proposed by several researchers. To date, however, there has been little research on a framework in which multiple transliteration models can operate simultaneously. Furthermore, there has been no comparison of the four models within the same framework and using the same data. We addressed these problems by 1) modeling the four models within the same framework, 2) comparing them under the same conditions, and 3) developing a way to improve machine transliteration through this comparison. Our comparison showed that the hybrid and correspondence-based models were the most effective and that the four models can be used in a complementary manner to improve machine transliteration performance.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {119–151},
numpages = {33}
}

@article{10.5555/1622572.1622576,
author = {Lapata, Mirella and Lascarides, Alex},
title = {Learning Sentence-Internal Temporal Relations},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., summarisation, question answering). Our method bypasses the need for manual coding by exploiting the presence of markers like after, which overtly signal a temporal relation. We first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudo-disambiguation task simulating temporal inference (during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates). Secondly, we assess whether the proposed approach holds promise for the semi-automatic creation of temporal annotations. Specifically, we use a model trained on noisy and approximate data (i.e., main and subordinate clauses) to predict intra-sentential relations present in TimeBank, a corpus annotated rich temporal information. Our experiments compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against gold standard corpora and also against human subjects.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {85–117},
numpages = {33}
}

@article{10.5555/1622572.1622575,
author = {Chai, Joyce Y. and Prasov, Zahar and Qu, Shaolin},
title = {Cognitive Principles in Robust Multimodal Interpretation},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {Multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture. To build effective multimodal interfaces, automated interpretation of user multimodal inputs is important. Inspired by the previous investigation on cognitive status in multimodal human machine interaction, we have developed a greedy algorithm for interpreting user referring expressions (i.e., multimodal reference resolution). This algorithm incorporates the cognitive principles of Conversational Implicature and Givenness Hierarchy and applies constraints from various sources (e.g., temporal, semantic, and contextual) to resolve references. Our empirical results have shown the advantage of this algorithm in efficiently resolving a variety of user references. Because of its simplicity and generality, this approach has the potential to improve the robustness of multimodal input interpretation.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {55–83},
numpages = {29}
}

@article{10.5555/1622572.1622574,
author = {Epshteyn, Arkady and DeJong, Gerald},
title = {Generative Prior Knowledge for Discriminative Classification},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {We present a novel framework for integrating prior knowledge into discriminative classifiers. Our framework allows discriminative classifiers such as Support Vector Machines (SVMs) to utilize prior knowledge specified in the generative setting. The dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program, which is solved (approximately) via iterative application of second-order cone programming. To test our approach, we consider the problem of using WordNet (a semantic database of English language) to improve low-sample classification accuracy of newsgroup categorization. WordNet is viewed as an approximate, but readily available source of background knowledge, and our framework is capable of utilizing it in a flexible way.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {25–53},
numpages = {29}
}

@article{10.5555/1622572.1622573,
author = {Geiger, Dan and Meek, Christopher and Wexler, Ydo},
title = {A Variational Inference Procedure Allowing Internal Structure for Overlapping Clusters and Deterministic Constraints},
year = {2006},
issue_date = {September 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {27},
number = {1},
issn = {1076-9757},
abstract = {We develop a novel algorithm, called VIP*, for structured variational approximate inference. This algorithm extends known algorithms to allow efficient multiple potential updates for overlapping clusters, and overcomes the difficulties imposed by deterministic constraints. The algorithm's convergence is proven and its applicability demonstrated for genetic linkage analysis.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–23},
numpages = {23}
}

@article{10.5555/1622559.1622571,
author = {Hoffmann, J\"{o}rg and Edelkamp, Stefan and Thi\'{e}baux, Sylvie and Englert, Roman and dos Santos Liporace, Frederico and Tr\"{u}g, Sebastian},
title = {Engineering Benchmarks for Planning: The Domains Used in the Deterministic Part of IPC-4},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {In a field of research about general reasoning mechanisms, it is essential to have appropriate benchmarks. Ideally, the benchmarks should reflect possible applications of the developed technology. In AI Planning, researchers more and more tend to draw their testing examples from the benchmark collections used in the International Planning Competition (IPC). In the organization of (the deterministic part of) the fourth IPC, IPC-4, the authors therefore invested significant effort to create a useful set of benchmarks. They come from five different (potential) real-world applications of planning: airport ground traffic control, oil derivative transportation in pipeline networks, model-checking safety properties, power supply restoration, and UMTS call setup. Adapting and preparing such an application for use as a benchmark in the IPC involves, at the time, inevitable (often drastic) simplifications, as well as careful choice between, and engineering of, domain encodings. For the first time in the IPC, we used compilations to formulate complex domain features in simple languages such as STRIPS, rather than just dropping the more interesting problem constraints in the simpler language subsets. The article explains and discusses the five application domains and their adaptation to form the PDDL test suites used in IPC-4. We summarize known theoretical results on structural properties of the domains, regarding their computational complexity and provable properties of their topology under the h+ function (an idealized version of the relaxed plan heuristic). We present new (empirical) results illuminating properties such as the quality of the most wide-spread heuristic functions (planning graph, serial planning graph, and relaxed plan), the growth of propositional representations over instance size, and the number of actions available to achieve each fact; we discuss these data in conjunction with the best results achieved by the different kinds of planners participating in IPC-4.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {453–541},
numpages = {89}
}

@article{10.5555/1622559.1622570,
author = {Davidov, Dmitry and Markovitch, Shaul},
title = {Multiple-Goal Heuristic Search},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new framework for anytime heuristic search where the task is to achieve as many goals as possible within the allocated resources. We show the inadequacy of traditional distance-estimation heuristics for tasks of this type and present alternative heuristics that are more appropriate for multiple-goal search. In particular, we introduce the marginal-utility heuristic, which estimates the cost and the benefit of exploring a subtree below a search node. We developed two methods for online learning of the marginal-utility heuristic. One is based on local similarity of the partial marginal utility of sibling nodes, and the other generalizes marginal-utility over the state feature space. We apply our adaptive and non-adaptive multiple-goal search algorithms to several problems, including focused crawling, and show their superiority over existing methods.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {417–451},
numpages = {35}
}

@article{10.5555/1622559.1622569,
author = {Giunchiglia, Enrico and Narizzano, Massimo and Tacchella, Armando},
title = {Clause/Term Resolution and Learning in the Evaluation of Quantified Boolean Formulas},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {Resolution is the rule of inference at the basis of most procedures for automated reasoning. In these procedures, the input formula is first translated into an equisatisfiable formula in conjunctive normal form (CNF) and then represented as a set of clauses. Deduction starts by inferring new clauses by resolution, and goes on until the empty clause is generated or satisfiability of the set of clauses is proven, e.g., because no new clauses can be generated.In this paper, we restrict our attention to the problem of evaluating Quantified Boolean Formulas (QBFs). In this setting, the above outlined deduction process is known to be sound and complete if given a formula in CNF and if a form of resolution, called "Q-resolution", is used. We introduce Q-resolution on terms, to be used for formulas in disjunctive normal form. We show that the computation performed by most of the available procedures for QBFs -based on the Davis-Logemann-Loveland procedure (DLL) for propositional satisfiability- corresponds to a tree in which Q-resolution on terms and clauses alternate. This poses the theoretical bases for the introduction of learning, corresponding to recording Q-resolution formulas associated with the nodes of the tree. We discuss the problems related to the introduction of learning in DLL based procedures, and present solutions extending state-of-the-art proposals coming from the literature on propositional satisfiability. Finally, we show that our DLL based solver extended with learning, performs significantly better on benchmarks used in the 2003 QBF solvers comparative evaluation.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {371–416},
numpages = {46}
}

@article{10.5555/1622559.1622568,
author = {Chen, Yixin and Wah, Benjamin W. and Hsu, Chih-Wei},
title = {Temporal Planning Using Subgoal Partitioning and Resolution in SGPlan},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we present the partitioning of mutual-exclusion (mutex) constraints in temporal planning problems and its implementation in the SGPlan4 planner. Based on the strong locality of mutex constraints observed in many benchmarks of the Fourth International Planning Competition (IPC4), we propose to partition the constraints of a planning problem into groups based on their subgoals. Constraint partitioning leads to significantly easier subproblems that are similar to the original problem and that can be efficiently solved by the same planner with some modifications to its objective function. We present a partition-and-resolve strategy that looks for locally optimal subplans in constraint-partitioned temporal planning subproblems and that resolves those inconsistent global constraints across the subproblems. We also discuss some implementation details of SGPlan4, which include the resolution of violated global constraints, techniques for handling producible resources, landmark analysis, path finding and optimization, search-space reduction, and modifications of Metric-FF when used as a basic planner in SGPlan4. Last, we show results on the sensitivity of each of these techniques in quality-time trade-offs and experimentally demonstrate that SGPlan4 is effective for solving the IPC3 and IPC4 benchmarks.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {323–369},
numpages = {47}
}

@article{10.5555/1622559.1622567,
author = {Ramani, Arathi and Markov, Igor L. and Sakallah, Karem A. and Aloul, Fadi A.},
title = {Breaking Instance-Independent Symmetries in Exact Graph Coloring},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {Code optimization and high level synthesis can be posed as constraint satisfaction and optimization problems, such as graph coloring used in register allocation. Graph coloring is also used to model more traditional CSPs relevant to AI, such as planning, time-tabling and scheduling. Provably optimal solutions may be desirable for commercial and defense applications. Additionally, for applications such as register allocation and code optimization, naturally-occurring instances of graph coloring are often small and can be solved optimally. A recent wave of improvements in algorithms for Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests generic problem-reduction methods, rather than problem-specific heuristics, because (1) heuristics may be upset by new constraints, (2) heuristics tend to ignore structure, and (3) many relevant problems are provably inapproximable.Problem reductions often lead to highly symmetric SAT instances, and symmetries are known to slow down SAT solvers. In this work, we compare several avenues for symmetry breaking, in particular when certain kinds of symmetry are present in all generated instances. Our focus on reducing CSPs to SAT allows us to leverage recent dramatic improvement in SAT solvers and automatically benefit from future progress. We can use a variety of black-box SAT solvers without modifying their source code because our symmetry-breaking techniques are static, i.e., we detect symmetries and add symmetry breaking predicates (SBPs) during pre-processing.An important result of our work is that among the types of instance-independent SBPs we studied and their combinations, the simplest and least complete constructions are the most effective. Our experiments also clearly indicate that instance-independent symmetries should mostly be processed together with instance-specific symmetries rather than at the specification level, contrary to what has been suggested in the literature.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {289–322},
numpages = {34}
}

@article{10.5555/1622559.1622566,
author = {Streeter, Matthew J. and Smith, Stephen F.},
title = {How the Landscape of Random Job Shop Scheduling Instances Depends on the Ratio of Jobs to Machines},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {We characterize the search landscape of random instances of the job shop scheduling problem (JSP). Specifically, we investigate how the expected values of (1) backbone size, (2) distance between near-optimal schedules, and (3) makespan of random schedules vary as a function of the job to machine ratio (N/M). For the limiting cases N/M → 0 and N/M → ∞ we provide analytical results, while for intermediate values of N/M we perform experiments. We prove that as N/M → 0, backbone size approaches 100%, while as N/M → ∞ the backbone vanishes. In the process we show that as N/M → 0 (resp. N/M → ∞), simple priority rules almost surely generate an optimal schedule, providing theoretical evidence of an "easy-hard-easy" pattern of typical-case instance difficulty in job shop scheduling. We also draw connections between our theoretical results and the "big valley" picture of JSP landscapes.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {247–287},
numpages = {41}
}

@article{10.5555/1622559.1622565,
author = {Helmert, Malte},
title = {The Fast Downward Planning System},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multivalued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.In this article, we give a full account of Fast Downward's approach to solving multivalued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downward's best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multiheuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.Fast Downward has proven remarkably successful: It won the "classical" (i. e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {191–246},
numpages = {56}
}

@article{10.5555/1622559.1622564,
author = {Heskes, Tom},
title = {Convexity Arguments for Efficient Minimization of the Bethe and Kikuchi Free Energies},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms have been shown to correspond to extrema of the Bethe and Kikuchi free energy, both of which are approximations of the exact Helmholtz free energy. However, belief propagation does not always converge, which motivates approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP and UPS.Here we describe a class of algorithms that solves this typically non-convex constrained minimization problem through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations. Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {153–190},
numpages = {38}
}

@article{10.5555/1622559.1622563,
author = {Booth, Richard and Meyer, Thomas},
title = {Admissible and Restrained Revision},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {As partial justification of their framework for iterated belief revision Darwiche and Pearl convincingly argued against Boutilier's natural revision and provided a prototypical revision operator that fits into their scheme. We show that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller class of operators which we refer to as admissible. Admissible revision ensures that the penultimate input is not ignored completely, thereby eliminating natural revision, but includes the Darwiche-Pearl operator, Nayak's lexicographic revision operator, and a newly introduced operator called restrained revision. We demonstrate that restrained revision is the most conservative of admissible revision operators, effecting as few changes as possible, while lexicographic revision is the least conservative, and point out that restrained revision can also be viewed as a composite operator, consisting of natural revision preceded by an application of a "backwards revision" operator previously studied by Papini. Finally, we propose the establishment of a principled approach for choosing an appropriate revision operator in different contexts and discuss future work.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {127–151},
numpages = {25}
}

@article{10.5555/1622559.1622562,
author = {Daum\'{e}, Hal and Marcu, Daniel},
title = {Domain Adaptation for Statistical Classifiers},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the "in-domain" test data is drawn from a distribution that is related, but not identical, to the "out-of-domain" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {101–126},
numpages = {26}
}

@article{10.5555/1622559.1622561,
author = {Bryce, Daniel and Kambhampati, Subbarao and Smith, David E.},
title = {Planning Graph Heuristics for Belief Space Search},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {Some recent works in conditional planning have proposed reachability heuristics to improve planner scalability, but many lack a formal description of the properties of their distance estimates. To place previous work in context and extend work on heuristics for conditional planning, we provide a formal basis for distance estimates between belief states. We give a definition for the distance between belief states that relies on aggregating underlying state distance measures. We give several techniques to aggregate state distances and their associated properties. Many existing heuristics exhibit a subset of the properties, but in order to provide a standardized comparison we present several generalizations of planning graph heuristics that are used in a single planner. We compliment our belief state distance estimate framework by also investigating efficient planning graph data structures that incorporate BDDs to compute the most effective heuristics.We developed two planners to serve as test-beds for our investigation. The first, CAltAlt, is a conformant regression planner that uses A* search. The second, POND, is a conditional progression planner that uses AO* search. We show the relative effectiveness of our heuristic techniques within these planners. We also compare the performance of these planners with several state of the art approaches in conditional planning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {35–99},
numpages = {65}
}

@article{10.5555/1622559.1622560,
author = {Halpern, Joseph Y. and Pucella, Riccardo},
title = {A Logic for Reasoning about Evidence},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {We introduce a logic for reasoning about evidence that essentially views evidence as a function from prior beliefs (before making an observation) to posterior beliefs (after making the observation). We provide a sound and complete axiomatization for the logic, and consider the complexity of the decision problem. Although the reasoning in the logic is mainly propositional, we allow variables representing numbers and quantification over them. This expressive power seems necessary to capture important properties of evidence.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–34},
numpages = {34}
}

@article{10.5555/1622543.1622558,
author = {Mailler, Roger and Lesser, Victor R.},
title = {Asynchronous Partial Overlay: A New Algorithm for Solving Distributed Constraint Satisfaction Problems},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {Distributed Constraint Satisfaction (DCSP) has long been considered an important problem in multi-agent systems research. This is because many real-world problems can be represented as constraint satisfaction and these problems often present themselves in a distributed form. In this article, we present a new complete, distributed algorithm called asynchronous partial overlay (APO) for solving DCSPs that is based on a cooperative mediation process. The primary ideas behind this algorithm are that agents, when acting as a mediator, centralize small, relevant portions of the DCSP, that these centralized subproblems overlap, and that agents increase the size of their subproblems along critical paths within the DCSP as the problem solving unfolds. We present empirical evidence that shows that APO outperforms other known, complete DCSP techniques.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {529–576},
numpages = {48}
}

@article{10.5555/1622543.1622557,
author = {Roy, Amitabha},
title = {Fault Tolerant Boolean Satisfiability},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {A δ-model is a satisfying assignment of a Boolean formula for which any small alteration, such as a single bit flip, can be repaired by flips to some small number of other bits, yielding a new satisfying assignment. These satisfying assignments represent robust solutions to optimization problems (e.g., scheduling) where it is possible to recover from unforeseen events (e.g., a resource becoming unavailable). The concept of δ-models was introduced by Ginsberg, Parkes, and Roy (1998), where it was proved that finding δ-models for general Boolean formulas is NP-complete. In this paper, we extend that result by studying the complexity of finding δ-models for classes of Boolean formulas which are known to have polynomial time satisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT, dual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the complexity of finding δ-models, e.g., while 2-SAT and Affine-SAT have polynomial time tests for δ-models, testing whether a Horn-SAT formula has one is NP-complete.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {503–527},
numpages = {25}
}

@article{10.5555/1622543.1622556,
author = {Blum, Ben and Shelton, Christian R. and Koller, Daphne},
title = {A Continuation Method for Nash Equilibria in Structured Games},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {Structured game representations have recently attracted interest as models for multi-agent artificial intelligence scenarios, with rational behavior most commonly characterized by Nash equilibria. This paper presents efficient, exact algorithms for computing Nash equilibria in structured game representations, including both graphical games and multi-agent influence diagrams (MAIDs). The algorithms are derived from a continuation method for normal-form and extensive-form games due to Govindan and Wilson; they follow a trajectory through a space of perturbed games and their equilibria, exploiting game structure through fast computation of the Jacobian of the payoff function. They are theoretically guaranteed to find at least one equilibrium of the game, and may find more. Our approach provides the first efficient algorithm for computing exact equilibria in graphical games with arbitrary topology, and the first algorithm to exploit fine-grained structural properties of MAIDs. Experimental results are presented demonstrating the effectiveness of the algorithms and comparing them to predecessors. The running time of the graphical game algorithm is similar to, and often better than, the running time of previous approximate algorithms. The algorithm for MAIDs can effectively solve games that are much larger than those solvable by previous methods.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {457–502},
numpages = {46}
}

@article{10.5555/1622543.1622555,
author = {Kersting, Kristian and De Raedt, Luc and Raiko, Tapani},
title = {Logical Hidden Markov Models},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models to deal with sequences of structured symbols in the form of logical atoms, rather than flat characters.This note formally introduces LOHMMs and presents solutions to the three central inference problems for LOHMMs: evaluation, most likely hidden state sequence and parameter estimation. The resulting representation and algorithms are experimentally evaluated on problems from the domain of bioinformatics.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {425–456},
numpages = {32}
}

@article{10.5555/1622543.1622554,
author = {Brafman, Ronen I. and Domshlak, Carmel and Shimony, Solomon E.},
title = {On Graphical Modeling of Preference and Importance},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {In recent years, CP-nets have emerged as a useful tool for supporting preference elicitation, reasoning, and representation. CP-nets capture and support reasoning with qualitative conditional preference statements, statements that are relatively natural for users to express. In this paper, we extend the CP-nets formalism to handle another class of very natural qualitative statements one often uses in expressing preferences in daily life - statements of relative importance of attributes. The resulting formalism, TCP-nets, maintains the spirit of CP-nets, in that it remains focused on using only simple and natural preference statements, uses the ceteris paribus semantics, and utilizes a graphical representation of this information to reason about its consistency and to perform, possibly constrained, optimization using it. The extra expressiveness it provides allows us to better model tradeoffs users would like to make, more faithfully representing their preferences.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {389–424},
numpages = {36}
}

@article{10.5555/1622543.1622553,
author = {Gutnik, Gery and Kaminka, Gal A.},
title = {Representing Conversations for Scalable Overhearing},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {Open distributed multi-agent systems are gaining interest in the academic community and in industry. In such open settings, agents are often coordinated using standardized agent conversation protocols. The representation of such protocols (for analysis, validation, monitoring, etc) is an important aspect of multi-agent applications. Recently, Petri nets have been shown to be an interesting approach to such representation, and radically different approaches using Petri nets have been proposed. However, their relative strengths and weaknesses have not been examined. Moreover, their scalability and suitability for different tasks have not been addressed. This paper addresses both these challenges. First, we analyze existing Petri net representations in terms of their scalability and appropriateness for overhearing, an important task in monitoring open multi-agent systems. Then, building on the insights gained, we introduce a novel representation using Colored Petri nets that explicitly represent legal joint conversation states and messages. This representation approach offers significant improvements in scalability and is particularly suitable for overhearing. Furthermore, we show that this new representation offers a comprehensive coverage of all conversation features of FIPA conversation standards. We also present a procedure for transforming AUML conversation protocol diagrams (a standard human-readable representation), to our Colored Petri net representation.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {349–387},
numpages = {39}
}

@article{10.5555/1622543.1622552,
author = {Endriss, Ulle and Maudet, Nicolas and Sadri, Fariba and Toni, Francesca},
title = {Negotiating Socially Optimal Allocations of Resources},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {A multiagent system may be thought of as an artificial society of autonomous software agents and we can apply concepts borrowed from welfare economics and social choice theory to assess the social welfare of such an agent society. In this paper, we study an abstract negotiation framework where agents can agree on multilateral deals to exchange bundles of indivisible resources. We then analyse how these deals affect social welfare for different instances of the basic framework and different interpretations of the concept of social welfare itself. In particular, we show how certain classes of deals are both sufficient and necessary to guarantee that a socially optimal allocation of resources will be reached eventually.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {315–348},
numpages = {34}
}

@article{10.5555/1622543.1622551,
author = {Adjiman, Philippe and Chatalic, Philippe and Goasdou\'{e}, Fran\c{c}ois and Rousset, Marie-Christine and Simon, Laurent},
title = {Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {In a peer-to-peer inference system, each peer can reason locally but can also solicit some of its acquaintances, which are peers sharing part of its vocabulary. In this paper, we consider peer-to-peer inference systems in which the local theory of each peer is a set of propositional clauses defined upon a local vocabulary. An important characteristic of peer-to-peer inference systems is that the global theory (the union of all peer theories) is not known (as opposed to partition-based reasoning systems). The main contribution of this paper is to provide the first consequence finding algorithm in a peer-to-peer setting: DECA. It is anytime and computes consequences gradually from the solicited peer to peers that are more and more distant. We exhibit a sufficient condition on the acquaintance graph of the peer-to-peer inference system for guaranteeing the completeness of this algorithm. Another important contribution is to apply this general distributed reasoning setting to the setting of the Semantic Web through the SOMEWHERE semantic peer-to-peer data management system. The last contribution of this paper is to provide an experimental analysis of the scalability of the peer-to-peer infrastructure that we propose, on large networks of 1000 peers.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {269–314},
numpages = {46}
}

@article{10.5555/1622543.1622550,
author = {Haslum, Patrik},
title = {Improving Heuristics through Relaxed Search: An Analysis of TP4 and HSP<sub>a</sub>* in the 2004 Planning Competition},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {The hm admissible heuristics for (sequential and temporal) regression planning are defined by a parameterized relaxation of the optimal cost function in the regression search space, where the parameter m offers a trade-off between the accuracy and computational cost of the heuristic. Existing methods for computing the hm heuristic require time exponential in m, limiting them to small values (m ≤ 2). The hm heuristic can also be viewed as the optimal cost funciton in a relaxation of the search space: this paper presents relaxed search, a method for computing this function partially by searching in the relaxed space. The relaxed search method, because it computes hm only partially, is computationally cheaper and therefore usable for higher values of m. The (complete) h2 heuristic is combined with partial hm heuristics, for m = 3,..., computed by relaxed search, resulting in a more accurate heuristic.This use of the relaxed search method to improve on the h2 heuristic is evaluated by comparing two optimal temporal planners: TP4, which does not use it, and HSP*a, which uses it but is otherwise identical to TP4. The comparison is made on the domains used in the 2004 International Planning Competition, in which both planners participated. Relaxed search is found to be cost effective in some of these domains, but not all. Analysis reveals a characterization of the domains in which relaxed search can be expected to be cost effective, in terms of two measures on the original and relaxed search spaces. In the domains where relaxed search is cost effective, expanding small states is computationally cheaper than expanding large states and small states tend to have small successor states.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {233–267},
numpages = {35}
}

@article{10.5555/1622543.1622549,
author = {Gerevini, Alfonso and Saetti, Alessandro and Serina, Ivan},
title = {An Approach to Temporal Planning and Scheduling in Domains with Predictable Exogenous Events},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {The treatment of exogenous events in planning is practically important in many real-world domains where the preconditions of certain plan actions are affected by such events. In this paper we focus on planning in temporal domains with exogenous events that happen at known times, imposing the constraint that certain actions in the plan must be executed during some predefined time windows. When actions have durations, handling such temporal constraints adds an extra difficulty to planning. We propose an approach to planning in these domains which integrates constraint-based temporal reasoning into a graph-based planning framework using local search. Our techniques are implemented in a planner that took part in the 4th International Planning Competition (IPC-4). A statistical analysis of the results of IPC-4 demonstrates the effectiveness of our approach in terms of both CPU-time and plan quality. Additional experiments show the good performance of the temporal reasoning techniques integrated into our planner.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {187–231},
numpages = {45}
}

@article{10.5555/1622543.1622548,
author = {Pullan, Wayne and Hoos, Holger H.},
title = {Dynamic Local Search for the Maximum Clique Problem},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we introduce DLS-MC, a new stochastic local search algorithm for the maximum clique problem. DLS-MC alternates between phases of iterative improvement, during which suitable vertices are added to the current clique, and plateau search, during which vertices of the current clique are swapped with vertices not contained in the current clique. The selection of vertices is solely based on vertex penalties that are dynamically adjusted during the search, and a perturbation mechanism is used to overcome search stagnation. The behaviour of DLS-MC is controlled by a single parameter, penalty delay, which controls the frequency at which vertex penalties are reduced. We show empirically that DLSMC achieves substantial performance improvements over state-of-the-art algorithms for the maximum clique problem over a large range of the commonly used DIMACS benchmark instances.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {159–185},
numpages = {27}
}

@article{10.5555/1622543.1622547,
author = {Bulitko, Vadim and Lee, Greg},
title = {Learning in Real-Time Search: A Unifying Framework},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {Real-time search methods are suited for tasks in which the agent is interacting with an initially unknown environment in real time. In such simultaneous planning and learning problems, the agent has to select its actions in a limited amount of time, while sensing only a local part of the environment centered at the agent's current location. Real-time heuristic search agents select actions using a limited lookahead search and evaluating the frontier states with a heuristic function. Over repeated experiences, they refine heuristic values of states to avoid infinite loops and to converge to better solutions. The wide spread of such settings in autonomous software and hardware agents has led to an explosion of real-time search algorithms over the last two decades. Not only is a potential user confronted with a hodgepodge of algorithms, but he also faces the choice of control parameters they use. In this paper we address both problems. The first contribution is an introduction of a simple three-parameter framework (named LRTS) which extracts the core ideas behind many existing algorithms. We then prove that LRTA*, Ε-LRTA* , SLA*, and γ-Trap algorithms are special cases of our framework. Thus, they are unified and extended with additional features. Second, we prove completeness and convergence of any algorithm covered by the LRTS framework. Third, we prove several upper-bounds relating the control parameters and solution quality. Finally, we analyze the influence of the three control parameters empirically in the realistic scalable domains of real-time navigation on initially unknown maps from a commercial role-playing game as well as routing in ad hoc sensor networks.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {119–157},
numpages = {39}
}

@article{10.5555/1622543.1622546,
author = {Fern, Alan and Yoon, Sungwook and Givan, Robert},
title = {Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {We study an approach to policy selection for large relational Markov Decision Processes (MDPs). We consider a variant of approximate policy iteration (API) that replaces the usual value-function learning step with a learning step in policy space. This is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions, which is often the case for the relational MDPs we are interested in. In order to apply API to such problems, we introduce a relational policy language and corresponding learner. In addition, we introduce a new bootstrapping routine for goal-based planning domains, based on random walks. Such bootstrapping is necessary for many large relational MDPs, where reward is extremely sparse, as API is ineffective in such domains when initialized with an uninformed policy. Our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational MDPs. The experiments also point to some limitations of our approach, suggesting future work.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {75–118},
numpages = {44}
}

@article{10.5555/1622543.1622545,
author = {Thi\'{e}baux, Sylvie and Gretton, Charles and Slaney, John and Price, David and Kabanza, Froduald},
title = {Decision-Theoretic Planning with Non-Markovian Rewards},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {A decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-Markovian rewards (NMRDP). In decision-theoretic planning, where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states, NMRDPs form a more natural model than the commonly adopted fully Markovian decision process (MDP) model. While the more tractable solution methods developed for MDPs do not directly apply in the presence of non-Markovian rewards, a number of solution methods for NMRDPs have been proposed in the literature. These all exploit a compact specification of the non-Markovian reward function in temporal logic, to automatically translate the NMRDP into an equivalent MDP which is solved using efficient MDP solution methods. This paper presents NMRDPP(Non-Markovian Reward Decision Process Planner), a software platform for the development and experimentation of methods for decision-theoretic planning with non-Markovian rewards. The current version of NMRDPP implements, under a single interface, a family of methods based on existing as well as new approaches which we describe in detail. These include dynamic programming, heuristic search, and structured methods. Using NMRDPP, we compare the methods and identify certain problem features that affect their performance. NMRDPP's treatment of non-Markovian rewards is inspired by the treatment of domain-specific search control knowledge in the TLPlan planner, which it incorporates as a special case. In the First International Probabilistic Planning Competition, NMRDPP was able to compete and perform well in both the domain-independent and hand-coded tracks, using search control knowledge in the latter.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {17–74},
numpages = {58}
}

@article{10.5555/1622543.1622544,
author = {Onder, Nilufer and Whelan, Garrett C. and Li, Li},
title = {Engineering a Conformant Probabilistic Planner},
year = {2006},
issue_date = {January 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {25},
number = {1},
issn = {1076-9757},
abstract = {We present a partial-order, conformant, probabilistic planner, Probapop which competed in the blind track of the Probabilistic Planning Competition in IPC-4. We explain how we adapt distance based heuristics for use with probabilistic domains. Probapop also incorporates heuristics based on probability of success. We explain the successes and diefficulties encountered during the design and implementation of Probapop.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–15},
numpages = {15}
}

@article{10.5555/1622519.1622542,
author = {Bonet, Blai and Geffner, H\'{e}ctor},
title = {MGPT: A Probabilistic Planner Based on Heuristic Search},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {We describe the version of the GPT planner used in the probabilistic track of the 4th International Planning Competition (IPC-4). This version, called mGPT, solves Markov Decision Processes specified in the PPDDL language by extracting and using different classes of lower bounds along with various heuristic-search algorithms. The lower bounds are extracted from deterministic relaxations where the alternative probabilistic effects of an action are mapped into different, independent, deterministic actions. The heuristic-search algorithms use these lower bounds for focusing the updates and delivering a consistent value function over all states reachable from the initial state and the greedy policy.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {933–944},
numpages = {12}
}

@article{10.5555/1622519.1622541,
author = {van den Briel, Menkes H. L. and Kambhampati, Subbarao},
title = {Optiplan: Unifying IP-Based and Graph-Based Planning},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {The Optiplan planning system is the first integer programming-based planner that successfully participated in the international planning competition. This engineering note describes the architecture of Optiplan and provides the integer programming formulation that enabled it to perform reasonably well in the competition. We also touch upon some recent developments that make integer programming encodings significantly more competitive.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {919–931},
numpages = {13}
}

@article{10.5555/1622519.1622540,
author = {Jaeger, Manfred},
title = {Ignorability in Statistical and Probabilistic Inference},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a "static" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Gr\"{u}nwald and Halpern. We then turn to a "procedural" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {889–917},
numpages = {29}
}

@article{10.5555/1622519.1622539,
author = {Younes, H\r{a}kan L. S. and Littman, Michael L. and Weissman, David and Asmuth, John},
title = {The First Probabilistic Track of the International Planning Competition},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {The 2004 International Planning Competition, IPC-4, included a probabilistic planning track for the first time. We describe the new domain specification language we created for the track, our evaluation methodology, the competition domains we developed, and the results of the participating teams.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {851–887},
numpages = {37}
}

@article{10.5555/1622519.1622538,
author = {Beetz, Michael and Grosskreutz, Henrik},
title = {Probabilistic Hybrid Action Models for Predicting Concurrent Percept-Driven Robot Behavior},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic causal model for predicting the behavior generated by modern percept-driven robot plans. PHAMs represent aspects of robot behavior that cannot be represented by most action models used in AI planning: the temporal structure of continuous control processes, their non-deterministic effects, several modes of their interferences, and the achievement of triggering conditions in closed-loop robot plans.The main contributions of this article are: (1) PHAMs, a model of concurrent percept-driven behavior, its formalization, and proofs that the model generates probably, qualitatively accurate predictions; and (2) a resource-efficient inference method for PHAMs based on sampling projections from probabilistic action models and state descriptions. We show how PHAMs can be applied to planning the course of action of an autonomous robot office courier based on analytical and experimental results.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {799–849},
numpages = {51}
}

@article{10.5555/1622519.1622537,
author = {Hoffmann, J\"{o}rg},
title = {Where "Ignoring Delete Lists" Works: Local Search Topology in Planning Benchmarks},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for.In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS and ADL benchmark domains, including all examples used in the first four international planning competitions. We prove connections between domain structure and local search topology - heuristic cost surface properties - under an idealized version of the heuristic functions used in modern planners. The idealized heuristic function is called h+, and differs from the practically used functions in that it returns the length of an optimal relaxed plan, which is NP-hard to compute. We identify several key characteristics of the topology under h+, concerning the existence/non-existence of unrecognized dead ends, as well as the existence/non-existence of constant upper bounds on the difficulty of escaping local minima and benches. These distinctions divide the (set of all) planning domains into a taxonomy of classes of varying h+ topology. As it turns out, many of the 30 investigated domains lie in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes where FF's search algorithm, provided with h+, is a polynomial solving mechanism.We also present results relating h+ to its approximation as implemented in FF. The behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h+ are largely inherited by the approximation. The overall investigation gives a rare example of a successful analysis of the connections between typical-case problem structure, and search performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some preliminary steps we made into that direction.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {685–758},
numpages = {74}
}

@article{10.5555/1622519.1622536,
author = {Samaras, Nikolaos and Stergiou, Kostas},
title = {Binary Encodings of Non-Binary Constraint Satisfaction Problems: Algorithms and Experimental Results},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {A non-binary Constraint Satisfaction Problem (CSP) can be solved directly using extended versions of binary techniques. Alternatively, the non-binary problem can be translated into an equivalent binary one. In this case, it is generally accepted that the translated problem can be solved by applying well-established techniques for binary CSPs. In this paper we evaluate the applicability of the latter approach. We demonstrate that the use of standard techniques for binary CSPs in the encodings of non-binary problems is problematic and results in models that are very rarely competitive with the non-binary representation. To overcome this, we propose specialized arc consistency and search algorithms for binary encodings, and we evaluate them theoretically and empirically. We consider three binary representations; the hidden variable encoding, the dual encoding, and the double encoding. Theoretical and empirical results show that, for certain classes of non-binary constraints, binary encodings are a competitive option, and in many cases, a better one than the non-binary representation.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {641–684},
numpages = {44}
}

@article{10.5555/1622519.1622535,
author = {Achlioptas, Dimitris and Jia, Haixia and Moore, Cristopher},
title = {Hiding Satisfying Assignments: Two Are Better than One},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k- SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, for a number of different algorithms, A acts as a stronger and stronger attractor as the formula's density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this "symmetrization" the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {623–639},
numpages = {17}
}

@article{10.5555/1622519.1622534,
author = {Botea, Adi and Enzenberger, Markus and M\"{u}ller, Martin and Schaeffer, Jonathan},
title = {Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {Despite recent progress in AI planning, many benchmarks remain challenging for current planners. In many domains, the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial PDDL formulation. In this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances. Our methods share a common four-step strategy. First, a domain is analyzed and structural information is extracted, then macro-operators are generated based on the previously discovered structure. A filtering and ranking procedure selects the most useful macro-operators. Finally, the selected macros are used to speed up future searches.We have successfully used such an approach in the fourth international planning competition IPC-4. Our system, Macro-FF, extends Hoffmann's state-of-the-art planner FF 2.3 with support for two kinds of macro-operators, and with engineering enhancements. We demonstrate the effectiveness of our ideas on benchmarks from international planning competitions. Our results indicate a large reduction in search effort in those complex domains where structural information can be inferred.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {581–621},
numpages = {41}
}

@article{10.5555/1622519.1622533,
author = {Hoffmann, J\"{o}rg and Edelkamp, Stefan},
title = {The Deterministic Part of IPC-4: An Overview},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {We provide an overview of the organization and results of the deterministic part of the 4th International Planning Competition, i.e., of the part concerned with evaluating systems doing deterministic planning. IPC-4 attracted even more competing systems than its already large predecessors, and the competition event was revised in several important respects. After giving an introduction to the IPC, we briefly explain the main differences between the deterministic part of IPC-4 and its predecessors. We then introduce formally the language used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed initial literals. We list the competing systems and overview the results of the competition. The entire set of data is far too large to be presented in full. We provide a detailed summary; the complete data is available in an online appendix. We explain how we awarded the competition prizes.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {519–579},
numpages = {61}
}

@article{10.5555/1622519.1622532,
author = {Vo, Quoc Bao and Foo, Norman Y.},
title = {Reasoning about Action: An Argumentation-Theoretic Approach},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {We present a uniform non-monotonic solution to the problems of reasoning about action on the basis of an argumentation-theoretic approach. Our theory is provably correct relative to a sensible minimisation policy introduced on top of a temporal propositional logic. Sophisticated problem domains can be formalised in our framework. As much attention of researchers in the field has been paid to the traditional and basic problems in reasoning about actions such as the frame, the qualification and the ramification problems, approaches to these problems within our formalisation lie at heart of the expositions presented in this paper.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {465–518},
numpages = {54}
}

@article{10.5555/1622519.1622531,
author = {Dutta, Partha S. and Jennings, Nicholas R. and Moreau, Luc},
title = {Cooperative Information Sharing to Improve Distributed Learning in Multi-Agent Systems},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {Effective coordination of agents' actions in partially-observable domains is a major challenge of multi-agent systems research. To address this, many researchers have developed techniques that allow the agents to make decisions based on estimates of the states and actions of other agents that are typically learnt using some form of machine learning algorithm. Nevertheless, many of these approaches fail to provide an actual means by which the necessary information is made available so that the estimates can be learnt. To this end, we argue that cooperative communication of state information between agents is one such mechanism. However, in a dynamically changing environment, the accuracy and timeliness of this communicated information determine the fidelity of the learned estimates and the usefulness of the actions taken based on these. Given this, we propose a novel information-sharing protocol, post-task-completion sharing, for the distribution of state information. vVe then show, through a formal analysis, the improvement in the quality of estimates produced using our strategy over the widely used protocol of sharing information between nearest neighbours. Moreover, communication heuristics designed around our information-sharing principle are subjected to empirical evaluation along with other benchmark strategies (including Littman's Q-routing and Stone's TPOT-RL) in a simulated call-routing application. These studies, conducted across a range of environmental settings, show that, compared to the different benchmarks used, our strategy generates an improvement of up to 60% in the call connection rate; of more than 1000% in the ability to connect long-distance calls; and incurs as low as 0.25 of the message overhead.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {407–463},
numpages = {57}
}

@article{10.5555/1622519.1622530,
author = {Gottlob, Georg and Greco, Gianluigi and Scarcello, Francesco},
title = {Pure Nash Equilibria: Hard and Easy Games},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {We investigate complexity issues related to pure Nash equilibria of strategic games. We show that, even in very restrictive settings, determining whether a game has a pure Nash Equilibrium is NP-hard, while deciding whether a game has a strong Nash equilibrium is Σ2P-complete. We then study practically relevant restrictions that lower the complexity. In particular, we are interested in quantitative and qualitative restrictions of the way each player's payoff depends on moves of other players. We say that a game has small neighborhood if the utility function for each player depends only on (the actions of) a logarithmically small number of other players. The dependency structure of a game G can be expressed by a graph G(G) or by a hypergraph H(G). By relating Nash equilibrium problems to constraint satisfaction problems (CSPs), we show that if G has small neighborhood and if H(G) has bounded hypertree width (or if G(G) has bounded treewidth), then finding pure Nash and Pareto equilibria is feasible in polynomial time. If the game is graphical, then these problems are LOGCFL-complete and thus in the class NC2 of highly parallelizable problems.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {357–406},
numpages = {50}
}

@article{10.5555/1622519.1622529,
author = {Khardon, Roni and Roth, Dan and Servedio, Rocco A.},
title = {Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm.We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions.We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnow's behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {341–356},
numpages = {16}
}

@article{10.5555/1622519.1622528,
author = {Cimiano, Philipp and Hotho, Andreas and Staab, Steffen},
title = {Learning Concept Hierarchies from Text Corpora Using Formal Concept Analysis},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information. We follow Harris' distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser. On the basis of this context information, FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy. The approach is evaluated by comparing the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism and finance. We also directly compare our approach with hierarchical agglomerative clustering as well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {305–339},
numpages = {35}
}

@article{10.5555/1622519.1622527,
author = {Bayer-Zubek, Valentina and Dietterich, Thomas G.},
title = {Integrating Learning from Examples into the Search for Diagnostic Policies},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {This paper studies the problem of learning diagnostic policies from training examples. A diagnostic policy is a complete description of the decision-making actions of a diagnostician (i.e., tests followed by a diagnostic decision) for all possible combinations of test results. An optimal diagnostic policy is one that minimizes the expected total cost, which is the sum of measurement costs and misdiagnosis costs. In most diagnostic settings, there is a tradeoff between these two kinds of costs.This paper formalizes diagnostic decision making as a Markov Decision Process (MDP). The paper introduces a new family of systematic search algorithms based on the AO* algorithm to solve this MDP. To make AO* efficient, the paper describes an admissible heuristic that enables AO* to prune large parts of the search space. The paper also introduces several greedy algorithms including some improvements over previously-published methods. The paper then addresses the question of learning diagnostic policies from examples. When the probabilities of diseases and test results are computed from training data, there is a great danger of overfitting. To reduce overfitting, regularizers are integrated into the search algorithms. Finally, the paper compares the proposed methods on five benchmark diagnostic data sets. The studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods. In addition, the studies show that for training sets of realistic size, the systematic search algorithms are practical on today's desktop computers.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {263–303},
numpages = {41}
}

@article{10.5555/1622519.1622526,
author = {Watson, Jean-Paul and Whitley, L. Darrell and Howe, Adele E.},
title = {Linking Search Space Structure, Run-Time Dynamics, and Problem Difficulty: A Step toward Demystifying Tabu Search},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {Tabu search is one of the most effective heuristics for locating high-quality solutions to a diverse array of NP-hard combinatorial optimization problems. Despite the widespread success of tabu search, researchers have a poor understanding of many key theoretical aspects of this algorithm, including models of the high-level run-time dynamics and identification of those search space features that influence problem difficulty. We consider these questions in the context of the job-shop scheduling problem (JSP), a domain where tabu search algorithms have been shown to be remarkably effective. Previously, we demonstrated that the mean distance between random local optima and the nearest optimal solution is highly correlated with problem difficulty for a well-known tabu search algorithm for the JSP introduced by Taillard. In this paper, we discuss various shortcomings of this measure and develop a new model of problem difficulty that corrects these deficiencies. We show that Taillard's algorithm can be modeled with high fidelity as a simple variant of a straightforward random walk. The random walk model accounts for nearly all of the variability in the cost required to locate both optimal and sub-optimal solutions to random JSPs, and provides an explanation for differences in the difficulty of random versus structured JSPs. Finally, we discuss and empirically substantiate two novel predictions regarding tabu search algorithm behavior. First, the method for constructing the initial solution is highly unlikely to impact the performance of tabu search. Second, tabu tenure should be selected to be as small as possible while simultaneously avoiding search stagnation; values larger than necessary lead to significant degradations in performance.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {221–261},
numpages = {41}
}

@article{10.5555/1622519.1622525,
author = {Spaan, Matthijs T. J. and Vlassis, Nikos},
title = {Perseus: Randomized Point-Based Value Iteration for POMDPs},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {Partially observable Markov decision processes (POMDPs) form an attractive and principled framework for agent planning under uncertainty. Point-based approximate techniques for POMDPs compute a policy based on a finite set of points collected in advance from the agent's belief space. We present a randomized point-based value iteration algorithm called PERSEUS. The algorithm performs approximate value backup stages, ensuring that in each backup stage the value of each point in the belief set is improved; the key observation is that a single backup may improve the value of many belief points. Contrary to other point-based methods, PERSEUS backs up only a (randomly selected) subset of points in the belief set, sufficient for improving the value of each belief point in the set. We show how the same idea can be extended to dealing with continuous action spaces. Experimental results show the potential of PERSEUS in large scale POMDP problems.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {195–220},
numpages = {26}
}

@article{10.5555/1622519.1622524,
author = {Jordan, Pamela W. and Walker, Marilyn A.},
title = {Learning Content Selection Rules for Generating Object Descriptions in Dialogue},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {A fundamental requirement of any task-oriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain. The subproblem of content selection for object descriptions in task-oriented dialogue has been the focus of much previous work and a large number of models have been proposed. In this paper, we use the annotated COCONUT corpus of task-oriented design dialogues to develop feature sets based on Dale and Reiter's (1995) incremental model, Brennan and Clark's (1996) conceptual pact model, and Jordan's (2000b) intentional influences model, and use these feature sets in a machine learning experiment to automatically learn a model of content selection for object descriptions. Since Dale and Reiter's model requires a representation of discourse structure, the corpus annotations are used to derive a representation based on Grosz and Sidner's (1986) theory of the intentional structure of discourse, as well as two very simple representations of discourse structure based purely on recency. We then apply the rule-induction program RIPPER to train and test the content selection component of an object description generator on a set of 393 object descriptions from the corpus. To our knowledge, this is the first reported experiment of a trainable content selection component for object description generation in dialogue. Three separate content selection models that are based on the three theoretical models, all independently achieve accuracies significantly above the MAJORITY CLASS baseline (17%) on unseen test data, with the intentional influences model (42.4%) performing significantly better than either the incremental model (30.4%) or the conceptual pact model (28.9%). But the best performing models combine all the feature sets, achieving accuracies near 60%. Surprisingly, a simple recency-based representation of discourse structure does as well as one based on intentional structure. To our knowledge, this is also the first empirical comparison of a representation of Grosz and Sidner's model of discourse structure with a simpler model for any generation task.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {157–194},
numpages = {38}
}

@article{10.5555/1622519.1622523,
author = {Hawkins, Peter and Lagoon, Vitaly and Stuckey, Peter J.},
title = {Solving Set Constraint Satisfaction Problems Using ROBDDs},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {In this paper we present a new approach to modeling finite set domain constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using ROBDDs. We demonstrate that the ROBDD-based approach provides unprecedented flexibility in modeling constraint satisfaction problems, leading to performance improvements. We also show that the ROBDD-based modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner. Since domain propagation is not always practical, we also show how to incorporate less strict consistency notions into the ROBDD framework, such as set bounds, cardinality bounds and lexicographic bounds consistency. Finally, we present experimental results that demonstrate the ROBDD-based solver performs better than various more conventional constraint solvers on several standard set constraint problems.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {109–156},
numpages = {48}
}

@article{10.5555/1622519.1622522,
author = {Geibel, Peter and Wysotzki, Fritz},
title = {Risk-Sensitive Reinforcement Learning Applied to Control under Constraints},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {81–108},
numpages = {28}
}

@article{10.5555/1622519.1622521,
author = {Gmytrasiewicz, Piotr J. and Doshi, Prashant},
title = {A Framework for Sequential Planning in Multi-Agent Settings},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {This paper extends the framework of partially observable Markov decision processes (POMDPs) to multi-agent settings by incorporating the notion of agent models into the state space. Agents maintain beliefs over physical states of the environment and over models of other agents, and they use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions. Models of other agents may include their belief states and are related to agent types considered in games of incomplete information. We express the agents' autonomy by postulating that their models are not directly manipulable or observable by other agents. We show that important properties of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework. Our approach complements a more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm. We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors. We do so at the cost of having to represent, process and continuously revise models of other agents. Since the agent's beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable. However, approximate belief updates and approximately optimal plans are computable. We illustrate our framework using a simple application domain, and we show examples of belief updates and value functions.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {49–79},
numpages = {31}
}

@article{10.5555/1622519.1622520,
author = {Ortiz-Boyer, Domingo and Herv\'{a}Mart\'{\i}nez, C\'{e}sar and Garc\'{\i}a-Pedrajas, Nicol\'{a}s},
title = {CIXL2: A Crossover Operator for Evolutionary Algorithms Based on Population Features},
year = {2005},
issue_date = {July 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {24},
number = {1},
issn = {1076-9757},
abstract = {In this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions. The operator is based on the theoretical distribution of the values of the genes of the best individuals in the population. The proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring. Our aim is the optimization of the balance between exploration and exploitation in the search process.In order to test the efficiency and robustness of this crossover, we have used a set of functions to be optimized with regard to different criteria, such as, multimodality, separability, regularity and epistasis. With this set of functions we can extract conclusions in function of the problem at hand. We analyze the results using ANOVA and multiple comparison statistical tests.As an example of how our crossover can be used to solve artificial intelligence problems, we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks. The results obtained are above the performance of standard methods.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–48},
numpages = {48}
}

@article{10.5555/1622503.1622518,
author = {Lutz, Carsten and Areces, Carlos and Horrocks, Ian and Sattler, Ulrike},
title = {Keys, Nominals, and Concrete Domains},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Many description logics (DLs) combine knowledge representation on an abstract, logical level with an interface to "concrete" domains like numbers and strings with built-in predicates such as &lt;, +, and prefix-of. These hybrid DLs have turned out to be useful in several application areas, such as reasoning about conceptual database models. We propose to further extend such DLs with key constraints that allow the expression of statements like "US citizens are uniquely identified by their social security number". Based on this idea, we introduce a number of natural description logics and perform a detailed analysis of their decidability and computational complexity. It turns out that naive extensions with key constraints easily lead to undecidability, whereas more careful extensions yield NExp-Time-complete DLs for a variety of useful concrete domains.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {667–726},
numpages = {60}
}

@article{10.5555/1622503.1622517,
author = {Barish, Greg and Knoblock, Craig A.},
title = {An Expressive Language and Efficient Execution System for Software Agents},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Software agents can be used to automate many of the tedious, time-consuming information processing tasks that humans currently have to complete manually. However, to do so, agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks. In addition, since these tasks can require integrating multiple sources of remote information -- typically, a slow, I/O-bound process -- it is desirable to make execution as efficient as possible. To address both of these needs, we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans. The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans (for modularity) and recursion (for indeterminate looping). The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime. We have implemented both the language and executor in a system called THESEUS. Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial (von Neumann) as well as nonstreaming dataflow-style execution that existing software and robot agent execution systems currently support. In addition, we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines. Finally, we demonstrate that the increased expressivity of our plan language does not hamper performance; specifically, we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a state-of-the-art streaming-dataflow network query engine.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {625–666},
numpages = {42}
}

@article{10.5555/1622503.1622516,
author = {Schroedl, Stefan},
title = {An Improved Search Algorithm for Optimal Multiple-Sequence Alignment},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance.In this paper, we first review two previous, complementary lines of research. Based on Hirschberg's algorithm, Dynamic Programming needs O(kNk-1) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic. However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost. Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list.In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kNk-1), as in Dynamic Programming. The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue. With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected.In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets. This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain.Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations.Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {587–623},
numpages = {37}
}

@article{10.5555/1622503.1622515,
author = {Zimmerman, Terry and Kambhampati, Subbarao},
title = {Using Memory to Transform Search on the Planning Graph},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {The Graphplan algorithm for generating optimal make-span plans containing parallel sets of actions remains one of the most effective ways to generate such plans. However, despite enhancements on a range of fronts, the approach is currently dominated in terms of speed, by state space planners that employ distance-based heuristics to quickly generate serial plans. We report on a family of strategies that employ available memory to construct a search trace so as to learn from various aspects of Graphplan's iterative search episodes in order to expedite search in subsequent episodes. The planning approaches can be partitioned into two classes according to the type and extent of search experience captured in the trace. The planners using the more aggressive tracing method are able to avoid much of Graphplan's redundant search effort, while planners in the second class trade off this aspect in favor of a much higher degree of freedom than Graphplan in traversing the space of 'states' generated during regression search on the planning graph. The tactic favored by the second approach, exploiting the search trace to transform the depth-first, IDA* nature of Graphplan's search into an iterative state space view, is shown to be the more powerful. We demonstrate that distance-based, state space heuristics can be adapted to informed traversal of the search trace used by the second class of planners and develop an augmentation targeted specifically at planning graph search. Guided by such a heuristic, the step-optimal version of the planner in this class clearly dominates even a highly enhanced version of Graphplan. By adopting beam search on the search trace we then show that virtually optimal parallel plans can be generated at speeds quite competitive with a modern heuristic state space planner.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {533–585},
numpages = {53}
}

@article{10.5555/1622503.1622514,
author = {Dixon, Heidi E. and Ginsberg, Matthew L. and Hofer, David and Luks, Eugene M. and Parkes, Andrew J.},
title = {Generalizing Boolean Satisfiability III: Implementation},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {This is the third of three papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high-performance solvers. The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal has been to define a representation in which this structure is apparent and can be exploited to improve computational performance. The first paper surveyed existing work that (knowingly or not) exploited problem structure to improve the performance of satisfiability engines, and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular Boolean theory. We conclude the series by discussing the techniques needed to implement our ideas, and by reporting on their performance on a variety of problem instances.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {441–531},
numpages = {91}
}

@article{10.5555/1622503.1622513,
author = {Larrosa, Javier and Morancho, Enric and Niso, David},
title = {On the Practical Use of Variable Elimination in Constraint Optimization Problems: 'still-Life' as a Case Study},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Variable elimination is a general technique for constraint processing. It is often discarded because of its high space complexity. However, it can be extremely useful when combined with other techniques. In this paper we study the applicability of variable elimination to the challenging problem of finding still-lifes. We illustrate several alternatives: variable elimination as a stand-alone algorithm, interleaved with search, and as a source of good quality lower bounds. We show that these techniques are the best known option both theoretically and empirically. In our experiments we have been able to solve the n = 20 instance, which is far beyond reach with alternative approaches.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {421–440},
numpages = {20}
}

@article{10.5555/1622503.1622512,
author = {Nair, Ranjit and Tambe, Milind},
title = {Hybrid BDI-POMDP Framework for Multiagent Teaming},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Many current large-scale multiagent team implementations can be characterized as following the "belief-desire-intention" (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance.Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps in such incomplete policies by searching for the best role allocation. Our second key contribution is a novel decomposition technique to further improve RMTDP policy search efficiency. Even though limited to searching role allocations, there are still combinatorially many role allocations, and evaluating each in RMTDP to identify the best is extremely difficult. Our decomposition technique exploits the structure in the BDI team plans to significantly prune the search space of role allocations. Our third key contribution is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal simulation and RoboCupRescue disaster rescue simulation.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {367–420},
numpages = {54}
}

@article{10.5555/1622503.1622511,
author = {Chawla, Nitesh V. and Karakoulas, Grigoris},
title = {Learning from Labeled and Unlabeled Data: An Empirical Study across Techniques and Domains},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {There has been increased interest in devising learning techniques that combine unlabeled data with labeled data -- i.e. semi-supervised learning. However, to the best of our knowledge, no study has been performed across various techniques and different types and amounts of labeled and unlabeled data. Moreover, most of the published work on semi-supervised learning techniques assumes that the labeled and unlabeled data come from the same distribution. It is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different. Not correcting for such bias can result in biased function approximation with potentially poor performance. In this paper, we present an empirical study of various semi-supervised learning techniques on a variety of datasets. We attempt to answer various questions such as the effect of independence or relevance amongst features, the effect of the size of the labeled and unlabeled sets and the effect of noise. We also investigate the impact of sample-selection bias on the semi -supervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {331–366},
numpages = {36}
}

@article{10.5555/1622503.1622510,
author = {Montoyo, Andres and Su\'{a}rez, Armando and Rigau, German and Palomar, Manuel},
title = {Combining Knowledge- and Corpus-Based Word-Sense-Disambiguation Methods},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {In this paper we concentrate on the resolution of the lexical ambiguity that arises when a given word has several different meanings. This specific task is commonly referred to as word sense disambiguation (WSD). The task of WSD consists of assigning the correct sense to words using an electronic dictionary as the source of word definitions. We present two WSD methods based on two main methodological approaches in this research area: a knowledge-based method and a corpus-based method. Our hypothesis is that word-sense disambiguation requires several knowledge sources in order to solve the semantic ambiguity of the words. These sources can be of different kinds-- for example, syntagmatic, paradigmatic or statistical information. Our approach combines various sources of knowledge, through combinations of the two WSD methods mentioned above. Mainly, the paper concentrates on how to combine these methods and sources of information in order to achieve good results in the disambiguation. Finally, this paper presents a comprehensive study and experimental work on evaluation of the methods and their combinations.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {299–330},
numpages = {32}
}

@article{10.5555/1622503.1622509,
author = {Cayrol, Claudette and Lagasquie-Schiex, Marie-Christine},
title = {Graduality in Argumentation},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Argumentation is based on the exchange and valuation of interacting arguments, followed by the selection of the most acceptable of them (for example, in order to take a decision, to make a choice). Starting from the framework proposed by Dung in 1995, our purpose is to introduce "graduality" in the selection of the best arguments, i.e. to be able to partition the set of the arguments in more than the two usual subsets of "selected" and "non-selected" arguments in order to represent different levels of selection. Our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers. First, we discuss general principles underlying a "gradual" valuation of arguments based on their interactions. Following these principles, we define several valuation models for an abstract argumentation system. Then, we introduce "graduality" in the concept of acceptability of arguments. We propose new acceptability classes and a refinement of existing classes taking advantage of an available "gradual" valuation.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {245–297},
numpages = {53}
}

@article{10.5555/1622503.1622508,
author = {Gabelaia, David and Kontchakov, Roman and Kurucz, Agi and Wolter, Frank and Zakharyaschev, Michael},
title = {Combining Spatial and Temporal Logics: Expressiveness vs. Complexity},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we construct and investigate a hierarchy of spatio-temporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic PT L, the spatial logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a clear picture of the trade-off between expressiveness and 'computational realisability' within the hierarchy. We demonstrate how different combining principles as well as spatial and temporal primitives can produce NP-, PSPACE-, EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out of components that are at most NP- or PSPACE-complete.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {167–243},
numpages = {77}
}

@article{10.5555/1622503.1622507,
author = {Zhang, Weihong and Zhang, Nevin L.},
title = {Restricted Value Iteration: Theory and Algorithms},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Value iteration is a popular algorithm for finding near optimal policies for POMDPs. It is ineffcient due to the need to account for the entire belief space, which necessitates the solution of large numbers of linear programs. In this paper, we study value iteration restricted to belief subsets. We show that, together with properly chosen belief subsets, restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time. We also apply restricted value iteration to two interesting classes of POMDPs, namely informative POMDPs and near-discernible POMDPs.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {123–165},
numpages = {43}
}

@article{10.5555/1622503.1622506,
author = {Porta, Josep M. and Celaya, Enric},
title = {Reinforcement Learning for Agents with Many Sensors and Actuators Acting in Categorizable Environments},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform parallel actions using many actuators as is the case in complex autonomous robots. We argue that reinforcement learning can only be successfully applied to this case if strong assumptions are made on the characteristics of the environment in which the learning is performed, so that the relevant sensor readings and motor commands can be readily identified. The introduction of such assumptions leads to strongly-biased learning systems that can eventually lose the generality of traditional reinforcement-learning algorithms.In this line, we observe that, in realistic situations, the reward received by the robot depends only on a reduced subset of all the executed actions and that only a reduced subset of the sensor inputs (possibly different in each situation and for each action) are relevant to predict the reward. We formalize this property in the so called categorizability assumption and we present an algorithm that takes advantage of the categorizability of the environment, allowing a decrease in the learning time with respect to existing reinforcement-learning algorithms. Results of the application of the algorithm to a couple of simulated realistic-] robotic problems (landmark-based navigation and the six-legged robot gait generation) are reported to validate our approach and to compare it to existing flat and generalization-based reinforcement-learning approaches.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {79–122},
numpages = {44}
}

@article{10.5555/1622503.1622505,
author = {Dunne, Paul E.},
title = {Extremal Behaviour in Multiagent Contract Negotiation},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {We examine properties of a model of resource allocation in which several agents exchange resources in order to optimise their individual holdings. The schemes discussed relate to well-known negotiation protocols proposed in earlier work and we consider a number of alternative notions of "rationality" covering both quantitative measures, e.g. cooperative and individual rationality and more qualitative forms, e.g. Pigou-Dalton transfers. While it is known that imposing particular rationality and structural restrictions may result in some reallocations of the resource set becoming unrealisable, in this paper we address the issue of the number of restricted rational deals that may be required to implement a particular reallocation when it is possible to do so. We construct examples showing that this number may be exponential (in the number of resources m), even when all of the agent utility functions are monotonic. We further show that k agents may achieve in a single deal a reallocation requiring exponentially many rational deals if at most k - 1 agents can participate, this same reallocation being unrealisable by any sequences of rational deals in which at most k - 2 agents are involved.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {41–78},
numpages = {38}
}

@article{10.5555/1622503.1622504,
author = {Roy, Nicholas and Gordon, Geoffrey and Thrun, Sebastian},
title = {Finding Approximate POMDP Solutions through Belief Compression},
year = {2005},
issue_date = {January 2005},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {23},
number = {1},
issn = {1076-9757},
abstract = {Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are generally considered to be intractable for large models. The intractability of these algorithms is to a large extent a consequence of computing an exact, optimal policy over the entire belief space. However, in real-world POMDP problems, computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes. The beliefs experienced by the controller often lie near a structured, low-dimensional subspace embedded in the high-dimensional belief space. Finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value function.We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space. We use Exponential family Principal Components Analysis (Collins, Dasgupta, &amp; Schapire, 2002) to represent sparse, high-dimensional belief spaces using small sets of learned features of the belief state. We then plan only in terms of the low-dimensional belief features. By planning in this low-dimensional space, we can find policies for POMDP models that are orders of magnitude larger than models that can be handled by conventional techniques.We demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–40},
numpages = {40}
}

@article{10.5555/1622487.1622502,
author = {Dixon, Heidi E. and Ginsberg, Matthew L. and Luks, Eugene M. and Parkes, Andrew J.},
title = {Generalizing Boolean Satisfiability II: Theory},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {This is the second of three planned papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers. The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance. This paper presents the theoretical basis for the ideas underlying ZAP, arguing that existing ideas in this area exploit a single, recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem. We argue that the group structure precisely captures the general structure at which earlier approaches hinted, and give numerous examples of its use. We go on to extend the Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and show that earlier computational improvements are either subsumed or left intact by the new method. The third paper in this series discusses ZAP's implementation and presents experimental performance results.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {481–534},
numpages = {54}
}

@article{10.5555/1622487.1622501,
author = {Erkan, G\"{u}nes and Radev, Dragomir R.},
title = {LexRank: Graph-Based Lexical Centrality as Salience in Text Summarization},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {457–479},
numpages = {23}
}

@article{10.5555/1622487.1622500,
author = {Becker, Raphen and Zilberstein, Shlomo and Lesser, Victor and Goldman, Claudia V.},
title = {Solving Transition Independent Decentralized Markov Decision Processes},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Formal treatment of collaborative multi-agent systems has been lagging behind the rapid progress in sequential decision making by individual agents. Recent work in the area of decentralized Markov Decision Processes (MDPs) has contributed to closing this gap, but the computational complexity of these models remains a serious obstacle. To overcome this complexity barrier, we identify a specific class of decentralized MDPs in which the agents' transitions are independent. The class consists of independent collaborating agents that are tied together through a structured global reward function that depends on all of their histories of states and actions. We present a novel algorithm for solving this class of problems and examine its properties, both as an optimal algorithm and as an anytime algorithm. To the best of our knowledge, this is the first algorithm to optimally solve a non-trivial subclass of decentralized MDPs. It lays the foundation for further work in this area on both exact and approximate algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {423–455},
numpages = {33}
}

@article{10.5555/1622487.1622499,
author = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
title = {On Prediction Using Variable Order Markov Models},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a "decomposed" CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {385–421},
numpages = {37}
}

@article{10.5555/1622487.1622498,
author = {Bowling, Michael and Veloso, Manuela},
title = {Existence of Multiagent Equilibria with Limited Agents},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Multiagent learning is a necessary yet challenging problem as multiagent systems become more prevalent and environments become more dynamic. Much of the groundbreaking work in this area draws on notable results from game theory, in particular, the concept of Nash equilibria. Learners that directly learn an equilibrium obviously rely on their existence. Learners that instead seek to play optimally with respect to the other players also depend upon equilibria since equilibria are fixed points for learning. From another perspective, agents with limitations are real and common. These may be undesired physical limitations as well as self-imposed rational limitations, such as abstraction and approximation techniques, used to make learning tractable. This article explores the interactions of these two important concepts: equilibria and limitations in learning. We introduce the question of whether equilibria continue to exist when agents have limitations. We look at the general effects limitations can have on agent behavior, and define a natural extension of equilibria that accounts for these limitations. Using this formalization, we make three major contributions: (i) a counterexample for the general existence of equilibria with limitations, (ii) sufficient conditions on limitations that preserve their existence, (iii) three general classes of games and limitations that satisfy these conditions. We then present empirical results from a specific multiagent learning algorithm applied to a specific instance of limited agents. These results demonstrate that learning with limitations is feasible, when the conditions outlined by our theoretical analysis hold.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {353–384},
numpages = {32}
}

@article{10.5555/1622487.1622497,
author = {Beame, Paul and Kautz, Henry and Sabharwal, Ashish},
title = {Towards Understanding and Harnessing the Potential of Clause Learning},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Effcient implementations of DPLL with the addition of clause learning are the fastest complete Boolean satisfiability solvers and can handle many significant real-world problems, such as verification, planning and design. Despite its importance, little is known of the ultimate strengths and limitations of the technique. This paper presents the first precise characterization of clause learning as a proof system (CL), and begins the task of understanding its power by relating it to the well-studied resolution proof system. In particular, we show that with a new learning scheme, CL can provide exponentially shorter proofs than many proper refinements of general resolution (RES) satisfying a natural property. These include regular and Davis-Putnam resolution, which are already known to be much stronger than ordinary DPLL. We also show that a slight variant of CL with unlimited restarts is as powerful as RES itself. Translating these analytical results to practice, however, presents a challenge because of the nondeterministic nature of clause learning algorithms. We propose a novel way of exploiting the underlying problem structure, in the form of a high level problem description such as a graph or PDDL specification, to guide clause learning algorithms toward faster solutions. We show that this leads to exponential speed-ups on grid and randomized pebbling problems, as well as substantial improvements on certain ordering formulas.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {319–351},
numpages = {33}
}

@article{10.5555/1622487.1622496,
author = {Felner, Ariel and Korf, Richard E. and Hanan, Sarit},
title = {Additive Pattern Database Heuristics},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {We explore a method for computing admissible heuristic evaluation functions for search problems. It utilizes pattern databases (Culberson &amp; Schaeffer, 1998), which are precomputed tables of the exact cost of solving various subproblems of an existing problem. Unlike standard pattern database heuristics, however, we partition our problems into disjoint sub-problems, so that the costs of solving the different subproblems can be added together without overestimating the cost of solving the original problem. Previously (Korf &amp; Felner, 2002) we showed how to statically partition the sliding-tile puzzles into disjoint groups of tiles to compute an admissible heuristic, using the same partition for each state and problem instance. Here we extend the method and show that it applies to other domains as well. We also present another method for additive heuristics which we call dynamically partitioned pattern databases. Here we partition the problem into disjoint subproblems for each state of the search dynamically. We discuss the pros and cons of each of these methods and apply both methods to three different problem domains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and finding an optimal vertex cover of a graph. We find that in some problem domains, static partitioning is most effective. while in others dynamic partitioning is a better choice. In each of these problem domains, either statically partitioned or dynamically partitioned pattern database heuristics are the best known heuristics for the problem.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {279–318},
numpages = {40}
}

@article{10.5555/1622487.1622495,
author = {Hoffmann, J\"{o}rg and Porteous, Julie and Sebastia, Laura},
title = {Ordered Landmarks in Planning},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Many known planning tasks have inherent constraints concerning the best order in which to achieve the goals. A number of research efiorts have been made to detect such constraints and to use them for guiding search, in the hope of speeding up the planning process.We go beyond the previous approaches by considering ordering constraints not only over the (top-level) goals, but also over the sub-goals that will necessarily arise during planning. Landmarks are facts that must be true at some point in every valid solution plan. We extend Koehler and Hoffmann's definition of reasonable orders between top level goals to the more general case of landmarks. We show how landmarks can be found, how their reasonable orders can be approximated, and how this information can be used to decompose a given planning task into several smaller sub-tasks. Our methodology is completely domain- and planner-independent. The implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around state-of-the-art sub-optimal planning systems, as exemplified by FF and LPG.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {215–278},
numpages = {64}
}

@article{10.5555/1622487.1622494,
author = {Park, Sunju and Durfee, Edmund H. and Birmingham, William P.},
title = {Use of Markov Chains to Design an Agent Bidding Strategy for Continuous Double Auctions},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {As computational agents are developed for increasingly complicated e-commerce applications, the complexity of the decisions they face demands advances in artificial intelligence techniques. For example, an agent representing a seller in an auction should try to maximize the seller's profit by reasoning about a variety of possibly uncertain pieces of information, such as the maximum prices various buyers might be willing to pay, the possible prices being offered by competing sellers, the rules by which the auction operates, the dynamic arrival and matching of offers to buy and sell, and so on. A na\"{\i}ve application of multiagent reasoning techniques would require the seller's agent to explicitly model all of the other agents through an extended time horizon, rendering the problem intractable for many realistically-sized problems. We have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable Markov chain model of the auction process. We have experimentally identified the conditions under which our new strategy works well, as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future. Our results show that our new strategy in general performs well, outperforming other tractable heuristic strategies in a majority of experiments, and is particularly effective in a "seller's market," where many buy offers are available.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {175–214},
numpages = {40}
}

@article{10.5555/1622487.1622493,
author = {Goldman, Claudia V. and Zilberstein, Shlomo},
title = {Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Decentralized control of cooperative systems captures the operation of a group of decision-makers that share a single global objective. The difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate. The general problem has been shown to be NEXP-complete. In this paper, we identify classes of decentralized control problems whose complexity ranges between NEXP and P. In particular, we study problems characterized by independent transitions, independent observations, and goal-oriented objective functions. Two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time. This paper also studies information sharing among the decision-makers, which can improve their performance. We distinguish between three ways in which agents can exchange information: indirect communication, direct communication and sharing state features that are not controlled by the agents. Our analysis shows that for every class of problems we consider, introducing direct or indirect communication does not change the worst-case complexity. The results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {143–174},
numpages = {32}
}

@article{10.5555/1622487.1622492,
author = {Derbeko, Philip and El-Yaniv, Ran and Meir, Ron},
title = {Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik's transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {117–142},
numpages = {26}
}

@article{10.5555/1622487.1622491,
author = {Chockler, Hana and Halpern, Joseph Y.},
title = {Responsibility and Blame: A Structural-Model Approach},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Causality is typically treated an all-or-nothing concept; either A is a cause of B or it is not. We extend the definition of causality introduced by Halpern and Pearl (2004a) to take into account the degree of responsibility of A for B. For example, if someone wins an election 11-0, then each person who votes for him is less responsible for the victory than if he had won 6-5. We then define a notion of degree of blame, which takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {93–115},
numpages = {23}
}

@article{10.5555/1622487.1622490,
author = {Meek, Colin J. and Birmingham, William P.},
title = {A Comprehensive Trainable Error Model for Sung Music Queries},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {We propose a model for errors in sung queries, a variant of the hidden Markov model (HMM). This is a solution to the problem of identifying the degree of similarity between a (typically error-laden) sung query and a potential target in a database of musical works, an important problem in the field of music information retrieval. Similarity metrics are a critical component of "query-by-humming" (QBH) applications which search audio and multimedia databases for strong matches to oral queries. Our model comprehensively expresses the types of error or variation between target and query: cumulative and noncumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. The model is not only expressive, but automatically trainable, or able to learn and generalize from query examples. We present results of simulations, designed to assess the discriminatory potential of the model, and tests with real sung queries, to demonstrate relevance to real-world applications.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {57–91},
numpages = {35}
}

@article{10.5555/1622487.1622489,
author = {Dubois, Didier and Fargier, Helene and Prade, Henri},
title = {Ordinal and Probabilistic Representations of Acceptance},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {An accepted belief is a proposition considered likely enough by an agent, to be inferred from as if it were true. This paper bridges the gap between probabilistic and logical representations of accepted beliefs. To this end, natural properties of relations on propositions, describing relative strength of belief are augmented with some conditions ensuring that accepted beliefs form a deductively closed set. This requirement turns out to be very restrictive. In particular, it is shown that the sets of accepted belief of an agent can always be derived from a family of possibility rankings of states. An agent accepts a proposition in a given context if this proposition is considered more possible than its negation in this context, for all possibility rankings in the family. These results are closely connected to the non-monotonic 'preferential' inference system of Kraus, Lehmann and Magidor and the so-called plausibility functions of Friedman and Halpern. The extent to which probability theory is compatible with acceptance relations is laid bare. A solution to the lottery paradox, which is considered as a major impediment to the use of non-monotonic inference is proposed using a special kind of probabilities (called lexicographic, or big-stepped). The setting of acceptance relations also proposes another way of approaching the theory of belief change after the works of Gauml;rdenfors and colleagues. Our view considers the acceptance relation as a primitive object from which belief sets are derived in various contexts.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {23–56},
numpages = {34}
}

@article{10.5555/1622487.1622488,
author = {Cohen, David and Cooper, Martin and Jeavons, Peter and Krokhin, Andrei},
title = {A Maximal Tractable Class of Soft Constraints},
year = {2004},
issue_date = {July 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {22},
number = {1},
issn = {1076-9757},
abstract = {Many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of (possibly conflicting) problem requirements. A soft constraint is a function defined on a collection of variables which associates some measure of desirability with each possible combination of values for those variables. However, the crucial question of the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention. In this paper we identify a class of soft binary constraints for which the problem of finding the optimal solution is tractable. In other words, we show that for any given set of such constraints, there exists a polynomial time algorithm to determine the assignment having the best overall combined measure of desirability. This tractable class includes many commonly-occurring soft constraints, such as "as near as possible" or "as soon as possible after", as well as crisp constraints such as "greater than". Finally, we show that this tractable class is maximal, in the sense that adding any other form of soft binary constraint which is not in the class gives rise to a class of problems which is NP-hard.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–22},
numpages = {22}
}

@article{10.5555/1622467.1622486,
author = {Felner, Ariel and Stern, Roni and Ben-Yair, Asaph and Kraus, Sarit and Netanyahu, Nathan},
title = {PHA*: Finding the Shortest Path with A* in an Unknown Physical Environment},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {We address the problem of finding the shortest path between two points in an unknown real physical environment, where a traveling agent must move around in the environment to explore unknown territory. We introduce the Physical-A* algorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes that A* would expand and returns the shortest path between the two points. However. due to the physical nature of the problem, the complexity of the algorithm is measured by the traveling effort of the moving agent and not by the number of generated nodes, as in standard A*. PHA* is presented a a two-level algorithm, such that its high level, A*, chooses the next node to be expanded and its low level directs the agent to that node in order to explore it. We present a number of variations for both the high-level and low-level procedures and evaluate their performance theoretically and experimentally. We show that the travel cost of our best variation is fairly dose to the optimal travel cost, assuming that the mandatory nodes of A* are known in advance. We then generalize our algorithm to the multi-agent case, where a number of cooperative agents are designed to solve the problem. Specifically, we provide an experimental implementation for such a system. It should be noted that the problem addressed here is not a navigation problem, but rather a problem of finding the shortest path between two points for future usage.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {631–670},
numpages = {40}
}

@article{10.5555/1622467.1622485,
author = {Babaioff, Moshe and Nisan, Noam},
title = {Concurrent Auctions across the Supply Chain},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {With the recent technological feasibility of electronic commerce over the Internet, much attention has been given to the design of electronic markets for various types of electronically-tradable goods. Such markets, however, will normally need to function in some relationship with markets for other related goods, usually those downstream or upstream in the supply chain. Thus, for example, an electronic market for rubber tires for trucks will likely need to be strongly influenced by the rubber market as well as by the truck market.In this paper we design protocols for exchange of information between a sequence of markets along a single supply chain. These protocols allow each of these markets to function separately, while the information exchanged ensures efficient global behavior across the supply chain. Each market that forms a link in the supply chain operates as a double auction, where the bids on one side of the double auction come from bidders in the corresponding segment of the industry, and the bids on the other side are synthetically generated by the protocol to express the combined information from all other links in the chain. The double auctions in each of the markets can be of several types, and we study several variants of incentive compatible double auctions, comparing them in terms of their efficiency and of the market revenue.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {595–629},
numpages = {35}
}

@article{10.5555/1622467.1622484,
author = {Borodin, Allan and El-Yaniv, Ran and Gogan, Vincent},
title = {Can We Learn to Beat the Best Stock},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {A novel algorithm for actively trading stocks is presented. While traditional expert advice and "universal" algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can "beat the market" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {579–594},
numpages = {16}
}

@article{10.5555/1622467.1622483,
author = {Liberatore, Paolo},
title = {On Polynomial Sized MDP Succinct Policies},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Policies of Markov Decision Processes (MDPs) determine the next action to execute from the current state and, possibly, the history (the past states). When the number of states is large, succinct representations are often used to compactly represent both the MDPs and the policies in a reduced amount of space. In this paper, some problems related to the size of succinctly represented policies are analyzed. Namely, it is shown that some MDPs have policies that can only be represented in space super-polynomial in the size of the MDP, unless the polynomial hierarchy collapses. This fact motivates the study of the problem of deciding whether a given MDP has a policy of a given size and reward. Since some algorithms for MDPs work by finding a succinct representation of the value function, the problem of deciding the existence of a succinct representation of a value function of a given size and reward is also considered.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {551–577},
numpages = {27}
}

@article{10.5555/1622467.1622482,
author = {Keppens, Jeroen and Shen, Qiang},
title = {Compositional Model Repositories via Dynamic Constraint Satisfaction with Order-of-Magnitude Preferences},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {The predominant knowledge-based approach to automated model construction, compositional modelling, employs a set of models of particular functional components. Its inference mechanism takes a scenario describing the constituent interacting components of a system and translates it into a useful mathematical model. This paper presents a novel compositional modelling approach aimed at building model repositories. It furthers the field in two respects. Firstly, it expands the application domain of compositional modelling to systems that can not be easily described in terms of interacting functional components, such as ecological systems. Secondly, it enables the incorporation of user preferences into the model selection process. These features are achieved by casting the compositional modelling problem as an activity-based dynamic preference constraint satisfaction problem, where the dynamic constraints describe the restrictions imposed over the composition of partial models and the preferences correspond to those of the user of the automated modeller. In addition, the preference levels are represented through the use of symbolic values that differ in orders of magnitude.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {499–550},
numpages = {52}
}

@article{10.5555/1622467.1622481,
author = {Zhang, Weixiong},
title = {Phase Transitions and Backbones of the Asymmetric Traveling Salesman Problem},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {In recent years, there has been much interest in phase transitions of combinatorial problems. Phase transitions have been successfully used to analyze combinatorial optimization problems, characterize their typical-case features and locate the hardest problem instances. In this paper, we study phase transitions of the asymmetric Traveling Salesman Problem (ATSP), an NP-hard combinatorial optimization problem that has many real-world applications. Using random instances of up to 1,500 cities in which intercity distances are uniformly distributed, we empirically show that many properties of the problem, including the optimal tour cost and backbone size, experience sharp transitions as the precision of intercity distances increases across a critical value. Our experimental results on the costs of the ATSP tours and assignment problem agree with the theoretical result that the asymptotic cost of assignment problem is π26 as the number of cities goes to infinity. In addition, we show that the average computational cost of the well-known branch-and-bound subtour elimination algorithm for the problem also exhibits a thrashing behavior, transitioning from easy to difficult as the distance precision increases. These results answer positively an open question regarding the existence of phase transitions in the ATSP, and provide guidance on how difficult ATSP problem instances should be generated.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {471–497},
numpages = {27}
}

@article{10.5555/1622467.1622480,
author = {Gorniak, Peter and Roy, Deb},
title = {Grounded Semantic Composition for Visual Scenes},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {We present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {429–470},
numpages = {42}
}

@article{10.5555/1622467.1622479,
author = {Thompson, Cynthia A. and G\"{o}ker, Mehmet H. and Langley, Pat},
title = {A Personalized System for Conversational Recommendations},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases. Recommendation systems help users find items of interest of a particular type, such as movies or restaurants, but are still somewhat awkward to use. Our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems, creating personalized aides. We present a system - the ADAPTIVE PLACE ADVISOR - that treats item selection as an interactive, conversational process, with the program inquiring about item attributes and the user responding. Individual, long-term user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user. We present a novel user model that influences both item search and the questions asked during a conversation. We demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item, as compared to a control group of users interacting with a non-adaptive version of the system.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {393–428},
numpages = {36}
}

@article{10.5555/1622467.1622477,
author = {Halpern, Joseph Y. and Koller, Daphne},
title = {Representation Dependence in Probabilistic Inference},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Non-deductive reasoning systems are often representation dependent: representing the same situation in two different ways may cause such a system to return two different answers. Some have viewed this as a significant problem. For example, the principle of maximum entropy has been subjected to much criticism due to its representation dependence. There has, however, been almost no work investigating representation dependence. In this paper, we formalize this notion and show that it is not a problem specific to maximum entropy. In fact, we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment, in a precise sense. Moreover, we show that representation independence is incompatible with even a weak default assumption of independence. We then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata, and provide a construction of a family of inference procedures that provides such restricted representation independence, using relative entropy.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {319–356},
numpages = {38}
}

@article{10.5555/1622467.1622476,
author = {Nederhof, Mark-Jan and Satta, Giorgio},
title = {IDL-Expressions: A Formalism for Representing and Parsing Finite Languages in Natural Language Processing},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {We propose a formalism for representation of finite languages, referred to as the class of IDL-expressions, which combines concepts that were only considered in isolation in existing formalisms. The suggested applications are in natural language processing, more specifically in surface natural language generation and in machine translation, where a sentence is obtained by first generating a large set of candidate sentences, represented in a compact way, and then filtering such a set through a parser. We study several formal properties of IDL-expressions and compare this new formalism with more standard ones. We also present a novel parsing algorithm for IDL-expressions and prove a non-trivial upper bound on its time complexity.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {287–317},
numpages = {31}
}

@article{10.5555/1622467.1622475,
author = {Arieli, Ofer and Denecker, Marc and Van Nuffelen, Bert and Bruynooghe, Maurice},
title = {Coherent Integration of Databases by Abductive Logic Programming},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {We introduce an abductive method for a coherent integration of independent datasources. The idea is to compute a list of data-facts that should be inserted to the amalgamated database or retracted from it in order to restore its consistency. This method is implemented by an abductive solver, called A system, that applies SLDNFA-resolution on a meta-theory that relates different, possibly contradicting, input databases. We also give a pure model-theoretic analysis of the possible ways to 'recover' consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible. This allows us to characterize the 'recovered databases' in terms of the 'preferred' (i.e., most consistent) models of the theory. The outcome is an abductive-based application that is sound and complete with respect to a corresponding model-based, preferential semantics, and - to the best of our knowledge - is more expressive (thus more general) than any other implementation of coherent integration of databases.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {245–286},
numpages = {42}
}

@article{10.5555/1622467.1622478,
author = {Hnich, Brahim and Smith, Barbara M. and Walsh, Toby},
title = {Dual Modelling of Permutation and Injection Problems},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {When writing a constraint program, we have to choose which variables should be the decision variables, and how to represent the constraints on these variables. In many cases, there is considerable choice for the decision variables. Consider, for example, permutation problems in which we have as many values as variables, and each variable takes an unique value. In such problems, we can choose between a primal and a dual viewpoint. In the dual viewpoint, each dual variable represents one of the primal values, whilst each dual value represemts one of the primal variables. Alternatively, by means of channelling constraints to link the primal and dual variables, we can have a combines model with both sets of variables. In this paper, we perform an extensive theoretical and empirical study of such primal, dual and combines models for two classes of problems: permutation problems and injection problems. Our results show that if often be advantageous to use multiple viewpoints, and to have constraints which channel between them to maintain consistency. They also illustrate a general methodology for comparing different constraint models.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {357–391},
numpages = {35}
}

@article{10.5555/1622467.1622474,
author = {Dixon, Heidi E. and Ginsberg, Matthew L. and Parkes, Andrew J.},
title = {Generalizing Boolean Satisfiability I: Background and Survey of Existing Work},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {This is the first of three planned papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high-performance solvers. The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance. This paper is a survey of the work underlying ZAP, and discusses previous attempts to improve the performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting the structure of the problem being solved. We examine existing ideas including extensions of the Boolean language to allow cardinality constraints, pseudo-Boolean representations, symmetry, and a limited form of quantification. While this paper is intended as a survey, our research results are contained in the two subsequent articles, with the theoretical structure of ZAP described in the second paper in this series, and ZAP's implementation described in the third.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {193–243},
numpages = {51}
}

@article{10.5555/1622467.1622473,
author = {Boutilier, Craig and Brafman, Ronen I. and Domshlak, Carmel and Hoos, Holger H. and Poole, David},
title = {CP-Nets: A Tool for Representing and Reasoning with Conditional Ceteris Paribus Preference Statements},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Information about user preferences plays a key role in automated decision making. In many domains it is desirable to assess such preferences in a qualitative rather than quantitative way. In this paper, we propose a qualitative graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus (all else being equal) interpretation. Such a representation is often compact and arguably quite natural in many circumstances. We provide a formal semantics for this model, and describe how the structure of the network can be exploited in several inference tasks, such as determining whether one outcome dominates (is preferred to) another, ordering a set outcomes according to the preference relation, and constructing the best outcome subject to available evidence.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {135–191},
numpages = {57}
}

@article{10.5555/1622467.1622472,
author = {Park, James D. and Darwiche, Adnan},
title = {Complexity Results and Approximation Strategies for MAP Explanations},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation (Pr), or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NPPP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated.Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference (Pr), and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {101–133},
numpages = {33}
}

@article{10.5555/1622467.1622471,
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
title = {Competitive Coevolution through Evolutionary Complexification},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {63–100},
numpages = {38}
}

@article{10.5555/1622467.1622470,
author = {Monderer, Dov and Tennenholtz, Moshe},
title = {K-Implementation},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {This paper discusses an interested party who wishes to influence the behavior of agents in a game (multi-agent interaction), which is not under his control. The interested party cannot design a new game, cannot enforce agents' behavior, cannot enforce payments by the agents, and cannot prohibit strategies available to the agents. However, he can influence the outcome of the game by committing to non-negative monetary transfers for the different strategy profiles that may be selected by the agents. The interested party assumes that agents are rational in the commonly agreed sense that they do not use dominated strategies. Hence, a certain subset of outcomes is implemented in a given game if by adding non-negative payments, rational players will necessarily produce an outcome in this subset. Obviously, by making sufficiently big payments one can implement any desirable outcome. The question is what is the cost of implementation? In this paper we introduce the notion of k-implementation of a desired set of strategy profiles, where k stands for the amount of payment that need to be actually made in order to implement desirable outcomes. A major point in k-implementation is that monetary offers need not necessarily materialize when following desired behaviors. We define and study k-implementation in the contexts of games with complete and incomplete information. In the latter case we mainly focus on the VCG games. Our setting is later extended to deal with mixed strategies using correlation devices. Together, the paper introduces and studies the implementation of desirable outcomes by a reliable party who cannot modify game rules (i.e. provide protocols), complementing previous work in mechanism design, while making it more applicable to many realistic CS settings.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {37–62},
numpages = {26}
}

@article{10.5555/1622467.1622469,
author = {Wellman, Michael P. and Reeves, Daniel M. and Lochner, Kevin M. and Vorobeychik, Yevgeniy},
title = {Price Prediction in a Trading Agent Competition},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {The 2002 Trading Agent Competition (TAC) presented a challenging market game in the domain of travel shopping. One of the pivotal issues in this domain is uncertainty about hotel prices, which have a significant influence on the relative cost of alternative trip schedules. Thus, virtually all participants employ some method for predicting hotel prices. We survey approaches employed in the tournament, finding that agents apply an interesting diversity of techniques, taking into account differing sources of evidence bearing on prices. Based on data provided by entrants on their agents' actual predictions in the TAC-02 finals and semifinals, we analyze the relative efficacy of these approaches. The results show that taking into account game-specific information about flight prices is a major distinguishing factor. Machine learning methods effectively induce the relationship between flight and hotel prices from game data, and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data. Employing a new measure of prediction quality, we relate absolute accuracy to bottom-line performance in the game.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {19–36},
numpages = {18}
}

@article{10.5555/1622467.1622468,
author = {Zhang, Nevin L. and Ko\v{c}ka, Tom\'{a}\v{s}},
title = {Effective Dimensions of Hierarchical Latent Class Models},
year = {2004},
issue_date = {January 2004},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {21},
number = {1},
issn = {1076-9757},
abstract = {Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent. There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models. Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the BIC score.Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–17},
numpages = {17}
}

@article{10.5555/1622434.1622451,
author = {Nigenda, Romeo Sanchez and Kambhampati, Subbarao},
title = {AltAlt<sup>p</sup>: Online Parallelization of Plans with Heuristic State Search},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Despite their near dominance, heuristic state search planners still lag behind disjunctive planners in the generation of parallel plans in classical planning. The reason is that directly searching for parallel solutions in state space planners would require the planners to branch on all possible subsets of parallel actions, thus increasing the branching factor exponentially. We present a variant of our heuristic state search planner AltAlt called AltAltp which generates parallel plans by using greedy online parallelization of partial plans. The greedy approach is significantly informed by the use of novel distance heuristics that AltAltp derives from a graphplan-style planning graph for the problem. While this approach is not guaranteed to provide optimal parallel plans, empirical results show that AltAltp is capable of generating good quality parallel plans at a fraction of the cost incurred by the disjunctive planners.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {631–657},
numpages = {27}
}

@article{10.5555/1622434.1622450,
author = {Price, Bob and Boutilier, Craig},
title = {Accelerating Reinforcement Learning through Implicit Imitation},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with difierent action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {569–629},
numpages = {61}
}

@article{10.5555/1622434.1622449,
author = {Walsh, William E. and Wellman, Michael P.},
title = {Decentralized Supply Chain Formation: A Market Protocol and Competitive Equilibrium Analysis},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel, multiagent production activity. We present a simple model of supply chains, highlighting two characteristic features: hierarchical subtask decomposition, and resource contention. To decentralize the formation process, we introduce a market price system over the resources produced along the chain. In a competitive equilibrium for this system, agents choose locally optimal allocations with respect to prices, and outcomes are optimal overall. To determine prices, we define a market protocol based on distributed, progressive auctions, and myopic, non-strategic agent bidding policies. In the presence of resource contention, this protocol produces better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature. The protocol often converges to high-value supply chains, and when competitive equilibria exist, typically to approximate competitive equilibria. However, complementarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their outputs. A subsequent decommitment phase recovers a significant fraction of the lost surplus.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {513–567},
numpages = {55}
}

@article{10.5555/1622434.1622448,
author = {Console, Luca and Picardi, Claudia and Dupr\'{e}, Daniele Theseider},
title = {Temporal Decision Trees: Model-Based Diagnosis of Dynamic Systems on-Board},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {The automatic generation of decision trees based on off-line reasoning on models of a domain is a reasonable compromise between the advantages of using a model-based approach in technical domains and the constraints imposed by embedded applications. In this paper we extend the approach to deal with temporal information. We introduce a notion of temporal decision tree, which is designed to make use of relevant information as long as it is acquired, and we present an algorithm for compiling such trees from a model-based reasoning system.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {469–512},
numpages = {44}
}

@article{10.5555/1622434.1622447,
author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
title = {Efficient Solution Algorithms for Factored MDPs},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 1040 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {399–468},
numpages = {70}
}

@article{10.5555/1622434.1622446,
author = {Wray, Robert E. and Laird, John E.},
title = {An Architectural Approach to Ensuring Consistency in Hierarchical Execution},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Hierarchical task decomposition is a method used in many agent systems to organize agent knowledge. This work shows how the combination of a hierarchy and persistent assertions of knowledge can lead to difficulty in maintaining logical consistency in asserted knowledge. We explore the problematic consequences of persistent assumptions in the reasoning process and introduce novel potential solutions. Having implemented one of the possible solutions, Dynamic Hierarchical Justification, its effectiveness is demonstrated with an empirical analysis.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {355–398},
numpages = {44}
}

@article{10.5555/1622434.1622445,
author = {Weiss, Gary M. and Provost, Foster},
title = {Learning When Training Data Are Costly: The Effect of Class Distribution on Tree Induction},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {For large, real-world inductive learning problems, the number of training examples often must be limited due to the costs associated with procuring, preparing, and storing the training examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only n training examples can be selected, in what proportion should the classes be represented? In this article we help to answer this question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data. We study twenty-six data sets and, for each, determine the best class distribution for learning. The naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate (0/1 loss). However, when the area under the ROC curve is used to evaluate classifier performance, a balanced distribution is shown to perform well. Since neither of these choices for class distribution always generates the best-performing classifier, we introduce a "budget-sensitive" progressive sampling algorithm for selecting training examples based on the class associated with each example. An empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good (nearly-optimal) classification performance.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {315–354},
numpages = {40}
}

@article{10.5555/1622434.1622444,
author = {Lin, Fangzhen},
title = {Compiling Causal Theories to Successor State Axioms and STRIPS-like Systems},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {We describe a system for specifying the effects of actions. Unlike those commonly used in AI planning, our system uses an action description language that allows one to specify the effects of actions using domain rules, which are state constraints that can entail new action effects from old ones. Declaratively, an action domain in our language corresponds to a nonmonotonic causal theory in the situation calculus. Procedurally, such an action domain is compiled into a set of logical theories, one for each action in the domain, from which fully instantiated successor state-like axioms and STRIPS-like systems are then generated. We expect the system to be a useful tool for knowledge engineers writing action specifications for classical AI planning systems, GOLOG systems, and other systems where formal specifications of actions are needed.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {279–314},
numpages = {36}
}

@article{10.5555/1622434.1622443,
author = {Gr\"{u}nwald, Peter D. and Halpern, Joseph Y.},
title = {Updating Probabilities},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a "naive space", which does not take into account the protocol used, can often lead to counterintuitive results. Here we examine why. A criterion known as CAR ("coarsening at random") in the statistical literature characterizes when "naive" conditioning in a naive space works. We show that the CAR condition holds rather infrequently, and we provide a procedural characterization of it, by giving a randomized algorithm that generates all and only distributions for which CAR holds. This substantially extends previous characterizations of CAR. We also consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE). We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, and show that there exist some very simple settings in which MRE essentially never gives the right results. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {243–278},
numpages = {36}
}

@article{10.5555/1622434.1622442,
author = {Stone, Peter and Schapire, Robert E. and Littman, Michael L. and Csirik, J\'{a}nos A. and McAllester, David},
title = {Decision-Theoretic Bidding Based on Learned Density Models in Simultaneous, Interacting Auctions},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids. We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, i.e., supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {209–242},
numpages = {34}
}

@article{10.5555/1622434.1622441,
author = {Wiewiora, Eric},
title = {Potential-Based Shaping and Q-Value Initialization Are Equivalent},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior.In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {205–208},
numpages = {4}
}

@article{10.5555/1622434.1622440,
author = {Maynard-Zhang, Pedrito and Lehmann, Daniel},
title = {Representing and Aggregating Conflicting Beliefs},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {We consider the two-fold problem of representing collective beliefs and aggregating these beliefs. We propose a novel representation for collective beliefs that uses modular, transitive relations over possible worlds. They allow us to represent conflicting opinions and they have a clear semantics, thus improving upon the quasi-transitive relations often used in social choice. We then describe a way to construct the belief state of an agent informed by a set of sources of varying degrees of reliability. This construction circumvents Arrow's Impossibility Theorem in a satisfactory manner by accounting for the explicitly encoded conflicts. We give a simple set-theory-based operator for combining the information of multiple agents. We show that this operator satisfies the desirable invariants of idempotence, commutativity, and associativity, and, thus, is well-behaved when iterated, and we describe a computationally effective way of computing the resulting belief state. Finally, we extend our framework to incorporate voting.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {155–203},
numpages = {49}
}

@article{10.5555/1622434.1622439,
author = {Leisink, Martijn and Kappen, Bert},
title = {Bound Propagation},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {In this article we present an algorithm to compute bounds on the marginals of a graphical model. For several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network. The range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds. As we will show, this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes. In this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals. We show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications, but for which exact computations are infeasible.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {139–154},
numpages = {16}
}

@article{10.5555/1622434.1622438,
author = {Finkelstein, Lev and Markovitch, Shaul and Rivlin, Ehud},
title = {Optimal Schedules for Parallelizing Anytime Algorithms: The Case of Shared Resources},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {The performance of anytime algorithms can be improved by simultaneously solving several instances of algorithm-problem pairs. These pairs may include different instances of a problem (such as starting from a different initial state), different algorithms (if several alternatives exist), or several runs of the same algorithm (for non-deterministic algorithms). In this paper we present a methodology for designing an optimal scheduling policy based on the statistical characteristics of the algorithms involved. We formally analyze the case where the processes share resources (a single-processor model), and provide an algorithm for optimal scheduling. We analyze, theoretically and empirically, the behavior of our scheduling algorithm for various distribution types. Finally, we present empirical results of applying our scheduling algorithm to the Latin Square problem.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {73–138},
numpages = {66}
}

@article{10.5555/1622434.1622437,
author = {Eiter, Thomas and Faber, Wolfgang and Leone, Nicola and Pfeifer, Gerald and Polleres, Axel},
title = {Answer Set Planning under Action Costs},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {Recently, planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems. In this paper, we present the language κc, which extends the declarative planning language κ by action costs. κc provides the notion of admissible and optimal plans, which are plans whose overall action costs are within a given limit resp. minimum over all plans (i.e., cheapest plans). As we demonstrate, this novel language allows for expressing some nontrivial planning tasks in a declarative way. Furthermore, it can be utilized for representing planning problems under other optimality criteria, such as computing "shortest" plans (with the least number of steps), and refinement combinations of cheapest and fastest plans. We study complexity aspects of the language κc and provide a transformation to logic programs, such that planning problems are solved via answer set programming. Furthermore, we report experimental results on selected problems. Our experience is encouraging that answer set planning may be a valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {25–71},
numpages = {47}
}

@article{10.5555/1622434.1622436,
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
title = {Learning to Coordinate Efficiently: A Model-Based Approach},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {In common-interest stochastic games all players receive an identical payoff. Players participating in such games must learn to coordinate with each other in order to receive the highest-possible value. A number of reinforcement learning algorithms have been proposed for this problem, and some have been shown to converge to good solutions in the limit. In this paper we show that using very simple model-based algorithms, much better (i.e., polynomial) convergence rates can be attained. Moreover, our model-based algorithms are guaranteed to converge to the optimal value, unlike many of the existing algorithms.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {11–23},
numpages = {13}
}

@article{10.5555/1622434.1622435,
author = {Zanuttini, Bruno},
title = {New Polynomial Classes for Logic-Based Abduction},
year = {2003},
issue_date = {July 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {19},
number = {1},
issn = {1076-9757},
abstract = {We address the problem of propositional logic-based abduction, i.e., the problem of searching for a best explanation for a given propositional observation according to a given propositional knowledge base. We give a general algorithm, based on the notion of projection; then we study restrictions over the representations of the knowledge base and of the query, and find new polynomial classes of abduction problems.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–10},
numpages = {10}
}

@article{10.5555/1622420.1622433,
author = {Reiter, Ehud and Sripada, Somayajulu G. and Robertson, Roma},
title = {Acquiring Correct Knowledge for Natural Language Generation},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {Natural language generation (NLG) systems are computer software systems that produce texts in English and other human languages, often from non-linguistic input data. NLG systems, like most ai systems, need substantial amounts of knowledge. However, our experience in two NLG projects suggests that it is difficult to acquire correct knowledge for NLG systems; indeed, every knowledge acquisition (KA) technique we tried had significant problems. In general terms, these problems were due to the complexity, novelty, and poorly understood nature of the tasks our systems attempted, and were worsened by the fact that people write so differently. This meant in particular that corpus-based KA approaches suffered because it was impossible to assemble a sizable corpus of high-quality consistent manually written texts in our domains; and structured expert-oriented KA techniques suffered because experts disagreed and because we could not get enough information about special and unusual cases to build robust systems. We believe that such problems are likely to affect many other NLG systems as well. In the long term, we hope that new KA techniques may emerge to help NLG system builders. In the shorter term, we believe that understanding how individual KA techniques can fail, and using a mixture of different KA techniques with different strengths and weaknesses, can help developers acquire NLG knowledge that is mostly correct.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {491–516},
numpages = {26}
}

@article{10.5555/1622420.1622432,
author = {Acid, Silvia and de Campos, Luis M.},
title = {Searching for Bayesian Network Structures in the Space of Restricted Acyclic Partially Directed Graphs},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {Although many algorithms have been designed to construct Bayesian network structures using different approaches and principles, they all employ only two methods: those based on independence criteria, and those based on a scoring function and a search procedure (although some methods combine the two). Within the score+search paradigm, the dominant approach uses local search methods in the space of directed acyclic graphs (DAGs), where the usual choices for defining the elementary modifications (local changes) that can be applied are arc addition, arc deletion, and arc reversal. In this paper, we propose a new local search method that uses a different search space, and which takes account of the concept of equivalence between network structures: restricted acyclic partially directed graphs (RPDAGs). In this way, the number of different configurations of the search space is reduced, thus improving efficiency. Moreover, although the final result must necessarily be a local optimum given the nature of the search method, the topology of the new search space, which avoids making early decisions about the directions of the arcs, may help to find better local optima than those obtained by searching in the DAG space. Detailed results of the evaluation of the proposed search method on several test problems, including the well-known Alarm Monitoring System, are also presented.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {445–490},
numpages = {46}
}

@article{10.5555/1622420.1622431,
author = {Lang, J\^{o}me and Liberatore, Paolo and Marquis, Pierre},
title = {Propositional Independence: Formula-Variable Independence and Forgetting},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {Independence - the study of what is relevant to a given problem of reasoning - has received an increasing attention from the AI community. In this paper, we consider two basic forms of independence, namely, a syntactic one and a semantic one. We show features and drawbacks of them. In particular, while the syntactic form of independence is computationally easy to check, there are cases in which things that intuitively are not relevant are not recognized as such. We also consider the problem of forgetting, i.e., distilling from a knowledge base only the part that is relevant to the set of queries constructed from a subset of the alphabet. While such process is computationally hard, it allows for a simplification of subsequent reasoning, and can thus be viewed as a form of compilation: once the relevant part of a knowledge base has been extracted, all reasoning tasks to be performed can be simplified.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {391–443},
numpages = {53}
}

@article{10.5555/1622420.1622430,
author = {Patel-Schneider, Peter F. and Sebastiani, Roberto},
title = {A New General Method to Generate Random Modal Formulae for Testing Decision Procedures},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {The recent emergence of heavily-optimized modal decision procedures has highlighted the key role of empirical testing in this domain. Unfortunately, the introduction of extensive empirical tests for modal logics is recent, and so far none of the proposed test generators is very satisfactory. To cope with this fact, we present a new random generation method that provides benefits over previous methods for generating empirical tests. It fixes and much generalizes one of the best-known methods, the random CNF???m test, allowing for generating a much wider variety of problems, covering in principle the whole input space. Our new method produces much more suitable test sets for the current generation of modal decision procedures. We analyze the features of the new method by means of an extensive collection of empirical tests.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {351–389},
numpages = {39}
}

@article{10.5555/1622420.1622429,
author = {Brafman, Ronen I. and Domshlak, Carmel},
title = {Structure and Complexity in Planning with Unary Operators},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {Unary operator domains - i.e., domains in which operators have a single effect - arise naturally in many control problems. In its most general form, the problem of STRIPS planning in unary operator domains is known to be as hard as the general STRIPS planning problem - both are PSPACE-complete. However, unary operator domains induce a natural structure, called the domain's causal graph. This graph relates between the preconditions and effect of each domain operator. Causal graphs were exploited by Williams and Nayak in order to analyze plan generation for one of the controllers in NASA's Deep-Space One spacecraft. There, they utilized the fact that when this graph is acyclic, a serialization ordering over any subgoal can be obtained quickly. In this paper we conduct a comprehensive study of the relationship between the structure of a domain's causal graph and the complexity of planning in this domain. On the positive side, we show that a non-trivial polynomial time plan generation algorithm exists for domains whose causal graph induces a polytree with a constant bound on its node indegree. On the negative side, we show that even plan existence is hard when the graph is a directed-path singly connected DAG. More generally, we show that the number of paths in the causal graph is closely related to the complexity of planning in the associated domain. Finally we relate our results to the question of complexity of planning with serializable subgoals.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {315–349},
numpages = {35}
}

@article{10.5555/1622420.1622428,
author = {Poole, David and Zhang, Nevin Lianwen},
title = {Exploiting Contextual Independence in Probabilistic Inference},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {Bayesian belief networks have grown to prominence because they provide compact representations for many problems for which probabilistic inference is appropriate, and there are algorithms to exploit this compactness. The next step is to allow compact representations of the conditional probabilities of a variable given its parents. In this paper we present such a representation that exploits contextual independence in terms of parent contexts; which variables act as parents may depend on the value of other variables. The internal representation is in terms of contextual factors (confactors) that is simply a pair of a context and a table. The algorithm, contextual variable elimination, is based on the standard variable elimination algorithm that eliminates the nonquery variables in turn, but when eliminating a variable, the tables that need to be multiplied can depend on the context. This algorithm reduces to standard variable elimination when there is no contextual independence structure to exploit. We show how this can be much more efficient than variable elimination when there is structure to exploit. We explain why this new method can exploit more structure than previous methods for structured belief network inference and an analogous algorithm that uses trees.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {263–313},
numpages = {51}
}

@article{10.5555/1622420.1622427,
author = {Wilkins, David E. and Lee, Thomas J. and Berry, Pauline},
title = {Interactive Execution Monitoring of Agent Teams},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {There is an increasing need for automated support for humans monitoring the activity of distributed teams of cooperating agents, both human and machine. We characterize the domain-independent challenges posed by this problem, and describe how properties of domains influence the challenges and their solutions. We will concentrate on dynamic, data-rich domains where humans are ultimately responsible for team behavior. Thus, the automated aid should interactively support effective and timely decision making by the human. We present a domain-independent categorization of the types of alerts a plan-based monitoring system might issue to a user, where each type generally requires different monitoring techniques. We describe a monitoring framework for integrating many domain-specific and task-specific monitoring techniques and then using the concept of value of an alert to avoid operator overload.We use this framework to describe an execution monitoring approach we have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains to assist a human in monitoring team behavior. One domain (Army small unit operations) has hundreds of mobile, geographically distributed agents, a combination of humans, robots, and vehicles. The other domain (teams of unmanned ground and air vehicles) has a handful of cooperating robots. Both domains involve unpredictable adversaries in the vicinity. Our approach customizes monitoring behavior for each specific task, plan, and situation, as well as for user preferences. Our EAs alert the human controller when reported events threaten plan execution or physically threaten team members. Alerts were generated in a timely manner without inundating the user with too many alerts (less than 10% of alerts are unwanted, as judged by domain experts).},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {217–261},
numpages = {45}
}

@article{10.5555/1622420.1622426,
author = {Tan, Kay Chen and Khor, Eik Fun and Lee, Tong Heng and Sathikannan, Ramasubramanian},
title = {An Evolutionary Algorithm with Advanced Goal and Priority Specification for Multi-Objective Optimization},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {This paper presents an evolutionary algorithm with a new goal-sequence domination scheme for better decision support in multi-objective optimization. The approach allows the inclusion of advanced hard/soft priority and constraint information on each objective component, and is capable of incorporating multiple specifications with overlapping or non-overlapping objective functions via logical "OR" and "AND" connectives to drive the search towards multiple regions of trade-off. In addition, we propose a dynamic sharing scheme that is simple and adaptively estimated according to the on-line population distribution without needing any a priori parameter setting. Each feature in the proposed algorithm is examined to show its respective contribution, and the performance of the algorithm is compared with other evolutionary optimization methods. It is shown that the proposed algorithm has performed well in the diversity of evolutionary search and uniform distribution of non-dominated individuals along the final trade-offs, without significant computational effort. The algorithm is also applied to the design optimization of a practical servo control system for hard disk drives with a single voice-coil-motor actuator. Results of the evolutionary designed servo control system show a superior closed-loop performance compared to classical PID or RPT approaches.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {183–215},
numpages = {33}
}

@article{10.5555/1622420.1622425,
author = {Lerman, Kristina and Minton, Steven N. and Knoblock, Craig A.},
title = {Wrapper Maintenance: A Machine Learning Approach},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {The proliferation of online information sources has led to an increased use of wrappers for extracting data from Web sources. While most of the previous research has focused on quick and efficient generation of wrappers, the development of tools for wrapper maintenance has received less attention. This is an important research problem because Web sources often change in ways that prevent the wrappers from extracting data correctly. We present an efficient algorithm that learns structural information about data from positive examples alone. We describe how this information can be used for two wrapper maintenance applications: wrapper verification and reinduction. The wrapper verification system detects when a wrapper is not extracting correct data, usually because the Web source has changed its format. The reinduction algorithm automatically recovers from changes in the Web source by identifying data on Web pages so that a new wrapper may be generated for this source. To validate our approach, we monitored 27 wrappers over a period of a year. The verification algorithm correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes, resulting in precision of 0.73 and recall of 0.95. We validated the reinduction algorithm on ten Web sources. We were able to successfully reinduce the wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data extraction task.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {149–181},
numpages = {33}
}

@article{10.5555/1622420.1622424,
author = {Peral, Jes\'{u} and Ferr\'{a}ez, Antonio},
title = {Translation of Pronominal Anaphora between English and Spanish: Discrepancies and Evaluation},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {This paper evaluates the different tasks carried out in the translation of pronominal anaphora in a machine translation (MT) system. The MT interlingua approach named AGIR (Anaphora Generation with an Interlingua Representation) improves upon other proposals presented to date because it is able to translate intersentential anaphors, detect co-reference chains, and translate Spanish zero pronouns into English--issues hardly considered by other systems. The paper presents the resolution and evaluation of these anaphora problems in AGIR with the use of different kinds of knowledge (lexical, morphological, syntactic, and semantic). The translation of English and Spanish anaphoric third-person personal pronouns (including Spanish zero pronouns) into the target language has been evaluated on unrestricted corpora. We have obtained a precision of 80.4% and 84.8% in the translation of Spanish and English pronouns, respectively. Although we have only studied the Spanish and English languages, our approach can be easily extended to other languages such as Portuguese, Italian, or Japanese.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {117–147},
numpages = {31}
}

@article{10.5555/1622420.1622423,
author = {Grumberg, Orna and Livne, Shlomi and Markovitch, Shaul},
title = {Learning to Order BDD Variables in Verification},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {The size and complexity of software and hardware systems have significantly increased in the past years. As a result, it is harder to guarantee their correct behavior. One of the most successful methods for automated verification of finite-state systems is model checking. Most of the current model-checking systems use binary decision diagrams (BDDs) for the representation of the tested model and in the verification process of its properties. Generally, BDDs allow a canonical compact representation of a boolean function (given an order of its variables). The more compact the BDD is, the better performance one gets from the verifier. However, finding an optimal order for a BDD is an NP-complete problem. Therefore, several heuristic methods based on expert knowledge have been developed for variable ordering.We propose an alternative approach in which the variable ordering algorithm gains "ordering experience" from training models and uses the learned knowledge for finding good orders. Our methodology is based on offine learning of pair precedence classifiers from training models, that is, learning which variable pair permutation is more likely to lead to a good order. For each training model, a number of training sequences are evaluated. Every training model variable pair permutation is then tagged based on its performance on the evaluated orders. The tagged permutations are then passed through a feature extractor and are given as examples to a classifier creation algorithm. Given a model for which an order is requested, the ordering algorithm consults each precedence classifier and constructs a pair precedence table which is used to create the order.Our algorithm was integrated with SMV, which is one of the most widely used verification systems. Preliminary empirical evaluation of our methodology, using real benchmark models, shows performance that is better than random ordering and is competitive with existing algorithms that use expert knowledge. We believe that in sub-domains of models (alu, caches, etc.) our system will prove even more valuable. This is because it features the ability to learn sub-domain knowledge, something that no other ordering algorithm does.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {83–116},
numpages = {34}
}

@article{10.5555/1622420.1622422,
author = {Cemgil, Ali Taylan and Kappen, Bert},
title = {Monte Carlo Methods for Tempo Tracking and Rhythm Quantization},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state space model. The switch variables correspond to discrete note locations as in a musical score. The continuous hidden variables denote the tempo. We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. Exact computation of posterior features such as the MAP state is intractable in this model class, so we introduce Monte Carlo methods for integration and optimization. We compare Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated annealing and iterative improvement) and sequential Monte Carlo methods (particle filters). Our simulation results suggest better results with sequential methods. The methods can be applied in both online and batch scenarios such as tempo tracking and transcription and are thus potentially useful in a number of music applications such as adaptive automatic accompaniment, score typesetting and music information retrieval.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {45–81},
numpages = {37}
}

@article{10.5555/1622420.1622421,
author = {Thompson, Cynthia A. and Mooney, Raymond J.},
title = {Acquiring Word-Meaning Mappings for Natural Language Interfaces},
year = {2003},
issue_date = {January 2003},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {18},
number = {1},
issn = {1076-9757},
abstract = {This paper focuses on a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of phrases paired with meaning representations. WOLFIE is part of an integrated system that learns to transform sentences into representations such as logical database queries.Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages. The usefulness of the lexicons learned by WOLFIE are compared to those acquired by a similar system, with results favorable to WOLFIE. A second set of experiments demonstrates WOLFIE's ability to scale to larger and more difficult, albeit artificially generated, corpora.In natural language acquisition, it is difficult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, most results to date for active learning have only considered standard classification tasks. To reduce annotation effort while maintaining accuracy, we apply active learning to semantic lexicons. We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–44},
numpages = {44}
}

@article{10.5555/1622810.1622825,
author = {Gamberger, Dragan and Lavrac, Nada},
title = {Expert-Guided Subgroup Discovery: Methodology and Application},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {This paper presents an approach to expert-guided subgroup discovery. The main step of the subgroup discovery process, the induction of subgroup descriptions, is performed by a heuristic beam search algorithm, using a novel parametrized definition of rule quality which is analyzed in detail. The other important steps of the proposed subgroup discovery process are the detection of statistically significant properties of selected subgroups and subgroup visualization: statistically significant properties are used to enrich the descriptions of induced subgroups, while the visualization shows subgroup properties in the form of distributions of the numbers of examples in the subgroups. The approach is illustrated by the results obtained for a medical problem of early detection of patient risk groups.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {501–527},
numpages = {27}
}

@article{10.5555/1622810.1622824,
author = {Bui, Hung H. and Venkatesh, Svetha and West, Geoff},
title = {Policy Recognition in the Abstract Hidden Markov Model},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we present a method for recognising an agent's behaviour in dynamic, noisy, uncertain domains, and across multiple levels of abstraction. We term this problem on-line plan recognition under uncertainty and view it generally as probabilistic inference on the stochastic process representing the execution of the agent's plan. Our contributions in this paper are twofold. In terms of probabilistic inference, we introduce the Abstract Hidden Markov Model (AHMM), a novel type of stochastic processes, provide its dynamic Bayesian network (DBN) structure and analyse the properties of this network. We then describe an application of the Rao-Blackwellised Particle Filter to the AHMM which allows us to construct an efficient, hybrid inference method for this model. In terms of plan recognition, we propose a novel plan recognition framework based on the AHMM as the plan execution model. The Rao-Blackwellised hybrid inference for AHMM can take advantage of the independence properties inherent in a model of plan execution, leading to an algorithm for online probabilistic plan recognition that scales well with the number of levels in the plan hierarchy. This illustrates that while stochastic models for plan execution can be complex, they exhibit special structures which, if exploited, can lead to efficient plan recognition algorithms. We demonstrate the usefulness of the AHMM framework via a behaviour recognition system in a complex spatial environment using distributed video surveillance data.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {451–499},
numpages = {49}
}

@article{10.5555/1622810.1622823,
author = {Fern, Alan and Givan, Robert and Siskind, Jeffrey Mark},
title = {Specific-to-General Learning for Temporal Events with Application to Learning Event Definitions from Video},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences. First, we introduce a simple, propositional, temporal, event-description language called AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning. We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas. We present a positive-examples-only specific-to-general learning method based on these algorithms. We also present a polynomial-time-computable "syntactic" subsumption test that implies semantic subsumption without being equivalent to it. A generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm. Finally, we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {379–449},
numpages = {71}
}

@article{10.5555/1622810.1622822,
author = {Tennenholtz, Moshe},
title = {Competitive Safety Analysis: Robust Decision-Making in Multi-Agent Systems},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Much work in AI deals with the selection of proper actions in a given (known or unknown) environment. However, the way to select a proper action when facing other agents is quite unclear. Most work in AI adopts classical game-theoretic equilibrium analysis to predict agent behavior in such settings. This approach however does not provide us with any guarantee for the agent. In this paper we introduce competitive safety analysis. This approach bridges the gap between the desired normative AI approach, where a strategy should be selected in order to guarantee a desired payoff, and equilibrium analysis. We show that a safety level strategy is able to guarantee the value obtained in a Nash equilibrium, in several classical computer science settings. Then, we discuss the concept of competitive safety strategies, and illustrate its use in a decentralized load balancing setting, typical to network problems. In particular, we show that when we have many agents, it is possible to guarantee an expected payoff which is a factor of 8/9 of the payoff obtained in a Nash equilibrium. Our discussion of competitive safety analysis for decentralized load balancing is further developed to deal with many communication links and arbitrary speeds. Finally, we discuss the extension of the above concepts to Bayesian games, and illustrate their use in a basic auctions setup.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {363–378},
numpages = {16}
}

@article{10.5555/1622810.1622820,
author = {Gao, Yong and Culberson, Joseph},
title = {An Analysis of Phase Transition in NK Landscapes},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we analyze the decision version of the NK landscape model from the perspective of threshold phenomena and phase transitions under two random distributions, the uniform probability model and the fixed ratio model. For the uniform probability model, we prove that the phase transition is easy in the sense that there is a polynomial algorithm that can solve a random instance of the problem with the probability asymptotic to 1 as the problem size tends to infinity. For the fixed ratio model, we establish several upper bounds for the solubility threshold, and prove that random instances with parameters above these upper bounds can be solved polynomially. This, together with our empirical study for random instances generated below and in the phase transition region, suggests that the phase transition of the fixed ratio model is also easy.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {309–332},
numpages = {24}
}

@article{10.5555/1622810.1622819,
author = {Bod, Rens},
title = {A Unified Model of Structural Organization in Language and Music},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Is there a general model that can predict the perceived phrase structure in language and music? While it is usually assumed that humans have separate faculties for language and music, this work focuses on the commonalities rather than on the differences between these modalities, aiming at finding a deeper "faculty". Our key idea is that the perceptual system strives for the simplest structure (the "simplicity principle"), but in doing so it is biased by the likelihood of previous structures (the "likelihood principle"). We present a series of data-oriented parsing (DOP) models that combine these two principles and that are tested on the Penn Treebank and the Essen Folksong Collection. Our experiments show that (1) a combination of the two principles outperforms the use of either of them, and (2) exactly the same model with the same parameter setting achieves maximum accuracy for both language and music. We argue that our results suggest an interesting parallel between linguistic and musical structuring.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {289–308},
numpages = {20}
}

@article{10.5555/1622810.1622818,
author = {Chan, Hei and Darwiche, Adnan},
title = {When Do Numbers Really Matter?},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Common wisdom has it that small distinctions in the probabilities (parameters) quantifying a belief network do not matter much for the results of probabilistic queries. Yet, one can develop realistic scenarios under which small variations in network parameters can lead to significant changes in computed queries. A pending theoretical question is then to analytically characterize parameter changes that do or do not matter. In this paper, we study the sensitivity of probabilistic queries to changes in network parameters and prove some tight bounds on the impact that such parameters can have on queries. Our analytic results pinpoint some interesting situations under which parameter changes do or do not matter. These results are important for knowledge engineers as they help them identify influential network parameters. They also help explain some of the previous experimental results and observations with regards to network robustness against parameter changes.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {265–287},
numpages = {23}
}

@article{10.5555/1622810.1622817,
author = {Darwiche, Adnan and Marquis, Pierre},
title = {A Knowledge Compilation Map},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {229–264},
numpages = {36}
}

@article{10.5555/1622810.1622816,
author = {Scerri, Paul and Pynadath, David V. and Tambe, Milind},
title = {Towards Adjustable Autonomy for the Real World},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Adjustable autonomy refers to entities dynamically varying their own autonomy, transferring decision-making control to other entities (typically agents transferring control to human users) in key situations. Determining whether and when such transfers-of-control should occur is arguably the fundamental research problem in adjustable autonomy. Previous work has investigated various approaches to addressing this problem but has often focused on individual agent-human interactions. Unfortunately, domains requiring collaboration between teams of agents and humans reveal two key shortcomings of these previous approaches. First, these approaches use rigid one-shot transfers of control that can result in unacceptable coordination failures in multiagent settings. Second, they ignore costs (e.g., in terms of time delays or effects on actions) to an agent's team due to such transfers-of-control.To remedy these problems, this article presents a novel approach to adjustable autonomy, based on the notion of a transfer-of-control strategy. A transfer-of-control strategy consists of a conditional sequence of two types of actions: (i) actions to transfer decision-making control (e.g., from an agent to a user or vice versa) and (ii) actions to change an agent's pre-specified coordination constraints with team members, aimed at minimizing miscoordination costs. The goal is for high-quality individual decisions to be made with minimal disruption to the coordination of the team. We present a mathematical model of transfer-of-control strategies. The model guides and informs the operationalization of the strategies using Markov Decision Processes, which select an optimal strategy, given an uncertain environment and costs to the individuals and teams. The approach has been carefully evaluated, including via its use in a real-world, deployed multi-agent system that assists a research group in its daily activities.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {171–228},
numpages = {58}
}

@article{10.5555/1622810.1622815,
author = {Nock, Richard},
title = {Inducing Interpretable Voting Classifiers without Trading Accuracy for Simplicity: Theoretical Results, Approximation Algorithms, and Experiments},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Recent advances in the study of voting classification algorithms have brought empirical and theoretical results clearly showing the discrimination power of ensemble classifiers. It has been previously argued that the search of this classification power in the design of the algorithms has marginalized the need to obtain interpretable classifiers. Therefore, the question of whether one might have to dispense with interpretability in order to keep classification strength is being raised in a growing number of machine learning or data mining papers. The purpose of this paper is to study both theoretically and empirically the problem. First, we provide numerous results giving insight into the hardness of the simplicity-accuracy tradeoff for voting classifiers. Then we provide an efficient "top-down and prune" induction heuristic, WIDC, mainly derived from recent results on the weak learning and boosting frameworks. It is to our knowledge the first attempt to build a voting classifier as a base formula using the weak learning framework (the one which was previously highly successful for decision tree induction), and not the strong learning framework (as usual for such classifiers with boosting-like approaches). While it uses a well-known induction scheme previously successful in other classes of concept representations, thus making it easy to implement and compare, WIDC also relies on recent or new results we give about particular cases of boosting known as partition boosting and ranking loss boosting. Experimental results on thirty-one domains, most of which readily available, tend to display the ability of WIDC to produce small, accurate, and interpretable decision committees.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {137–170},
numpages = {34}
}

@article{10.5555/1622810.1622814,
author = {Kaminka, Gal A. and Pynadath, David V. and Tambe, Milind},
title = {Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Recent years are seeing an increasing need for on-line monitoring of teams of cooperating agents, e.g., for visualization, or performance tracking. However, in monitoring deployed teams, we often cannot rely on the agents to always communicate their state to the monitoring system. This paper presents a non-intrusive approach to monitoring by overhearing, where the monitored team's state is inferred (via plan-recognition) from team-members' routine communications, exchanged as part of their coordinated task execution, and observed (overheard) by the monitoring system. Key challenges in this approach include the demanding run-time requirements of monitoring, the scarceness of observations (increasing monitoring uncertainty), and the need to scale-up monitoring to address potentially large teams. To address these, we present a set of complementary novel techniques, exploiting knowledge of the social structures and procedures in the monitored team: (i) an efficient probabilistic plan-recognition algorithm, well-suited for processing communications as observations; (ii) an approach to exploiting knowledge of the team's social behavior to predict future observations during execution (reducing monitoring uncertainty); and (iii) monitoring algorithms that trade expressivity for scalability, representing only certain useful monitoring hypotheses, but allowing for any number of agents and their different activities to be represented in a single coherent entity. We present an empirical evaluation of these techniques, in combination and apart, in monitoring a deployed team of agents, running on machines physically distributed across the country, and engaged in complex, dynamic task execution. We also compare the performance of these techniques to human expert and novice monitors, and show that the techniques presented are capable of monitoring at human-expert levels, despite the difficulty of the task.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {83–135},
numpages = {53}
}

@article{10.5555/1622810.1622813,
author = {Halpern, Joseph Y. and Pucella, Riccardo},
title = {A Logic for Reasoning about Upper Probabilities},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {We present a propositional logic to reason about the uncertainty of events, where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event. We give a sound and complete axiomatization for the logic, and show that the satisfiability problem is NP-complete, no harder than satisfiability for propositional logic.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {57–81},
numpages = {25}
}

@article{10.5555/1622810.1622812,
author = {Barzilay, Regina and Elhadad, Noemie and McKeown, Kathleen R.},
title = {Inferring Strategies for Sentence Ordering in Multidocument News Summarization},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {The problem of organizing information for multidocument summarization so that the generated summary is coherent has received relatively little attention. While sentence ordering for single document summarization can be determined from the ordering of sentences in the input article, this is not the case for multidocument summarization where summary sentences may be drawn from different input articles. In this paper, we propose a methodology for studying the properties of ordering information in the news genre and describe experiments done on a corpus of multiple acceptable orderings we developed for the task. Based on these experiments, we implemented a strategy for ordering information that combines constraints from chronological order of events and topical relatedness. Evaluation of our augmented algorithm shows a significant improvement of the ordering over two baseline strategies.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {35–55},
numpages = {21}
}

@article{10.5555/1622810.1622821,
author = {Al-Ani, Ahmed and Deriche, Mohamed},
title = {A New Technique for Combining Multiple Classifiers Using the Dempster-Shafer Theory of Evidence},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new classifier combination technique based on the Dempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a powerful method for combining measures of evidence from different classifiers. However, since each of the available methods that estimates the evidence of classifiers has its own limitations, we propose here a new implementation which adapts to training data so that the overall mean square error is minimized. The proposed technique is shown to outperform most available classifier combination methods when tested on three different classification problems.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {333–361},
numpages = {29}
}

@article{10.5555/1622810.1622811,
author = {Howe, Adele E. and Dahlman, Eric},
title = {A Critical Assessment of Benchmark Comparison in Planning},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {Recent trends in planning research have led to empirical comparison becoming commonplace. The field has started to settle into a methodology for such comparisons, which for obvious practical reasons requires running a subset of planners on a subset of problems. In this paper, we characterize the methodology and examine eight implicit assumptions about the problems, planners and metrics used in many of these comparisons. The problem assumptions are: PR1) the performance of a general purpose planner should not be penalized/biased if executed on a sampling of problems and domains, PR2) minor syntactic differences in representation do not affect performance, and PR3) problems should be solvable by STRIPS capable planners unless they require ADL. The planner assumptions are: PL1) the latest version of a planner is the best one to use, PL2) default parameter settings approximate good performance, and PL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1) performance degrades similarly for each planner when run on degraded runtime environments (e.g., machine platform) and M2) the number of plan steps distinguishes performance. We find that most of these assumptions are not supported empirically; in particular, that planners are affected differently by these assumptions. We conclude with a call to the community to devote research resources to improving the state of the practice and especially to enhancing the available benchmark problems.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–33},
numpages = {33}
}

@article{10.5555/1622407.1622419,
author = {Baget, Jean-Fran\c{c}s and Mugnier, Marie-Laure},
title = {Extensions of Simple Conceptual Graphs: The Complexity of Rules and Constraints},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Simple conceptual graphs are considered as the kernel of most knowledge representation formalisms built upon Sowa's model. Reasoning in this model can be expressed by a graph homomorphism called projection, whose semantics is usually given in terms of positive, conjunctive, existential FOL. We present here a family of extensions of this model, based on rules and constraints, keeping graph homomorphism as the basic operation. We focus on the formal definitions of the different models obtained, including their operational semantics and relationships with FOL, and we analyze the decidability and complexity of the associated problems (consistency and deduction). As soon as rules are involved in reasonings, these problems are not decidable, but we exhibit a condition under which they fall in the polynomial hierarchy. These results extend and complete the ones already published by the authors. Moreover we systematically study the complexity of some particular cases obtained by restricting the form of constraints and/or rules.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {425–465},
numpages = {41}
}

@article{10.5555/1622407.1622418,
author = {Pynadath, David V. and Tambe, Milind},
title = {The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories and Models},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Despite the significant progress in multiagent teamwork, existing research does not address the optimality of its prescriptions nor the complexity of the teamwork problem. Without a characterization of the optimality-complexity tradeoffs, it is impossible to determine whether the assumptions and approximations made by a particular theory gain enough efficiency to justify the losses in overall performance. To provide a tool for use by multiagent researchers in evaluating this tradeoff, we present a unified framework, the COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model combines and extends existing multiagent theories, such as decentralized partially observable Markov decision processes and economic team theory. In addition to their generality of representation, COM-MTDPs also support the analysis of both the optimality of team performance and the computational complexity of the agents' decision problem. In analyzing complexity, we present a breakdown of the computational complexity of constructing optimal teams under various classes of problem domains, along the dimensions of observability and communication cost. In analyzing optimality, we exploit the COM-MTDP's ability to encode existing teamwork theories and models to encode two instantiations of joint intentions theory taken from the literature. Furthermore, the COM-MTDP model provides a basis for the development of novel team coordination algorithms. We derive a domain-independent criterion for optimal communication and provide a comparative analysis of the two joint intentions instantiations with respect to this optimal policy. We have implemented a reusable, domain-independent software package based on COM-MTDPs to analyze teamwork coordination strategies, and we demonstrate its use by encoding and evaluating the two joint intentions strategies within an example domain.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {389–423},
numpages = {35}
}

@article{10.5555/1622407.1622417,
author = {Wolpert, David H. and Tumer, Kagan},
title = {Collective Intelligence, Data Routing and Braess' Paradox},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of designing the the utility functions of the utility-maximizing agents in a multi-agent system (MAS) so that they work synergistically to maximize a global utility. The particular problem domain we explore is the control of network routing by placing agents on all the routers in the network. Conventional approaches to this task have the agents all use the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many cases, due to the side-effects of one agent's actions on another agent's performance, having agents use ISPA's is suboptimal as far as global aggregate cost is concerned, even when they are only used to route infinitesimally small amounts of traffic. The utility functions of the individual agents are not "aligned" with the global utility, intuitively speaking. As a particular example of this we present an instance of Braess' paradox in which adding new links to a network whose agents all use the ISPA results in a decrease in overall throughput. We also demonstrate that load-balancing, in which the agents' decisions are collectively made to optimize the global cost incurred by all traffic currently being routed, is suboptimal as far as global cost averaged across time is concerned. This is also due to "side-effects", in this case of current routing decision on future traffic. The mathematics of Collective Intelligence (COIN) is concerned precisely with the issue of avoiding such deleterious side-effects in multi-agent systems, both over time and space. We present key concepts from that mathematics and use them to derive an algorithm whose ideal version should have better performance than that of having all agents use the ISPA, even in the infinitesimal limit. We present experiments verifying this, and also showing that a machine-learning-based version of this COIN algorithm in which costs are only imprecisely estimated via empirical means (a version potentially applicable in the real world) also outperforms the ISPA, despite having access to less information than does the ISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {359–387},
numpages = {29}
}

@article{10.5555/1622407.1622416,
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
title = {SMOTE: Synthetic Minority over-Sampling Technique},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {321–357},
numpages = {37}
}

@article{10.5555/1622407.1622415,
author = {Walker, Marilyn A. and Langkilde-Geary, Irene and Hastie, Helen Wright and Wright, Jerry and Gorin, Allen},
title = {Automatically Training a Problematic Dialogue Predictor for a Spoken Dialogue System},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Spoken dialogue systems promise efficient and natural access to a large variety of information sources and services from any phone. However, current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on automatically training a Problematic Dialogue Predictor to predict problematic human-computer dialogues using a corpus of 4692 dialogues collected with the How May I Help YouSM spoken dialogue system. The Problematic Dialogue Predictor can be immediately applied to the system's decision of whether to transfer the call to a human customer care agent, or be used as a cue to the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We show that a Problematic Dialogue Predictor using automatically-obtainable features from the first two exchanges in the dialogue can predict problematic dialogues 13.2% more accurately than the baseline.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {293–319},
numpages = {27}
}

@article{10.5555/1622407.1622414,
author = {Xu, Xin and He, Han-gen and Hu, Dewen},
title = {Efficient Reinforcement Learning Using Recursive Least-Squares Methods},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence speed, which is considered to be optimal in practice. In this paper, RLS methods are used to solve reinforcement learning problems, where two new reinforcement learning algorithms using linear value function approximators are proposed and analyzed. The two algorithms are called RLS-TD(λ) and Fast-AHC (Fast Adaptive Heuristic Critic), respectively. RLS-TD(λ) can be viewed as the extension of RLS-TD(0) from λ =0 to general 0≤ λ ≤1, so it is a multi-step temporal-difference (TD) learning algorithm using RLS methods. The convergence with probability one and the limit of convergence of RLS-TD(λ) are proved for ergodic Markov chains. Compared to the existing LS-TD(λ) algorithm, RLS-TD(λ) has advantages in computation and is more suitable for online learning. The effectiveness of RLS-TD(λ) is analyzed and verified by learning prediction experiments of Markov chains with a wide range of parameter settings.The Fast-AHC algorithm is derived by applying the proposed RLS-TD(λ) algorithm in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use of RLS methods to improve the learning-prediction efficiency in the critic. Learning control experiments of the cart-pole balancing and the acrobot swing-up problems are conducted to compare the data efficiency of Fast-AHC with conventional AHC. From the experimental results, it is shown that the data efficiency of learning control can also be improved by using RLS methods in the learning-prediction process of the critic. The performance of Fast-AHC is also compared with that of the AHC method using LS-TD(λ). Furthermore, it is demonstrated in the experiments that different initial values of the variance matrix in RLS-TD(λ) are required to get better performance not only in learning prediction but also in learning control. The experimental results are analyzed based on the existing theoretical work on the transient phase of forgetting factor RLS methods.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {259–292},
numpages = {34}
}

@article{10.5555/1622407.1622413,
author = {Di Sciascio, Eugenio and Donini, Francesco M. and Mongiello, Marina},
title = {Structured Knowledge Representation for Image Retrieval},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {We propose a structured approach to the problem of retrieval of images by content and present a description logic that has been devised for the semantic indexing and retrieval of images containing complex objects.As other approaches do, we start from low-level features extracted with image analysis to detect and characterize regions in an image. However, in contrast with feature-based approaches, we provide a syntax to describe segmented regions as basic objects and complex objects as compositions of basic ones. Then we introduce a companion extensional semantics for defining reasoning services, such as retrieval, classification, and subsumption. These services can be used for both exact and approximate matching, using similarity measures.Using our logical approach as a formal specification, we implemented a complete client-server image retrieval system, which allows a user to pose both queries by sketch and queries by example. A set of experiments has been carried out on a testbed of images to assess the retrieval capabilities of the system in comparison with expert users ranking. Results are presented adopting a well-established measure of quality borrowed from textual information retrieval.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {209–257},
numpages = {49}
}

@article{10.5555/1622407.1622412,
author = {Shatkay, Hagit and Kaelbling, Leslie Pack},
title = {Learning Geometrically-Constrained Hidden Markov Models for Robot Navigation: Bridging the Topological-Geometrical Gap},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Hidden Markov models (HMMs) and partially observable Markov decision processes (POMDPs) provide useful tools for modeling dynamical systems. They are particularly useful for representing the topology of environments such as road networks and office buildings, which are typical for robot navigation and planning. The work presented here describes a formal framework for incorporating readily available odometric information and geometrical constraints into both the models and the algorithm that learns them. By taking advantage of such information, learning HMMs/POMDPs can be made to generate better solutions and require fewer iterations, while being robust in the face of data reduction. Experimental results, obtained from both simulated and real robot data, demonstrate the effectiveness of the approach.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {167–207},
numpages = {41}
}

@article{10.5555/1622407.1622411,
author = {Blockeel, Hendrik and Dehaspe, Luc and Demoen, Bart and Janssens, Gerda and Ramon, Jan and Vandecasteele, Henk},
title = {Improving the Efficiency of Inductive Logic Programming through the Use of Query Packs},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Inductive logic programming, or relational learning, is a powerful paradigm for machine learning or data mining. However, in order for ILP to become practically useful, the efficiency of ILP systems must improve substantially. To this end, the notion of a query pack is introduced: it structures sets of similar queries. Furthermore, a mechanism is described for executing such query packs. A complexity analysis shows that considerable efficiency improvements can be achieved through the use of this query pack execution mechanism. This claim is supported by empirical results obtained by incorporating support for query pack execution in two existing learning systems.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {135–166},
numpages = {32}
}

@article{10.5555/1622407.1622410,
author = {Singh, Satinder and Litman, Diane and Kearns, Michael and Walker, Marilyn},
title = {Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {105–133},
numpages = {29}
}

@article{10.5555/1622407.1622409,
author = {Drummond, Chris},
title = {Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {This paper discusses a system that accelerates reinforcement learning by using transfer from related tasks. Without such transfer, even if two tasks are very similar at some abstract level, an extensive re-learning effort is required. The system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution. The system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task. These features are stable and easy to recognize early in the learning process. They generate a partitioning of the state space and thus the function. The partition is represented as a graph. This is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task. Experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {59–104},
numpages = {46}
}

@article{10.5555/1622407.1622408,
author = {Baader, Franz and Lutz, Carsten and Sturm, Holger and Wolter, Frank},
title = {Fusions of Description Logics and Abstract Description Systems},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {Fusions are a simple way of combining logics. For normal modal logics, fusions have been investigated in detail. In particular, it is known that, under certain conditions, decidability transfers from the component logics to their fusion. Though description logics are closely related to modal logics, they are not necessarily normal. In addition, ABox reasoning in description logics is not covered by the results from modal logics.In this paper, we extend the decidability transfer results from normal modal logics to a large class of description logics. To cover different description logics in a uniform way, we introduce abstract description systems, which can be seen as a common generalization of description and modal logics, and show the transfer results in this general setting.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–58},
numpages = {58}
}

@article{10.5555/1622845.1622858,
author = {Sato, Taisuke and Kameya, Yoshitaka},
title = {Parameter Learning of Logic Programs for Symbolic-Statistical Modeling},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. definite clause programs containing probabilistic facts with a parameterized distribution. It extends the traditional least Herbrand model semantics in logic programming to distribution semantics, possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs and Bayesian networks.We also propose a new EM algorithm, the graphical EM algorithm, that runs for a class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent. It runs on a new data structure called support graphs describing the logical relationship between observations and their explanations, and learns parameters by computing inside and outside probability generalized for logic programs.The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks that have been developed independently in each research field. Learning experiments with PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can significantly outperform the Inside-Outside algorithm.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {391–454},
numpages = {64}
}

@article{10.5555/1622845.1622857,
author = {Meek, Christopher},
title = {Finding a Path is Harder than Finding a Tree},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {I consider the problem of learning an optimal path graphical model from data and show the problem to be NP-hard for the maximum likelihood and minimum description length approaches and a Bayesian approach. This hardness result holds despite the fact that the problem is a restriction of the polynomially solvable problem of finding the optimal tree graphical model.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {383–389},
numpages = {7}
}

@article{10.5555/1622845.1622856,
author = {Baxter, Jonathan and Bartlett, Peter L. and Weaver, Lex},
title = {Experiments with Infinite-Horizon, Policy-Gradient Estimation},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter &amp; Bartlett, 2001), which computes biased estimates of the performance gradient in POMDPs. The algorithm's chief advantages are that it uses only one free parameter β ∈ [0, 1], which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of Baxter and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {351–381},
numpages = {31}
}

@article{10.5555/1622845.1622855,
author = {Baxter, Jonathan and Bartlett, Peter L.},
title = {Infinite-Horizon Policy-Gradient Estimation},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter β ∈ [0, 1] (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter β is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, &amp; Weaver, 2001) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {319–350},
numpages = {32}
}

@article{10.5555/1622845.1622854,
author = {Renz, Jochen and Nebel, Bernhard},
title = {Efficient Methods for Qualitative Spatial Reasoning},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {The theoretical properties of qualitative spatial reasoning in the RCC-8 framework have been analyzed extensively. However, no empirical investigation has been made yet. Our experiments show that the adaption of the algorithms used for qualitative temporal reasoning can solve large RCC-8 instances, even if they are in the phase transition region - provided that one uses the maximal tractable subsets of RCC-8 that have been identified by us. In particular, we demonstrate that the orthogonal combination of heuristic methods is successful in solving almost all apparently hard instances in the phase transition region up to a certain size in reasonable time.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {289–318},
numpages = {30}
}

@article{10.5555/1622845.1622853,
author = {Palomar, Manuel and Mart\'{\i}nez-Barco, Patricio},
title = {Computational Approach to Anaphora Resolution in Spanish Dialogues},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {This paper presents an algorithm for identifying noun-phrase antecedents of pronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora resolution requires numerous sources of information in order to find the correct antecedent of the anaphor. These sources can be of different kinds, e.g., linguistic information, discourse/dialogue structure information, or topic information. For this reason, our algorithm uses various different kinds of information (hybrid information). The algorithm is based on linguistic constraints and preferences and uses an anaphoric accessibility space within which the algorithm finds the noun phrase. We present some experiments related to this algorithm and this space using a corpus of 204 dialogues. The algorithm is implemented in Prolog. According to this study, 95.9% of antecedents were located in the proposed space, a precision of 81.3% was obtained for pronominal anaphora resolution, and 81.5% for adjectival anaphora.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {263–287},
numpages = {25}
}

@article{10.5555/1622845.1622852,
author = {Ambite, Jos\'{e} Luis and Knoblock, Craig A.},
title = {Planning by Rewriting},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {Domain-independent planning is a hard combinatorial problem. Taking into account plan quality makes the task even more dificult. This article introduces Planning by Rewriting (PbR), a new paradigm for efficient high-quality domain-independent planning. PbR exploits declarative plan-rewriting rules and efficient local search techniques to transform an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition to addressing the issues of planning efficiency and plan quality, this framework offers a new anytime planning algorithm. We have implemented this planner and applied it to several existing domains. The experimental results show that the PbR approach provides significant savings in planning effort while generating high-quality plans.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {207–261},
numpages = {55}
}

@article{10.5555/1622845.1622851,
author = {Stone, Peter and Littman, Michael L. and Singh, Satinder and Kearns, Michael},
title = {ATTac-2000: An Adaptive Autonomous Bidding Agent},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {The First Trading Agent Competition (TAC) was held from June 22nd to July 8th, 2000. TAC was designed to create a benchmark problem in the complex domain of e-marketplaces and to motivate researchers to apply unique approaches to a common task. This article describes ATTac-2000, the first-place finisher in TAC. ATTac-2000 uses a principled bidding strategy that includes several elements of adaptivity. In addition to the success at the competition, isolated empirical results are presented indicating the robustness and effectiveness of ATTac-2000's adaptive strategy.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {189–206},
numpages = {18}
}

@article{10.5555/1622845.1622850,
author = {Elomaa, Tapio and K\"{a}\"{a}ri\"{a}inen, Matti},
title = {An Analysis of Reduced Error Pruning},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {Top-down induction of decision trees has been observed to suffer from the inadequate functioning of the pruning phase. In particular, it is known that the size of the resulting tree grows linearly with the sample size, even though the accuracy of the tree does not improve. Reduced Error Pruning is an algorithm that has been used as a representative technique in attempts to explain the problems of decision tree learning.In this paper we present analyses of Reduced Error Pruning in three different settings. First we study the basic algorithmic properties of the method, properties that hold independent of the input decision tree and pruning examples. Then we examine a situation that intuitively should lead to the subtree under consideration to be replaced by a leaf node, one in which the class label and attribute values of the pruning examples are independent of each other. This analysis is conducted under two different assumptions. The general analysis shows that the pruning probability of a node fitting pure noise is bounded by a function that decreases exponentially as the size of the tree grows. In a specific analysis we assume that the examples are distributed uniformly to the tree. This assumption lets us approximate the number of subtrees that are pruned because they do not receive any pruning examples.This paper clarifies the different variants of the Reduced Error Pruning algorithm, brings new insight to its algorithmic properties, analyses the algorithm with less imposed assumptions than before, and includes the previously overlooked empty subtrees to the analysis.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {163–187},
numpages = {25}
}

@article{10.5555/1622845.1622849,
author = {Refanidis, Ioannis and Vlahavas, Ioannis},
title = {The GRT Planning System: Backward Heuristic Construction in Forward State-Space Planning},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {This paper presents GRT, a domain-independent heuristic planning system for STRIPS worlds. GRT solves problems in two phases. In the pre-processing phase, it estimates the distance between each fact and the goals of the problem, in a backward direction. Then, in the search phase, these estimates are used in order to further estimate the distance between each intermediate state and the goals, guiding so the search process in a forward direction and on a best-first basis. The paper presents the benefits from the adoption of opposite directions between the preprocessing and the search phases, discusses some difficulties that arise in the pre-processing phase and introduces techniques to cope with them. Moreover, it presents several methods of improving the efficiency of the heuristic, by enriching the representation and by reducing the size of the problem. Finally, a method of overcoming local optimal states, based on domain axioms, is proposed. According to it, difficult problems are decomposed into easier sub-problems that have to be solved sequentially. The performance results from various domains, including those of the recent planning competitions, show that GRT is among the fastest planners.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {115–161},
numpages = {47}
}

@article{10.5555/1622845.1622848,
author = {Bhattacharyya, Chiranjib and Keerthi, S. Sathiya},
title = {Mean-Field Methods for a Special Class of Belief Networks},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {The chief aim of this paper is to propose mean-field approximations for a broad class of Belief networks, of which sigmoid and noisy-or networks can be seen as special cases. The approximations are based on a powerful mean-field theory suggested by Plefka. We show that Saul, Jaakkola, and Jordan's approach is the first order approximation in Plefka's approach, via a variational derivation. The application of Plefka's theory to belief networks is not computationally tractable. To tackle this problem we propose new approximations based on Taylor series. Small scale experiments show that the proposed schemes are attractive.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {91–114},
numpages = {24}
}

@article{10.5555/1622845.1622846,
author = {Hong, Jun},
title = {Goal Recognition through Goal Graph Analysis},
year = {2001},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
abstract = {We present a novel approach to goal recognition based on a two-stage paradigm of graph construction and analysis. First, a graph structure called a Goal Graph is constructed to represent the observed actions, the state of the world, and the achieved goals as well as various connections between these nodes at consecutive time steps. Then, the Goal Graph is analysed at each time step to recognise those partially or fully achieved goals that are consistent with the actions observed so far. The Goal Graph analysis also reveals valid plans for the recognised goals or part of these goals.Our approach to goal recognition does not need a plan library. It does not suffer from the problems in the acquisition and hand-coding of large plan libraries, neither does it have the problems in searching the plan space of exponential size. We describe two algorithms for Goal Graph construction and analysis in this paradigm. These algorithms are both provably sound, polynomial-time, and polynomial-space. The number of goals recognised by our algorithms is usually very small after a sequence of observed actions has been processed. Thus the sequence of observed actions is well explained by the recognised goals with little ambiguity. We have evaluated these algorithms in the UNIX domain, in which excellent performance has been achieved in terms of accuracy, efficiency, and scalability.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–30},
numpages = {30}
}

@article{10.5555/1622394.1622406,
author = {Halpern, Joseph Y.},
title = {Conditional Plausibility Measures and Bayesian Networks},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {359–389},
numpages = {31}
}

@article{10.5555/1622394.1622405,
author = {Ginsberg, Matthew L.},
title = {GIB: Imperfect Information in a Computationally Challenging Game},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {This paper investigates the problems arising in the construction of a program to play the game of contract bridge. These problems include both the difficulty of solving the game's perfect information variant, and techniques needed to address the fact that bridge is not, in fact, a perfect information game. GIB, the program being described, involves five separate technical advances: partition search, the practical application of Monte Carlo techniques to realistic problems, a focus on achievable sets to solve problems inherent in the Monte Carlo approach, an extension of alpha-beta pruning from total orders to arbitrary distributive lattices, and the use of squeaky wheel optimization to find approximately optimal solutions to cardplay problems.GIB is currently believed to be of approximately expert caliber, and is currently the strongest computer bridge program in the world.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {303–358},
numpages = {56}
}

@article{10.5555/1622394.1622404,
author = {Hoffmann, J\"{o}rg and Nebel, Bernhard},
title = {The FF Planning System: Fast Plan Generation through Heuristic Search},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP's heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {253–302},
numpages = {50}
}

@article{10.5555/1622394.1622403,
author = {Basu, Chumki and Hirsh, Haym and Cohen, William W. and Nevill-Manning, Craig},
title = {Technical Paper Recommendation: A Study in Combining Multiple Information Sources},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {The growing need to manage and exploit the proliferation of online data sources is opening up new opportunities for bringing people closer to the resources they need. For instance, consider a recommendation service through which researchers can receive daily pointers to journal papers in their fields of interest. We survey some of the known approaches to the problem of technical paper recommendation and ask how they can be extended to deal with multiple information sources. More specifically, we focus on a variant of this problem - recommending conference paper submissions to reviewing committee members - which offers us a testbed to try different approaches. Using WHIRL - an information integration system - we are able to implement different recommendation algorithms derived from information retrieval principles. We also use a novel autonomous procedure for gathering reviewer interest information from the Web. We evaluate our approach and compare it to other methods using preference data provided by members of the AAAI-98 conference reviewing committee along with data about the actual submissions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {231–252},
numpages = {22}
}

@article{10.5555/1622394.1622402,
author = {Debruyne, Romuald and Bessi\'{e}re, Christian},
title = {Domain Filtering Consistencies},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {Enforcing local consistencies is one of the main features of constraint reasoning. Which level of local consistency should be used when searching for solutions in a constraint network is a basic question. Arc consistency and partial forms of arc consistency have been widely studied, and have been known for sometime through the forward checking or the MAC search algorithms. Until recently, stronger forms of local consistency remained limited to those that change the structure of the constraint graph, and thus, could not be used in practice, especially on large networks. This paper focuses on the local consistencies that are stronger than arc consistency, without changing the structure of the network, i.e., only removing inconsistent values from the domains. In the last five years, several such local consistencies have been proposed by us or by others. We make an overview of all of them, and highlight some relations between them. We compare them both theoretically and experimentally, considering their pruning efficiency and the time required to enforce them.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {205–230},
numpages = {26}
}

@article{10.5555/1622394.1622401,
author = {K\"{u}sters, Ralf and Borgida, Alex},
title = {What's in an Attribute? Consequences for the Least Common Subsumer},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {Functional relationships between objects, called "attributes", are of considerable importance in knowledge representation languages, including Description Logics (DLs). A study of the literature indicates that papers have made, often implicitly, different assumptions about the nature of attributes: whether they are always required to have a value, or whether they can be partial functions. The work presented here is the first explicit study of this difference for subclasses of the CLASSIC DL, involving the same-as concept constructor. It is shown that although determining subsumption between concept descriptions has the same complexity (though requiring different algorithms), the story is different in the case of determining the least common subsumer (lcs). For attributes interpreted as partial functions, the lcs exists and can be computed relatively easily; even in this case our results correct and extend three previous papers about the lcs of DLs. In the case where attributes must have a value, the lcs may not exist, and even if it exists it may be of exponential size. Interestingly, it is possible to decide in polynomial time if the lcs exists.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {167–203},
numpages = {37}
}

@article{10.5555/1622394.1622400,
author = {Straccia, Umberto},
title = {Reasoning within Fuzzy Description Logics},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {Description Logics (DLs) are suitable, well-known, logics for managing structured knowledge. They allow reasoning about individuals and well defined concepts, i.e. set of individuals with common properties. The experience in using DLs in applications has shown that in many cases we would like to extend their capabilities. In particular, their use in the context of Multimedia Information Retrieval (MIR) leads to the convincement that such DLs should allow the treatment of the inherent imprecision in multimedia object content representation and retrieval.In this paper we will present a fuzzy extension of ALC, combining Zadeh's fuzzy logic with a classical DL. In particular, concepts becomes fuzzy and, thus, reasoning about imprecise concepts is supported. We will define its syntax, its semantics, describe its properties and present a constraint propagation calculus for reasoning in it.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {137–166},
numpages = {30}
}

@article{10.5555/1622394.1622399,
author = {Boutilier, Craig and Brafman, Ronen I.},
title = {Partial-Order Planning with Concurrent Interacting Actions},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {In order to generate plans for agents with multiple actuators, agent teams, or distributed controllers, we must be able to represent and plan using concurrent actions with interacting effects. This has historically been considered a challenging task requiring a temporal planner with the ability to reason explicitly about time. We show that with simple modifications, the STRIPS action representation language can be used to represent interacting actions. Moreover, algorithms for partial-order planning require only small modifications in order to be applied in such multiagent domains. We demonstrate this fact by developing a sound and complete partial-order planner for planning with concurrent interacting actions, POMP, that extends existing partial-order planners in a straightforward way. These results open the way to the use of partial-order planners for the centralized control of cooperative multiagent systems.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {105–136},
numpages = {32}
}

@article{10.5555/1622394.1622398,
author = {Lusena, Christopher and Goldsmith, Judy and Mundhenk, Martin},
title = {Nonapproximability Results for Partially Observable Markov Decision Processes},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {We show that for several variations of partially observable Markov decision processes, polynomial-time algorithms for finding control policies are unlikely to or simply don't have guarantees of finding policies within a constant factor or a constant summand of optimal. Here "unlikely" means "unless some complexity classes collapse," where the collapses considered are P = NP, P = PSPACE, or P = EXP. Until or unless these collapses are shown to hold, any control-policy designer must choose between such performance guarantees and efficient computation.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {83–103},
numpages = {21}
}

@article{10.5555/1622394.1622397,
author = {Chen, Xinguang and van Beek, Peter},
title = {Conflict-Directed Backjumping Revisited},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {In recent years, many improvements to backtracking algorithms for solving constraint satisfaction problems have been proposed. The techniques for improving backtracking algorithms can be conveniently classified as look-ahead schemes and look-back schemes. Unfortunately, look-ahead and look-back schemes are not entirely orthogonal as it has been observed empirically that the enhancement of look-ahead techniques is sometimes counterproductive to the effects of look-back techniques. In this paper, we focus on the relationship between the two most important look-ahead techniques--using a variable ordering heuristic and maintaining a level of local consistency during the backtracking search--and the look-back technique of conflict-directed backjumping (CBJ). We show that there exists a "perfect" dynamic variable ordering such that CBJ becomes redundant. We also show theoretically that as the level of local consistency that is maintained in the backtracking search is increased, the less that backjumping will be an improvement. Our theoretical results partially explain why a backtracking algorithm doing more in the look-ahead phase cannot benefit more from the backjumping look-back scheme. Finally, we show empirically that adding CBJ to a backtracking algorithm that maintains generalized arc consistency (GAC), an algorithm that we refer to as GAC-CBJ, can still provide orders of magnitude speedups. Our empirical results contrast with Bessi\`{e}re and R\'{e}gin's conclusion (1996) that CBJ is useless to an algorithm that maintains arc consistency.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {53–81},
numpages = {29}
}

@article{10.5555/1622394.1622396,
author = {Zhang, Nevin L. and Zhang, Weihong},
title = {Speeding up the Convergence of Value Iteration in Partially Observable Markov Decision Processes},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {Partially observable Markov decision processes (POMDPs) have recently become popular among many AI researchers because they serve as a natural model for planning under uncertainty. Value iteration is a well-known algorithm for finding optimal policies for POMDPs. It typically takes a large number of iterations to converge. This paper proposes a method for accelerating the convergence of value iteration. The method has been evaluated on an array of benchmark problems and was found to be very effective: It enabled value iteration to converge after only a few iterations on all the test problems.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {29–51},
numpages = {23}
}

@article{10.5555/1622394.1622395,
author = {Brafman, Ronen I.},
title = {On Reachability, Relevance, and Resolution in the Planning as Satisfiability Approach},
year = {2001},
issue_date = {January 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {14},
number = {1},
issn = {1076-9757},
abstract = {In recent years, there is a growing awareness of the importance of reachability and relevance-based pruning techniques for planning, but little work specifically targets these techniques. In this paper, we compare the ability of two classes of algorithms to propagate and discover reachability and relevance constraints in classical planning problems. The first class of algorithms operates on SAT encoded planning problems obtained using the linear and GRAPHPLAN encoding schemes. It applies unit-propagation and more general resolution steps (involving larger clauses) to these plan encodings. The second class operates at the plan level and contains two families of pruning algorithms: Reachable-k and Relevant-k . Reachable-k provides a coherent description of a number of existing forward pruning techniques used in numerous algorithms, while Relevant-k captures different grades of backward pruning. Our results shed light on the ability of different plan-encoding schemes to propagate information forward and backward and on the relative merit of plan-level and SAT-level pruning methods.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–28},
numpages = {28}
}

@article{10.5555/1622262.1622269,
author = {Cimatti, Alessandro and Roveri, Marco},
title = {Conformant Planning via Symbolic Model Checking},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {We tackle the problem of planning in nondeterministic domains, by presenting a new approach to conformant planning. Conformant planning is the problem of finding a sequence of actions that is guaranteed to achieve the goal despite the nondeterminism of the domain. Our approach is based on the representation of the planning domain as a finite state automaton. We use Symbolic Model Checking techniques, in particular Binary Decision Diagrams, to compactly represent and efficiently search the automaton. In this paper we make the following contributions. First, we present a general planning algorithm for conformant planning, which applies to fully nondeterministic domains, with uncertainty in the initial condition and in action effects. The algorithm is based on a breadth-first, backward search, and returns conformant plans of minimal length, if a solution to the planning problem exists, otherwise it terminates concluding that the problem admits no conformant solution. Second, we provide a symbolic representation of the search space based on Binary Decision Diagrams (BDDs), which is the basis for search techniques derived from symbolic model checking. The symbolic representation makes it possible to analyze potentially large sets of states and transitions in a single computation step, thus providing for an efficient implementation. Third, we present CMBP (Conformant Model Based Planner), an efficient implementation of the data structures and algorithm described above, directly based on BDD manipulations, which allows for a compact representation of the search layers and an efficient implementation of the search steps. Finally, we present an experimental comparison of our approach with the state-of-the-art conformant planners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems from the distribution packages of these systems, plus other problems defined to stress a number of specific factors. Our approach appears to be the most effective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all the problems where a comparison is possible, CMBP outperforms its competitors, sometimes by orders of magnitude.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {305–338},
numpages = {34}
}

@article{10.5555/1622262.1622268,
author = {Dietterich, Thomas G.},
title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics--as a subroutine hierarchy--and a declarative semantics--as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this nonhierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {227–303},
numpages = {77}
}

@article{10.5555/1622262.1622267,
author = {Jensen, Rune M. and Veloso, Manuela M.},
title = {OBDD-Based Universal Planning for Synchronized Agents in Non-Deterministic Domains},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {Recently model checking representation and search techniques were shown to be efficiently applicable to planning, in particular to non-deterministic planning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDS) to encode a planning domain as a non-deterministic finite automaton and then apply fast algorithms from model checking to search for a solution. OBDDS can effectively scale and can provide universal plans for complex planning domains. We are particularly interested in addressing the complexities arising in non-deterministic, multi-agent domains. In this article, we present UMOP, a new universal OBDD-based planning framework for non-deterministic, multi-agent domains. We introduce a new planning domain description language, NADL, to specify non-deterministic, multi-agent domains. The language contributes the explicit definition of controllable agents and uncontrollable environment agents. We describe the syntax and semantics of NADL and show how to build an efficient OBDD-based representation of an NADL description. The UMOP planning system uses NADL and different OBDD-based universal planning algorithms. It includes the previously developed strong and strong cyclic planning algorithms. In addition, we introduce our new optimistic planning algorithm that relaxes optimality guarantees and generates plausible universal plans in some domains where no strong nor strong cyclic solution exists. We present empirical results applying UMOP to domains ranging from deterministic and single-agent with no environment actions to non-deterministic and multi-agent with complex environment actions. UMOP is shown to be a rich and efficient planning system.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {189–226},
numpages = {38}
}

@article{10.5555/1622262.1622266,
author = {Cheng, Jian and Druzdzel, Marek J.},
title = {AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {Stochastic sampling algorithms, while an attractive alternative to exact algorithms in very large Bayesian network models, have been observed to perform poorly in evidential reasoning with extremely unlikely evidence. To address this problem, we propose an adaptive importance sampling algorithm, AIS-BN, that shows promising convergence rates even under extreme conditions and seems to outperform the existing sampling algorithms consistently. Three sources of this performance improvement are (1) two heuristics for initialization of the importance function that are based on the theoretical properties of importance sampling in finite-dimensional integrals and the structural advantages of Bayesian networks, (2) a smooth learning method for the importance function, and (3) a dynamic weighting function for combining samples from different stages of the algorithm.We tested the performance of the AIS-BN algorithm along with two state of the art general purpose sampling algorithms, likelihood weighting (Fung &amp; Chang, 1989; Shachter &amp; Peot, 1989) and self-importance sampling (Shachter &amp; Peot, 1989). We used in our tests three large real Bayesian network models available to the scientific community: the CPCS network (Pradhan et al., 1994), the PATHFINDER network (Heckerman, Horvitz, &amp; Nathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, &amp; Druzdzel, 1997), with evidence as unlikely as 10-41. While the AIS-BN algorithm always performed better than the other two algorithms, in the majority of the test cases it achieved orders of magnitude improvement in precision of the results. Improvement in speed given a desired precision is even more dramatic, although we are unable to report numerical results here, as the other algorithms almost never achieved the precision reached even by the first few iterations of the AIS-BN algorithm.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {155–188},
numpages = {34}
}

@article{10.5555/1622262.1622265,
author = {Gordon, Diana F.},
title = {Asimovian Adaptive Agents},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {The goal of this research is to develop agents that are adaptive and predictable and timely. At first blush, these three requirements seem contradictory. For example, adaptation risks introducing undesirable side effects, thereby making agents' behavior less predictable. Furthermore, although formal verification can assist in ensuring behavioral predictability, it is known to be time-consuming.Our solution to the challenge of satisfying all three requirements is the following. Agents have finite-state automaton plans, which are adapted online via evolutionary learning (perturbation) operators. To ensure that critical behavioral constraints are always satisfied, agents' plans are first formally verified. They are then reverified after every adaptation. If reverification concludes that constraints are violated, the plans are repaired. The main objective of this paper is to improve the efficiency of reverification after learning, so that agents have a sufficiently rapid response time. We present two solutions: positive results that certain learning operators are a priori guaranteed to preserve useful classes of behavioral assurance constraints (which implies that no reverification is needed for these operators), and efficient incremental reverification algorithms for those learning operators that have negative a priori results.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {95–153},
numpages = {59}
}

@article{10.5555/1622262.1622264,
author = {Hauskrecht, Milos},
title = {Value-Function Approximations for Partially Observable Markov Decision Processes},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {Partially observable Markov decision processes (POMDPs) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic domains in which states of the system are observable only indirectly, via a set of imperfect or noisy observations. The modeling advantage of POMDPs, however, comes at a price -- exact methods for solving them are computationally very expensive and thus applicable in practice only to very simple problems. We focus on efficient approximation (heuristic) methods that attempt to alleviate the computational problem and trade off accuracy for speed. We have two objectives here. First, we survey various approximation methods, analyze their properties and relations and provide some new insights into their differences. Second, we present a number of new approximation methods and novel refinements of existing techniques. The theoretical results are supported by experiments on a problem from the agent navigation domain.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {33–94},
numpages = {62}
}

@article{10.5555/1622262.1622263,
author = {Cadoli, Marco and Donini, Francesco M. and Liberatore, Paolo and Schaerf, Marco},
title = {Space Efficiency of Propositional Knowledge Representation Formalisms},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
abstract = {We investigate the space efficiency of a Propositional Knowledge Representation (PKR) formalism. Intuitively, the space efficiency of a formalism F in representing a certain piece of knowledge α, is the size of the shortest formula of F that represents α. In this paper we assume that knowledge is either a set of propositional interpretations (models) or a set of propositional formulae (theorems). We provide a formal way of talking about the relative ability of PKR formalisms to compactly represent a set of models or a set of theorems. We introduce two new compactness measures, the corresponding classes, and show that the relative space efficiency of a PKR formalism in representing models/theorems is directly related to such classes. In particular, we consider formalisms for nonmonotonic reasoning, such as circumscription and default logic, as well as belief revision operators and the stable model semantics for logic programs with negation. One interesting result is that formalisms with the same time complexity do not necessarily belong to the same space efficiency class.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {1–31},
numpages = {31}
}

@article{10.5555/1622248.1622261,
author = {Walker, Marilyn A.},
title = {An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {This paper describes a novel method by which a spoken dialogue system can learn to choose an optimal dialogue strategy from its experience interacting with human users. The method is based on a combination of reinforcement learning and performance modeling of spoken dialogue systems. The reinforcement learning component applies Q-learning (Watkins, 1989), while the performance modeling component applies the PARADISE evaluation framework (Walker et al., 1997) to learn the performance function (reward) used in reinforcement learning. We illustrate the method with a spoken dialogue system named elvis (EmaiL Voice Interactive System), that supports access to email over the phone. We conduct a set of experiments for training an optimal dialogue strategy on a corpus of 219 dialogues in which human users interact with elvis over the phone. We then test that strategy on a corpus of 18 dialogues. We show that elvis can learn to optimize its strategy selection for agent initiative, for reading messages, and for summarizing email folders.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {387–416},
numpages = {30}
}

@article{10.5555/1622248.1622260,
author = {Koehler, Jana and Hoffmann, J\"{o}rg},
title = {On Reasonable and Forced Goal Orderings and Their Use in an Agenda-Driven Planning Algorithm},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {The paper addresses the problem of computing goal orderings, which is one of the longstanding issues in AI planning. It makes two new contributions. First, it formally defines and discusses two different goal orderings, which are called the reasonable and the forced ordering. Both orderings are defined for simple STRIPS operators as well as for more complex ADL operators supporting negation and conditional effects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two different methods to compute reasonable goal orderings are developed. One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals. This can lead to an exponential complexity reduction, as the solution to a complex planning problem is found by solving easier subproblems. Since only a polynomial overhead is caused by the goal agenda computation, a potential exists to dramatically speed up planning algorithms as we demonstrate in the empirical evaluation, where we use this method in the IPP planner.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {339–386},
numpages = {48}
}

@article{10.5555/1622248.1622259,
author = {Halpern, Joseph Y.},
title = {Axiomatizing Causal Reasoning},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {Causal models defined in terms of a collection of equations, as defined by Pearl, are axiomatized here. Axiomatizations are provided for three successively more general classes of causal models: (1) the class of recursive theories (those without feedback), (2) the class of theories where the solutions to the equations are unique, (3) arbitrary theories (where the equations may not have solutions and, if they do, they are not necessarily unique). It is shown that to reason about causality in the most general third class, we must extend the language used by Galles and Pearl (1997, 1998). In addition, the complexity of the decision procedures is characterized for all the languages and classes of models considered.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {317–337},
numpages = {21}
}

@article{10.5555/1622248.1622258,
author = {Nebel, Bernhard},
title = {On the Compilability and Expressive Power of Propositional Planning Formalisms},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {The recent approaches of extending the GRAPHPLAN algorithm to handle more expressive planning formalisms raise the question of what the formal meaning of "expressive power" is. We formalize the intuition that expressive power is a measure of how concisely planning domains and plans can be expressed in a particular formalism by introducing the notion of "compilation schemes" between planning formalisms. Using this notion, we analyze the expressiveness of a large family of propositional planning formalisms, ranging from basic STRIPS to a formalism with conditional effects, partial state specifications, and propositional formulae in the preconditions. One of the results is that conditional effects cannot be compiled away if plan size should grow only linearly but can be compiled away if we allow for polynomial growth of the resulting plans. This result confirms that the recently proposed extensions to the GRAPHPLAN algorithm concerning conditional effects are optimal with respect to the "compilability" framework. Another result is that general propositional formulae cannot be compiled into conditional effects if the plan size should be preserved linearly. This implies that allowing general propositional formulae in preconditions and effect conditions adds another level of difficulty in generating a plan.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {271–315},
numpages = {45}
}

@article{10.5555/1622248.1622257,
author = {Singer, Josh and Gent, Ian P. and Smaill, Alan},
title = {Backbone Fragility and the Local Search Cost Peak},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {The local search algorithm WSAT is one of the most successful algorithms for solving the satisfiability (SAT) problem. It is notably effective at solving hard Random 3-SAT instances near the so-called 'satisfiability threshold', but still shows a peak in search cost near the threshold and large variations in cost over different instances. We make a number of significant contributions to the analysis of WSAT on high-cost random instances, using the recently-introduced concept of the backbone of a SAT instance. The backbone is the set of literals which are entailed by an instance. We find that the number of solutions predicts the cost well for small-backbone instances but is much less relevant for the large-backbone instances which appear near the threshold and dominate in the overconstrained region. We show a very strong correlation between search cost and the Hamming distance to the nearest solution early in WSAT's search. This pattern leads us to introduce a measure of the backbone fragility of an instance, which indicates how persistent the backbone is as clauses are removed. We propose that high-cost random instances for local search are those with very large backbones which are also backbone-fragile. We suggest that the decay in cost beyond the satisfiability threshold is due to increasing backbone robustness (the opposite of backbone fragility). Our hypothesis makes three correct predictions. First, that the backbone robustness of an instance is negatively correlated with the local search cost when other factors are controlled for. Second, that backbone-minimal instances (which are 3-SAT instances altered so as to be more backbone-fragile) are unusually hard for WSAT. Third, that the clauses most often unsatisfied during search are those whose deletion has the most effect on the backbone. In understanding the pathologies of local search methods, we hope to contribute to the development of new and better techniques.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {235–270},
numpages = {36}
}

@article{10.5555/1622248.1622256,
author = {Becker, Ann and Bar-Yehuda, Reuven and Geiger, Dan},
title = {Randomized Algorithms for the Loop Cutset Problem},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {We show how to find a minimum weight loop cutset in a Bayesian network with high probability. Finding such a loop cutset is the first step in the method of conditioning for inference. Our randomized algorithm for finding a loop cutset outputs a minimum loop cutset after O(c 6kkn) steps with probability at least 1 - (1 - 1/6k)c6k, where c &gt; 1 is a constant specified by the user, k is the minimal size of a minimum weight loop cutset, and n is the number of vertices. We also show empirically that a variant of this algorithm often finds a loop cutset that is closer to the minimum weight loop cutset than the ones found by the best deterministic algorithms known.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {219–234},
numpages = {16}
}

@article{10.5555/1622248.1622255,
author = {Tobies, Stephan},
title = {The Complexity of Reasoning with Cardinality Restrictions and Nominals in Expressive Description Logics},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {We study the complexity of the combination of the Description Logics ALCQ and ALCQI with a terminological formalism based on cardinality restrictions on concepts. These combinations can naturally be embedded into C2, the two variable fragment of predicate logic with counting quantifiers, which yields decidability in NExpTime. We show that this approach leads to an optimal solution for ALCQI, as ALCQI with cardinality restrictions has the same complexity as C2 (NExpTime-complete). In contrast, we show that for ALCQ, the problem can be solved in ExpTime. This result is obtained by a reduction of reasoning with cardinality restrictions to reasoning with the (in general weaker) terminological formalism of general axioms for ALCQ extended with nominals. Using the same reduction, we show that, for the extension of ALCQI with nominals, reasoning with general axioms is a NExpTime-complete problem. Finally, we sharpen this result and show that pure concept satisfiability for ALCQI with nominals is NExpTime-complete. Without nominals, this problem is known to be PSpace-complete.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {199–217},
numpages = {19}
}

@article{10.5555/1622248.1622254,
author = {Baxter, Jonathan},
title = {A Model of Inductive Bias Learning},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {149–198},
numpages = {50}
}

@article{10.5555/1622248.1622253,
author = {Kaminka, Gal A. and Tambe, Milind},
title = {Robust Agent Teams via Socially-Attentive Monitoring},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {Agents in dynamic multi-agent environments must monitor their peers to execute individual and group plans. A key open question is how much monitoring of other agents' states is required to be effective: The Monitoring Selectivity Problem. We investigate this question in the context of detecting failures in teams of cooperating agents, via Socially-Attentive Monitoring, which focuses on monitoring for failures in the social relationships between the agents. We empirically and analytically explore a family of socially-attentive teamwork monitoring algorithms in two dynamic, complex, multi-agent domains, under varying conditions of task distribution and uncertainty. We show that a centralized scheme using a complex algorithm trades correctness for completeness and requires monitoring all teammates. In contrast, a simple distributed teamwork monitoring algorithm results in correct and complete detection of teamwork failures, despite relying on limited, uncertain knowledge, and monitoring only key agents in a team. In addition, we report on the design of a socially-attentive monitoring system and demonstrate its generality in monitoring several coordination relationships, diagnosing detected failures, and both on-line and off-line applications.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {105–147},
numpages = {43}
}

@article{10.5555/1622248.1622252,
author = {Xu, Ke and Li, Wei},
title = {Exact Phase Transitions in Random Constraint Satisfaction Problems},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {In this paper we propose a new type of random CSP model, called Model RB, which is a revision to the standard Model B. It is proved that phase transitions from a region where almost all problems are satisfiable to a region where almost all problems are unsatisfiable do exist for Model RB as the number of variables approaches infinity. Moreover, the critical values at which the phase transitions occur are also known exactly. By relating the hardness of Model RB to Model B, it is shown that there exist a lot of hard instances in Model RB.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {93–103},
numpages = {11}
}

@article{10.5555/1622248.1622251,
author = {Neal, Radford M.},
title = {On Deducing Conditional Independence from D-Separation in Causal Graphs with Feedback},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {Pearl and Dechter (1996) claimed that the d-separation criterion for conditional independence in acyclic causal networks also applies to networks of discrete variables that have feedback cycles, provided that the variables of the system are uniquely determined by the random disturbances. I show by example that this is not true in general. Some condition stronger than uniqueness is needed, such as the existence of a causal dynamics guaranteed to lead to the unique solution.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {87–91},
numpages = {5}
}

@article{10.5555/1622248.1622250,
author = {Barber, Federico},
title = {Reasoning on Interval and Point-Based Disjunctive Metric Constraints in Temporal Contexts},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {We introduce a temporal model for reasoning on disjunctive metric constraints on intervals and time points in temporal contexts. This temporal model is composed of a labeled temporal algebra and its reasoning algorithms. The labeled temporal algebra defines labeled disjunctive metric point-based constraints, where each disjunct in each input disjunctive constraint is univocally associated to a label. Reasoning algorithms manage labeled constraints, associated label lists, and sets of mutually inconsistent disjuncts. These algorithms guarantee consistency and obtain a minimal network. Additionally, constraints can be organized in a hierarchy of alternative temporal contexts. Therefore, we can reason on context-dependent disjunctive metric constraints on intervals and points. Moreover, the model is able to represent non-binary constraints, such that logical dependencies on disjuncts in constraints can be handled. The computational cost of reasoning algorithms is exponential in accordance with the underlying problem complexity, although some improvements are proposed.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {35–86},
numpages = {52}
}

@article{10.5555/1622248.1622249,
author = {Kambhampati, Subbarao},
title = {Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and Other CSP Search Techniques in Graphplan},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {This paper reviews the connections between Graphplan's planning-graph and the dynamic constraint satisfaction problem and motivates the need for adapting CSP search techniques to the Graphplan algorithm. It then describes how explanation based learning, dependency directed backtracking, dynamic variable ordering, forward checking, sticky values and random-restart search strategies can be adapted to Graphplan. Empirical results are provided to demonstrate that these augmentations improve Graphplan's performance significantly (up to 1000x speedups) on several benchmark problems. Special attention is paid to the explanation-based learning and dependency directed backtracking techniques as they are empirically found to be most useful in improving the performance of Graphplan.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {1–34},
numpages = {34}
}

@article{10.5555/3013545.3013557,
author = {Halpern, Joseph Y.},
title = {Cox's Theorem Revisited},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {429–435},
numpages = {7}
}

@article{10.5555/3013545.3013556,
author = {Fox, Dieter and Burgard, Wolfram and Thrun, Sebastian},
title = {Markov Localization for Mobile Robots in Dynamic Environments},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {Localization, that is the estimation of a robot's location from sensor data, is a fundamental problem in mobile robotics. This papers presents a version of Markov localization which provides accurate position estimates and which is tailored towards dynamic environments. The key idea of Markov localization is to maintain a probability density over the space of all locations of a robot in its environment. Our approach represents this space metrically, using a fine-grained grid to approximate densities. It is able to globally localize the robot from scratch and to recover from localization failures. It is robust to approximate models of the environment (such as occupancy grid maps) and noisy sensors (such as ultrasound sensors). Our approach also includes a filtering technique which allows a mobile robot to reliably estimate its position even in densely populated environments in which crowds of people block the robot's sensors for extended periods of time. The method described here has been implemented and tested in several real-world applications of mobile robots, including the deployments of two mobile robots as interactive museum tour-guides.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {391–427},
numpages = {37}
}

@article{10.5555/3013545.3013555,
author = {Cristani, Matteo},
title = {The Complexity of Reasoning about Spatial Congruence},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {In the recent literature of Artificial Intelligence, an intensive research effort has been spent, for various algebras of qualitative relations used in the representation of temporal and spatial knowledge, on the problem of classifying the computational complexity of reasoning problems for subsets of algebras. The main purpose of these researches is to describe a restricted set of maximal tractable subalgebras, ideally in an exhaustive fashion with respect to the hosting algebras.In this paper we introduce a novel algebra for reasoning about Spatial Congruence, show that the satisfiability problem in the spatial algebra MC-4 is NP-complete, and present a complete classification of tractability in the algebra, based on the individuation of three maximal tractable subclasses, one containing the basic relations. The three algebras are formed by 14, 10 and 9 relations out of 16 which form the full algebra.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {361–390},
numpages = {30}
}

@article{10.5555/3013545.3013554,
author = {Argamon-Engelson, Shlomo and Dagan, Ido},
title = {Committee-Based Sample Selection for Probabilistic Classifiers},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {In many real-world learning tasks it is expensive to acquire a sufficient number of labeled examples for training. This paper investigates methods for reducing annotation cost by sample selection. In this approach, during training the learning program examines many unlabeled examples and selects for labeling only those that are most informative at each stage. This avoids redundantly labeling examples that contribute little new information.Our work follows on previous research on Query By Committee, and extends the committee-based paradigm to the context of probabilistic classification. We describe a family of empirical methods for committee-based sample selection in probabilistic classification models, which evaluate the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set labeled so far.The method was applied to the real-world natural language processing task of stochastic part-of-speech tagging. We find that all variants of the method achieve a significant reduction in annotation cost, although their computational efficiency differs. In particular, the simplest variant, a two member committee with no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {335–360},
numpages = {26}
}

@article{10.5555/3013545.3013553,
author = {Ygge, Fredrik and Akkermans, Hans},
title = {Decentralized Markets versus Central Control: A Comparative Study},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {Multi-Agent Systems (MAS) promise to offer solutions to problems where established, older paradigms fall short. In order to validate such claims that are repeatedly made in software agent publications, empirical in-depth studies of advantages and weaknesses of multi-agent solutions versus conventional ones in practical applications are needed. Climate control in large buildings is one application area where multi-agent systems, and market-oriented programming in particular, have been reported to be very successful, although central control solutions are still the standard practice. We have therefore constructed and implemented a variety of market designs for this problem, as well as different standard control engineering solutions. This article gives a detailed analysis and comparison, so as to learn about differences between standard versus agent approaches, and yielding new insights about benefits and limitations of computational markets. An important outcome is that "local information plus market communication produces global control".},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {301–333},
numpages = {33}
}

@article{10.5555/3013545.3013552,
author = {Rosati, Riccardo},
title = {Reasoning about Minimal Belief and Negation as Failure},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {We investigate the problem of reasoning in the propositional fragment of MBNF, the logic of minimal belief and negation as failure introduced by Lifschitz, which can be considered as a unifying framework for several nonmonotonic formalisms, including default logic, autoepistemic logic, circumscription, epistemic queries, and logic programming. We characterize the complexity and provide algorithms for reasoning in propositional MBNF. In particular, we show that skeptical entailment in propositional MBNF is Π3p-complete, hence it is harder than reasoning in all the above mentioned propositional formalisms for nonmonotonic reasoning. We also prove the exact correspondence between negation as failure in MBNF and negative introspection in Moore's autoepistemic logic.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {277–300},
numpages = {24}
}

@article{10.5555/3013545.3013551,
author = {Moriarty, David E. and Schultz, Alan C. and Grefenstette, John J.},
title = {Evolutionary Algorithms for Reinforcement Learning},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary approach to reinforcement learning are presented, along with a survey of representative applications.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {241–276},
numpages = {36}
}

@article{10.5555/3013545.3013550,
author = {Calvanese, Diego and Lenzerini, Maurizio and Nardi, Daniele},
title = {Unifying Class-Based Representation Formalisms},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts. The proposed formalism is expressed in the style of description logics, which have been introduced in knowledge representation as a means to provide a semantically well-founded basis for the structural aspects of knowledge representation systems. The description logic considered in this paper is a subset of first order logic with nice computational characteristics. It is quite expressive and features a novel combination of constructs that has not been studied before. The distinguishing constructs are number restrictions, which generalize existence and functional dependencies, inverse roles, which allow one to refer to the inverse of a relationship, and possibly cyclic assertions, which are necessary for capturing real world domains. We are able to show that it is precisely such combination of constructs that makes our logic powerful enough to model the essential set of features for defining class structures that are common to frame systems, object-oriented database languages, and semantic data models. As a consequence of the established correspondences, several significant extensions of each of the above formalisms become available. The high expressiveness of the logic we propose and the need for capturing the reasoning in different contexts forces us to distinguish between unrestricted and finite model reasoning. A notable feature of our proposal is that reasoning in both cases is decidable. We argue that, by virtue of the high expressive power and of the associated reasoning capabilities on both unrestricted and finite models, our logic provides a common core for class-based representation formalisms.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {199–240},
numpages = {42}
}

@article{10.5555/3013545.3013549,
author = {Opitz, David and Maclin, Richard},
title = {Popular Ensemble Methods: An Empirical Study},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {An ensemble consists of a set of individually trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman, 1996c) and Boosting (Freund &amp; Schapire, 1996; Schapire, 1990) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm. Our results clearly indicate a number of conclusions. First, while Bagging is almost always more accurate than a single classifier, it is sometimes much less accurate than Boosting. On the other hand, Boosting can create ensembles that are less accurate than a single classifier - especially when using neural networks. Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined. In fact, further results show that Boosting ensembles may overfit noisy data sets, thus decreasing its performance. Finally, consistent with previous studies, our work suggests that most of the gain in an ensemble's performance comes in the first few classifiers combined; however, relatively large gains can be seen up to 25 classifiers when Boosting decision trees.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {169–198},
numpages = {30}
}

@article{10.5555/3013545.3013548,
author = {Brodley, Carla E. and Friedl, Mark A.},
title = {Identifying Mislabeled Training Data},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new approach to identifying and eliminating mislabeled training instances for supervised learning. The goal of this approach is to improve classification accuracies produced by learning algorithms by improving the quality of the training data. Our approach uses a set of learning algorithms to create classifiers that serve as noise filters for the training data. We evaluate single algorithm, majority vote and consensus filters on five datasets that are prone to labeling errors. Our experiments illustrate that filtering significantly improves classification accuracy for noise levels up to 30%. An analytical and empirical evaluation of the precision of our approach shows that consensus filters are conservative at throwing away good data at the expense of retaining bad data and that majority filters are better at detecting bad data at the expense of throwing away good data. This suggests that for situations in which there is a paucity of data, consensus filters are preferable, whereas majority vote filters are preferable for situations with an abundance of data.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {131–167},
numpages = {37}
}

@article{10.5555/3013545.3013547,
author = {Resnik, Philip},
title = {Semantic Similarity in a Taxonomy: An Information-Based Measure and Its Application to Problems of Ambiguity in Natural Language},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their effectiveness.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {95–130},
numpages = {36}
}

@article{10.5555/3013545.3013546,
author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
title = {Decision-Theoretic Planning: Structural Assumptions and Computational Leverage},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory.This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations.Specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. Certain AI techniques-- in particular those based on the use of structured, intensional representations--can be viewed in this way. This paper surveys several types of representations for both classical and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–94},
numpages = {94}
}

@article{10.5555/1622859.1622875,
author = {Birnbaum, Elazar and Lozinskii, Eliezer L.},
title = {The Good Old Davis-Putnam Procedure Helps Counting Models},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {As was shown recently, many important AI problems require counting the number of models of propositional formulas. The problem of counting models of such formulas is, according to present knowledge, computationally intractable in a worst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP, that computes the exact number of models of a propositional CNF or DNF formula F. Let m and n be the number of clauses and variables of F, respectively, and let p denote the probability that a literal l of F occurs in a clause C of F, then the average running time of CDP is shown to be O(mdn), where d=[-1/log2(1-p)].The practical performance of CDP has been estimated in a series of experiments on a wide variety of CNF formulas.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {457–477},
numpages = {21}
}

@article{10.5555/1622859.1622874,
author = {Barber, David and van de Laar, Pi\"{e}rre},
title = {Variational Cumulant Expansions for Intractable Distributions},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {Intractable distributions present a common difficulty in inference within the probabilistic knowledge representation framework and variational methods have recently been popular in providing an approximate solution. In this article, we describe a perturbational approach in the form of a cumulant expansion which, to lowest order, recovers the standdard Kullback-Leibler variational bound. Higher-order terms describe corrections on the variational approach without incurring much further computational cost. The relationship to other perturbational approaches such as TAP is also elucidated. We demonstrate the method on a particular class of undirected graphical models, Boltzmann machines, for which our simulation results confirm improved accuracy and enhanced stability during learning.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {435–455},
numpages = {21}
}

@article{10.5555/1622859.1622873,
author = {Borgida, Alex},
title = {Extensible Knowledge Representation: The Case of Description Reasoners},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {This paper offers an approach to extensible knowledge representation and reasoning for the Description Logic family of formalisms. The approach is based on the notion of adding new concept constructors, and includes a heuristic methodology for specifying the desired extensions, as well as a modularized software architecture that supports implementing extensions. The architecture detailed here falls in the normalize-compared paradigm, and supports both intentional reasoning (subsumption) involving concepts, and extensional reasoning involving individuals after incremental updates to the knowledge base.The resulting approach can be used to extend the reasoner with specialized notions that are motivated by specific problems or application areas, such as reasoning about dates, plans, etc. In addition, it provides an opportunity to implement constructors that are not currently yet sufficiently well understood theoretically, but are needed in practice. Also, for constructors that are provably hard to reason with (e.g., ones whose presence would lead to undecidability), it allows the implementation of incomplete reasoners where the incompleteness is tailored to be acceptable for the application at hand.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {399–434},
numpages = {36}
}

@article{10.5555/1622859.1622872,
author = {Chien, Steve and Stechert, Andre and Mutz, Darren},
title = {Efficient Heuristic Hypothesis Ranking},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {This paper considers the problem of learning the ranking of a set of stochastic alternatives based upon incomplete information (i.e., a limited number of samples). We describe a system that, at each decision cycle, outputs either a complete ordering on the hypotheses or decides to gather additional information (i.e., observations) at some cost. The ranking problem is a generalization of the previously studied hypothesis selection problem -- in selection, an algorithm must select the single best hypothesis, while in ranking, an algorithm must order all the hypotheses.The central problem we address is achieving the desired ranking quality while minimizing the cost of acquiring additional samples. We describe two algorithms for hypothesis ranking and their application for the probably approximately correct (PAC) and expected loss (EL) learning criteria. Empirical results are provided to demonstrate the effectiveness of these ranking procedures on both synthetic and real-world datasets.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {375–397},
numpages = {23}
}

@article{10.5555/1622859.1622871,
author = {Joslin, David E. and Clements, David P.},
title = {"Squeaky Wheel" Optimization},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {We describe a general approach to optimization which we term "Squeaky Wheel" Optimization (SWO). In SWO, a greedy algorithm is used to construct a solution which is then analyzed to find the trouble spots, i.e., those elements, that, if improved, are likely to improve the objective function score. The results of the analysis are used to generate new priorities that determine the order in which the greedy algorithm constructs the next solution. This Construct/Analyze/Prioritize cycle continues until some limit is reached, or an acceptable solution is found.SWO can be viewed as operating on two search spaces: solutions and prioritizations. Successive solutions are only indirectly related, via the re-prioritization that results from analyzing the prior solution. Similarly, successive prioritizations are generated by constructing and analyzing solutions. This "coupled search" has some interesting properties, which we discuss.We report encouraging experimental results on two domains, scheduling problems that arise in fiber-optic cable manufacturing, and graph coloring problems. The fact that these domains are very different supports our claim that SWO is a general technique for optimization.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {353–373},
numpages = {21}
}

@article{10.5555/1622859.1622870,
author = {Rintanen, Jussi},
title = {Constructing Conditional Plans by a Theorem-Prover},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {The research on conditional planning rejects the assumptions that there is no uncertainty or incompleteness of knowledge with respect to the state and changes of the system the plans operate on. Without these assumptions the sequences of operations that achieve the goals depend on the initial state and the outcomes of nondeterministic changes in the system. This setting raises the questions of how to represent the plans and how to perform plan search. The answers are quite different from those in the simpler classical framework. In this paper, we approach conditional planning from a new viewpoint that is motivated by the use of satisfiability algorithms in classical planning. Translating conditional planning to formulae in the propositional logic is not feasible because of inherent computational limitations. Instead, we translate conditional planning to quantified Boolean formulae. We discuss three formalizations of conditional planning as quantified Boolean formulae, and present experimental results obtained with a theorem-prover.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {323–352},
numpages = {30}
}

@article{10.5555/1622859.1622869,
author = {Jaakkola, Tommi S. and Jordan, Michael I.},
title = {Variational Probabilistic Inference and the QMR-DT Network},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {We describe a variational approximation method for efficient inference in large-scale probabilistic models. Variational methods are deterministic procedures that provide approximations to marginal and conditional probabilities of interest. They provide alternatives to approximate inference methods based on stochastic sampling or search. We describe a variational approach to the problem of diagnostic inference in the "Quick Medical Reference" (QMR) network. The QMR network is a large-scale probabilistic graphical model built on statistical and expert knowledge. Exact probabilistic inference is infeasible in this model for all but a small set of cases. We evaluate our variational inference algorithm on a large set of diagnostic test cases, comparing the algorithm to a state-of-the-art stochastic sampling method.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {291–322},
numpages = {32}
}

@article{10.5555/1622859.1622868,
author = {Ting, Kai Ming and Witten, Ian H.},
title = {Issues in Stacked Generalization},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones.We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {271–289},
numpages = {19}
}

@article{10.5555/1622859.1622867,
author = {Cohen, William W. and Schapire, Robert E. and Singer, Yoram},
title = {Learning to Order Things},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order instances given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a binary preference function indicating whether it is advisable to rank one instance before another. Here we consider an on-line algorithm for learning preference functions that is based on Freund and Schapire's "Hedge" algorithm. In the second stage, new instances are ordered so as to maximize agreement with the learned preference function. We show that the problem of finding the ordering that agrees best with a learned preference function is NP-complete. Nevertheless, we describe simple greedy algorithms that are guaranteed to find a good approximation. Finally, we show how metasearch can be formulated as an ordering problem, and present experimental results on learning a combination of "search experts," each of which is a domain-specific query expansion strategy for a web search engine.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {243–270},
numpages = {28}
}

@article{10.5555/1622859.1622866,
author = {Lukasiewicz, Thomas},
title = {Probabilistic Deduction with Conditional Constraints over Basic Events},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very efficient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {199–241},
numpages = {43}
}

@article{10.5555/1622859.1622865,
author = {Fuchs, Dirk and Fuchs, Marc},
title = {Cooperation between Top-down and Bottom-up Theorem Provers},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {Top-down and bottom-up theorem proving approaches each have specific advantages and disadvantages. Bottom-up provers profit from strong redundancy control but suffer from the lack of goal-orientation, whereas top-down provers are goal-oriented but often have weak calculi when their proof lengths are considered. In order to integrate both approaches, we try to achieve cooperation between a top-down and a bottom-up prover in two different ways: The first technique aims at supporting a bottom-up with a top-down prover. A top-down prover generates subgoal clauses, they are then processed by a bottom-up prover. The second technique deals with the use of bottom-up generated lemmas in a top-down prover. We apply our concept to the areas of model elimination and superposition. We discuss the ability of our techniques to shorten proofs as well as to reorder the search space in an appropriate manner. Furthermore, in order to identify subgoal clauses and lemmas which are actually relevant for the proof task, we develop methods for a relevancy-based filtering. Experiments with the provers SETHEO and SPASS performed in the problem library TPTP reveal the high potential of our cooperation approaches.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {169–198},
numpages = {30}
}

@article{10.5555/1622859.1622864,
author = {Friedman, Nir and Halpern, Joseph Y.},
title = {Modeling Belief in Dynamic Systems Part II: Revision and Update},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman &amp; Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno &amp; Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {117–167},
numpages = {51}
}

@article{10.5555/1622859.1622863,
author = {Long, Derek and Fox, Maria},
title = {Efficient Implementation of the Plan Graph in STAN},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {STAN is a Graphplan-based planner, so-called because it uses a variety of STate ANalysis techniques to enhance its performance. STAN competed in the AIPS-98 planning competition where it compared well with the other competitors in terms of speed, finding solutions fastest to many of the problems posed. Although the domain analysis techniques STAN exploits are an important factor in its overall performance, we believe that the speed at which STAN solved the competition problems is largely due to the implementation of its plan graph. The implementation is based on two insights: that many of the graph construction operations can be implemented as bit-level logical operations on bit vectors, and that the graph should not be explicitly constructed beyond the fix point. This paper describes the implementation of STAN's plan graph and provides experimental results which demonstrate the circumstances under which advantages can be obtained from using this implementation.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {87–115},
numpages = {29}
}

@article{10.5555/1622859.1622862,
author = {Halpern, Joseph Y.},
title = {A Counterexample to Theorems of Cox and Fine},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {Cox's well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox's assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {67–85},
numpages = {19}
}

@article{10.1613/jair.790,
author = {Siskind, Jeffrey Mark},
title = {Grounding the Lexical Semantics of Verbs in Visual Perception Using Force Dynamics and Event Logic},
year = {1999},
issue_date = {July 2001},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {15},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.790},
doi = {10.1613/jair.790},
abstract = {This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. Additionally, an efficient procedure using this representation is presented for inferring occurrences of compound events, described with event-logic expressions, from occurrences of primitive events. Using force dynamics and event logic to specify the lexical semantics of events allows the system to be more robust than prior systems based on motion profile.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {31–90},
numpages = {60}
}

@article{10.5555/1622859.1622860,
author = {Davis, Ernest},
title = {Order of Magnitude Comparisons of Distance},
year = {1999},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
abstract = {Order of magnitude reasoning -- reasoning by rough comparisons of the sizes of quantities -- is often called "back of the envelope calculation", with the implication that the calculations are quick though approximate. This paper exhibits an interesting class of constraint sets in which order of magnitude reasoning is demonstrably fast. Specifically, we present a polynomial-time algorithm that can solve a set of constraints of the form "Points a and b are much closer together than points c and d". We prove that this algorithm can be applied if "much closer together" is interpreted either as referring to an infinite difference in scale or as referring to a finite difference in scale, as long as the difference in scale is greater than the number of variables in the constraint set. We also prove that the first-order theory over such constraints is decidable.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–38},
numpages = {38}
}

@article{10.5555/1622797.1622808,
author = {Rintanen, Jussi},
title = {Complexity of Prioritized Default Logics},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {In default reasoning, usually not all possible ways of resolving conflicts between default rules are acceptable. Criteria expressing acceptable ways of resolving the conflicts may be hardwired in the inference mechanism, for example specificity in inheritance reasoning can be handled this way, or they may be given abstractly as an ordering on the default rules. In this article we investigate formalizations of the latter approach in Reiter's default logic. Our goal is to analyze and compare the computational properties of three such formalizations in terms of their computational complexity: the prioritized default logics of Baader and Hollunder, and Brewka, and a prioritized default logic that is based on lexicographic comparison. The analysis locates the propositional variants of these logics on the second and third levels of the polynomial hierarchy, and identifies the boundary between tractable and intractable inference for restricted classes of prioritized default theories.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {423–461},
numpages = {39}
}

@article{10.5555/1622797.1622807,
author = {Fox, Maria and Long, Derek},
title = {The Automatic Inference of State Invariants in TIM},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {As planning is applied to larger and richer domains the effort involved in constructing domain descriptions increases and becomes a significant burden on the human application designer. If general planners are to be applied successfully to large and complex domains it is necessary to provide the domain designer with some assistance in building correctly encoded domains. One way of doing this is to provide domain-independent techniques for extracting, from a domain description, knowledge that is implicit in that description and that can assist domain designers in debugging domain descriptions. This knowledge can also be exploited to improve the performance of planners: several researchers have explored the potential of state invariants in speeding up the performance of domain-independent planners. In this paper we describe a process by which state invariants can be extracted from the automatically inferred type structure of a domain. These techniques are being developed for exploitation by STAN, a Graphplan based planner that employs state analysis techniques to enhance its performance.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {367–421},
numpages = {55}
}

@article{10.5555/1622797.1622806,
author = {Di Caro, Gianni and Dorigo, Marco},
title = {AntNet: Distributed Stigmergetic Control for Communications Networks},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {317–365},
numpages = {49}
}

@article{10.5555/1622797.1622805,
author = {Mazer, Emmanuel and Ahuactzin, Juan Manuel and Bessi\'{e}re, Pierre},
title = {The Ariadne's Clew Algorithm},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {We present a new approach to path planning, called the "Ariadne's clew algorithm". It is designed to find paths in high-dimensional continuous spaces and applies to robots with many degrees of freedom in static, as well as dynamic environments -- ones where obstacles may move. The Ariadne's clew algorithm comprises two sub-algorithms, called SEARCH and EXPLORE, applied in an interleaved manner. EXPLORE builds a representation of the accessible space while SEARCH looks for the target. Both are posed as optimization problems. We describe a real implementation of the algorithm to plan paths for a six degrees of freedom arm in a dynamic environment where another six degrees of freedom arm is used as a moving obstacle. Experimental results show that a path is found in about one second without any pre-processing.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {295–316},
numpages = {22}
}

@article{10.5555/1622797.1622804,
author = {Wiebe, Janyce M. and O'Hara, Thomas P. and \"{O}hrstr\"{o}m-Sandgren, Thorsten and McKeever, Kenneth J.},
title = {An Empirical Approach to Temporal Reference Resolution},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {Scheduling dialogs, during which people negotiate the times of appointments, are common in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four phases of this work: data annotation and evaluation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data, and then applied later to a much more complex data set, to assess the generalizability of the model for the task being performed. Many different types of empirical methods are applied to pinpoint the strengths and weaknesses of the approach. Detailed annotation instructions were developed and an intercoder reliability study was performed, showing that naive annotators can reliably perform the targeted annotations. A fully automatic system has been developed and evaluated on unseen test data, with good results on both data sets. We adopt a pure realization of a recency-based focus model to identify precisely when it is and is not adequate for the task being addressed. In addition to system results, an in-depth evaluation of the model itself is presented, based on detailed manual annotations. The results are that few errors occur specifically due to the model of focus being used, and the set of anaphoric relations defined in the model are low in ambiguity for both data sets.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {247–294},
numpages = {48}
}

@article{10.5555/1622797.1622803,
author = {Vandegriend, Basil and Culberson, Joseph},
title = {The G<sub>n,m</sub>Phase Transition is Not Hard for the Hamiltonian Cycle Problem},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {Using an improved backtrack algorithm with sophisticated pruning techniques, we revise previous observations correlating a high frequency of hard to solve Hamiltonian cycle instances with the Gn,m phase transition between Hamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500 vertices are easily solved.When we artificially restrict the degree sequence with a bounded maximum degree, although there is some increase in difficulty, the frequency of hard graphs is still low. When we consider more regular graphs based on a generalization of knight's tours, we observe frequent instances of really hard graphs, but on these the average degree is bounded by a constant. We design a set of graphs with a feature our algorithm is unable to detect and so are very hard for our algorithm, but in these we can vary the average degree from O(1) to O(n). We have so far found no class of graphs correlated with the Gn,m phase transition which asymptotically produces a high frequency of hard instances.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {219–245},
numpages = {27}
}

@article{10.5555/1622797.1622802,
author = {Ruiz, Alberto and L\'{o}pez-de-Teruel, Pedro E. and Garrido, M. Carmen},
title = {Probabilistic Inference from Arbitrary Uncertainty Using Mixtures of Factorized Generalized Gaussians},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a general and efficient framework for probabilistic inference and learning from arbitrary uncertain information. It exploits the calculation properties of finite mixture models, conjugate families and factorization. Both the joint probability density of the variables and the likelihood function of the (objective or subjective) observation are approximated by a special mixture model, in such a way that any desired conditional distribution can be directly obtained without numerical integration. We have developed an extended version of the expectation maximization (EM) algorithm to estimate the parameters of mixture models from uncertain training examples (indirect observations). As a consequence, any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages. This ability, extremely useful in certain situations, is not found in most alternative methods. The proposed framework is formally justified from standard probabilistic principles and illustrative examples are provided in the fields of nonparametric pattern classification, nonlinear regression and pattern completion. Finally, experiments on a real application and comparative results over standard databases provide empirical evidence of the utility of the method in a wide range of applications.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {167–217},
numpages = {51}
}

@article{10.5555/1622797.1622801,
author = {Cook, Diane J. and Varnell, R. Craig},
title = {Adaptive Parallel Iterative Deepening Search},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {Many of the artificial intelligence techniques developed to date rely on heuristic search through large spaces. Unfortunately, the size of these spaces and the corresponding computational effort reduce the applicability of otherwise novel and effective algorithms. A number of parallel and distributed approaches to search have considerably improved the performance of the search process.Our goal is to develop an architecture that automatically selects parallel search strategies for optimal performance on a variety of search problems. In this paper we describe one such architecture realized in the EUREKA system, which combines the benefits of many different approaches to parallel heuristic search. Through empirical and theoretical analyses we observe that features of the problem space directly affect the choice of optimal parallel search strategy. We then employ machine learning techniques to select the optimal parallel search strategy for a given problem space. When a new search task is input to the system, EUREKA uses features describing the search space and the chosen architecture to automatically select the appropriate search strategy. EUREKA has been tested on a MIMD parallel processor, a distributed network of workstations, and a single workstation using multithreading. Results generated from fifteen puzzle problems, robot arm motion problems, artificial search spaces, and planning problems indicate that EUREKA outperforms any of the tested strategies used exclusively for all problem instances and is able to greatly reduce the search time for these applications.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {139–166},
numpages = {28}
}

@article{10.5555/1622797.1622800,
author = {B\"{a}ckstr\"{o}m, Christer},
title = {Computational Aspects of Reordering Plans},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {This article studies the problem of modifying the action ordering of a plan in order to optimise the plan according to various criteria. One of these criteria is to make a plan less constrained and the other is to minimize its parallel execution time. Three candidate definitions are proposed for the first of these criteria, constituting a sequence of increasing optimality guarantees. Two of these are based on deordering plans, which means that ordering relations may only be removed, not added, while the third one uses reordering, where arbitrary modifications to the ordering are allowed. It is shown that only the weakest one of the three criteria is tractable to achieve, the other two being NP-hard and even difficult to approximate. Similarly, optimising the parallel execution time of a plan is studied both for deordering and reordering of plans. In the general case, both of these computations are NP-hard. However, it is shown that optimal deorderings can be computed in polynomial time for a class of planning languages based on the notions of producers, consumers and threats, which includes most of the commonly used planning languages. Computing optimal reorderings can potentially lead to even faster parallel executions, but this problem remains NP-hard and difficult to approximate even under quite severe restrictions.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {99–137},
numpages = {39}
}

@article{10.5555/1622797.1622799,
author = {Ledeniov, Oleg and Markovitch, Shaul},
title = {The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic Inference},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {It is common to view programs as a combination of logic and control: the logic part defines what the program must do, the control part - how to do it. The Logic Programming paradigm was developed with the intention of separating the logic from the control. Recently, extensive research has been conducted on automatic generation of control for logic programs. Only a few of these works considered the issue of automatic generation of control for improving the efficiency of logic programs. In this paper we present a novel algorithm for automatic finding of lowest-cost subgoal orderings. The algorithm works using the divide-and-conquer strategy. The given set of subgoals is partitioned into smaller sets, based on co-occurrence of free variables. The subsets are ordered recursively and merged, yielding a provably optimal order. We experimentally demonstrate the utility of the algorithm by testing it in several domains, and discuss the possibilities of its cooperation with other existing methods.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {37–97},
numpages = {61}
}

@article{10.5555/1622797.1622809,
author = {Artale, Alessandro and Franconi, Enrico},
title = {A Temporal Description Logic for Reasoning about Actions and Plans},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {A class of interval-based temporal languages for uniformly representing and reasoning about actions and plans is presented. Actions are represented by describing what is true while the action itself is occurring, and plans are constructed by temporally relating actions and world states. The temporal languages are members of the family of Description Logics, which are characterized by high expressivity combined with good computational properties. The subsumption problem for a class of temporal Description Logics is investigated and sound and complete decision procedures are given. The basic language TL-F is considered first: it is the composition of a temporal logic TL - able to express interval temporal networks - together with the non-temporal logic F - a Feature Description Logic. It is proven that subsumption in this language is an NP-complete problem. Then it is shown how to reason with the more expressive languages TLU-FU and TL-ALCF. The former adds disjunction both at the temporal and non-temporal sides of the language, the latter extends the non-temporal side with set-valued features (i.e., roles) and a propositionally complete language.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {463–506},
numpages = {44}
}

@article{10.5555/1622797.1622798,
author = {Littman, Michael L. and Goldsmith, Judy and Mundhenk, Martin},
title = {The Computational Complexity of Probabilistic Planning},
year = {1998},
issue_date = {August 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {9},
number = {1},
issn = {1076-9757},
abstract = {We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NPPP, co-NPPP, and PSPACE. In the process of proving that certain planning problems are complete for NPPP, we introduce a new basic NPPP -complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {1–36},
numpages = {36}
}

@article{10.5555/1622788.1622796,
author = {Finkelstein, Lev and Markovitch, Shaul},
title = {A Selective Macro-Learning Algorithm and Its Application to the N \texttimes{} N Sliding-Tile Puzzle},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of N \texttimes{} N sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {223–263},
numpages = {41}
}

@article{10.5555/1622788.1622795,
author = {Darwiche, Adnan},
title = {Model-Based Diagnosis Using Structured System Descriptions},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {165–222},
numpages = {58}
}

@article{10.5555/1622788.1622794,
author = {F\"{u}rnkranz, Johannes},
title = {Integrative Windowing},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behaviour of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to archieve run-time gains in a set of experiments in a simple domain with artificial noise.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {129–164},
numpages = {36}
}

@article{10.5555/1622788.1622793,
author = {Srivastava, Biplav and Kambhampati, Subbarao},
title = {Synthesizing Customized Planners from Specifications},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {Existing plan synthesis approaches in artificial intelligence fall into two categories - domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati &amp; Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {93–128},
numpages = {36}
}

@article{10.5555/1622788.1622792,
author = {Moore, Andrew and Lee, Mary Soon},
title = {Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table.We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves.We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {67–91},
numpages = {25}
}

@article{10.5555/1622788.1622791,
author = {Argamon-Engelson, Shlomo and Koppel, Moshe},
title = {Tractability of Theory Patching},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {In this paper we consider the problem of theory patching, in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is stable regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {39–65},
numpages = {27}
}

@article{10.5555/1622788.1622790,
author = {Gogic, Goran and Papadimitriou, Christos H. and Sideri, Martha},
title = {Incremental Recompilation of Knowledge},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of "knowledge compilation," supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {23–37},
numpages = {15}
}

@article{10.5555/1622788.1622789,
author = {Engelfriet, Joeri},
title = {Monotonicity and Persistence in Preferential Logics},
year = {1998},
issue_date = {January 1998},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {8},
number = {1},
issn = {1076-9757},
abstract = {An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–21},
numpages = {21}
}

@article{10.5555/1622776.1622787,
author = {Kaindl, Hermann and Kainz, Gerhard},
title = {Bidirectional Heuristic Search Reconsidered},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {283–317},
numpages = {35}
}

@article{10.5555/1622776.1622786,
author = {Frank, Jeremy and Cheeseman, Peter and Stutz, John},
title = {When Gravity Fails: Local Search Topology},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {249–281},
numpages = {33}
}

@article{10.5555/1622776.1622785,
author = {Monderer, Dov and Tennenholtz, Moshe},
title = {Dynamic Non-Bayesian Decision Making},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them - the perfect monitoring case - the agent is able to observe the previous environment state as part of his feedback, while in the other - the imperfect monitoring case - all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {231–248},
numpages = {18}
}

@article{10.5555/1622776.1622784,
author = {Zhang, Nevin L. and Liu, Wenju},
title = {A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {199–230},
numpages = {32}
}

@article{10.5555/1622776.1622783,
author = {Ihrig, Laurie H. and Kambhampati, Subbarao},
title = {Storing and Indexing Plan Derivations through Explanation-Based Analysis of Retrieval Failures},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {161–198},
numpages = {38}
}

@article{10.5555/1622776.1622782,
author = {Leherte, Laurence and Glasgow, Janice and Baxter, Kim and Steeg, Evan and Fortier, Suzanne},
title = {Analysis of Three-Dimensional Protein Images},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {A fundamental goal of research in molecular biology is to understand protein structure. Protein crystallography is currently the most successful method for determining the three-dimensional (3D) conformation of a protein, yet it remains labor intensive and relies on an expert's ability to derive and evaluate a protein scene model. In this paper, the problem of protein structure determination is formulated as an exercise in scene analysis. A computational methodology is presented in which a 3D image of a protein is segmented into a graph of critical points. Bayesian and certainty factor approaches are described and used to analyze critical point graphs and identify meaningful substructures, such as α-helices and β-sheets. Results of applying the methodologies to protein images at low and medium resolution are reported. The research is related to approaches to representation, segmentation and classification in vision, as well as to top-down approaches to protein structure prediction.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {125–159},
numpages = {35}
}

@article{10.5555/1622776.1622781,
author = {Tambe, Milind},
title = {Towards Flexible Teamwork},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability.Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen &amp; Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz &amp; Kraus's partial Shared-Plans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {83–124},
numpages = {42}
}

@article{10.5555/1622776.1622780,
author = {Nevill-Manning, Craig G. and Witten, Ian H.},
title = {Identifying Hierarchical Structure in Sequences: A Linear-Time Algorithm},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {67–82},
numpages = {16}
}

@article{10.5555/1622776.1622779,
author = {Mammen, Dorothy L. and Hogg, Tad},
title = {A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search Difficulty},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {47–66},
numpages = {20}
}

@article{10.5555/1622776.1622778,
author = {Drakengren, Thomas and Jonsson, Peter},
title = {Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {25–45},
numpages = {21}
}

@article{10.5555/1622776.1622777,
author = {Halpern, Joseph Y.},
title = {Defining Relative Likelihood in Partially-Ordered Preferential Structures},
year = {1997},
issue_date = {July 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {7},
number = {1},
issn = {1076-9757},
abstract = {Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {1–24},
numpages = {24}
}

@article{10.5555/1622767.1622775,
author = {Pollack, Martha E. and Joslin, David and Paolucci, Massimo},
title = {Flaw Selection Strategies for Partial-Order Planning},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {223–262},
numpages = {40}
}

@article{10.5555/1622767.1622774,
author = {Jonsson, Peter and Drakengren, Thomas},
title = {A Complete Classification of Tractability in RCC-5},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {211–221},
numpages = {11}
}

@article{10.5555/1622767.1622773,
author = {Opitz, David W. and Shavlik, Jude W.},
title = {Connectionist Theory Refinement: Genetically Searching the Space of Network Topologies},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the Regent algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {177–209},
numpages = {33}
}

@article{10.5555/1622767.1622772,
author = {Darwiche, Adnan and Provan, Gregory},
title = {Query DAGs: A Practical Paradigm for Implementing Belief-Network Inference},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks -- we show how they can be generated using clustering and conditioning algorithms. The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {147–176},
numpages = {30}
}

@article{10.5555/1622767.1622771,
author = {Agre, Philip and Horswill, Ian},
title = {Lifeworld Analysis},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the TOAST system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {111–145},
numpages = {35}
}

@article{10.5555/1622767.1622770,
author = {De Giacomo, Giuseppe and Lenzerini, Maurizio},
title = {A Uniform Framework for Concept Definitions in Description Logics},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic µALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualidied number restrictions, µALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of µALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {87–110},
numpages = {24}
}

@article{10.5555/1622767.1622769,
author = {Wermter, Stefan and Weber, Volker},
title = {SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken-language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component.In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the screen system which is based on this new robust, learned and flat analysis.In this paper, we focus on a detailed description of screen's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {35–85},
numpages = {51}
}

@article{10.5555/1622767.1622768,
author = {Wilson, D. Randall and Martinez, Tony R.},
title = {Improved Heterogeneous Distance Functions},
year = {1997},
issue_date = {January 1997},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {6},
number = {1},
issn = {1076-9757},
abstract = {Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–34},
numpages = {34}
}

@article{10.5555/1622756.1622766,
author = {Schlimmer, Jeffrey C. and Wells, Patricia Crane},
title = {Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a person's name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {329–349},
numpages = {21}
}

@article{10.5555/1622756.1622765,
author = {Zhang, Nevin Lianwen and Poole, David},
title = {Exploiting Causal Independence in Bayesian Network Inference},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as "or", "sum" or "max", on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {301–328},
numpages = {28}
}

@article{10.5555/1622756.1622764,
author = {de Campos, Luis M.},
title = {Characterizations of Decomposable Dependency Models},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {289–300},
numpages = {12}
}

@article{10.5555/1622756.1622763,
author = {Helzerman, Randall A. and Harper, Mary P.},
title = {MUSE CSP: An Extension to the Constraint Satisfaction Problem},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {239–288},
numpages = {50}
}

@article{10.5555/1622756.1622762,
author = {Zlotkin, Gilad and Rosenschein, Jeffrey S.},
title = {Mechanisms for Automated Negotiation in State Oriented Domains},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point).A Unified Negotiation Protocol (UNP) is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals.Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one "strict," the other "tolerant," and analyze their affects on the stability and efficiency of negotiation outcomes.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {163–238},
numpages = {76}
}

@article{10.5555/1622756.1622761,
author = {Quinlan, J. R.},
title = {Learning First-Order Definitions of Functions},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {139–161},
numpages = {23}
}

@article{10.5555/1622756.1622760,
author = {Gerevini, Alfonso and Schubert, Lenhart},
title = {Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by ucpop to select plans for refinement. The other is based on preferring "zero commitment" (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of ucpop, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default ucpop search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {95–137},
numpages = {43}
}

@article{10.5555/1622756.1622759,
author = {Litman, Diane J.},
title = {Cue Phrase Classification Using Machine Learning},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {53–94},
numpages = {42}
}

@article{10.5555/1622756.1622758,
author = {Ben-Eliyahu, Rachel},
title = {A Hierarchy of Tractable Subsets for Computing Stable Models},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Ω1, Ω2,..., with the following properties: first, Ω1 is the class of all stratified knowledge bases; second, if a knowledge base Π is in Ωk, then Π has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Π, third, for an arbitrary knowledge base Π, we can find the minimum k such that Π belongs to Ωk in time polynomial in the size of Π, and, last, where κ is the class of all knowledge bases, it is the case that ∪i=1∞ Ωi= κ, that is, every knowledge base belongs to some class in the hierarchy.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {27–52},
numpages = {26}
}

@article{10.5555/1622756.1622757,
author = {Yip, Kenneth and Zhao, Feng},
title = {Spatial Aggregation: Theory and Applications},
year = {1996},
issue_date = {August 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {5},
number = {1},
issn = {1076-9757},
abstract = {Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods.We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers - kam, maps, and hipair - in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {1–26},
numpages = {26}
}

@article{10.5555/1622737.1622755,
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
title = {On Partially Controlled Multi-Agent Systems},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {477–507},
numpages = {31}
}

@article{10.5555/1622737.1622754,
author = {Tadepalli, Prasad and Natarajan, Balas K.},
title = {A Formal Framework for Speedup Learning from Problems and Solutions},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {445–475},
numpages = {31}
}

@article{10.5555/1622737.1622753,
author = {Bhansali, Sanjay and Kramer, Glenn A. and Hoar, Tim J.},
title = {A Principled Approach towards Symbolic Geometric Constraint Satisfaction},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {419–443},
numpages = {25}
}

@article{10.5555/1622737.1622752,
author = {Webb, Geoffrey I.},
title = {Further Experimental Evidence against the Utility of Occam's Razor},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {This paper presents new experimental evidence against the utility of Occam's razor. A systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {397–417},
numpages = {21}
}

@article{10.5555/1622737.1622751,
author = {Gratch, Jonathan and Chien, Steve},
title = {Adaptive Problem-Solving for Large-Scale Scheduling Problems: A Case Study},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {365–396},
numpages = {32}
}

@article{10.5555/1622737.1622750,
author = {Nienhuys-Cheng, Shan-Hwei and de Wolf, Ronald},
title = {Least Generalizations and Greatest Specializations of Sets of Clauses},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own.Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one nontautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {341–363},
numpages = {23}
}

@article{10.5555/1622737.1622749,
author = {Pryor, Louise and Collins, Gregg},
title = {Planning for Contingencies: A Decision-Based Approach},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which diffierent actions are performed in diffierent circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of diffierent decision-making procedures.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {287–339},
numpages = {53}
}

@article{10.5555/1622737.1622748,
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
title = {Reinforcement Learning: A Survey},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {237–285},
numpages = {49}
}

@article{10.5555/1622737.1622747,
author = {Walsh, Toby},
title = {A Divergence Critic for Inductive Proof},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a "difference matching" procedure. The critic then proposes lemmas and generalizations which "ripple" these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {209–235},
numpages = {27}
}

@article{10.5555/1622737.1622746,
author = {Marchiori, Elena},
title = {Practical Methods for Proving Termination of General Logic Programs},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t, the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {179–208},
numpages = {30}
}

@article{10.5555/1622737.1622745,
author = {Fisher, Doug},
title = {Iterative Optimization and Simplification of Hierarchical Clusterings},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a 'tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts - often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to 'externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {147–179},
numpages = {33}
}

@article{10.5555/1622737.1622744,
author = {Cohn, David A. and Ghahramani, Zoubin and Jordan, Michael I.},
title = {Active Learning with Statistical Models},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {For many types of machine learning algorithms, one can compute the statistically "optimal" way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {129–145},
numpages = {17}
}

@article{10.5555/1622737.1622743,
author = {Hogg, Tad},
title = {Quantum Computing and Phase Transitions in Combinatorial Search},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {91–128},
numpages = {38}
}

@article{10.5555/1622737.1622742,
author = {Quinlan, J. R.},
title = {Improved Use of Continuous Attributes in C4.5},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {77–90},
numpages = {14}
}

@article{10.5555/1622737.1622741,
author = {Saul, Lawrence K. and Jaakkola, Tommi and Jordan, Michael I.},
title = {Mean Field Theory for Sigmoid Belief Networks},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {61–76},
numpages = {16}
}

@article{10.5555/1622737.1622740,
author = {Delcher, Arthur L. and Grove, Adam J. and Kasif, Simon and Pearl, Judea},
title = {Logarithmic-Time Updates and Queries in Probabilistic Networks},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in time O(1) and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {37–59},
numpages = {23}
}

@article{10.5555/1622737.1622739,
author = {Brewka, Gerhard},
title = {Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {19–36},
numpages = {18}
}

@article{10.5555/1622737.1622738,
author = {van Beek, Peter and Manchak, Dennis W.},
title = {The Design and Experimental Analysis of Algorithms for Temporal Reasoning},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
issn = {1076-9757},
abstract = {Many applications-from planning and scheduling to problems in molecular biology-rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–18},
numpages = {18}
}

@article{10.5555/1622620.1622636,
author = {Idestam-Almquist, Peter},
title = {Generalization of Clauses under Implication},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation θ-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under θ-subsumption, but not under implication. However generalization under θ-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs.We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under θ-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under θ-subsumption of the expansion.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {467–489},
numpages = {23}
}

@article{10.5555/1622620.1622635,
author = {Webb, Geoffrey I.},
title = {OPUS: An Efficient Admissible Algorithm for Unordered Search},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {431–465},
numpages = {35}
}

@article{10.5555/1622620.1622634,
author = {Heckerman, David and Shachter, Ross},
title = {Decision-Theoretic Foundations for Causal Reasoning},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {405–430},
numpages = {26}
}

@article{10.5555/1622620.1622633,
author = {Weiss, Sholom M. and Indurkhya, Nitin},
title = {Rule-Based Machine Learning Methods for Functional Prediction},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {383–403},
numpages = {21}
}

@article{10.5555/1622620.1622632,
author = {Buro, Michael},
title = {Statistical Feature Combination for the Evaluation of Game Positions},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression -- which is used here for the first time in the context of game playing - leads to better results than the other approaches.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {373–382},
numpages = {10}
}

@article{10.5555/1622620.1622631,
author = {Khardon, Roni},
title = {Translating between Horn Representations and Their Characteristic Models},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems.Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression.We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {349–372},
numpages = {24}
}

@article{10.5555/1622620.1622630,
author = {Broggi, Alberto and Bert\'{e}, Simona},
title = {Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs.This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel Simd architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {325–348},
numpages = {24}
}

@article{10.5555/1622620.1622629,
author = {Huffman, Scott B. and Laird, John E.},
title = {Flexibly Instructable Agents},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called INSTRUCTO-SOAR that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. INSTRUCTO-SOAR meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {271–324},
numpages = {54}
}

@article{10.5555/1622620.1622628,
author = {Bengio, Yoshua and Frasconi, Paolo},
title = {Diffusion of Context and Credit Information in Markovian Models},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {249–270},
numpages = {22}
}

@article{10.5555/1622620.1622627,
author = {Pinkas, Gadi and Dechter, Rina},
title = {Improving Connectionist Energy Minimization},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimumfrom any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {223–248},
numpages = {26}
}

@article{10.5555/1622620.1622626,
author = {Woods, Kevin and Cook, Diane and Hall, Lawrence and Bowyer, Kevin and Stark, Louise},
title = {Learning Membership Functions in a Function-Based Object Recognition System},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a "measure of goodness" or "membership value" with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of diffierent properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as GRUFF, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the GRUFF system, called OMLET, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {187–222},
numpages = {36}
}

@article{10.5555/1622620.1622625,
author = {Giraud-Carrier, Christophe G. and Martinez, Tony R.},
title = {An Integrated Framework for Learning and Reasoning},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {147–185},
numpages = {39}
}

@article{10.5555/1622620.1622624,
author = {Zhao, Qi and Nishida, Toyoaki},
title = {Using Qualitative Hypotheses to Identify Inaccurate Data},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {119–145},
numpages = {27}
}

@article{10.5555/1622620.1622623,
author = {Bergmann, Ralph and Wilke, Wolfgang},
title = {Building and Refining Abstract Planning Cases by Change of Representation Language},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {Abstraction is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description - as used in most hierarchical planners - has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of PARIS (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.},
journal = {J. Artif. Int. Res.},
month = jul,
pages = {53–118},
numpages = {66}
}

@article{10.5555/1622620.1622622,
author = {Veloso, Manuela and Stone, Peter},
title = {FLECS: Planning with a Flexible Commitment Strategy},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the "best" possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, flecs, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows flecs to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. flecs can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. flecs represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {25–52},
numpages = {28}
}

@article{10.5555/1622620.1622621,
author = {Mooney, Raymond J. and Califf, Mary Elaine},
title = {Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs},
year = {1995},
issue_date = {June 1995},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {3},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called Foidl, is based on Foil (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. Foidl is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {1–24},
numpages = {24}
}

@article{10.5555/1622826.1622843,
author = {Cohen, William W.},
title = {Pac-Learning Recursive Logic Programs: Negative Results},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of paclearnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {541–573},
numpages = {33}
}

@article{10.5555/1622826.1622842,
author = {Cohen, William W.},
title = {Pac-Learning Recursive Logic Programs: Efficient Algorithms},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate nonrecursive clause are also learnable, if an additional "basecase" oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {501–539},
numpages = {39}
}

@article{10.5555/1622826.1622841,
author = {Schaerf, Andrea and Shoham, Yoav and Tennenholtz, Moshe},
title = {Adaptive Load Balancing: A Study in Multi-Agent Learning},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {475–500},
numpages = {26}
}

@article{10.5555/1622826.1622840,
author = {David, Philippe},
title = {Using Pivot Consistency to Decompose and Solve Functional CSPS},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n2d2 complexity (instead of O(n3d3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {447–474},
numpages = {28}
}

@article{10.5555/1622826.1622839,
author = {Donoho, Steven K. and Rendell, Larry A.},
title = {Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required.Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {411–446},
numpages = {36}
}

@article{10.5555/1622826.1622838,
author = {Turney, Peter D.},
title = {Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification -- EG2, CS-ID3, and IDX -- and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {369–409},
numpages = {41}
}

@article{10.5555/1622826.1622837,
author = {Ortega, Julio},
title = {On the Informativeness of the DNA Promoter Sequences Domain Theory},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {The DNA promoter sequences domain theory and database have become popular for testing systems that integrate empirical and analytical learning. This note reports a simple change and reinterpretation of the domain theory in terms of M-of-N concepts, involving no learning, that results in an accuracy of 93.4% on the 106 items of the database. Moreover, an exhaustive search of the space of M-of-N domain theory interpretations indicates that the expected accuracy of a randomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2% is achieved in 12 cases. This demonstrates the informativeness of the domain theory, without the complications of understanding the interactions between various learning algorithms and the theory. In addition, our results help characterize the difficulty of learning using the DNA promoters theory.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {361–367},
numpages = {7}
}

@article{10.5555/1622826.1622836,
author = {Hanks, Steve and Weld, Daniel S.},
title = {A Domain-Independent Algorithm for Plan Adaptation},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation -- modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature.Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan--an arbitrary node in the plan graph--is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {319–360},
numpages = {42}
}

@article{10.5555/1622826.1622844,
author = {Russell, Stuart J. and Subramanian, Devika},
title = {Provably Bounded-Optimal Agents},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Since its inception, artificial intelligence has relied upon a theoretical foundation centred around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {575–609},
numpages = {35}
}

@article{10.5555/1622826.1622835,
author = {Cichosz, Pawe\l{}},
title = {Truncating Temporal Differences: On the Efficient Implementation of TD (λ) for Reinforcement Learning},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor λ. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(λ) for arbitrary λ, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(λ), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using λ &gt; 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {287–318},
numpages = {32}
}

@article{10.5555/1622826.1622834,
author = {Dietterich, Thomas G. and Bakiri, Ghulum},
title = {Solving Multiclass Learning Problems via Error-Correcting Output Codes},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt; 2 values (i.e., k "classes"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {263–286},
numpages = {24}
}

@article{10.5555/1622826.1622833,
author = {Minton, Steven and Bresina, John and Drummond, Mark},
title = {Total-Order and Partial-Order Planning: A Comparative Analysis},
year = {1995},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {227–262},
numpages = {36}
}

@article{10.5555/1622826.1622831,
author = {Soderland, Stephen and Lehnert, Wendy},
title = {Wrap-Up: A Trainable Discourse Module for Information Extraction},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {131–158},
numpages = {28}
}

@article{10.1613/jair.62,
author = {Buntine, Wray L.},
title = {Operations for Learning with Graphical Models},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.62},
doi = {10.1613/jair.62},
abstract = {This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks frorn data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented.The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {159–225},
numpages = {67}
}

@article{10.1613/jair.574,
author = {Hogg, Tad},
title = {Solving Highly Constrained Search Problems with Quantum Computers},
year = {1994},
issue_date = {January 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {10},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.574},
doi = {10.1613/jair.574},
abstract = {A previously developed quantum search algorithm for solving 1-SAT problems in a single step is generalized to apply to a range of highly constrained k-SAT problems. We identify a bound on the number of clauses in satisfiahility problems for which the generalized algorithm can find a solution in a constant number of steps as the number of variables increases. This performance contrasts with the linear growth in the number of steps required by the best classical algorithms, and the exponential number required by classical and quantum methods that ignore the problem structure. In some cases, the algorithm can also guarantee that insoluble problems in fact have no solutions, unlike previously proposed quantum search algorithms.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {39–66},
numpages = {28}
}

@article{10.5555/1622826.1622830,
author = {Safra, Shmuel and Tennenholtz, Moshe},
title = {On Planning While Learning},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent.We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable.We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {111–129},
numpages = {19}
}

@article{10.5555/1622826.1622829,
author = {Kitani, Tsuyoshi and Eriguchi, Yoshio and Hara, Masami},
title = {Pattern Matching and Discourse Processing in Information Extraction from Japanese Text},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {89–110},
numpages = {22}
}

@article{10.5555/1622826.1622828,
author = {Grove, Adam J. and Halpern, Joseph Y. and Koller, Daphne},
title = {Random Worlds and Maximum Entropy},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Φ holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,..., N} that satisfy KB, and compute the fraction of them in which Φ is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Φ and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {33–88},
numpages = {56}
}

@article{10.5555/1622826.1622827,
author = {Murthy, Sreerama K. and Kasif, Simon and Salzberg, Steven},
title = {A System for Induction of Oblique Decision Trees},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
abstract = {This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {1–32},
numpages = {32}
}

@article{10.5555/1618595.1618608,
author = {Sebastiani, Roberto},
title = {Applying GSAT to Non-Clausal Formulas},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {In this paper we describe how to modify GSAT so that it can be applied to nonclausal formulas. The idea is to use a particular "score" function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {309–314},
numpages = {6}
}

@article{10.5555/1618595.1618607,
author = {Borgida, Alex and Patel-Schneider, Peter F.},
title = {A Semantics and Complete Algorithm for Subsumption in the Classic Description Logic},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.},
journal = {J. Artif. Int. Res.},
month = jun,
pages = {277–308},
numpages = {32}
}

@article{10.5555/1618595.1618606,
author = {Murphy, Patrick M. and Pazzani, Michael J.},
title = {Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {257–275},
numpages = {19}
}

@article{10.5555/1618595.1618605,
author = {Cook, Diane J. and Holder, Lawrence B.},
title = {Substructure Discovery Using Minimum Description Length and Background Knowledge},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimumdescription length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {231–255},
numpages = {25}
}

@article{10.5555/1618595.1618604,
author = {Ling, Charles X.},
title = {Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {209–229},
numpages = {21}
}

@article{10.5555/1618595.1618603,
author = {Koppel, Moshe and Feldman, Ronen and Segre, Alberto Maria},
title = {Bias-Driven Revision of Logical Domain Theories},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the "flow" of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.},
journal = {J. Artif. Int. Res.},
month = feb,
pages = {159–208},
numpages = {50}
}

@article{10.5555/1618595.1618602,
author = {Nilsson, Nils J.},
title = {Teleo-Reactive Programs for Agent Control},
year = {1994},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {139–158},
numpages = {20}
}

@article{10.5555/1618595.1618601,
author = {Buchheit, Martin and Donini, Francesco M. and Schaerf, Andrea},
title = {Decidable Reasoning in Terminological Knowledge Representation Systems},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.},
journal = {J. Artif. Int. Res.},
month = dec,
pages = {109–138},
numpages = {30}
}

@article{10.5555/1618595.1618600,
author = {Bergadano, Francesco and Gunetti, Daniele and Trinchero, Umberto},
title = {The Difficulties of Learning Logic Programs with Cut},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {91–107},
numpages = {17}
}

@article{10.5555/1618595.1618599,
author = {Schlimmer, Jeffrey C. and Hermens, Leonard A.},
title = {Software Agents: Completing Patterns and Constructing User Interfaces},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software-agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface.},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {61–89},
numpages = {29}
}

@article{10.5555/1618595.1618598,
author = {Gent, Ian P. and Walsh, Toby},
title = {An Empirical Analysis of Search in GSAT},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3-SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {47–59},
numpages = {13}
}

@article{10.5555/1618595.1618597,
author = {Ginsberg, Matthew L.},
title = {Dynamic Backtracking},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {25–46},
numpages = {22}
}

@article{10.5555/1618595.1618596,
author = {Wellman, Michael P.},
title = {A Market-Oriented Programming Environment and Its Application to Distributed Multicommodity Flow Problems},
year = {1993},
issue_date = {August 1993},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {1},
number = {1},
issn = {1076-9757},
abstract = {Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.},
journal = {J. Artif. Int. Res.},
month = aug,
pages = {1–23},
numpages = {23}
}

@article{10.1613/jair.1.11675,
author = {Jauhiainen, Tommi and Lui, Marco and Zampieri, Marcos and Baldwin, Timothy and Lind\'{e}n, Krister},
title = {Automatic Language Identification in Texts: A Survey},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11675},
doi = {10.1613/jair.1.11675},
abstract = {Language identification ("LI") is the problem of determining the natural language that a document or part thereof is written in. Automatic LI has been extensively researched for over fifty years. Today, LI is a key part of many text processing pipelines, as text processing techniques generally assume that the language of the input text is known. Research in this area has recently been especially active. This article provides a brief history of LI research, and an extensive survey of the features and methods used in the LI literature. We describe the features and methods using a unified notation, to make the relationships between methods clearer. We discuss evaluation methods, applications of LI, as well as off-the-shelf LI systems that do not require training by the end user. Finally, we identify open issues, survey the work to date on each issue, and propose future directions for research in LI.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {675–682},
numpages = {8}
}

@article{10.1613/jair.1.11640,
author = {Ruder, Sebastian and Vuli\'{c}, Ivan and S\o{}gaard, Anders},
title = {A Survey of Cross-Lingual Word Embedding Models},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11640},
doi = {10.1613/jair.1.11640},
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {569–630},
numpages = {62}
}

@article{10.1613/jair.1.11635,
author = {Piacentini, Chiara and Bernardini, Sara and Beck, J. Christopher},
title = {Autonomous Target Search with Multiple Coordinated UAVs},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11635},
doi = {10.1613/jair.1.11635},
abstract = {Search and tracking is the problem of locating a moving target and following it to its destination. In this work, we consider a scenario in which the target moves across a large geographical area by following a road network and the search is performed by a team of unmanned aerial vehicles (UAVs). We formulate search and tracking as a combinatorial optimization problem and prove that the objective function is submodular. We exploit this property to devise a greedy algorithm. Although this algorithm does not offer strong theoretical guarantees because of the presence of temporal constraints that limit the feasibility of the solutions, it presents remarkably good performance, especially when several UAVs are available for the mission. As the greedy algorithm suffers when resources are scarce, we investigate two alternative optimization techniques: Constraint Programming (CP) and AI planning. Both approaches struggle to cope with large problems, and so we strengthen them by leveraging the greedy algorithm. We use the greedy solution to warm start the CP model and to devise a domain-dependent heuristic for planning. Our extensive experimental evaluation studies the scalability of the different techniques and identifies the conditions under which one approach becomes preferable to the others.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {519–568},
numpages = {50}
}

@article{10.1613/jair.1.11592,
author = {Zhang, Dianmu and Hannaford, Blake},
title = {IKBT: Solving Symbolic Inverse Kinematics with Behavior Tree},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11592},
doi = {10.1613/jair.1.11592},
abstract = {Inverse kinematics solves the problem of how to control robot arm joints to achieve desired end effector positions, which is critical to any robot arm design and implementations of control algorithms. It is a common misunderstanding that closed-form inverse kinematics analysis is solved. Popular software and algorithms, such as gradient descent or any multi-variant equations solving algorithm, claims solving inverse kinematics but only on the numerical level. While the numerical inverse kinematics solutions are relatively straightforward to obtain, these methods often fail, due to dependency on specific numerical values, even when the inverse kinematics solutions exist. Therefore, closed-form inverse kinematics analysis is superior, but there is no generalized automated algorithm. Up till now, the high-level logical reasoning involved in solving closed-form inverse kinematics made it hard to automate, so it's handled by human experts. We developed IKBT, a knowledge-based intelligent system that can mimic human experts' behaviors in solving closed-from inverse kinematics using Behavior Tree. Knowledge and rules used by engineers when solving closed-from inverse kinematics are encoded as actions in Behavior Tree. The order of applying these rules is governed by higher level composite nodes, which resembles the logical reasoning process of engineers. It is also the first time that the dependency of joint variables, an important issue in inverse kinematics analysis, is automatically tracked in graph form. Besides generating closed-form solutions, IKBT also explains its solving strategies in human (engineers) interpretable form. This is a proof-of-concept of using Behavior Trees to solve high-cognitive problems.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {457–486},
numpages = {30}
}

@article{10.1613/jair.1.11583,
author = {Fioretto, Ferdinando and Van Hentenryck, Pascal},
title = {Optstream: Releasing Time Series Privately},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11583},
doi = {10.1613/jair.1.11583},
abstract = {Many applications of machine learning and optimization operate on data streams. While these datasets are fundamental to fuel decision-making algorithms, often they contain sensitive information about individuals, and their usage poses significant privacy risks. Motivated by an application in energy systems, this paper presents OPTSTREAM, a novel algorithm for releasing differentially private data streams under the w-event model of privacy. OPTSTREAM is a 4-step procedure consisting of sampling, perturbation, reconstruction, and post-processing modules. First, the sampling module selects a small set of points to access in each period of interest. Then, the perturbation module adds noise to the sampled data points to guarantee privacy. Next, the reconstruction module reassembles non-sampled data points from the perturbed sample points. Finally, the post-processing module uses convex optimization over the privacy-preserving output of the previous modules, as well as the privacy-preserving answers of additional queries on the data stream, to improve accuracy by redistributing the added noise. OPTSTREAM is evaluated on a test case involving the release of a real data stream from the largest European transmission operator. Experimental results show that OPTSTREAM may not only improve the accuracy of state-of-the-art methods by at least one order of magnitude but also supports accurate load forecasting on the privacy-preserving data.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {423–456},
numpages = {34}
}

@article{10.1613/jair.1.11582,
author = {Ismaili, Anisse and Hamada, Naoto and Zhang, Yuzhe and Suzuki, Takamasa and Yokoo, Makoto},
title = {Weighted Matching Markets with Budget Constraints},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11582},
doi = {10.1613/jair.1.11582},
abstract = {We investigate markets with a set of students on one side and a set of colleges on the other. A student and college can be linked by a weighted contract that defines the student's wage, while a college's budget for hiring students is limited. Stability is a crucial requirement for matching mechanisms to be applied in the real world. A standard stability requirement is coalitional stability, i.e., no pair of a college and group of students has any incentive to deviate. We find that a coalitionally stable matching is not guaranteed to exist, verifying the coalitional stability for a given matching is coNP-complete, and the problem of finding whether a coalitionally stable matching exists in a given market, is Σ2P-complete: NPNP-complete. Other negative results also hold when blocking coalitions contain at most two students and one college. Given these computational hardness results, we pursue a weaker stability requirement called pairwise stability, where no pair of a college and single student has an incentive to deviate. Unfortunately, a pairwise stable matching is not guaranteed to exist either. Thus, we consider a restricted market called a typed weighted market, in which students are partitioned into types that induce their possible wages. We then design a strategy-proof and Pareto efficient mechanism that works in polynomial-time for computing a pairwise stable matching in typed weighted markets.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {393–421},
numpages = {29}
}

@article{10.1613/jair.1.11576,
author = {Gnad, Daniel and Hoffmann, J\"{o}rg and Wehrle, Martin},
title = {Strong Stubborn Set Pruning for Star-Topology Decoupled State Space Search},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11576},
doi = {10.1613/jair.1.11576},
abstract = {Analyzing reachability in large discrete transition systems is an important sub-problem in several areas of AI, and of CS in general. State space search is a basic method for conducting such an analysis. A wealth of techniques have been proposed to reduce the search space without affecting the existence of (optimal) solution paths. In particular, strong stubborn set (SSS) pruning is a prominent such method, analyzing action dependencies to prune commutative parts of the search space. We herein show how to apply this idea to star-topology decoupled state space search, a recent search reformulation method invented in the context of classical AI planning.Star-topology decoupled state space search, short decoupled search, addresses planning tasks where a single center component interacts with several leaf components. The search exploits a form of conditional independence arising in this setting: given a fixed path πC of transitions by the center, the possible leaf moves compliant with πC are independent across the leaves. Decoupled search thus searches over center paths only, maintaining the compliant paths for each leaf separately. This avoids the enumeration of combined states across leaves.Just like standard search, decoupled search is adversely affected by commutative parts of its search space. The adaptation of strong stubborn set pruning is challenging due to the more complex structure of the search space, and the resulting ways in which action dependencies may affect the search. We spell out how to address this challenge, designing optimality-preserving decoupled strong stubborn set (DSSS) pruning methods. We introduce a design for star topologies in full generality, as well as simpler design variants for the practically relevant fork and inverted fork special cases. We show that there are cases where DSSS pruning is exponentially more effective than both, decoupled search and SSS pruning, exhibiting true synergy where the whole is more than the sum of its parts. Empirically, DSSS pruning reliably inherits the best of its components, and sometimes outperforms both.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {343–392},
numpages = {50}
}

@article{10.1613/jair.1.11569,
author = {Pineda, Luis and Zilberstein, Shlomo},
title = {Probabilistic Planning with Reduced Models},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11569},
doi = {10.1613/jair.1.11569},
abstract = {Reduced models are simplified versions of a given domain, designed to accelerate the planning process. Interest in reduced models has grown since the surprising success of determinization in the first international probabilistic planning competition, leading to the development of several enhanced determinization techniques. To address the drawbacks of previous determinization methods, we introduce a family of reduced models in which probabilistic outcomes are classified as one of two types: primary and exceptional. In each model that belongs to this family of reductions, primary outcomes can occur an unbounded number of times per trajectory, while exceptions can occur at most a finite number of times, specified by a parameter. Distinct reduced models are characterized by two parameters: the maximum number of primary outcomes per action, and the maximum number of occurrences of exceptions per trajectory. This family of reductions generalizes the well-known most-likely-outcome determinization approach, which includes one primary outcome per action and zero exceptional outcomes per plan. We present a framework to determine the benefits of planning with reduced models, and develop a continual planning approach that handles situations where the number of exceptions exceeds the specified bound during plan execution. Using this framework, we compare the performance of various reduced models and consider the challenge of generating good ones automatically. We show that each one of the dimensions--allowing more than one primary outcome or planning for some limited number of exceptions--could improve performance relative to standard determinization. The results place previous work on determinization in a broader context and lay the foundation for a systematic exploration of the space of model reductions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {271–306},
numpages = {36}
}

@article{10.1613/jair.1.11551,
author = {Keren, Sarah and Gal, Avigdor and Karpas, Erez},
title = {Goal Recognition Design in Deterministic Environments},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11551},
doi = {10.1613/jair.1.11551},
abstract = {Goal recognition design (GRD) facilitates understanding the goals of acting agents through the analysis and redesign of goal recognition models, thus offering a solution for assessing and minimizing the maximal progress of any agent in the model before goal recognition is guaranteed. In a nutshell, given a model of a domain and a set of possible goals, a solution to a GRD problem determines (1) the extent to which actions performed by an agent within the model reveal the agent's objective; and (2) how best to modify the model so that the objective of an agent can be detected as early as possible. This approach is relevant to any domain in which rapid goal recognition is essential and the model design can be controlled. Applications include intrusion detection, assisted cognition, computer games, and human-robot collaboration.A GRD problem has two components: the analyzed goal recognition setting, and a design model specifying the possible ways the environment in which agents act can be modified so as to facilitate recognition. This work formulates a general framework for GRD in deterministic and partially observable environments, and offers a toolbox of solutions for evaluating and optimizing model quality for various settings.For the purpose of evaluation we suggest the worst case distinctiveness (WCD) measure, which represents the maximal cost of a path an agent may follow before its goal can be inferred by a goal recognition system. We offer novel compilations to classical planning for calculating WCD in settings where agents are bounded-suboptimal. We then suggest methods for minimizing WCD by searching for an optimal redesign strategy within the space of possible modifications, and using pruning to increase efficiency. We support our approach with an empirical evaluation that measures WCD in a variety of GRD settings and tests the efficiency of our compilation-based methods for computing it. We also examine the effectiveness of reducing WCD via redesign and the performance gain brought about by our proposed pruning strategy.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {209–269},
numpages = {61}
}

@article{10.1613/jair.1.11529,
author = {Peitl, Tom\'{a}\v{s} and Slivovsky, Friedrich and Szeider, Stefan},
title = {Dependency Learning for QBF},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11529},
doi = {10.1613/jair.1.11529},
abstract = {Quantified Boolean Formulas (QBFs) can be used to succinctly encode problems from domains such as formal verification, planning, and synthesis. One of the main approaches to QBF solving is Quantified Conflict Driven Clause Learning (QCDCL). By default, QCDCL assigns variables in the order of their appearance in the quantifier prefix so as to account for dependencies among variables. Dependency schemes can be used to relax this restriction and exploit independence among variables in certain cases, but only at the cost of nontrivial interferences with the proof system underlying QCDCL. We introduce dependency learning, a new technique for exploiting variable independence within QCDCL that allows solvers to learn variable dependencies on the fly. The resulting version of QCDCL enjoys improved propagation and increased flexibility in choosing variables for branching while retaining ordinary (long-distance) Q-resolution as its underlying proof system. We show that dependency learning can achieve exponential speedups over ordinary QCDCL. Experiments on standard benchmark sets demonstrate the effectiveness of this technique.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {181–208},
numpages = {28}
}

@article{10.1613/jair.1.11524,
author = {Sridharan, Mohan and Gelfond, Michael and Zhang, Shiqi and Wyatt, Jeremy},
title = {REBA: A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11524},
doi = {10.1613/jair.1.11524},
abstract = {This article describes REBA, a knowledge representation and reasoning architecture for robots that is based on tightly-coupled transition diagrams of the domain at two different levels of granularity. An action language is extended to support non-boolean fluents and non-deterministic causal laws, and used to describe the domain's transition diagrams, with the fine-resolution transition diagram being defined as a refinement of the coarse-resolution transition diagram. The coarse-resolution system description, and a history that includes prioritized defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this abstract transition. The zoomed fine-resolution system description, and a probabilistic representation of the uncertainty in sensing and actuation, are used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract transition as a sequence of concrete actions. The fine-resolution outcomes of executing these concrete actions are used to infer coarse-resolution outcomes that are added to the coarse-resolution history and used for subsequent coarse-resolution reasoning. The architecture thus combines the complementary strengths of declarative programming and probabilistic graphical models to represent and reason with non-monotonic logic-based and probabilistic descriptions of uncertainty and incomplete domain knowledge. In addition, we describe a general methodology for the design of software components of a robot based on these knowledge representation and reasoning tools, and provide a path for proving the correctness of these components. The architecture is evaluated in simulation and on a mobile robot finding and moving target objects to desired locations in indoor domains, to show that the architecture supports reliable and efficient reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {87–180},
numpages = {94}
}

@article{10.1613/jair.1.11494,
author = {Telang, Pankaj R. and Singh, Munindar P. and Yorke-Smith, Neil},
title = {A Coupled Operational Semantics for Goals and Commitments},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11494},
doi = {10.1613/jair.1.11494},
abstract = {Commitments capture how an agent relates to another agent, whereas goals describe states of the world that an agent is motivated to bring about. Commitments are elements of the social state of a set of agents whereas goals are elements of the private states of individual agents. It makes intuitive sense that goals and commitments are understood as being complementary to each other. More importantly, an agent's goals and commitments ought to be coherent, in the sense that an agent's goals would lead it to adopt or modify relevant commitments and an agent's commitments would lead it to adopt or modify relevant goals. However, despite the intuitive naturalness of the above connections, they have not been adequately studied in a formal framework. This article provides a combined operational semantics for goals and commitments by relating their respective life cycles as a basis for how these concepts (1) cohere for an individual agent and (2) engender cooperation among agents. Our semantics yields important desirable properties of convergence of the configurations of cooperating agents, thereby delineating some theoretically well-founded yet practical modes of cooperation in a multiagent system.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {31–85},
numpages = {55}
}

@article{10.1613/jair.1.11478,
author = {Fran\c{c}ois-Lavet, Vincent and Rabusseau, Guillaume and Pineau, Joelle and Ernst, Damien and Fonteneau, Raphael},
title = {On Overfitting and Asymptotic Bias in Batch Reinforcement Learning with Partial Observability},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11478},
doi = {10.1613/jair.1.11478},
abstract = {This paper provides an analysis of the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data) in the context of reinforcement learning with partial observability. Our theoretical analysis formally characterizes that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting. This analysis relies on expressing the quality of a state representation by bounding L1 error terms of the associated belief states. Theoretical results are empirically illustrated when the state representation is a truncated history of observations, both on synthetic POMDPs and on a large-scale POMDP in the context of smartgrids, with real-world data. Finally, similarly to known results in the fully observable setting, we also briefly discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting in the partially observable context.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–30},
numpages = {30}
}

@article{10.1613/jair.1.11422,
author = {Botea, Adi and Kishimoto, Akihiro and Nikolova, Evdokia and Braghin, Stefano and Berlingerio, Michele and Daly, Elizabeth},
title = {Computing Multi-Modal Journey Plans under Uncertainty},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11422},
doi = {10.1613/jair.1.11422},
abstract = {Multi-modal journey planning, which allows multiple types of transport within a single trip, is becoming increasingly popular, due to a strong practical interest and an increasing availability of data. In real life, transport networks feature uncertainty. Yet, most approaches assume a deterministic environment, making plans more prone to failures such as missed connections and major delays in the arrival.This paper presents an approach to computing optimal contingent plans in multi-modal journey planning. The problem is modeled as a search in an And/Or state space. We describe search enhancements used on top of the AO* algorithm. Enhancements include admissible heuristics, multiple types of pruning that preserve the completeness and the optimality, and a hybrid search approach with a deterministic and a nondeterministic search. We demonstrate an NP-hardness result, with the hardness stemming from the dynamically changing distributions of the travel time random variables. We perform a detailed empirical analysis on realistic transport networks from cities such as Montpellier, Rome and Dublin. The results demonstrate the effectiveness of our algorithmic contributions, and the benefits of contingent plans as compared to standard sequential plans, when the arrival and departure times of buses are characterized by uncertainty.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {633–674},
numpages = {42}
}

@article{10.1613/jair.1.11366,
author = {Balaban, Edward and Johnson, Stephen B. and Kochenderfer, Mykel J.},
title = {Unifying System Health Management and Automated Decision Making},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11366},
doi = {10.1613/jair.1.11366},
abstract = {Health management of complex dynamic systems has evolved from simple automated alarms into a subfield of artificial intelligence with techniques for analyzing off-nominal conditions and generating responses. This evolution took place largely apart from the development of automated system control, planning, and scheduling (generally referred to in this work as decision making). While there have been efforts to establish an information exchange between system health management and decision making, successful practical implementations of integrated architectures remain limited. This article proposes that rather than being treated as connected yet distinct entities, system health management and decision making should be unified in their formulations. Enabled by advances in modeling and algorithms, we believe that a unified approach will increase systems' resilience to faults and improve their effectiveness. We overview the prevalent system health management methodology, illustrate its limitations through numerical examples, and describe a proposed unified approach. We then show how typical system health management concepts are accommodated in the proposed approach without loss of functionality or generality. A computational complexity analysis of the unified approach is also provided.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {487–518},
numpages = {32}
}

@article{10.1613/jair.1.11349,
author = {Jacobs, Bart},
title = {The Mathematics of Changing One's Mind, via Jeffrey's or via Pearl's Update Rule},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11349},
doi = {10.1613/jair.1.11349},
abstract = {Evidence in probabilistic reasoning may be 'hard' or 'soft', that is, it may be of yes/no form, or it may involve a strength of belief, in the unit interval [0, 1]. Reasoning with soft, [0, 1]-valued evidence is important in many situations but may lead to different, confusing interpretations. This paper intends to bring more mathematical and conceptual clarity to the field by shifting the existing focus from specification of soft evidence to accomodation of soft evidence. There are two main approaches, known as Jeffrey's rule and Pearl's method; they give different outcomes on soft evidence. This paper argues that they can be understood as correction and as improvement. It describes these two approaches as different ways of updating with soft evidence, highlighting their differences, similarities and applications. This account is based on a novel channel-based approach to Bayesian probability. Proper understanding of these two update mechanisms is highly relevant for inference, decision tools and probabilistic programming languages.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {783–806},
numpages = {24}
}

@article{10.1613/jair.1.11324,
author = {Walraven, Erwin and Spaan, Matthijs T. J.},
title = {Point-Based Value Iteration for Finite-Horizon POMDPs},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11324},
doi = {10.1613/jair.1.11324},
abstract = {Partially Observable Markov Decision Processes (POMDPs) are a popular formalism for sequential decision making in partially observable environments. Since solving POMDPs to optimality is a difficult task, point-based value iteration methods are widely used. These methods compute an approximate POMDP solution, and in some cases they even provide guarantees on the solution quality, but these algorithms have been designed for problems with an infinite planning horizon. In this paper we discuss why state-of-the-art point-based algorithms cannot be easily applied to finite-horizon problems that do not include discounting. Subsequently, we present a general point-based value iteration algorithm for finite-horizon problems which provides solutions with guarantees on solution quality. Furthermore, we introduce two heuristics to reduce the number of belief points considered during execution, which lowers the computational requirements. In experiments we demonstrate that the algorithm is an effective method for solving finite-horizon POMDPs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {307–341},
numpages = {35}
}

@article{10.1613/jair.1.11468,
author = {Leite, Allan R. and Enembreck, Fabr\'{\i}cio},
title = {COOPT: Using Collective Behavior of Coupled Oscillators for Solving DCOP},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11468},
doi = {10.1613/jair.1.11468},
abstract = {The distributed constraint optimization problem (DCOP) has emerged as one of the most promising coordination techniques in multiagent systems. However, because DCOP is known to be NP-hard, the existing DCOP techniques are often unsuitable for large-scale applications, which require distributed and scalable algorithms to deal with severely limited computing and communication. In this paper, we present a novel approach to provide approximate solutions for large-scale, complex DCOPs. This approach introduces concepts of synchronization of coupled oscillators for speeding up the convergence process towards high-quality solutions. We propose a new anytime local search DCOP algorithm, called Coupled Oscillator OPTimization (COOPT), which amounts to iteratively solving a DCOP by agents exchanging local information that brings them to a consensus. We empirically evaluate COOPT on constraint networks involving hundreds of variables with different topologies, domains, and densities. Our experimental results demonstrate that COOPT outperforms other incomplete state-of-the-art DCOP algorithms, especially in terms of the agents' communication cost and solution quality.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {987–1023},
numpages = {37}
}

@article{10.1613/jair.1.11453,
author = {Combi, Carlo and Posenato, Roberto and Vigan\`{o}, Luca and Zavatteri, Matteo},
title = {Conditional Simple Temporal Networks with Uncertainty and Resources},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11453},
doi = {10.1613/jair.1.11453},
abstract = {Conditional simple temporal networks with uncertainty (CSTNUs) allow for the representation of temporal plans subject to both conditional constraints and uncertain durations. Dynamic controllability (DC) of CSTNUs ensures the existence of an execution strategy able to execute the network in real time (i.e., scheduling the time points under control) depending on how these two uncontrollable parts behave. However, CSTNUs do not deal with resources.In this paper, we define conditional simple temporal networks with uncertainty and resources (CSTNURs) by injecting resources and runtime resource constraints (RRCs) into the specification. Resources are mandatory for executing the time points and their availability is represented through temporal expressions, whereas RRCs restrict resource availability by further temporal constraints among resources.We provide a fully-automated encoding to translate any CSTNUR into an equivalent timed game automaton in polynomial time for a sound and complete DC-checking.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {931–985},
numpages = {55}
}

@article{10.1613/jair.1.11446,
author = {Grandi, Umberto and Grossi, Davide and Turrini, Paolo},
title = {Negotiable Votes Pre-Vote Negotiations in Binary Voting with Non-Manipulable Rules},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11446},
doi = {10.1613/jair.1.11446},
abstract = {We study voting games on binary issues, where voters hold an objective over the outcome of the collective decision and are allowed, before the vote takes place, to negotiate their ballots with the other participants. We analyse the voters' rational behaviour in the resulting two-phase game when ballots are aggregated via non-manipulable rules and, more specifically, quota rules. We show under what conditions undesirable equilibria can be removed and desirable ones sustained as a consequence of the pre-vote phase.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {895–929},
numpages = {35}
}

@article{10.1613/jair.1.11420,
author = {Eggensperger, Katharina and Lindauer, Marius and Hutter, Frank},
title = {Pitfalls and Best Practices in Algorithm Configuration},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11420},
doi = {10.1613/jair.1.11420},
abstract = {Good parameter settings are crucial to achieve high performance in many areas of artificial intelligence (AI), such as propositional satisfiability solving, AI planning, scheduling, and machine learning (in particular deep learning). Automated algorithm configuration methods have recently received much attention in the AI community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. However, practical applications of algorithm configuration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineffective. We identify several common issues and propose best practices for avoiding them. As one possibility for automatically handling as many of these as possible, we also propose a tool called GenericWrapper4AC.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {861–893},
numpages = {33}
}

@article{10.1613/jair.1.11418,
author = {Amato, Christopher and Konidaris, George and Kaelbling, Leslie P. and How, Jonathan P.},
title = {Modeling and Planning with Macro-Actions in Decentralized POMDPs},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11418},
doi = {10.1613/jair.1.11418},
abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for decentralized multi-agent decision making under uncertainty. However, they typically model a problem at a low level of granularity, where each agent's actions are primitive operations lasting exactly one time step. We address the case where each agent has macro-actions: temporally extended actions that may require different amounts of time to execute. We model macro-actions as options in a Dec-POMDP, focusing on actions that depend only on information directly available to the agent during execution. Therefore, we model systems where coordination decisions only occur at the level of deciding which macro-actions to execute. The core technical difficulty in this setting is that the options chosen by each agent no longer terminate at the same time. We extend three leading Dec-POMDP algorithms for policy generation to the macro-action case, and demonstrate their effectiveness in both standard benchmarks and a multi-robot coordination problem. The results show that our new algorithms retain agent coordination while allowing high-quality solutions to be generated for significantly longer horizons and larger state-spaces than previous Dec-POMDP methods. Furthermore, in the multi-robot domain, we show that, in contrast to most existing methods that are specialized to a particular problem class, our approach can synthesize control policies that exploit opportunities for coordination while balancing uncertainty, sensor information, and information about other agents.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {817–859},
numpages = {43}
}

@article{10.1613/jair.1.11408,
author = {Asuncion, Vernon and Zhang, Yan and Zhang, Heng and Li, Ruixuan},
title = {Polynomial and Exponential Bounded Logic Programs with Function Symbols: Some New Decidable Classes},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11408},
doi = {10.1613/jair.1.11408},
abstract = {A logic program with function symbols is called finitely ground if there is a finite propositional logic program whose stable models are exactly the same as the stable models of this program. Finite groundability is an important property for logic programs with function symbols because it makes feasible to compute such programs' stable models using traditional ASP solvers. In this paper, we introduce new decidable classes of finitely ground programs called poly-bounded and k-EXP-bounded programs, which, to the best of our knowledge, strictly contain all other decidable classes of finitely ground programs discovered so far in the literature. We also study the relevant complexity properties for these classes of programs. We prove that the membership complexities for poly-bounded and k-EXP-bounded programs are EXPTIME-complete and (k+1)-EXPTIME-complete, respectively.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {749–815},
numpages = {67}
}

@article{10.1613/jair.1.11400,
author = {Nguyen, Duc Thien and Yeoh, William and Lau, Hoong Chuin and Zivan, Roie},
title = {Distributed Gibbs: A Linear-Space Sampling-Based DCOP Algorithm},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11400},
doi = {10.1613/jair.1.11400},
abstract = {Researchers have used distributed constraint optimization problems (DCOPs) to model various multi-agent coordination and resource allocation problems. Very recently, Ottens et al. proposed a promising new approach to solve DCOPs that is based on confidence bounds via their Distributed UCT (DUCT) sampling-based algorithm. Unfortunately, its memory requirement per agent is exponential in the number of agents in the problem, which prohibits it from scaling up to large problems. Thus, in this article, we introduce two new sampling-based DCOP algorithms called Sequential Distributed Gibbs (SD-Gibbs) and Parallel Distributed Gibbs (PD-Gibbs). Both algorithms have memory requirements per agent that is linear in the number of agents in the problem. Our empirical results show that our algorithms can find solutions that are better than DUCT, run faster than DUCT, and solve some large problems that DUCT failed to solve due to memory limitations.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {705–748},
numpages = {44}
}

@article{10.1613/jair.1.11396,
author = {Da Silva, Felipe Leno and Costa, Anna Helena Reali},
title = {A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11396},
doi = {10.1613/jair.1.11396},
abstract = {Multiagent Reinforcement Learning (RL) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. However, learning a complex task from scratch is impractical due to the huge sample complexity of RL algorithms. For this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent RL algorithms. This survey provides a unifying view of the literature on knowledge reuse in multiagent RL. We define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). We aim at encouraging the community to work towards reusing all the knowledge sources available in a MAS. For that, we provide an in-depth discussion of current lines of research and open questions.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {645–703},
numpages = {59}
}

@article{10.1613/jair.1.11395,
author = {Bienvenu, Meghyn and Bourgaux, Camille and Goasdou\'{e}, Fran\c{c}ois},
title = {Computing and Explaining Query Answers over Inconsistent DL-Lite Knowledge Bases},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11395},
doi = {10.1613/jair.1.11395},
abstract = {Several inconsistency-tolerant semantics have been introduced for querying inconsistent description logic knowledge bases. The first contribution of this paper is a practical approach for computing the query answers under three well-known such semantics, namely the AR, IAR and brave semantics, in the lightweight description logic DL-LiteR. We show that query answering under the intractable AR semantics can be performed efficiently by using IAR and brave semantics as tractable approximations and encoding the AR entailment problem as a propositional satisfiability (SAT) problem. The second issue tackled in this work is explaining why a tuple is a (non-)answer to a query under these semantics. We define explanations for positive and negative answers under the brave, AR and IAR semantics. We then study the computational properties of explanations in DL-LiteR. For each type of explanation, we analyze the data complexity of recognizing (preferred) explanations and deciding if a given assertion is relevant or necessary. We establish tight connections between intractable explanation problems and variants of SAT, enabling us to generate explanations by exploiting solvers for Boolean satisfaction and optimization problems. Finally, we empirically study the efficiency of our query answering and explanation framework using a benchmark we built upon the well-established LUBM benchmark.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {563–644},
numpages = {82}
}

@article{10.1613/jair.1.11388,
author = {Hern\'{a}ndez-Orallo, Jos\'{e}},
title = {AI Generality and Spearman's Law of Diminishing Returns},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11388},
doi = {10.1613/jair.1.11388},
abstract = {Many areas of AI today use benchmarks and competitions with larger and wider sets of tasks. This tries to deter AI systems (and research effort) from specialising to a single task, and encourage them to be prepared to solve previously unseen tasks. It is unclear, however, whether the methods with best performance are actually those that are most general and, in perspective, whether the trend moves towards more general AI systems. This question has a striking similarity with the analysis of the so-called positive manifold and general factors in the area of human intelligence. In this paper, we first show how the existence of a manifold (positive average pairwise task correlation) can also be analysed in AI, and how this relates to the notion of agent generality, from the individual and the populational points of view. From the populational perspective, we analyse the following question: is this manifold correlation higher for the most or for the least able group of agents? We contrast this analysis with one of the most controversial issues in human intelligence research, the so-called Spearman's Law of Diminishing Returns (SLODR), which basically states that the relevance of a general factor diminishes for most able human groups. We perform two empirical studies on these issues in AI. We analyse the results of the 2015 general video game AI (GVGAI) competition, with games as tasks and "controllers" as agents, and the results of a synthetic setting, with modified elementary cellular automata (ECA) rules as tasks and simple interactive programs as agents. In both cases, we see that SLODR does not appear. The data, and the use of just two scenarios, does not clearly support the reverse either, a Universal Law of Augmenting Returns (ULOAR), but calls for more experiments on this question.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {529–562},
numpages = {34}
}

@article{10.1613/jair.1.11376,
author = {Nebel, Bernhard and Bolander, Thomas and Engesser, Thorsten and Mattm\"{u}ller, Robert},
title = {Implicitly Coordinated Multi-Agent Path Finding under Destination Uncertainty: Success Guarantees and Computational Complexity},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11376},
doi = {10.1613/jair.1.11376},
abstract = {In multi-agent path finding (MAPF), it is usually assumed that planning is performed centrally and that the destinations of the agents are common knowledge. We will drop both assumptions and analyze under which conditions it can be guaranteed that the agents reach their respective destinations using implicitly coordinated plans without communication. Furthermore, we will analyze what the computational costs associated with such a coordination regime are. As it turns out, guarantees can be given assuming that the agents are of a certain type. However, the implied computational costs are quite severe. In the distributed setting, we either have to solve a sequence of NP-complete problems or have to tolerate exponentially longer executions. In the setting with destination uncertainty, bounded plan existence becomes PSPACE-complete. This clearly demonstrates the value of communicating about plans before execution starts.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {497–527},
numpages = {31}
}

@article{10.1613/jair.1.11375,
author = {Cui, Jing and Haslum, Patrik},
title = {Dynamic Controllability of Controllable Conditional Temporal Problems with Uncertainty},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11375},
doi = {10.1613/jair.1.11375},
abstract = {Dynamic Controllability (DC) of a Simple Temporal Problem with Uncertainty (STPU) uses a dynamic decision strategy, rather than a fixed schedule, to tackle temporal uncertainty. We extend this concept to the Controllable Conditional Temporal Problem with Uncertainty (CCTPU), which extends the STPU by conditioning temporal constraints on the assignment of controllable discrete variables. We define dynamic controllability of a CCTPU as the existence of a strategy that decides on both the values of discrete choice variables and the scheduling of controllable time points dynamically. This contrasts with previous work, which made a static assignment of choice variables and dynamic decisions over time points only. We propose an algorithm to find such a fully dynamic strategy. The algorithm computes the "envelope" of outcomes of temporal uncertainty in which a particular assignment of discrete variables is feasible, and aggregates these over all choices. When an aggregated envelope covers all uncertain situations of the CCTPU, the problem is dynamically controllable. However, the algorithm is complete only under certain assumptions. Experiments on an existing set of CCTPU benchmarks show that there are cases in which making both discrete and temporal decisions dynamically it is feasible to satisfy the problem constraints while assigning the discrete variables statically it is not.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {445–495},
numpages = {51}
}

@article{10.1613/jair.1.11370,
author = {Burch, Neil and Moravcik, Matej and Schmid, Martin},
title = {Revisiting CFR+ and Alternating Updates},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11370},
doi = {10.1613/jair.1.11370},
abstract = {The CFR+ algorithm for solving imperfect information games is a variant of the popular CFR algorithm, with faster empirical performance on a range of problems. It was introduced with a theoretical upper bound on solution error, but subsequent work showed an error in one step of the proof. We provide updated proofs to recover the original bound.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {429–443},
numpages = {15}
}

@article{10.1613/jair.1.11369,
author = {Song, Wen and Kang, Donghun and Zhang, Jie and Cao, Zhiguang and Xi, Hui},
title = {A Sampling Approach for Proactive Project Scheduling under Generalized Time-Dependent Workability Uncertainty},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11369},
doi = {10.1613/jair.1.11369},
abstract = {In real-world project scheduling applications, activity durations are often uncertain. Proactive scheduling can effectively cope with the duration uncertainties, by generating robust baseline solutions according to a priori stochastic knowledge. However, most of the existing proactive approaches assume that the duration uncertainty of an activity is not related to its scheduled start time, which may not hold in many real-world scenarios. In this paper, we relax this assumption by allowing the duration uncertainty to be time-dependent, which is caused by the uncertainty of whether the activity can be executed on each time slot. We propose a stochastic optimization model to find an optimal Partial-order Schedule (POS) that minimizes the expected makespan. This model can cover both the time-dependent uncertainty studied in this paper and the traditional time-independent duration uncertainty. To circumvent the underlying complexity in evaluating a given solution, we approximate the stochastic optimization model based on Sample Average Approximation (SAA). Finally, we design two efficient branch-and-bound algorithms to solve the NP-hard SAA problem. Empirical evaluation confirms that our approach can generate high-quality proactive solutions for a variety of uncertainty distributions.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {385–427},
numpages = {43}
}

@article{10.1613/jair.1.11361,
author = {Wright, James R. and Leyton-Brown, Kevin},
title = {Level-0 Models for Predicting Human Behavior in Games},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11361},
doi = {10.1613/jair.1.11361},
abstract = {Behavioral game theory seeks to describe the way actual people (as compared to idealized, "rational" agents) act in strategic situations. Our own recent work has identified iterative models, such as quantal cognitive hierarchy, as the state of the art for predicting human play in unrepeated, simultaneous-move games. Iterative models predict that agents reason iteratively about their opponents, building up from a specification of nonstrategic behavior called level-0. A modeler is in principle free to choose any description of level-0 behavior that makes sense for a given setting. However, in practice almost all existing work specifies this behavior as a uniform distribution over actions. In most games it is not plausible that even nonstrategic agents would choose an action uniformly at random, nor that other agents would expect them to do so. A more accurate model for level-0 behavior has the potential to dramatically improve predictions of human behavior, since a substantial fraction of agents may play level-0 strategies directly, and furthermore since iterative models ground all higher-level strategies in responses to the level-0 strategy. Our work considers models of the way in which level-0 agents construct a probability distribution over actions, given an arbitrary game. We considered a large space of alternatives and, in the end, recommend a model that achieved excellent performance across the board: a linear weighting of four binary features, each of which is general in the sense that it can be computed from any normal form game. Adding real-valued variants of the same four features yielded further improvements in performance, albeit with a corresponding increase in the number of parameters needing to be estimated. We evaluated the effects of combining these new level-0 models with several iterative models and observed large improvements in predictive accuracy.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {357–383},
numpages = {27}
}

@article{10.1613/jair.1.11358,
author = {Garg, Nikhil and Kamble, Vijay and Goel, Ashish and Marn, David and Munagala, Kamesh},
title = {Iterative Local Voting for Collective Decision-Making in Continuous Spaces},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11358},
doi = {10.1613/jair.1.11358},
abstract = {Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a algorithm called Iterative Local Voting for collective decision-making in this setting. In this algorithm, voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm, with the size of the ball shrinking at a specified rate.We first prove the convergence of this algorithm under appropriate choices of neighborhoods to Pareto optimal solutions with desirable fairness properties in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution.We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 2,000 workers, employing neighborhoods defined by various L-Norm balls. We make several observations that inform future implementations of such a procedure.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {315–355},
numpages = {41}
}

@article{10.1613/jair.1.11355,
author = {Grau, Bernardo Cuenca and Kostylev, Egor V.},
title = {Logical Foundations of Linked Data Anonymisation},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11355},
doi = {10.1613/jair.1.11355},
abstract = {The widespread adoption of the Linked Data paradigm has been driven by the increasing demand for information exchange between organisations, as well as by regulations in domains such as health care and governance that require certain data to be published. In this setting, sensitive information is at high risk of disclosure since published data can be often seamlessly linked with arbitrary external data sources.In this paper we lay the logical foundations of anonymisation in the context of Linked Data. We consider anonymisations of RDF graphs (and, more generally, relational datasets with labelled nulls) and define notions of policy-compliant and linkage-safe anonymisations. Policy compliance ensures that an anonymised dataset does not reveal any sensitive information as specified by a policy query. Linkage safety ensures that an anonymised dataset remains compliant even if it is linked to (possibly unknown) external datasets available on the Web, thus providing provable protection guarantees against data linkage attacks. We establish the computational complexity of the underpinning decision problems both under the open-world semantics inherent to RDF and under the assumption that an attacker has complete, closed-world knowledge over some parts of the original data.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {253–314},
numpages = {62}
}

@article{10.1613/jair.1.11345,
author = {Zanzotto, Fabio Massimo},
title = {Viewpoint: Human-in-the-Loop Artificial Intelligence},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11345},
doi = {10.1613/jair.1.11345},
abstract = {Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {243–252},
numpages = {10}
}

@article{10.1613/jair.1.11343,
author = {Masters, Peta and Sardina, Sebastian},
title = {Cost-Based Goal Recognition in Navigational Domains},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11343},
doi = {10.1613/jair.1.11343},
abstract = {Goal recognition is the problem of determining an agent's intent by observing her behaviour. Contemporary solutions for general task-planning relate the probability of a goal to the cost of reaching it. We adapt this approach to goal recognition in the strict context of path-planning. We show (1) that a simpler formula provides an identical result to current state-of-the-art in less than half the time under all but one set of conditions. Further, we prove (2) that the probability distribution based on this technique is independent of an agent's past behaviour and present a revised formula that achieves goal recognition by reference to the agent's starting point and current location only. Building on this, we demonstrate (3) that a Radius of Maximum Probability (i.e., the distance from a goal within which that goal is guaranteed to be the most probable) can be calculated from relative cost-distances between the candidate goals and a start location, without needing to calculate any actual probabilities. In this extended version of earlier work, we generalise our framework to the continuous domain and discuss our results, including the conditions under which our findings can be generalised back to goal recognition in general task-planning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {197–242},
numpages = {46}
}

@article{10.1613/jair.1.11338,
author = {Tian, Yan and Wang, Xun and Wu, Jiachen and Wang, Ruili and Yang, Bailin},
title = {Multi-Scale Hierarchical Residual Network for Dense Captioning},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11338},
doi = {10.1613/jair.1.11338},
abstract = {Recent research on dense captioning based on the recurrent neural network and the convolutional neural network has made a great progress. However, mapping from an image feature space to a description space is a nonlinear and multimodel task, which makes it difficult for the current methods to get accurate results. In this paper, we put forward a novel approach for dense captioning based on hourglass-structured residual learning. Discriminant feature maps are obtained by incorporating dense connected networks and residual learning in our model. Finally, the performance of the approach on the Visual Genome V1.0 dataset and the region labelled MS-COCO (Microsoft Common Objects in Context) dataset are demonstrated. The experimental results have shown that our approach outperforms most current methods.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {181–196},
numpages = {16}
}

@article{10.1613/jair.1.11337,
author = {Zhuang, Zhiqiang and Wang, Zhe and Wang, Kewen and Delgrande, James},
title = {A Generalisation of AGM Contraction and Revision to Fragments of First-Order Logic},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11337},
doi = {10.1613/jair.1.11337},
abstract = {AGM contraction and revision assume an underlying logic that contains propositional logic. Consequently, this assumption excludes many useful logics such as the Horn fragment of propositional logic and most description logics. Our goal in this paper is to generalise AGM contraction and revision to (near-)arbitrary fragments of classical first-order logic. To this end, we first define a very general logic that captures these fragments. In so doing, we make the modest assumptions that a logic contains conjunction and that information is expressed by closed formulas or sentences. The resulting logic is called first-order conjunctive logic or FC logic for short. We then take as the point of departure the AGM approach of constructing contraction functions through epistemic entrenchment, that is the entrenchment-based contraction. We redefine entrenchment-based contraction in ways that apply to any FC logic, which we call FC contraction. We prove a representation theorem showing its compliance with all the AGM contraction postulates except for the controversial recovery postulate. We also give methods for constructing revision functions through epistemic entrenchment which we call FC revision; which also apply to any FC logic. We show that if the underlying FC logic contains tautologies then FC revision complies with all the AGM revision postulates. Finally, in the context of FC logic, we provide three methods for generating revision functions via a variant of the Levi Identity, which we call contraction, withdrawal and cut generated revision, and explore the notion of revision equivalence. We show that withdrawal and cut generated revision coincide with FC revision and so does contraction generated revision under a finiteness condition.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {147–179},
numpages = {33}
}

@article{10.1613/jair.1.11335,
author = {Keller, Orgad and Hassidim, Avinatan and Hazon, Noam},
title = {New Approximations for Coalitional Manipulation in Scoring Rules},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11335},
doi = {10.1613/jair.1.11335},
abstract = {We study the problem of coalitional manipulation--where k manipulators try to manipulate an election on m candidates--for any scoring rule, with focus on the Borda protocol. We do so in both the weighted and unweighted settings. For these problems, recent approximation approaches have tried to minimize k, the number of manipulators needed to make some preferred candidate p win (thus assuming that the number of manipulators is not limited in advance). In contrast, we focus on minimizing the score margin of p which is the difference between the maximum score of a candidate and the score of p.We provide algorithms that approximate the optimum score margin, which are applicable to any scoring rule. For the specific case of the Borda protocol in the unweighted setting, our algorithm provides a superior approximation factor for lower values of k.Our methods are novel and adapt techniques from multiprocessor scheduling by carefully rounding an exponentially-large configuration linear program that is solved by using the ellipsoid method with an Efficient separation oracle. We believe that such methods could be benEfficial in other social choice settings as well.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {109–145},
numpages = {37}
}

@article{10.1613/jair.1.11323,
author = {Laing, Kathryn and Thwaites, Peter Adam and Gosling, John Paul},
title = {Rank Pruning for Dominance Queries in CP-Nets},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11323},
doi = {10.1613/jair.1.11323},
abstract = {Conditional preference networks (CP-nets) are a graphical representation of a person's (conditional) preferences over a set of discrete features. In this paper, we introduce a novel method of quantifying preference for any given outcome based on a CP-net representation of a user's preferences. We demonstrate that these values are useful for reasoning about user preferences. In particular, they allow us to order (any subset of) the possible outcomes in accordance with the user's preferences. Further, these values can be used to improve the efficiency of outcome dominance testing. That is, given a pair of outcomes, we can determine which the user prefers more efficiently. Through experimental results, we show that this method is more effective than existing techniques for improving dominance testing efficiency. We show that the above results also hold for CP-nets that express indifference between variable values.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {55–107},
numpages = {53}
}

@article{10.1613/jair.1.11305,
author = {Trouillon, Th\'{e}o and Gaussier, \'{E}ric and Dance, Christopher R. and Bouchard, Guillaume},
title = {On Inductive Abilities of Latent Factor Models for Relational Learning},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11305},
doi = {10.1613/jair.1.11305},
abstract = {Latent factor models are increasingly popular for modeling multi-relational knowledge graphs. By their vectorial nature, it is not only hard to interpret why this class of models works so well, but also to understand where they fail and how they might be improved. We conduct an experimental survey of state-of-the-art models, not towards a purely comparative end, but as a means to get insight about their inductive abilities. To assess the strengths and weaknesses of each model, we create simple tasks that exhibit first, atomic properties of binary relations, and then, common inter-relational inference through synthetic genealogies. Based on these experimental results, we propose new research directions to improve on existing models.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {21–53},
numpages = {33}
}

@article{10.1613/jair.1.11291,
author = {Farhadi, Alireza and Ghodsi, Mohammad and Hajiaghayi, MohammadTaghi and Lahaie, S\'{e}bastien and Pennock, David and Seddighin, Masoud and Seddighin, Saeed and Yami, Hadi},
title = {Fair Allocation of Indivisible Goods to Asymmetric Agents},
year = {2019},
issue_date = {January 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {64},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11291},
doi = {10.1613/jair.1.11291},
abstract = {We study fair allocation of indivisible goods to agents with unequal entitlements. Fair allocation has been the subject of many studies in both divisible and indivisible settings. Our emphasis is on the case where the goods are indivisible and agents have unequal entitlements. This problem is a generalization of the work by Procaccia and Wang (Procaccia &amp; Wang, 2014) wherein the agents are assumed to be symmetric with respect to their entitlements. Although Procaccia and Wang show an almost fair (constant approximation) allocation exists in their setting, our main result is in sharp contrast to their observation. We show that, in some cases with n agents, no allocation can guarantee better than 1/n approximation of a fair allocation when the entitlements are not necessarily equal. Furthermore, we devise a simple algorithm that ensures a 1/n approximation guarantee.Our second result is for a restricted version of the problem where the valuation of every agent for each good is bounded by the total value he wishes to receive in a fair allocation. Although this assumption might seem without loss of generality, we show it enables us to find a 1/2 approximation fair allocation via a greedy algorithm. Finally, we run some experiments on real-world data and show that, in practice, a fair allocation is likely to exist. We also support our experiments by showing positive results for two stochastic variants of the problem, namely stochastic agents and stochastic items.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–20},
numpages = {20}
}

@article{10.1613/jair.1.11270,
author = {Bachrach, Yoram and Elkind, Edith and Malizia, Enrico and Meir, Reshef and Pasechnik, Dmitrii and Rosenschein, Jeffrey S. and Rothe, J\"{o}rg and Zuckerman, Michael},
title = {Bounds on the Cost of Stabilizing a Cooperative Game},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11270},
doi = {10.1613/jair.1.11270},
abstract = {A key issue in cooperative game theory is coalitional stability, usually captured by the notion of the core--the set of outcomes that are resistant to group deviations. However, some coalitional games have empty cores, and any outcome in such a game is unstable. We investigate the possibility of stabilizing a coalitional game by using subsidies. We consider scenarios where an external party that is interested in having the players work together offers a supplemental payment to the grand coalition, or, more generally, a particular coalition structure. This payment is conditional on players not deviating from this coalition structure, and may be divided among the players in any way they wish. We define the cost of stability as the minimum external payment that stabilizes the game. We provide tight bounds on the cost of stability, both for games where the coalitional values are nonnegative (profit-sharing games) and for games where the coalitional values are nonpositive (cost-sharing games), under natural assumptions on the characteristic function, such as superadditivity, anonymity, or both. We also investigate the relationship between the cost of stability and several variants of the least core. Finally, we study the computational complexity of problems related to the cost of stability, with a focus on weighted voting games.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {987–1023},
numpages = {37}
}

@article{10.1613/jair.1.11268,
author = {Goldwaser, Adrian and Schutt, Andreas},
title = {Optimal Torpedo Scheduling},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11268},
doi = {10.1613/jair.1.11268},
abstract = {We consider the torpedo scheduling problem in steel production, which is concerned with the transport of hot metal from a blast furnace to an oxygen converter. A schedule must satisfy, amongst other considerations, resource capacity constraints along the path and the locations traversed as well as the sulfur level of the hot metal. The goal is first to minimize the number of torpedo cars used during the planning horizon and second to minimize the time spent desulfurizing the hot metal. We propose an exact solution method based on Logic-based Benders Decomposition using Mixed-Integer and Constraint Programming, which optimally solves and proves, for the first time, the optimality of all instances from the ACP Challenge 2016 within 10 minutes. In addition, we adapted our method to handle large-scale instances and instances with a more general rail network. This adaptation optimally solved all challenge instances within one minute and was able to solve instances of up to 100,000 hot metal pickups.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {955–986},
numpages = {32}
}

@article{10.1613/jair.1.11266,
author = {Zhang, Meishan and Zhang, Yue and Fu, Guohong},
title = {Transition-Based Neural Word Segmentation Using Word-Level Features},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11266},
doi = {10.1613/jair.1.11266},
abstract = {Character-based and word-based methods are two different solutions for Chinese word segmentation, the former exploiting sequence labeling models over characters and the latter using word-level features. Neural models have been exploited for character-based Chinese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering. In this paper, we study a neural model for word-based Chinese word segmentation, by replacing the manually-designed discrete features with neural features in a transition-based word segmentation framework. Experimental results demonstrate that word features lead to comparable performance to the best systems in the literature, and a further combination of discrete and neural features obtains top accuracies on several benchmarks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {923–953},
numpages = {31}
}

@article{10.1613/jair.1.11265,
author = {Marinescu, Radu and Lee, Junkyu and Dechter, Rina and Ihler, Alexander},
title = {AND/OR Search for Marginal MAP},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11265},
doi = {10.1613/jair.1.11265},
abstract = {Mixed inference such as the marginal MAP query (some variables marginalized by summation and others by maximization) is key to many prediction and decision models. It is known to be extremely hard; the problem is NPPP-complete while the decision problem for MAP is only NP-complete and the summation problem is #P-complete. Consequently, approximation anytime schemes are essential. In this paper, we show that the framework of heuristic AND/OR search, which exploits conditional independence in the graphical model, coupled with variational-based mini-bucket heuristics can be extended to this task and yield powerful state-of-the-art schemes. Specifically, we explore the complementary properties of best-first search for reducing the number of conditional sums and providing time-improving upper bounds, with depth-first search for rapidly generating and improving solutions and lower bounds. We show empirically that a class of solvers that interleaves depth-first with best-first schemes emerges as the most competitive anytime scheme.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {875–921},
numpages = {47}
}

@article{10.1613/jair.1.11263,
author = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
title = {Grounding Language for Transfer in Deep Reinforcement Learning},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11263},
doi = {10.1613/jair.1.11263},
abstract = {In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {849–874},
numpages = {26}
}

@article{10.1613/jair.1.11261,
author = {L\"{u}dtke, Stefan and Schr\"{o}der, Max and Kr\"{u}ger, Frank and Bader, Sebastian and Kirste, Thomas},
title = {State-Space Abstractions for Probabilistic Inference: A Systematic Review},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11261},
doi = {10.1613/jair.1.11261},
abstract = {Tasks such as social network analysis, human behavior recognition, or modeling biochemical reactions, can be solved elegantly by using the probabilistic inference framework. However, standard probabilistic inference algorithms work at a propositional level, and thus cannot capture the symmetries and redundancies that are present in these tasks.Algorithms that exploit those symmetries have been devised in different research fields, for example by the lifted inference-, multiple object tracking-, and modeling and simulation-communities. The common idea, that we call state space abstraction, is to perform inference over compact representations of sets of symmetric states. Although they are concerned with a similar topic, the relationship between these approaches has not been investigated systematically.This survey provides the following contributions. We perform a systematic literature review to outline the state of the art in probabilistic inference methods exploiting symmetries. From an initial set of more than 4,000 papers, we identify 116 relevant papers. Furthermore, we provide new high-level categories that classify the approaches, based on common properties of the approaches. The research areas underlying each of the categories are introduced concisely. Researchers from different fields that are confronted with a state space explosion problem in a probabilistic system can use this classification to identify possible solutions. Finally, based on this conceptualization, we identify potentials for future research, as some relevant application domains are not addressed by current approaches.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {789–848},
numpages = {60}
}

@article{10.1613/jair.1.11259,
author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
title = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11259},
doi = {10.1613/jair.1.11259},
abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {743–788},
numpages = {46}
}

@article{10.1613/jair.1.11258,
author = {Banaee, Hadi and Schaffernicht, Erik and Loutfi, Amy},
title = {Data-Driven Conceptual Spaces: Creating Semantic Representations for Linguistic Descriptions of Numerical Data},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11258},
doi = {10.1613/jair.1.11258},
abstract = {There is an increasing need to derive semantics from real-world observations to facilitate natural information sharing between machine and human. Conceptual spaces theory is a possible approach and has been proposed as mid-level representation between symbolic and sub-symbolic representations, whereby concepts are represented in a geometrical space that is characterised by a number of quality dimensions. Currently, much of the work has demonstrated how conceptual spaces are created in a knowledge-driven manner, relying on prior knowledge to form concepts and identify quality dimensions. This paper presents a method to create semantic representations using data-driven conceptual spaces which are then used to derive linguistic descriptions of numerical data. Our contribution is a principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori. This novelty of the approach is the ability to select and group semantic features to discriminate between concepts in a data-driven manner while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans. Two data sets representing leaf images and time series signals are used to evaluate the method. An empirical evaluation for each case study assesses how well linguistic descriptions generated from the conceptual spaces identify unknown observations. Furthermore, comparisons are made with descriptions derived on alternative approaches for generating semantic models.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {691–742},
numpages = {52}
}

@article{10.1613/jair.1.11257,
author = {Bate, Andrew and Motik, Boris and Grau, Bernardo Cuenca and Cucala, David Tena and Siman\v{c}\'{\i}k, Franti\v{s}ek and Horrocks, Ian},
title = {Consequence-Based Reasoning for Description Logics with Disjunctions and Number Restrictions},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11257},
doi = {10.1613/jair.1.11257},
abstract = {Classification of description logic (DL) ontologies is a key computational problem in modern data management applications, so considerable effort has been devoted to the development and optimisation of practical reasoning calculi. Consequence-based calculi combine ideas from hypertableau and resolution in a way that has proved very effective in practice. However, existing consequence-based calculi can handle either Horn DLs (which do not support disjunction) or DLs without number restrictions. In this paper, we overcome this important limitation and present the first consequence-based calculus for deciding concept subsumption in the DL ALCHIQ+. Our calculus runs in exponential time assuming unary coding of numbers, and on ELH ontologies it runs in polynomial time. The extension to disjunctions and number restrictions is technically involved: we capture the relevant consequences using first-order clauses, and our inference rules adapt paramodulation techniques from first-order theorem proving. By using a well-known preprocessing step, the calculus can also decide concept subsumptions in SRIQ--a rich DL that covers all features of OWL 2 DL apart from nominals and datatypes. We have implemented our calculus in a new reasoner called Sequoia. We present the architecture of our reasoner and discuss several novel and important implementation techniques such as clause indexing and redundancy elimination. Finally, we present the results of an extensive performance evaluation, which revealed Sequoia to be competitive with existing reasoners. Thus, the calculus and the techniques we present in this paper provide an important addition to the repertoire of practical implementation techniques for description logic reasoning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {625–690},
numpages = {66}
}

@article{10.1613/jair.1.11256,
author = {Fionda, Valeria and Greco, Gianluigi},
title = {LTL on Finite and Process Traces: Complexity Results and a Practical Reasoner},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11256},
doi = {10.1613/jair.1.11256},
abstract = {Linear temporal logic (LTL) is a modal logic where formulas are built over temporal operators relating events happening in different time instants. According to the standard semantics, LTL formulas are interpreted on traces spanning over an infinite timeline. However, applications related to the specification and verification of business processes have recently pointed out the need for defining and reasoning about a variant of LTL, which we name LTLp, whose semantics is defined over process traces, that is, over finite traces such that, at each time instant, precisely one propositional variable (standing for the execution of some given activity) evaluates true.The paper investigates the theoretical underpinnings of LTLp and of a related logic formalism, named LTLf, which had already attracted attention in the literature and where formulas have the same syntax as in LTLp and are evaluated over finite traces, but without any constraint on the number of variables simultaneously evaluating true. The two formalisms are comparatively analyzed, by pointing out similarities and differences. In addition, a thorough complexity analysis has been conducted for reasoning problems about LTLp and LTLf, by considering arbitrary formulas as well as classes of formulas defined in terms of restrictions on the temporal operators that are allowed. Finally, based on the theoretical findings of the paper, a practical reasoner specifically tailored for LTLp and LTLf has been developed by leveraging state-of-the-art SAT solvers. The behavior of the reasoner has been experimentally compared with other systems available in the literature.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {557–623},
numpages = {67}
}

@article{10.1613/jair.1.11254,
author = {Fujita, Etsushi and Lesca, Julien and Sonoda, Akihisa and Todo, Taiki and Yokoo, Makoto},
title = {A Complexity Approach for Core-Selecting Exchange under Conditionally Lexicographic Preferences},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11254},
doi = {10.1613/jair.1.11254},
abstract = {Core-selection is a crucial property of rules in the literature of resource allocation. It is also desirable, from the perspective of mechanism design, to address the incentive of agents to cheat by misreporting their preferences. This paper investigates the exchange problem where (i) each agent is initially endowed with (possibly multiple) indivisible goods, (ii) agents' preferences are assumed to be conditionally lexicographic, and (iii) side payments are prohibited. We propose an exchange rule called augmented top-trading-cycles (ATTC), based on the original TTC procedure. We first show that ATTC is core-selecting and runs in polynomial time with respect to the number of goods. We then show that finding a beneficial misreport under ATTC is NP-hard. We finally clarify relationship of misreporting with splitting and hiding, two different types of manipulations, under ATTC.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {515–555},
numpages = {41}
}

@article{10.1613/jair.1.11253,
author = {Cygan, Marek and Kowalik, undefinedukasz and Soca\l{}a, Arkadiusz and Sornat, Krzysztof},
title = {Approximation and Parameterized Complexity of Minimax Approval Voting},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11253},
doi = {10.1613/jair.1.11253},
abstract = {We present three results on the complexity of MINNIMAX APPROVAL VOTING. First, we study MINNIMAX APPROVAL VOTING parameterized by the Hamming distance d from the solution to the votes. We show MINNIMAX APPROVAL VOTING admits no algorithm running in time O*(2o(d log d)), unless the Exponential Time Hypothesis (ETH) fails. This means that the O*(d2d) algorithm of Misra, Nabeel and Singh is essentially optimal. Motivated by this, we then show a parameterized approximation scheme, running in time O*((3/ε)2d), which is essentially tight assuming ETH. Finally, we get a new polynomial-time randomized approximation scheme for MINNIMAX APPROVAL VOTING, which runs in time nO(1/ε2·log(1/ε)) · poly(m), where n is a number of voters and m is a number of alternatives. It almost matches the running time of the fastest known PTAS for Closest String due to Ma and Sun.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {495–513},
numpages = {19}
}

@article{10.1613/jair.1.11251,
author = {Liu, Bo and Gemp, Ian and Ghavamzadeh, Mohammad and Liu, Ji and Mahadevan, Sridhar and Petrik, Marek},
title = {Proximal Gradient Temporal Difference Learning: Stable Reinforcement Learning with Polynomial Sample Complexity},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11251},
doi = {10.1613/jair.1.11251},
abstract = {In this paper, we introduce proximal gradient temporal difference learning, which provides a principled way of designing and analyzing true stochastic gradient temporal difference learning algorithms. We show how gradient TD (GTD) reinforcement learning methods can be formally derived, not by starting from their original objective functions, as previously attempted, but rather from a primal-dual saddle-point objective function. We also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. Previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence, and do not provide any finite-sample analysis. We also propose an accelerated algorithm, called GTD2-MP, that uses proximal "mirror maps" to yield an improved convergence rate. The results of our theoretical analysis imply that the GTD family of algorithms are comparable and may indeed be preferred over existing least squares TD methods for off-policy learning, due to their linear complexity. We provide experimental results showing the improved performance of our accelerated gradient TD methods.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {461–494},
numpages = {34}
}

@article{10.1613/jair.1.11249,
author = {Laskey, Kathryn Blackmond and Sun, Wei and Hanson, Robin and Twardy, Charles and Matsumoto, Shou and Goldfedder, Brandon},
title = {Graphical Model Market Maker for Combinatorial Prediction Markets},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11249},
doi = {10.1613/jair.1.11249},
abstract = {We describe algorithms for use by prediction markets in forming a crowd consensus joint probability distribution over thousands of related events. Equivalently, we describe market mechanisms to efficiently crowdsource both structure and parameters of a Bayesian network. Prediction markets are among the most accurate methods to combine forecasts; forecasters form a consensus probability distribution by trading contingent securities. A combinatorial prediction market forms a consensus joint distribution over many related events by allowing conditional trades or trades on Boolean combinations of events. Explicitly representing the joint distribution is infeasible, but standard inference algorithms for graphical probability models render it tractable for large numbers of base events. We show how to adapt these algorithms to compute expected assets conditional on a prospective trade, and to find the conditional state where a trader has minimum assets, allowing full asset reuse. We compare the performance of three algorithms: the straightforward algorithm from the DAGGRE (Decomposition-Based Aggregation) prediction market for geopolitical events, the simple block-merge model from the SciCast market for science and technology forecasting, and a more sophisticated algorithm we developed for future markets.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {421–460},
numpages = {40}
}

@article{10.1613/jair.1.11248,
author = {Landeiro, Virgile and Culotta, Aron},
title = {Robust Text Classification under Confounding Shift},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11248},
doi = {10.1613/jair.1.11248},
abstract = {As statistical classifiers become integrated into real-world applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. Although identifying and controlling for confounding variables Z-correlated with both the input X of a classifier and its output Y-has been assiduously studied in empirical social science, it is often neglected in text classification. This can be understood by the fact that, if we assume that the impact of confounding variables does not change between the time we fit a model and the time we use it, then prediction accuracy should only be slightly affected. We show in this paper that this assumption often does not hold and that when the influence of a confounding variable changes from training time to prediction time (i.e. under confounding shift), the classifier accuracy can degrade rapidly. We use Pearl's back-door adjustment as a predictive framework to develop a model robust to confounding shift under the condition that Z is observed at training time. Our approach does not make any causal conclusions but by experimenting on 6 datasets, we show that our approach is able to outperform baselines 1) in controlled cases where confounding shift is manually injected between fitting time and prediction time 2) in natural experiments where confounding shift appears either abruptly or gradually 3) in cases where there is one or multiple confounders. Finally, we discuss multiple issues we encountered during this research such as the effect of noise in the observation of Z and the importance of only controlling for confounding variables.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {391–419},
numpages = {29}
}

@article{10.1613/jair.1.11244,
author = {Le, Duong and Plaku, Erion},
title = {Cooperative, Dynamics-Based and Abstraction-Guided Multi-Robot Motion Planning},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11244},
doi = {10.1613/jair.1.11244},
abstract = {This paper presents an effective, cooperative, and probabilistically-complete multirobot motion planner that enables each robot to move to a desired location while avoiding collisions with obstacles and other robots. The approach takes into account not only the geometric constraints arising from collision avoidance, but also the differential constraints imposed by the motion dynamics of each robot. This makes it possible to generate collision-free and dynamically-feasible trajectories that can be executed in the physical world.The salient aspect of the approach is the coupling of sampling-based motion planning to handle the complexity arising from the obstacles and robot dynamics with multi-agent search to find solutions over a suitable discrete abstraction. The discrete abstraction is obtained by constructing roadmaps to solve a relaxed problem that accounts for the obstacles but not the dynamics. Sampling-based motion planning expands a motion tree in the composite state space of all the robots by adding collision-free and dynamically-feasible trajectories as branches. Efficiency is obtained by using multi-agent search to find non-conflicting routes over the discrete abstraction which serve as heuristics to guide the motion-tree expansion. When little or no progress is made, the routes are penalized and the multi-agent search is invoked again to find alternative routes. This synergistic coupling makes it possible to effectively plan collision-free and dynamically-feasible motions that enable each robot to reach its goal. Experiments using vehicle models with nonlinear dynamics operating in complex environments, where cooperation among robots is required, show significant speedups over related work.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {361–390},
numpages = {30}
}

@article{10.1613/jair.1.11243,
author = {Levine, Steven J. and Williams, Brian C.},
title = {Watching and Acting Together: Concurrent Plan Recognition and Adaptation for Human-Robot Teams},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11243},
doi = {10.1613/jair.1.11243},
abstract = {There is huge demand for robots to work alongside humans in heterogeneous teams. To achieve a high degree of fluidity, robots must be able to (1) recognize their human co-worker's intent, and (2) adapt to this intent accordingly, providing useful aid as a teammate. The literature to date has made great progress in these two areas-recognition and adaptation-but largely as separate research activities. In this work, we present a unified approach to these two problems, in which recognition and adaptation occur concurrently and holistically within the same framework. We introduce Pike, an executive for human-robot teams, that allows the robot to continuously and concurrently reason about what a human is doing as execution proceeds, as well as adapt appropriately. The result is a mixed-initiative execution where humans and robots interact fluidly to complete task goals.Key to our approach is our task model: a contingent, temporally-flexible team-plan with explicit choices for both the human and robot. This allows a single set of algorithms to find implicit constraints between sets of choices for the human and robot (as determined via causal link analysis and temporal reasoning), narrowing the possible decisions a rational human would take (hence achieving intent recognition) as well as the possible actions a robot could consistently take (hence achieving adaptation). Pike makes choices based on the preconditions of actions in the plan, temporal constraints, unanticipated disturbances, and choices made previously (by either agent).Innovations of this work include (1) a framework for concurrent intent recognition and adaptation for contingent, temporally-flexible plans, (2) the generalization of causal links for contingent, temporally-flexible plans along with related extraction algorithms, and (3) extensions to a state-of-the-art dynamic execution system to utilize these causal links for decision making.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {281–359},
numpages = {79}
}

@article{10.1613/jair.1.11242,
author = {Hellerstein, Lisa and Kletenik, Devorah},
title = {Revisiting the Approximation Bound for Stochastic Submodular Cover},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11242},
doi = {10.1613/jair.1.11242},
abstract = {Deshpande et al. presented a k(ln R+1) approximation bound for Stochastic Submodular Cover, where k is the state set size, R is the maximum utility of a single item, and the utility function is integer-valued. This bound is similar to the (ln Q/η + 1) bound given by Golovin and Krause, whose analysis was recently found to have an error. Here Q ≥ R is the goal utility and η is the minimum gap between Q and any attainable utility Q′ &lt; Q. We revisit the proof of the k(ln R+1) bound of Deshpande et al., fill in the details of the proof of a key lemma, and prove two bounds for real-valued utility functions: k(ln R1 +1) and (ln RE+1). Here R1 equals the maximum ratio between the largest increase in utility attainable from a single item, and the smallest non-zero increase attainable from that same item (in the same state). The quantity RE equals the maximum ratio between the largest expected increase in utility from a single item, and the smallest non-zero expected increase in utility from that same item. Our bounds apply only to the stochastic setting with independent states.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {265–279},
numpages = {15}
}

@article{10.1613/jair.1.11240,
author = {Amarilli, Antone and Benedikt, Michael and Bourhis, Pierre and Boom, Michael Vanden},
title = {Query Answering with Transitive and Linear-Ordered Data},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11240},
doi = {10.1613/jair.1.11240},
abstract = {We consider entailment problems involving powerful constraint languages such as frontier-guarded existential rules in which we impose additional semantic restrictions on a set of distinguished relations. We consider restricting a relation to be transitive, restricting a relation to be the transitive closure of another relation, and restricting a relation to be a linear order. We give some natural variants of guardedness that allow inference to be decidable in each case, and isolate the complexity of the corresponding decision problems. Finally we show that slight changes in these conditions lead to undecidability.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {191–264},
numpages = {74}
}

@article{10.1613/jair.1.11239,
author = {Tarkowski, Mateusz K. and Szczepa\'{n}ski, Piotr L. and Michalak, Tomasz P. and Harrenstein, Paul and Wooldridge, Michael},
title = {Efficient Computation of Semivalues for Game-Theoretic Network Centrality},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11239},
doi = {10.1613/jair.1.11239},
abstract = {Some game-theoretic solution concepts such as the Shapley value and the Banzhaf index have recently gained popularity as measures of node centrality in networks. While this direction of research is promising, the computational problems that surround it are challenging and have largely been left open. To date there are only a few positive results in the literature, which show that some game-theoretic extensions of degree-, closeness- and betweenness-centrality measures are computable in polynomial time, i.e., without the need to enumerate the exponential number of all possible coalitions. In this article, we show that these results can be extended to a much larger class of centrality measures that are based on a family of solution concepts known as semivalues. The family of semivalues includes, among others, the Shapley value and the Banzhaf index. To this end, we present a generic framework for defining game-theoretic network centralities and prove that all centrality measures that can be expressed in this framework are computable in polynomial time. Using our framework, we present a number of new and polynomial-time computable game-theoretic centrality measures.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {145–189},
numpages = {45}
}

@article{10.1613/jair.1.11238,
author = {Liberatore, Paolo},
title = {Belief Integration and Source Reliability Assessment},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11238},
doi = {10.1613/jair.1.11238},
abstract = {Merging beliefs requires the plausibility of the sources of the information to be merged. They are typically assumed equally reliable when nothing suggests otherwise. A recent line of research has spun from the idea of deriving this information from the revision process itself. In particular, the history of previous revisions and previous merging examples provide information for performing subsequent merging operations.Yet, no examples or previous revisions may be available. In spite of the apparent lack of information, something can still be inferred by a try-and-check approach: a relative reliability ordering is assumed, the sources are integrated according to it and the result is compared with the original information. The final check may contradict the original ordering, like when the result of merging implies the negation of a formula coming from a source initially assumed reliable, or it implies a formula coming from a source assumed unreliable. In such cases, the reliability ordering assumed in the first place can be excluded from consideration.Such a scenario is proved real under the classifications of source reliability and definitions of belief integration considered in this article: sources divided in two, three or multiple reliability classes; integration is mostly by maximal consistent subsets but also weighted distance is considered. Other results mainly concern the integration by maximal consistent subsets and partitions of two and three reliability classes.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {87–143},
numpages = {57}
}

@article{10.1613/jair.1.11236,
author = {Saldanha, Emmanuelle-Anna Dietz and H\"{o}lldobler, Steffen and Ramli, Carroline Dewi Puspa Kencana and Medinacelli, Luis Palacios},
title = {A Core Method for the Weak Completion Semantics with Skeptical Abduction},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11236},
doi = {10.1613/jair.1.11236},
abstract = {The Weak Completion Semantics is a novel cognitive theory which has been successfully applied to the suppression task, the selection task, syllogistic reasoning, the belief bias effect, spatial reasoning as well as reasoning with conditionals. It is based on logic programming with skeptical abduction. Each program admits a least model under the three-valued undefinedukasiewicz logic, which can be computed as the least fixed point of an appropriate semantic operator. The semantic operator can be represented by a three-layer feed-forward network using the CORE method. Its least fixed point is the unique stable state of a recursive network which is obtained from the three-layer feed-forward core by mapping the activation of the output layer back to the input layer. The recursive network is embedded into a novel network to compute skeptical abduction. This paper presents a fully connectionist realization of the Weak Completion Semantics.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {51–86},
numpages = {36}
}

@article{10.1613/jair.1.11233,
author = {Gombolay, Matthew and Jensen, Reed and Stigile, Jessica and Golen, Toni and Shah, Neel and Son, Sung-Hyun and Shah, Julie},
title = {Human-Machine Collaborative Optimization via Apprenticeship Scheduling},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11233},
doi = {10.1613/jair.1.11233},
abstract = {Coordinating agents to complete a set of tasks with intercoupled temporal and resource constraints is computationally challenging, yet human domain experts can solve these difficult scheduling problems using paradigms learned through years of apprenticeship. A process for manually codifying this domain knowledge within a computational framework is necessary to scale beyond the "single-expert, single-trainee" apprenticeship model. However, human domain experts often have difficulty describing their decision-making processes. We propose a new approach for capturing this decision-making process through counterfactual reasoning in pairwise comparisons. Our approach is model-free and does not require iterating through the state space. We demonstrate that this approach accurately learns multifaceted heuristics on a synthetic and real world data sets. We also demonstrate that policies learned from human scheduling demonstration via apprenticeship learning can substantially improve the efficiency of schedule optimization. We employ this human-machine collaborative optimization technique on a variant of the weapon-to-target assignment problem. We demonstrate that this technique generates optimal solutions up to 9.5 times faster than a state-of-the-art optimization algorithm.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–49},
numpages = {49}
}

@article{10.1613/jair.1.11229,
author = {Brandt, Sebastian and Kalayci, Elem G\"{u}zel and Ryzhikov, Vladislav and Xiao, Guohui and Zakharyaschev, Michael},
title = {Querying Log Data with Metric Temporal Logic},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11229},
doi = {10.1613/jair.1.11229},
abstract = {We propose a novel framework for ontology-based access to temporal log data using a datalog extension datalogMTL of the Horn fragment of the metric temporal logic MTL. We show that datalogMTL is EXPSPACE-complete even with punctual intervals, in which case full MTL is known to be undecidable. We also prove that nonrecursive datalogMTL is PSPACE-complete for combined complexity and in AC0 for data complexity. We demonstrate by two real-world use cases that nonrecursive datalogMTL programs can express complex temporal concepts from typical user queries and thereby facilitate access to temporal log data. Our experiments with Siemens turbine data and MesoWest weather data show that datalogMTL ontology-mediated queries are efficient and scale on large datasets.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {829–877},
numpages = {49}
}

@article{10.1613/jair.1.11228,
author = {Salmer\'{o}n, Antonio and Rum\'{\i}, Rafael and Langseth, Helge and Nielsen, Thomas D. and Madsen, Anders L.},
title = {A Review of Inference Algorithms for Hybrid Bayesian Networks},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11228},
doi = {10.1613/jair.1.11228},
abstract = {Hybrid Bayesian networks have received an increasing attention during the last years. The difference with respect to standard Bayesian networks is that they can host discrete and continuous variables simultaneously, which extends the applicability of the Bayesian network framework in general. However, this extra feature also comes at a cost: inference in these types of models is computationally more challenging and the underlying models and updating procedures may not even support closed-form solutions. In this paper we provide an overview of the main trends and principled approaches for performing inference in hybrid Bayesian networks. The methods covered in the paper are organized and discussed according to their methodological basis. We consider how the methods have been extended and adapted to also include (hybrid) dynamic Bayesian networks, and we end with an overview of established software systems supporting inference in these types of models.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {799–828},
numpages = {30}
}

@article{10.1613/jair.1.11227,
author = {Segovia-Aguas, Javier and Jim\'{e}nez, Sergio and Jonsson, Anders},
title = {Computing Hierarchical Finite State Controllers with Classical Planning},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11227},
doi = {10.1613/jair.1.11227},
abstract = {Finite State Controllers (FSCs) are an effective way to compactly represent sequential plans. By imposing appropriate conditions on transitions, FSCs can also represent generalized plans (plans that solve a range of planning problems from a given domain). In this paper we introduce the concept of hierarchical FSCs for planning by allowing controllers to call other controllers. This call mechanism allows hierarchical FSCs to represent generalized plans more compactly than individual FSCs, to compute controllers in a modular fashion or even more, to compute recursive controllers. The paper introduces a classical planning compilation for computing hierarchical FSCs that solve challenging generalized planning tasks. The compilation takes as input a finite set of classical planning problems from a given domain. The output of the compilation is a single classical planning problem whose solution induces: (1) a hierarchical FSC and (2), the corresponding validation of that controller on the input classical planning problems.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {755–797},
numpages = {43}
}

@article{10.1613/jair.1.11222,
author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
title = {Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11222},
doi = {10.1613/jair.1.11222},
abstract = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {729–754},
numpages = {26}
}

@article{10.1613/jair.1.11221,
author = {Eiter, Thomas and Kaminski, Tobias and Redl, Christoph and Weinzierl, Antonius},
title = {Exploiting Partial Assignments for Efficient Evaluation of Answer Set Programs with External Source Access},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11221},
doi = {10.1613/jair.1.11221},
abstract = {Answer Set Programming (ASP) is a well-known declarative problem solving approach based on nonmonotonic logic programs, which has been successfully applied to a wide range of applications in artificial intelligence and beyond. To address the needs of modern applications, HEX-programs were introduced as an extension of ASP with external atoms for accessing information outside programs via an API style bi-directional interface mechanism. To evaluate such programs, conflict-driving learning algorithms for SAT and ASP solving have been extended in order to capture the semantics of external atoms. However, a drawback of the state-of-the-art approach is that external atoms are only evaluated under complete assignments (i.e., input to the external source) while in practice, their values often can be determined already based on partial assignments alone (i.e., from incomplete input to the external source). This prevents early backtracking in case of conflicts, and hinders more efficient evaluation of HEX-programs. We thus extend the notion of external atoms to allow for three-valued evaluation under partial assignments, while the two-valued semantics of the overall HEX-formalism remains unchanged. This paves the way for three enhancements: first, to evaluate external sources at any point during model search, which can trigger learning knowledge about the source behavior and/or early backtracking in the spirit of theory propagation in SAT modulo theories (SMT). Second, to optimize the knowledge learned in terms of so-called nogoods, which roughly speaking are impossible input-output configurations. Shrinking nogoods to their relevant input part leads to more effective search space pruning. And third, to make a necessary minimality check of candidate answer sets more efficient by exploiting early external evaluation calls. As this check usually accounts for a large share of the total runtime, optimization is here particularly important. We further present an experimental evaluation of an implementation of a novel HEX-algorithm that incorporates these enhancements using a benchmark suite. Our results demonstrate a clear efficiency gain over the state-of-the-art HEX-solver for the benchmarks, and provide insights regarding the most effective combinations of solver configurations.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {665–727},
numpages = {63}
}

@article{10.1613/jair.1.11219,
author = {Fern\'{a}ndez-Gonz\'{a}lez, Enrique and Williams, Brian and Karpas, Erez},
title = {ScottyActivity: Mixed Discrete-Continuous Planning with Convex Optimization},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11219},
doi = {10.1613/jair.1.11219},
abstract = {The state of the art practice in robotics planning is to script behaviors manually, where each behavior is typically generated using trajectory optimization. However, in order for robots to be able to act robustly and adapt to novel situations, they need to plan these activity sequences autonomously. Since the conditions and effects of these behaviors are tightly coupled through time, state and control variables, many problems require that the tasks of activity planning and trajectory optimization are considered together. There are two key issues underlying effective hybrid activity and trajectory planning: the sufficiently accurate modeling of robot dynamics and the capability of planning over long horizons. Hybrid activity and trajectory planners that employ mixed integer programming within a discrete time formulation are able to accurately model complex dynamics for robot vehicles, but are often restricted to relatively short horizons. On the other hand, current hybrid activity planners that employ continuous time formulations can handle longer horizons but they only allow actions to have continuous effects with constant rate of change, and restrict the allowed state constraints to linear inequalities. This is insufficient for many robotic applications and it greatly limits the expressivity of the problems that these approaches can solve. In this work we present the ScottyActivity planner, that is able to generate practical hybrid activity and motion plans over long horizons by employing recent methods in convex optimization combined with methods for planning with relaxed plan graphs and heuristic forward search. Unlike other continuous time planners, ScottyActivity can solve a broad class of robotic planning problems by supporting convex quadratic constraints on state variables and control variables that are jointly constrained and that affect multiple state variables simultaneously. In order to support planning over long horizons, ScottyActivity does not resort to time, state or control variable discretization. While straightforward formulations of consistency checks are not convex and do not scale, we present an efficient convex formulation, in the form of a Second Order Cone Program (SOCP), that is very fast to solve. We also introduce several new realistic domains that demonstrate the capabilities and scalability of our approach, and their simplified linear versions, that we use to compare with other state of the art planners. This work demonstrates the power of integrating advanced convex optimization techniques with discrete search methods and paves the way for extensions dealing with non-convex disjoint constraints, such as obstacle avoidance.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {579–664},
numpages = {86}
}

@article{10.1613/jair.1.11217,
author = {Seipp, Jendrik and Helmert, Malte},
title = {Counterexample-Guided Cartesian Abstraction Refinement for Classical Planning},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11217},
doi = {10.1613/jair.1.11217},
abstract = {Counterexample-guided abstraction refinement (CEGAR) is a method for incrementally computing abstractions of transition systems. We propose a CEGAR algorithm for computing abstraction heuristics for optimal classical planning. Starting from a coarse abstraction of the planning task, we iteratively compute an optimal abstract solution, check if and why it fails for the concrete planning task and refine the abstraction so that the same failure cannot occur in future iterations. A key ingredient of our approach is a novel class of abstractions for classical planning tasks that admits efficient and very fine-grained refinement. Since a single abstraction usually cannot capture enough details of the planning task, we also introduce two methods for producing diverse sets of heuristics within this framework, one based on goal atoms, the other based on landmarks. In order to sum their heuristic estimates admissibly we introduce a new cost partitioning algorithm called saturated cost partitioning. We show that the resulting heuristics outperform other state-of-the-art abstraction heuristics in many benchmark domains.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {535–577},
numpages = {43}
}

@article{10.1613/jair.1.11216,
author = {Walraven, Erwin and Spaan, Matthijs T. J.},
title = {Column Generation Algorithms for Constrained POMDPs},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11216},
doi = {10.1613/jair.1.11216},
abstract = {In several real-world domains it is required to plan ahead while there are finite resources available for executing the plan. The limited availability of resources imposes constraints on the plans that can be executed, which need to be taken into account while computing a plan. A Constrained Partially Observable Markov Decision Process (Constrained POMDP) can be used to model resource-constrained planning problems which include uncertainty and partial observability. Constrained POMDPs provide a framework for computing policies which maximize expected reward, while respecting constraints on a secondary objective such as cost or resource consumption. Column generation for linear programming can be used to obtain Constrained POMDP solutions. This method incrementally adds columns to a linear program, in which each column corresponds to a POMDP policy obtained by solving an unconstrained subproblem. Column generation requires solving a potentially large number of POMDPs, as well as exact evaluation of the resulting policies, which is computationally difficult. We propose a method to solve subproblems in a two-stage fashion using approximation algorithms. First, we use a tailored point-based POMDP algorithm to obtain an approximate subproblem solution. Next, we convert this approximate solution into a policy graph, which we can evaluate efficiently. The resulting algorithm is a new approximate method for Constrained POMDPs in single-agent settings, but also in settings in which multiple independent agents share a global constraint. Experiments based on several domains show that our method outperforms the current state of the art.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {489–533},
numpages = {45}
}

@article{10.1613/jair.1.11215,
author = {Fotakis, Dimitris and Krysta, Piotr and Ventre, Carmine},
title = {The Power of Verification for Greedy Mechanism Design},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11215},
doi = {10.1613/jair.1.11215},
abstract = {Greedy algorithms are known to provide, in polynomial time, near optimal approximation guarantees for Combinatorial Auctions (CAs) with multidimensional bidders. It is known that truthful greedy-like mechanisms for CAs with multi-minded bidders do not achieve good approximation guarantees.In this work, we seek a deeper understanding of greedy mechanism design and investigate under which general assumptions, we can have efficient and truthful greedy mechanisms for CAs. Towards this goal, we use the framework of priority algorithms and weak and strong verification, where the bidders are not allowed to overbid on their winning set or on any subset of this set, respectively. We provide a complete characterization of the power of weak verification showing that it is sufficient and necessary for any greedy fixed priority algorithm to become truthful with the use of money or not, depending on the ordering of the bids. Moreover, we show that strong verification is sufficient and necessary to obtain a 2-approximate truthful mechanism with money, based on a known greedy algorithm, for the problem of submodular CAs in finite bidding domains. Our proof is based on an interesting structural analysis of the strongly connected components of the declaration graph.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {459–488},
numpages = {30}
}

@article{10.1613/jair.1.11214,
author = {Alechina, Natasha and Halpern, Joseph Y. and Kash, Ian A. and Logan, Brian},
title = {Incentive-Compatible Mechanisms for Norm Monitoring in Open Multi-Agent Systems},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11214},
doi = {10.1613/jair.1.11214},
abstract = {We consider the problem of detecting norm violations in open multi-agent systems (MAS). We show how, using ideas from scrip systems, we can design mechanisms where the agents comprising the MAS are incentivised to monitor the actions of other agents for norm violations. The cost of providing the incentives is not borne by the MAS and does not come from fines charged for norm violations (fines may be impossible to levy in a system where agents are free to leave and rejoin again under a different identity). Instead, monitoring incentives come from (scrip) fees for accessing the services provided by the MAS. In some cases, perfect monitoring (and hence enforcement) can be achieved: no norms will be violated in equilibrium. In other cases, we show that, while it is impossible to achieve perfect enforcement, we can get arbitrarily close; we can make the probability of a norm violation in equilibrium arbitrarily small. We show using simulations that our theoretical results, which apply to systems with a large number of agents, hold for multi-agent systems with as few as 1000 agents--the system rapidly converges to the steady-state distribution of scrip tokens necessary to ensure monitoring and then remains close to the steady state.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {433–458},
numpages = {26}
}

@article{10.1613/jair.1.11213,
author = {Haslum, Patrik and Ivankovic, Franc and Ram\'{\i}rez, Miquel and Gordon, Dan and Thi\'{e}baux, Sylvie and Shivashankar, Vikas and Nau, Dana S.},
title = {Extending Classical Planning with State Constraints: Heuristics and Search for Optimal Planning},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11213},
doi = {10.1613/jair.1.11213},
abstract = {We present a principled way of extending a classical AI planning formalism with systems of state constraints, which relate - sometimes determine - the values of variables in each state traversed by the plan. This extension occupies an attractive middle ground between expressivity and complexity. It enables modelling a new range of problems, as well as formulating more efficient models of classical planning problems. An example of the former is planning-based control of networked physical systems - power networks, for example - in which a local, discrete control action can have global effects on continuous quantities, such as altering flows across the entire network. At the same time, our extension remains decidable as long as the satisfiability of sets of state constraints is decidable, including in the presence of numeric state variables, and we demonstrate that effective techniques for cost-optimal planning known in the classical setting - in particular, relaxation-based admissible heuristics - can be adapted to the extended formalism. In this paper, we apply our approach to constraints in the form of linear or non-linear equations over numeric state variables, but the approach is independent of the type of state constraints, as long as there exists a procedure that decides their consistency. The planner and the constraint solver interact through a well-defined, narrow interface, in which the solver requires no specialisation to the planning context. Furthermore, we present an admissible search algorithm - a variant of A* - that is able to make use of additional information provided by the search heuristic, in the form of preferred actions. Although preferred actions have been widely used in satisficing planning, we are not aware of any previous use of them in optimal planning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {373–431},
numpages = {59}
}

@article{10.1613/jair.1.11212,
author = {Botea, Adi and Bonusi, Davide and Surynek, Pavel},
title = {Solving Multi-Agent Path Finding on Strongly Biconnected Digraphs},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11212},
doi = {10.1613/jair.1.11212},
abstract = {Much of the literature on suboptimal, polynomial-time algorithms for multi-agent path finding focuses on undirected graphs, where motion is permitted in both directions along a graph edge. Despite this, traveling on directed graphs is relevant in navigation domains, such as path finding in games, and asymmetric communication networks.We consider multi-agent path finding on strongly biconnected directed graphs. We show that all instances with at least two unoccupied positions have a solution, except for a particular, degenerate subclass where the graph has a cyclic shape. We present diBOX, an algorithm for multi-agent path finding on strongly biconnected directed graphs. diBOX runs in polynomial time, computes suboptimal solutions and is complete for instances on strongly biconnected digraphs with at least two unoccupied positions. We theoretically analyze properties of the algorithm and properties of strongly biconnected directed graphs that are relevant to our approach. We perform a detailed empirical analysis of diBOX, showing a good scalability. To our knowledge, our work is the first study of multi-agent path finding focused on directed graphs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {273–314},
numpages = {42}
}

@article{10.1613/jair.1.11211,
author = {Bil\`{o}, Vittorio and Fanelli, Angelo and Flammini, Michele and Monaco, Gianpiero and Moscardelli, Luca},
title = {Nash Stable Outcomes in Fractional Hedonic Games: Existence, Efficiency and Computation},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11211},
doi = {10.1613/jair.1.11211},
abstract = {We consider fractional hedonic games, a subclass of coalition formation games that can be succinctly modeled by means of a graph in which nodes represent agents and edge weights the degree of preference of the corresponding endpoints. The happiness or utility of an agent for being in a coalition is the average value she ascribes to its members. We adopt Nash stable outcomes as the target solution concept; that is we focus on states in which no agent can improve her utility by unilaterally changing her own group. We provide existence, efficiency and complexity results for games played on both general and specific graph topologies. As to the efficiency results, we mainly study the quality of the best Nash stable outcome and refer to the ratio between the social welfare of an optimal coalition structure and the one of such an equilibrium as to the price of stability. In this respect, we remark that a best Nash stable outcome has a natural meaning of stability, since it is the optimal solution among the ones which can be accepted by selfish agents. We provide upper and lower bounds on the price of stability for different topologies, both in case of weighted and unweighted edges. Beside the results for general graphs, we give refined bounds for various specific cases, such as triangle-free, bipartite graphs and tree graphs. For these families, we also show how to efficiently compute Nash stable outcomes with provable good social welfare.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {315–371},
numpages = {57}
}

@article{10.1613/jair.1.11210,
author = {Halaweh, Mohanad},
title = {Viewpoint: Artificial Intelligence Government (Gov. 3.0): The UAE Leading Model},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11210},
doi = {10.1613/jair.1.11210},
abstract = {The United Arab Emirates (UAE) is the first country in the world to appoint a State Minister for Artificial Intelligence (AI). The UAE is embracing AI in society at the governmental level, which is leading to a new generations of digital government (which we are labeling Gov. 3.0). This paper argues that the decision to embrace AI will lead to positive impacts on society, including businesses, organizations and individuals, as well as on the AI industry itself. This paper discusses the societal impacts of AI at a macro (country-wide) level.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {269–272},
numpages = {4}
}

@article{10.1613/jair.1.11209,
author = {Hatem, Matthew and Burns, Ethan and Ruml, Wheeler},
title = {Solving Large Problems with Heuristic Search: General-Purpose Parallel External-Memory Search},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11209},
doi = {10.1613/jair.1.11209},
abstract = {Classic best-first heuristic search algorithms, like A*, record every unique state they encounter in RAM, making them infeasible for solving large problems. In this paper, we demonstrate how best-first search can be scaled to solve much larger problems by exploiting disk storage and parallel processing and, in some cases, slightly relaxing the strict best-first node expansion order. Some previous disk-based search algorithms abandon best-first search order in an attempt to increase efficiency. We present two case studies showing that A*, when augmented with Delayed Duplicate Detection, can actually be more efficient than these non-best-first search orders. First, we present a straightforward external variant of A*, called PEDAL, that slightly relaxes best-first order in order to be I/O efficient in both theory and practice, even on problems featuring real-valued node costs. Because it is easy to parallelize, PEDAL can be faster than in-memory IDA* even on domains with few duplicate states, such as the sliding-tile puzzle. Second, we present a variant of PEDAL, called PE2A*, that uses partial expansion to handle problems that have large branching factors. When tested on the problem of Multiple Sequence Alignment, PE2A* is the first algorithm capable of solving the entire Reference Set 1 of the standard BAliBASE benchmark using a biologically accurate cost function. This work shows that classic best-first algorithms like A* can be applied to large real-world problems. We also provide a detailed implementation guide with source code both for generic parallel disk-based best-first search and for Multiple Sequence Alignment with a biologically accurate cost function. Given its effectiveness as a general-purpose problem-solving method, we hope that this makes parallel and disk-based search accessible to a wider audience.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {233–268},
numpages = {36}
}

@article{10.1613/jair.1.11208,
author = {Baier, Hendrik and Winands, Mark H. M.},
title = {MCTS-Minimax Hybrids with State Evaluations},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11208},
doi = {10.1613/jair.1.11208},
abstract = {Monte-Carlo Tree Search (MCTS) has been found to show weaker play than minimax-based search in some tactical game domains. This is partly due to its highly selective search and averaging value backups, which make it susceptible to traps. In order to combine the strategic strength of MCTS and the tactical strength of minimax, MCTS-minimax hybrids have been introduced, embedding shallow minimax searches into the MCTS framework. Their results have been promising even without making use of domain knowledge such as heuristic evaluation functions. This article continues this line of research for the case where evaluation functions are available. Three different approaches are considered, employing minimax with an evaluation function in the rollout phase of MCTS, as a replacement for the rollout phase, and as a node prior to bias move selection. The latter two approaches are newly proposed. Furthermore, all three hybrids are enhanced with the help of move ordering and k-best pruning for minimax. Results show that the use of enhanced minimax for computing node priors results in the strongest MCTS-minimax hybrid investigated in the three test domains of Othello, Breakthrough, and Catch the Lion. This hybrid, called MCTS-IP-M-k, also outperforms enhanced minimax as a standalone player in Breakthrough, demonstrating that at least in this domain, MCTS and minimax can be combined to an algorithm stronger than its parts. Using enhanced minimax for computing node priors is therefore a promising new technique for integrating domain knowledge into an MCTS framework.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {193–231},
numpages = {39}
}

@article{10.1613/jair.1.11206,
author = {Criado, Natalia},
title = {Resource-Bounded Norm Monitoring in Multi-Agent Systems},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11206},
doi = {10.1613/jair.1.11206},
abstract = {Norms allow system designers to specify the desired behaviour of a sociotechnical system. In this way, norms regulate what the social and technical agents in a sociotechnical system should (not) do. In this context, a vitally important question is the development of mechanisms for monitoring whether these agents comply with norms. Proposals on norm monitoring often assume that monitoring has no costs and/or that monitors have unlimited resources to observe the environment and the actions performed by agents. In this paper, we challenge this assumption and propose the first practical resource-bounded norm monitor. Our monitor is capable of selecting the resources to be deployed and use them to check norm compliance with incomplete information about the actions performed and the state of the world. We formally demonstrate the correctness and soundness of our norm monitor and study its complexity. We also demonstrate in randomised simulations and benchmark experiments that our monitor can select monitored resources effectively and efficiently, detecting more norm violations and fulfilments than other tractable optimization approaches and obtaining slightly worse results than intractable optimal approaches.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {153–192},
numpages = {40}
}

@article{10.1613/jair.1.11205,
author = {Bistaffa, Filippo and Farinelli, Alessandro},
title = {A COP Model for Graph-Constrained Coalition Formation},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11205},
doi = {10.1613/jair.1.11205},
abstract = {We consider Graph-Constrained Coalition Formation (GCCF), a widely studied subproblem of coalition formation in which the set of valid coalitions is restricted by a graph. We propose COP-GCCF, a novel approach that models GCCF as a COP, and we solve such COP with a highly-parallel approach based on Bucket Elimination executed on the GPU, which is able to exploit the high constraint tightness of COP-GCCF. Results show that our approach outperforms state of the art algorithms (i.e., DyCE and IDPG) by at least one order of magnitude on realistic graphs, i.e., a crawl of the Twitter social graph, both in terms of runtime and memory.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {133–153},
numpages = {21}
}

@article{10.1613/jair.1.11204,
author = {Apt, Krzysztof R. and Wojtczak, Dominik},
title = {Verification of Distributed Epistemic Gossip Protocols},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11204},
doi = {10.1613/jair.1.11204},
abstract = {Gossip protocols aim at arriving, by means of point-to-point or group communications, at a situation in which all the agents know each other secrets. Distributed epistemic gossip protocols use as guards formulas from a simple epistemic logic and as statements calls between the agents. They are natural examples of knowledge based programs.We prove here that these protocols are implementable, that their partial correctness is decidable and that termination and two forms of fair termination of these protocols are decidable, as well. To establish these results we show that the definition of semantics and of truth of the underlying logic are decidable.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {101–132},
numpages = {32}
}

@article{10.1613/jair.1.11203,
author = {\v{S}ourek, Gustav and Aschenbrenner, Vojt\v{e}ch and \v{Z}elezn\'{y}, Filip and Schockaert, Steven and Ku\v{z}elka, Ond\v{r}ej},
title = {Lifted Relational Neural Networks: Efficient Learning of Latent Relational Structures},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11203},
doi = {10.1613/jair.1.11203},
abstract = {We propose a method to combine the interpretability and expressive power of first-order logic with the effectiveness of neural network learning. In particular, we introduce a lifted framework in which first-order rules are used to describe the structure of a given problem setting. These rules are then used as a template for constructing a number of neural networks, one for each training and testing example. As the different networks corresponding to different examples share their weights, these weights can be efficiently learned using stochastic gradient descent. Our framework provides a exible way for implementing and combining a wide variety of modelling constructs. In particular, the use of first-order logic allows for a declarative specification of latent relational structures, which can then be efficiently discovered in a given data set using neural network learning. Experiments on 78 relational learning benchmarks clearly demonstrate the effectiveness of the framework.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {69–100},
numpages = {32}
}

@article{10.1613/jair.1.11202,
author = {Skibski, Oskar and Michalak, Tomasz P. and Rahwan, Talal},
title = {Axiomatic Characterization of Game-Theoretic Centrality},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11202},
doi = {10.1613/jair.1.11202},
abstract = {One of the fundamental research challenges in network science is centrality analysis, i.e., identifying the nodes that play the most important roles in the network. In this article, we focus on the game-theoretic approach to centrality analysis. While various centrality indices have been recently proposed based on this approach, it is still unknown how general is the game-theoretic approach to centrality and what distinguishes some game-theoretic centralities from others. In this article, we attempt to answer this question by providing the first axiomatic characterization of game-theoretic centralities. Specifically, we show that every possible centrality measure can be obtained following the game-theoretic approach. Furthermore, we study three natural classes of game-theoretic centrality, and prove that they can be characterized by certain intuitive properties pertaining to the well-known notion of Fairness due to Myerson.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {33–68},
numpages = {36}
}

@article{10.1613/jair.1.11201,
author = {Oztok, Umut and Darwiche, Adnan},
title = {An Exhaustive DPLL Algorithm for Model Counting},
year = {2018},
issue_date = {May 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {62},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11201},
doi = {10.1613/jair.1.11201},
abstract = {State-of-the-art model counters are based on exhaustive DPLL algorithms, and have been successfully used in probabilistic reasoning, one of the key problems in AI. In this article, we present a new exhaustive DPLL algorithm with a formal semantics, a proof of correctness, and a modular design. The modular design is based on the separation of the core model counting algorithm from SAT solving techniques. We also show that the trace of our algorithm belongs to the language of Sentential Decision Diagrams (SDDs), which is a subset of Decision-DNNFs, the trace of existing state-of-the-art model counters. Still, our experimental analysis shows comparable results against state-of-the-art model counters. Furthermore, we obtain the first top-down SDD compiler, and show orders-of-magnitude improvements in SDD construction time against the existing bottom-up SDD compiler.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–32},
numpages = {32}
}

@article{10.5555/3241691.3241715,
author = {Costa-juss\`{a}, Marta R.},
title = {From Feature to Paradigm: Deep Learning in Machine Translation},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {In the last years, deep learning algorithms have highly revolutionized several areas including speech, image and natural language processing. The specific field of Machine Translation (MT) has not remained invariant. Integration of deep learning in MT varies from re-modeling existing features into standard statistical systems to the development of a new architecture. Among the different neural networks, research works use feedforward neural networks, recurrent neural networks and the encoder-decoder schema. These architectures are able to tackle challenges as having low-resources or morphology variations.This manuscript focuses on describing how these neural networks have been integrated to enhance different aspects and models from statistical MT, including language modeling, word alignment, translation, reordering, and rescoring. Then, we report the new neural MT approach together with a description of the foundational related works and recent approaches on using subword, characters and training with multilingual languages, among others. Finally, we include an analysis of the corresponding challenges and future work in using deep learning in MT.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {947–974},
numpages = {28}
}

@article{10.5555/3241691.3241714,
author = {Alhama, Raquel G. and Zuidema, Willem},
title = {Pre-Wiring and Pre-Training: What Does a Neural Network Need to Learn Truly General Identity Rules?},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {In an influential paper ("Rule learning by seven-month-old infants"), Marcus, Vijayan, Rao and Vishton claimed that connectionist models cannot account for human success at learning tasks that involved generalization of abstract knowledge such as grammatical rules. This claim triggered a heated debate, centered mostly around variants of the Simple Recurrent Network model. In our work, we revisit this unresolved debate and analyze the underlying issues from a different perspective. We argue that, in order to simulate human-like learning of grammatical rules, a neural network model should not be used as a tabula rasa, but rather, the initial wiring of the neural connections and the experience acquired prior to the actual task should be incorporated into the model. We present two methods that aim to provide such initial state: a manipulation of the initial connections of the network in a cognitively plausible manner (concretely, by implementing a "delay-line" memory), and a pre-training algorithm that incrementally challenges the network with novel stimuli. We implement such techniques in an Echo State Network (ESN), and we show that only when combining both techniques the ESN is able to learn truly general identity rules. Finally, we discuss the relation between these cognitively motivated techniques and recent advances in Deep Learning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {927–946},
numpages = {20}
}

@article{10.5555/3241691.3241713,
author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
title = {Visualisation and ‘Diagnostic Classifiers’ Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artifical task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can implement a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task: the network learns to predict the outcome of the arithmetic expressions with high accuracy, although performance deteriorates somewhat with increasing length. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This in turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {907–926},
numpages = {20}
}

@article{10.5555/3241691.3241712,
author = {Fern\'{a}ndez, Alberto and Garc\'{\i}a, Salvador and Herrera, Francisco and Chawla, Nitesh V.},
title = {SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-Year Anniversary},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The Synthetic Minority Oversampling Technique (SMOTE) preprocessing algorithm is considered "de facto" standard in the framework of learning from imbalanced data. This is due to its simplicity in the design of the procedure, as well as its robustness when applied to different type of problems. Since its publication in 2002, SMOTE has proven successful in a variety of applications from several different domains. SMOTE has also inspired several approaches to counter the issue of class imbalance, and has also significantly contributed to new supervised learning paradigms, including multilabel classification, incremental learning, semi-supervised learning, multi-instance learning, among others. It is standard benchmark for learning from imbalanced data. It is also featured in a number of different software packages -- from open source to commercial. In this paper, marking the fifteen year anniversary of SMOTE, we reect on the SMOTE journey, discuss the current state of affairs with SMOTE, its applications, and also identify the next set of challenges to extend SMOTE for Big Data problems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {863–905},
numpages = {43}
}

@article{10.5555/3241691.3241711,
author = {Yaghoobzadeh, Yadollah and Adel, Heike and Sch\"{u}tze, Hinrich},
title = {Corpus-Level Fine-Grained Entity Typing},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Extracting information about entities remains an important research area. This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class, such as "food" or "artist". The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that computes scores based on global information of an entity and (ii) a context model that first evaluates the individual occurrences of an entity and then aggregates the scores.Each of the two proposed models has specific properties. For the global model, learning high-quality entity representations is crucial because it is the only source used for the predictions. Therefore, we introduce representations using the name and contexts of entities on the three levels of entity, word, and character. We show that each level provides complementary information and a multi-level representation performs best. For the context model, we need to use distant supervision since there are no context-level labels available for entities. Distantly supervised labels are noisy and this harms the performance of models. Therefore, we introduce and apply new algorithms for noise mitigation using multi-instance learning. We show the effectiveness of our models on a large entity typing dataset built from Freebase.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {835–862},
numpages = {28}
}

@article{10.5555/3241691.3241710,
author = {Creignou, Nadia and Ktari, Ra\"{\i}da and Papini, Odile},
title = {Belief Update within Propositional Fragments},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Belief change within the framework of fragments of propositional logic is one of the main and recent challenges in the knowledge representation research area. While previous research works focused on belief revision, belief merging, and belief contraction, the problem of belief update within fragments of classical logic has not been addressed so far. In the context of revision, it has been proposed to refine existing operators so that they operate within propositional fragments, and that the result of revision remains in the fragment under consideration. This approach is not restricted to the Horn fragment but also applicable to other propositional fragments like Krom and affine fragments. We generalize this notion of refinement to any belief change operator. We then focus on a specific belief change operation, namely belief update. We investigate the behavior of the refined update operators with respect to satisfaction of the KM postulates and highlight differences between revision and update in this context.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {807–834},
numpages = {28}
}

@article{10.5555/3241691.3241709,
author = {Raue, Federico and Dengel, Andreas and Breuel, Thomas M. and Liwicki, Marcus},
title = {Symbol Grounding Association in Multimodal Sequences with Missing Elements},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {787–806},
numpages = {20}
}

@article{10.5555/3241691.3241708,
author = {Maximov, Yury and Amini, Massih-Reza and Harchaoui, Zaid},
title = {Rademacher Complexity Bounds for a Penalized Multi-Class Semi-Supervised Algorithm},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {We propose Rademacher complexity bounds for multi-class classifiers trained with a two-step semi-supervised model. In the first step, the algorithm partitions the partially labeled data and then identifies dense clusters containing κ predominant classes using the labeled training examples such that the proportion of their non-predominant classes is below a fixed threshold stands for clustering consistency. In the second step, a classifier is trained by minimizing a margin empirical loss over the labeled training set and a penalization term measuring the disability of the learner to predict the κ predominant classes of the identified clusters. The resulting data-dependent generalization error bound involves the margin distribution of the classifier, the stability of the clustering technique used in the first step and Rademacher complexity terms corresponding to partially labeled training data. Our theoretical result exhibit convergence rates extending those proposed in the literature for the binary case, and experimental results on different multi-class classification problems show empirical evidence that supports the theory.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {761–786},
numpages = {26}
}

@article{10.5555/3241691.3241707,
author = {McCreesh, Ciaran and Prosser, Patrick and Solnon, Christine and Trimble, James},
title = {When Subgraph Isomorphism is Really Hard, and Why This Matters for Graph Databases},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The subgraph isomorphism problem involves deciding whether a copy of a pattern graph occurs inside a larger target graph. The non-induced version allows extra edges in the target, whilst the induced version does not. Although both variants are NP-complete, algorithms inspired by constraint programming can operate comfortably on many real-world problem instances with thousands of vertices. However, they cannot handle arbitrary instances of this size. We show how to generate "really hard" random instances for subgraph isomorphism problems, which are computationally challenging with a couple of hundred vertices in the target, and only twenty pattern vertices. For the non-induced version of the problem, these instances lie on a satisfiable / unsatisfiable phase transition, whose location we can predict; for the induced variant, much richer behaviour is observed, and constrainedness gives a better measure of difficulty than does proximity to a phase transition. These results have practical consequences: we explain why the widely researched "filter / verify" indexing technique used in graph databases is founded upon a misunderstanding of the empirical hardness of NP-complete problems, and cannot be beneficial when paired with any reasonable subgraph isomorphism algorithm.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {723–759},
numpages = {37}
}

@article{10.5555/3241691.3241706,
author = {Booth, Richard and Hunter, Aaron},
title = {Trust as a Precursor to Belief Revision},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Belief revision is concerned with incorporating new information into a pre-existing set of beliefs. When the new information comes from another agent, we must first determine if that agent should be trusted. In this paper, we define trust as a pre-processing step before revision. We emphasize that trust in an agent is often restricted to a particular domain of expertise. We demonstrate that this form of trust can be captured by associating a state partition with each agent, then relativizing all reports to this partition before revising. We position the resulting family of trust-sensitive revision operators within the class of selective revision operators of Ferm\'{e} and Hansson, and we prove a representation result that characterizes the class of trust-sensitive revision operators in terms of a set of postulates. We also show that trust-sensitive revision is manipulable, in the sense that agents can sometimes have incentive to pass on misleading information.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {699–722},
numpages = {24}
}

@article{10.5555/3241691.3241705,
author = {Fioretto, Ferdinando and Pontelli, Enrico and Yeoh, William},
title = {Distributed Constraint Optimization Problems and Applications: A Survey},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The field of multi-agent system (MAS) is an active area of research within artificial intelligence, with an increasingly important impact in industrial and other real-world applications. In a MAS, autonomous agents interact to pursue personal interests and/or to achieve common objectives. Distributed Constraint Optimization Problems (DCOPs) have emerged as a prominent agent model to govern the agents' autonomous behavior, where both algorithms and communication models are driven by the structure of the specific problem. During the last decade, several extensions to the DCOP model have been proposed to enable support of MAS in complex, real-time, and uncertain environments.This survey provides an overview of the DCOP model, offering a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of DCOPs. The proposed classification suggests several future perspectives for DCOP extensions and identifies challenges in the design of efficient resolution algorithms, possibly through the adaptation of strategies from different areas.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {623–698},
numpages = {76}
}

@article{10.5555/3241691.3241704,
author = {D’Ippolito, Nicol\'{a}s and Rodr\'{\i}guez, Natalia and Sardina, Sebastian},
title = {Fully Observable Non-Deterministic Planning as Assumption-Based Reactive Synthesis},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {We contribute to recent efforts in relating two approaches to automatic synthesis, namely, automated planning and discrete reactive synthesis. First, we develop a declarative characterization of the standard "fairness" assumption on environments in non-deterministic planning, and show that strong-cyclic plans are correct solution concepts for fair environments. This complements, and arguably completes, the existing foundational work on non-deterministic planning, which focuses on characterizing (and computing) plans enjoying special "structural" properties, namely loopy but closed policy structures. Second, we provide an encoding suitable for reactive synthesis that avoids the naive exponential state space blowup. To do so, special care has to be taken to specify the fairness assumption on the environment in a succinct manner.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {593–621},
numpages = {29}
}

@article{10.5555/3241691.3241703,
author = {Garcia-Gasulla, Dario and Par\'{e}s, Ferran and Vilalta, Armand and Moreno, Jonatan and Ayguad\'{e}, Eduard and Labarta, Jes\'{u}s and Cort\'{e}s, Ulises and Suzumura, Toyotaro},
title = {On the Behavior of Convolutional Nets for Feature Extraction},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classi_er. This approach has provided consistent results, although its relevance is limited to classi_cation tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results con_rm that low and middle level features may behave di_erently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {563–592},
numpages = {30}
}

@article{10.5555/3241691.3241702,
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
title = {Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-pro_le success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523–562},
numpages = {40}
}

@article{10.5555/3241691.3241701,
author = {Fi\v{s}er, Daniel and Komenda, Anton\'{\i}n},
title = {Fact-Alternating Mutex Groups for Classical Planning},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Mutex groups are defined in the context of STRIPS planning as sets of facts out of which, maximally, one can be true in any state reachable from the initial state. The importance of computing and exploiting mutex groups was repeatedly pointed out in many studies. However, the theoretical analysis of mutex groups is sparse in current literature. This work provides a complexity analysis showing that inference of mutex groups is as hard as planning itself (PSPACE-Complete) and it also shows a tight relationship between mutex groups and graph cliques. This result motivates us to propose a new type of mutex group called a fact-alternating mutex group (fam-group) of which inference is NP-Complete. Moreover, we introduce an algorithm for the inference of fam-groups based on integer linear programming that is complete with respect to the maximal fam-groups and we demonstrate how beneficial fam-groups can be in the translation of planning tasks into finite domain representation. Finally, we show that fam-groups can be used for the detection of deadend states and we propose a simple algorithm for the pruning of operators and facts as a preprocessing step that takes advantage of the properties of fam-groups. The experimental evaluation of the pruning algorithm shows a substantial increase in a number of solved tasks in domains from the optimal deterministic track of the last two planning competitions (IPC 2011 and 2014).},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {475–521},
numpages = {47}
}

@article{10.5555/3241691.3241700,
author = {Zenonos, Alexandros and Stein, Sebastian and Jennings, Nicholas R.},
title = {Coordinating Measurements for Environmental Monitoring in Uncertain Participatory Sensing Settings},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Environmental monitoring allows authorities to understand the impact of potentially harmful phenomena, such as air pollution, excessive noise and radiation. Recently, there has been considerable interest in participatory sensing as a paradigm for such large-scale data collection because it is cost-effective and able to capture more fine-grained data than traditional approaches that use stationary sensors scattered in cities. In this approach, ordinary citizens (non-expert contributors) collect environmental data using low-cost mobile devices. However, these participants are generally self-interested actors that have their own goals and make local decisions about when and where to take measurements. This can lead to highly ine_cient outcomes, where observations are either taken redundantly or do not provide sufficient information about key areas of interest. To address these challenges, it is necessary to guide and to coordinate participants, so they take measurements when it is most informative. To this end, we develop a computationally-e_cient coordination algorithm (adaptive Best-Match) that suggests to users when and where to take measurements. Our algorithm exploits probabilistic knowledge of human mobility patterns, but explicitly considers the uncertainty of these patterns and the potential unwillingness of people to take measurements when requested to do so. In particular, our algorithm uses a local search technique, clustering and random simulations to map participants to measurements that need to be taken in space and time. We empirically evaluate our algorithm on a real-world human mobility and air quality dataset and show that it outperforms the current state of the art by up to 24% in terms of utility gained.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {433–474},
numpages = {42}
}

@article{10.5555/3241691.3241699,
author = {Zwicker, William S.},
title = {Cycles and Intractability in a Large Class of Aggregation Rules},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {We introduce the (j, k)-Kemeny rule - a generalization of Kemeny's voting rule that aggregates j-chotomous weak orders into a k-chotomous weak order. Special cases of (j, k)- Kemeny include approval voting, the mean rule and Borda mean rule, as well as the Borda count and plurality voting. Why, then, is the winner problem computationally tractable for each of these other rules, but intractable for Kemeny? We show that intractability of winner determination for the (j, k)-Kemeny rule first appears at the j = 3, k = 3 level. The proof rests on a reduction of max cut to a related problem on weighted tournaments, and reveals that computational complexity arises from the cyclic part in the fundamental decomposition of a weighted tournament into cyclic and cocyclic components. Thus the existence of majority cycles - the engine driving both Arrow's impossibility theorem and the Gibbard-Satterthwaite theorem - also serves as a source of computational complexity in social choice.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {407–431},
numpages = {25}
}

@article{10.5555/3241691.3241698,
author = {Kara, Yunus Emre and Genc, Gaye and Aran, Oya and Akarun, Lale},
title = {Actively Estimating Crowd Annotation Consensus},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The rapid growth of storage capacity and processing power has caused machine learning applications to increasingly rely on using immense amounts of labeled data. It has become more important than ever to have fast and inexpensive ways to annotate vast amounts of data. With the emergence of crowdsourcing services, the research direction has gravitated toward putting the wisdom of crowds to better use. Unfortunately, spammers and inattentive annotators pose a threat to the quality and trustworthiness of the consensus. Thus, high quality consensus estimation from crowd annotated data requires a meticulous choice of the candidate annotator and the sample in need of a new annotation. Due to time and budget limitations, it is of utmost importance that this choice is carried out while the annotation collection is in progress. We call this process active crowd-labeling. To this end, we propose an active crowd-labeling approach for actively estimating consensus from continuous-valued crowd annotations. Our method is based on annotator models with unknown parameters, and Bayesian inference is employed to reach a consensus in the form of ordinal, binary, or continuous values. We introduce ranking functions for choosing the candidate annotator and sample pair for requesting an annotation. In addition, we propose a penalizing method for preventing annotator domination, investigate the explore-exploit trade-off for incorporating new annotators into the system, and study the effects of inducing a stopping criterion based on consensus quality. We also introduce the crowd-labeled Head Pose Annotations datasets. Experimental results on the benchmark datasets used in the literature and the Head Pose Annotations datasets suggest that our method provides high-quality consensus by using as few as one _fth of the annotations (~ 80% cost reduction), thereby providing a budget and time-sensitive solution to the crowd-labeling problem.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {363–405},
numpages = {43}
}

@article{10.5555/3241691.3241697,
author = {Gaschler, Andre and Petrick, Ronald P. A. and Khatib, Oussama and Knoll, Alois},
title = {KABouM: Knowledge-Level Action and Bounding Geometry Motion Planner},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {For robots to solve real world tasks, they often require the ability to reason about both symbolic and geometric knowledge. We present a framework, called KABouM, for integrating knowledge-level task planning and motion planning in a bounding geometry. By representing symbolic information at the knowledge level, we can model incomplete information, sensing actions and information gain; by representing all geometric entities-- objects, robots and swept volumes of motions|by sets of convex polyhedra, we can efficiently plan manipulation actions and raise reasoning about geometric predicates, such as collisions, to the symbolic level. At the geometric level, we take advantage of our bounded convex decomposition and swept volume computation with quadratic convergence, and fast collision detection of convex bodies. We evaluate our approach on a wide set of problems using real robots, including tasks with multiple manipulators, sensing and branched plans, and mobile manipulation.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {323–362},
numpages = {40}
}

@article{10.5555/3241691.3241696,
author = {Kimura, Kei and Makino, Kazuhisa},
title = {Linear Satisfiability Preserving Assignments},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we study several classes of satisfiability preserving assignments to the constraint satisfaction problem (CSP). In particular, we consider fixable, autark and satisfying assignments. Since it is in general NP-hard to find a nontrivial (i.e., nonempty) satisfiability preserving assignment, we introduce linear satisfiability preserving assignments, which are defined by polyhedral cones in an associated vector space. The vector space is obtained by the identi_cation, introduced by Kullmann, of assignments with real vectors. We consider arbitrary polyhedral cones, where only restricted classes of cones for autark assignments are considered in the literature. We reveal that cones in certain classes are maximal as a convex subset of the set of the associated vectors, which can be regarded as extensions of Kullmann's results for autark assignments of CNFs. As algorithmic results, we present a pseudo-polynomial time algorithm that computes a linear fixable assignment for a given integer linear system, which implies the well known pseudo-polynomial solvability for integer linear systems such as two-variable-per-inequality (TVPI), Horn and q-Horn systems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {291–321},
numpages = {31}
}

@article{10.5555/3241691.3241695,
author = {Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
title = {From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of constructing abstract representations for planning in high-dimensional, continuous environments. We assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions.We first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. We define the specific collection of sets that is necessary and su_cient for planning, and use them to construct a grounded abstract symbolic representation that is provably suitable for deterministic planning. The resulting representation can be expressed in PDDL, a canonical high-level planning domain language; we construct such a representation for the Playroom domain and solve it in milliseconds using an off-the-shelf planner.We then consider probabilistic planning, which we show requires generalizing from sets of states to distributions over states. We identify the specific distributions required for planning, and use them to construct a grounded abstract symbolic representation that correctly estimates the expected reward and probability of success of any plan. In addition, we show that learning the relevant probability distributions corresponds to specific instances of probabilistic density estimation and probabilistic classification. We construct an agent that autonomously learns the correct abstract representation of a computer game domain, and rapidly solves it.Finally, we apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from sensorimotor data--point clouds, map locations, and joint angles--and then plans using that representation. Together, these results establish a principled link between high-level actions and abstract representations, a concrete theoretical foundation for constructing abstract representations with provable properties, and a practical mechanism for autonomously learning abstract high-level representations.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {215–289},
numpages = {75}
}

@article{10.5555/3241691.3241694,
author = {Abriola, Sergio and Barcel\'{o}, Pablo and Figueira, Diego and Figueira, Santiago},
title = {Bisimulations on Data Graphs},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Bisimulation provides structural conditions to characterize indistinguishability from an external observer between nodes on labeled graphs. It is a fundamental notion used in many areas, such as verification, graph-structured databases, and constraint satisfaction. However, several current applications use graphs where nodes also contain data (the so called "data graphs"), and where observers can test for equality or inequality of data values (e.g., asking the attribute 'name' of a node to be different from that of all its neighbors). The present work constitutes a first investigation of "data aware" bisimulations on data graphs. We study the problem of computing such bisimulations, based on the observational indistinguishability for XPath "a language that extends modal logics like PDL with tests for data equality" with and without transitive closure operators. We show that in general the problem is PSPACE-complete, but identify several restrictions that yield better complexity bounds (CO-NP, PTIME) by controlling suitable parameters of the problem, namely the amount of non-locality allowed, and the class of models considered (graphs, DAGs, trees). In particular, this analysis yields a hierarchy of tractable fragments.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {171–213},
numpages = {43}
}

@article{10.5555/3241691.3241693,
author = {Gatt, Albert and Krahmer, Emiel},
title = {Survey of the State of the Art in Natural Language Generation: Core Tasks, Applications and Evaluation},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past two decades, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artifical intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {65–170},
numpages = {106}
}

@article{10.5555/3241691.3241692,
author = {Evans, Richard and Grefenstette, Edward},
title = {Learning Explanatory Rules from Noisy Data},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous over_tting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–64},
numpages = {64}
}

@article{10.5555/3207692.3207718,
author = {Delgrande, James P.},
title = {A Knowledge Level Account of Forgetting},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Forgetting is an operation on knowledge bases that has been addressed in different areas of Knowledge Representation and with respect to different formalisms, including classical propositional and first-order logic, modal logics, logic programming, and description logics. Definitions of forgetting have been expressed in terms of manipulation of formulas, sets of postulates, isomorphisms between models, bisimulations, second-order quantification, elementary equivalence, and others. In this paper, forgetting is regarded as an abstract belief change operator, independent of the underlying logic. The central thesis is that forgetting amounts to a reduction in the language, specifically the signature, of a logic. The main definition is simple: the result of forgetting a portion of a signature in a theory is given by the set of logical consequences of this theory over the reduced language. This definition offers several advantages. Foremost, it provides a uniform approach to forgetting, with a definition that is applicable to any logic with a well-defined consequence relation. Hence it generalises a disparate set of logic-specific definitions with a general, high-level definition. Results obtained in this approach are thus applicable to all subsumed formal systems, and many results are obtained much more straightforwardly. This view also leads to insights with respect to specific logics: for example, forgetting in first-order logic is somewhat different from the accepted approach. Moreover, the approach clarifies the relation between forgetting and related operations, including belief contraction.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1165–1213},
numpages = {49}
}

@article{10.5555/3207692.3207717,
author = {Basat, Ran Ben and Tennenholtz, Moshe and Kurland, Oren},
title = {A Game Theoretic Analysis of the Adversarial Retrieval Setting},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {The main goal of search engines is ad hoc retrieval: ranking documents in a corpus by their relevance to the information need expressed by a query. The Probability Ranking Principle (PRP) -- ranking the documents by their relevance probabilities -- is the theoretical foundation of most existing ad hoc document retrieval methods. A key observation that motivates our work is that the PRP does not account for potential post-ranking effects; specifically, changes to documents that result from a given ranking. Yet, in adversarial retrieval settings such as the Web, authors may consistently try to promote their documents in rankings by changing them. We prove that, indeed, the PRP can be sub-optimal in adversarial retrieval settings. We do so by presenting a novel game theoretic analysis of the adversarial setting. The analysis is performed for different types of documents (single-topic and multi-topic) and is based on different assumptions about the writing qualities of documents' authors. We show that in some cases, introducing randomization into the document ranking function yields an overall user utility that transcends that of applying the PRP.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1127–1164},
numpages = {38}
}

@article{10.5555/3207692.3207716,
author = {Zaffalon, Marco and Miranda, Enrique},
title = {Axiomatising Incomplete Preferences through Sets of Desirable Gambles},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We establish the equivalence of two very general theories: the first is the decision-theoretic formalisation of incomplete preferences based on the mixture independence axiom; the second is the theory of coherent sets of desirable gambles (bounded variables) developed in the context of imprecise probability and extended here to vector-valued gambles. Such an equivalence allows us to analyse the theory of incomplete preferences from the point of view of desirability. Among other things, this leads us to uncover an unexpected and clarifying relation: that the notion of state independence -- the traditional assumption that we can have separate models for beliefs (probabilities) and values (utilities) -- coincides with that of strong independence in imprecise probability; this connection leads us also to propose much weaker, and arguably more realistic, notions of state independence. Then we simplify the treatment of complete beliefs and values by putting them on a more equal footing. We study the role of the Archimedean condition -- which allows us to actually talk of expected utility--, identify some weaknesses and propose alternatives that solve these. More generally speaking, we show that desirability is a valuable alternative foundation to preferences for decision theory that streamlines and unifies a number of concepts while preserving great generality. In addition, the mentioned equivalence shows for the first time how to extend the theory of desirability to imprecise non-linear utility, thus enabling us to formulate one of the most powerful self-consistent theories of reasoning and decision-making available today.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1057–1126},
numpages = {70}
}

@article{10.5555/3207692.3207715,
author = {De Rosa, Rocco and Cesa-Bianchi, Nicol\`{o}},
title = {Confidence Decision Trees via Online and Active Learning for Streaming Data},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Decision tree classifiers are a widely used tool in data stream mining. The use of confidence intervals to estimate the gain associated with each split leads to very effective methods, like the popular Hoeffding tree algorithm. From a statistical viewpoint, the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf. Although some of the issues in the statistical analysis of Hoeffding trees have been already clarified, a general and rigorous study of confidence intervals for splitting criteria is missing. We fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria: entropy, Gini index, and a third index proposed by Kearns and Mansour. We also extend our confidence analysis to a selective sampling setting, in which the decision tree learner adaptively decides which labels to query in the stream. We provide theoretical guarantees bounding the probability that the decision tree learned via our selective sampling strategy classifies suboptimally the next example in the stream. Experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by state-of-the-art techniques. In addition to that, our active learning module empirically uses fewer labels without significantly hurting the performance.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1031–1055},
numpages = {25}
}

@article{10.5555/3207692.3207714,
author = {Kiela, Douwe and Clark, Stephen},
title = {Learning Neural Audio Embeddings for Grounding Semantics in Auditory Perception},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Multi-modal semantics, which aims to ground semantic representations in perception, has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics. After having shown the quality of such auditorily grounded representations, we show how they can be applied to tasks where auditory perception is relevant, including two unsupervised categorization experiments, and provide further analysis. We find that features transfered from deep neural networks outperform bag of audio words approaches. To our knowledge, this is the first work to construct multi-modal models from a combination of textual information and auditory information extracted from deep neural networks, and the first work to evaluate the performance of tri-modal (textual, visual and auditory) semantic models.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1003–1030},
numpages = {28}
}

@article{10.5555/3207692.3207713,
author = {Chen, Jiehua and Faliszewski, Piotr and Niedermeier, Rolf and Talmon, Nimrod},
title = {Elections with Few Voters: Candidate Control Can Be Easy},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We study the computational complexity of candidate control in elections with few voters, that is, we consider the parameterized complexity of candidate control in elections with respect to the number of voters as a parameter. We consider both the standard scenario of adding and deleting candidates, where one asks whether a given candidate can become a winner (or, in the destructive case, can be precluded from winning) by adding or deleting few candidates, as well as a combinatorial scenario where adding/deleting a candidate automatically means adding or deleting a whole group of candidates. Considering several fundamental voting rules, our results show that the parameterized complexity of candidate control, with the number of voters as the parameter, is much more varied than in the setting with many voters.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {937–1002},
numpages = {66}
}

@article{10.5555/3207692.3207712,
author = {Vodopivec, Tom and Samothrakis, Spyridon and \v{S}ter, Branko},
title = {On Monte Carlo Tree Search and Reinforcement Learning},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Fuelled by successes in Computer Go, Monte Carlo tree search (MCTS) has achieved wide-spread adoption within the games community. Its links to traditional reinforcement learning (RL) methods have been outlined in the past; however, the use of RL techniques within tree search has not been thoroughly studied yet. In this paper we re-examine in depth this close relation between the two fields; our goal is to improve the cross-awareness between the two communities. We show that a straightforward adaptation of RL semantics within tree search can lead to a wealth of new algorithms, for which the traditional MCTS is only one of the variants. We confirm that planning methods inspired by RL in conjunction with online search demonstrate encouraging results on several classic board games and in arcade video game competitions, where our algorithm recently ranked first. Our study promotes a unified view of learning, planning, and search.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {881–936},
numpages = {56}
}

@article{10.5555/3207692.3207711,
author = {Echenim, Mnacho and Peltier, Nicolas and Tourret, Sophie},
title = {Prime Implicate Generation in Equational Logic},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We present an algorithm for the generation of prime implicates in equational logic, that is, of the most general consequences of formul\ae{} containing equations and disequations between first-order terms. This algorithm is defined by a calculus that is proved to be correct and complete. We then focus on the case where the considered clause set is ground, i.e., contains no variables, and devise a specialized tree data structure that is designed to efficiently detect and delete redundant implicates. The corresponding algorithms are presented along with their termination and correctness proofs. Finally, an experimental evaluation of this prime implicate generation method is conducted in the ground case, including a comparison with state-of-the-art propositional and first-order prime implicate generation tools.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {827–880},
numpages = {54}
}

@article{10.5555/3207692.3207710,
author = {Caminada, Martin and Schulz, Claudia},
title = {On the Equivalence between Assumption-Based Argumentation and Logic Programming},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Assumption-Based Argumentation (ABA) has been shown to subsume various other non-monotonic reasoning formalisms, among them normal logic programming (LP). We re-examine the relationship between ABA and LP and show that normal LP also subsumes (flat) ABA. More precisely, we specify a procedure that given a (flat) ABA framework yields an associated logic program with almost the same syntax whose semantics coincide with those of the ABA framework. That is, the 3-valued stable (respectively well-founded, regular, 2-valued stable, and ideal) models of the associated logic program coincide with the complete (respectively grounded, preferred, stable, and ideal) assumption labellings and extensions of the ABA framework. Moreover, we show how our results on the translation from ABA to LP can be reapplied for a reverse translation from LP to ABA, and observe that some of the existing results in the literature are in fact special cases of our work. Overall, we show that (flat) ABA frameworks can be seen as normal logic programs with a slightly different syntax. This implies that methods developed for one of these formalisms can be equivalently applied to the other by simply modifying the syntax.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {779–825},
numpages = {47}
}

@article{10.5555/3207692.3207709,
author = {Hostetler, Jesse and Fern, Alan and Dietterich, Thomas},
title = {Sample-Based Tree Search with Fixed and Adaptive State Abstractions},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Sample-based tree search (SBTS) is an approach to solving Markov decision problems based on constructing a lookahead search tree using random samples from a generative model of the MDP. It encompasses Monte Carlo tree search (MCTS) algorithms like UCT as well as algorithms such as sparse sampling. SBTS is well-suited to solving MDPs with large state spaces due to the relative insensitivity of SBTS algorithms to the size of the state space. The limiting factor in the performance of SBTS tends to be the exponential dependence of sample complexity on the depth of the search tree. The number of samples required to build a search tree is O((|A|B)d), where |A| is the number of available actions, B is the number of possible random outcomes of taking an action, and d is the depth of the tree. State abstraction can be used to reduce B by aggregating random outcomes together into abstract states. Recent work has shown that abstract tree search often performs substantially better than tree search conducted in the ground state space.This paper presents a theoretical and empirical evaluation of tree search with both fixed and adaptive state abstractions. We derive a bound on regret due to state abstraction in tree search that decomposes abstraction error into three components arising from properties of the abstraction and the search algorithm. We describe versions of popular SBTS algorithms that use fixed state abstractions, and we introduce the Progressive Abstraction Refinement in Sparse Sampling (PARSS) algorithm, which adapts its abstraction during search. We evaluate PARSS as well as sparse sampling with fixed abstractions on 12 experimental problems, and find that PARSS outperforms search with a fixed abstraction and that search with even highly inaccurate fixed abstractions outperforms search without abstraction. These results establish progressive abstraction refinement as a promising basis for new tree search algorithms, and we propose directions for future work within the progressive refinement framework.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {717–777},
numpages = {61}
}

@article{10.5555/3207692.3207708,
author = {Skowron, Piotr and Faliszewski, Piotr},
title = {Chamberlin-Courant Rule with Approval Ballots: Approximating the Maxcover Problem with Bounded Frequencies in FPT Time},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of winner determination under Chamberlin-Courant's multiwinner voting rule with approval utilities. This problem is equivalent to the well-known NP-complete MaxCover problem and, so, the best polynomial-time approximation algorithm for it has approximation ratio 1-1/e. We show exponential-time/FPT approximation algorithms that, on one hand, achieve arbitrarily good approximation ratios and, on the other hand, have running times much better than known exact algorithms. We focus on the cases where the voters have to approve of at most/at least a given number of candidates.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {687–716},
numpages = {30}
}

@article{10.5555/3207692.3207707,
author = {Inclezan, Daniela and Pr\'{a}danos, Luis I.},
title = {Viewpoint: A Critical View on Smart Cities and AI},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {AI developments on smart cities, if not critical, risk making a flawed urban model more efficient. Instead, we suggest that AI should challenge the mainstream techno-optimistic approach to solving urban problems by dialoguing with other academic fields, questioning the dominant urban paradigm, and creating transformative solutions. We claim that doing differently, rather than doing better, may be smarter for cities and the common good.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {681–686},
numpages = {6}
}

@article{10.5555/3207692.3207706,
author = {Dickerson, John P. and Sandholm, Tuomas},
title = {Multi-Organ Exchange},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Kidney exchange, where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand. While fielded kidney exchanges see huge benefit from altruistic kidney donors (who give an organ without a paired needy candidate), a significantly higher medical risk to the donor deters similar altruism with livers. In this paper, we begin by exploring the idea of large-scale liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level. We then propose cross-organ donation where kidneys and livers can be bartered for each other. We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges. This linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool; it exists even when only a small but constant portion of the donors on the kidney side of the pool are willing to donate a liver lobe. We support this result experimentally on demographically accurate multi-organ exchanges. We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {639–679},
numpages = {41}
}

@article{10.5555/3207692.3207705,
author = {B\"{a}ckstr\"{o}m, Christer and Jonsson, Peter},
title = {Time and Space Bounds for Planning},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {There is an extensive literature on the complexity of planning, but explicit bounds on time and space complexity are very rare. On the other hand, problems like the constraint satisfaction problem (CSP) have been thoroughly analysed in this respect. We provide a number of upper- and lower-bound results (the latter based on various complexity-theoretic assumptions such as the Exponential Time Hypothesis) for both satisficing and optimal planning. We show that many classes of planning instances exhibit a dichotomy: either they can be solved in polynomial time or they cannot be solved in subexponential time and thus require O(2cn) time for some c &gt; 0. In many cases, we can even prove closely matching upper and lower bounds; for every ε &gt; 0, the problem can be solved in time O(2(1+ε)n) but not in time O(2(1-ε)n). Our results also indicate, analogously to CSPs, the existence of sharp phase transitions. We finally study and discuss the trade-off between time and space. In particular, we show that depth-first search may sometimes be a viable option for planning under severe space constraints.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {595–638},
numpages = {44}
}

@article{10.5555/3207692.3207704,
author = {Paetzold, Gustavo H. and Specia, Lucia},
title = {A Survey on Lexical Simplification},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Lexical Simplification is the process of replacing complex words in a given sentence with simpler alternatives of equivalent meaning. This task has wide applicability both as an assistive technology for readers with cognitive impairments or disabilities, such as Dyslexia and Aphasia, and as a pre-processing tool for other Natural Language Processing tasks, such as machine translation and summarisation. The problem is commonly framed as a pipeline of four steps: the identification of complex words, the generation of substitution candidates, the selection of those candidates that fit the context, and the ranking of the selected substitutes according to their simplicity. In this survey we review the literature for each step in this typical Lexical Simplification pipeline and provide a benchmarking of existing approaches for these steps on publicly available datasets. We also provide pointers for datasets and resources available for the task.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {549–593},
numpages = {45}
}

@article{10.5555/3207692.3207703,
author = {Jinnai, Yuu and Fukunaga, Alex},
title = {On Hash-Based Work Distribution Methods for Parallel Best-First Search},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Parallel best-first search algorithms such as Hash Distributed A* (HDA*) distribute work among the processes using a global hash function. We analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although Zobrist hashing, the standard hash function used by HDA*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. We show that Abstract Zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. We then propose GRAZHDA*, a graph-partitioning based approach to automatically generating feature projection functions. GRAZHDA* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. We show that GRAZHDA* outperforms previous methods on domain-independent planning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {491–548},
numpages = {58}
}

@article{10.5555/3207692.3207702,
author = {Yu, Peng and Williams, Brian and Fang, Cheng and Cui, Jing and Haslum, Patrik},
title = {Resolving Over-Constrained Temporal Problems with Uncertainty through Conflict-Directed Relaxation},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Over-subscription, that is, being assigned too many things to do, is commonly encountered in temporal scheduling problems. As human beings, we often want to do more than we can actually do, and underestimate how long it takes to perform each task. Decision makers can benefit from aids that identify when these failure situations are likely, the root causes of these failures, and resolutions to these failures.In this paper, we present a decision assistant that helps users resolve oversubscribed temporal problems. The system works like an experienced advisor that can quickly identify the cause of failure underlying temporal problems and compute resolutions. The core of the decision assistant is the Best-first Con ict-Directed Relaxation (BCDR) algorithm, which can detect conflicting sets of constraints within temporal problems, and computes continuous relaxations for them that weaken constraints to the minimum extent, instead of removing them completely. BCDR is an extension to the Con ict-Directed A* algorithm, first developed in the model-based reasoning community to compute most likely system diagnoses or reconfigurations. It generalizes the discrete conflicts and relaxations, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relaxations to both discrete and continuous relaxable constraints. In addition, BCDR is capable of handling temporal uncertainty, expressed as either set-bounded or probabilistic durations, and can compute preferred trade-offs between the risk of violating a schedule requirement, versus the loss of utility by weakening those requirements.BCDR has been applied to several decision support applications in different domains, including deep-sea exploration, urban travel planning and transit system management. It has demonstrated its effectiveness in helping users resolve over-subscribed scheduling problems and evaluate the robustness of existing solutions. In our benchmark experiments, BCDR has also demonstrated its efficiency on solving large-scale scheduling problems in the aforementioned domains. Thanks to its conflict-driven approach for computing relaxations, BCDR achieves one to two orders of magnitude improvements on runtime performance when compared to state-of-the-art numerical solvers.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {425–490},
numpages = {66}
}

@article{10.5555/3207692.3207701,
author = {Eiter, Thomas and Weinzierl, Antonius},
title = {Preference-Based Inconsistency Management in Multi-Context Systems},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Multi-Context Systems (MCS) are a powerful framework for interlinking possibly heterogeneous, autonomous knowledge bases, where information can be exchanged among knowledge bases by designated bridge rules with negation as failure. An acknowledged issue with MCS is inconsistency that arises due to the information exchange. To remedy this problem, inconsistency removal has been proposed in terms of repairs, which modify bridge rules based on suitable notions for diagnosis of inconsistency. In general, multiple diagnoses and repairs do exist; this leaves the user, who arguably may oversee the inconsistency removal, with the task of selecting some repair among all possible ones. To aid in this regard, we extend the MCS framework with preference information for diagnoses, such that undesired diagnoses are filtered out and diagnoses that are most preferred according to a preference ordering are selected. We consider preference information at a generic level and develop meta-reasoning techniques on diagnoses in MCS that can be exploited to reduce preference-based selection of diagnoses to computing ordinary subset-minimal diagnoses in an extended MCS. We describe two meta-reasoning encodings for preference orders: the first is conceptually simple but may incur an exponential blowup. The second is increasing only linearly in size and based on duplicating the original MCS. The latter requires nondeterministic guessing if a subset-minimal among all most preferred diagnoses should be computed. However, a complexity analysis of diagnoses shows that this is worst-case optimal, and that in general, preferred diagnoses have the same complexity as subset-minimal ordinary diagnoses. Furthermore, (subset-minimal) filtered diagnoses and (subset-minimal) ordinary diagnoses also have the same complexity.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {347–424},
numpages = {78}
}

@article{10.5555/3207692.3207700,
author = {Lam, William and Kask, Kalev and Larrosa, Javier and Dechter, Rina},
title = {Residual-Guided Look-Ahead in and/or Search for Graphical Models},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We introduce the concept of local bucket error for the mini-bucket heuristics and show how it can be used to improve the power of AND/OR search for combinatorial optimization tasks in graphical models (e.g. MAP/MPE or weighted CSPs). The local bucket error illuminates how the heuristic errors are distributed in the search space, guided by the mini-bucket heuristic. We present and analyze methods for compiling the local bucket-errors (exactly and approximately) and show that they can be used to yield an effective tool for balancing look-ahead overhead during search. This can be especially instrumental when memory is restricted, accommodating the generation of only weak compiled heuristics. We illustrate the impact of the proposed schemes in an extensive empirical evaluation for both finding exact solutions and anytime suboptimal solutions.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {287–346},
numpages = {60}
}

@article{10.5555/3207692.3207699,
author = {Kariotoglou, Nikolaos and Kamgarpour, Maryam and Summers, Tyler H. and Lygeros, John},
title = {The Linear Programming Approach to Reach-Avoid Problems for Markov Decision Processes},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {One of the most fundamental problems in Markov decision processes is analysis and control synthesis for safety and reachability specifications. We consider the stochastic reach-avoid problem, in which the objective is to synthesize a control policy to maximize the probability of reaching a target set at a given time, while staying in a safe set at all prior times. We characterize the solution to this problem through an infinite dimensional linear program. We then develop a tractable approximation to the infinite dimensional linear program through finite dimensional approximations of the decision space and constraints. For a large class of Markov decision processes modeled by Gaussian mixtures kernels we show that through a proper selection of the finite dimensional space, one can further reduce the computational complexity of the resulting linear program. We validate the proposed method and analyze its potential with numerical case studies.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {263–285},
numpages = {23}
}

@article{10.5555/3207692.3207698,
author = {Cozman, Fabio Gagliardi and Mau\'{a}, Denis Deratani},
title = {On the Semantics and Complexity of Probabilistic Logic Programs},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {We examine the meaning and the complexity of probabilistic logic programs that consist of a set of rules and a set of independent probabilistic facts (that is, programs based on Sato's distribution semantics). We focus on two semantics, respectively based on stable and on well-founded models. We show that the semantics based on stable models (referred to as the "credal semantics") produces sets of probability measures that dominate infinitely monotone Choquet capacities; we describe several useful consequences of this result. We then examine the complexity of inference with probabilistic logic programs. We distinguish between the complexity of inference when a probabilistic program and a query are given (the inferential complexity), and the complexity of inference when the probabilistic program is fixed and the query is given (the query complexity, akin to data complexity as used in database theory). We obtain results on the inferential and query complexity for acyclic, stratified, and normal propositional and relational programs; complexity reaches various levels of the counting hierarchy and even exponential levels.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {221–262},
numpages = {42}
}

@article{10.5555/3207692.3207697,
author = {Yang, Yinfei and Nenkova, Ani},
title = {Combining Lexical and Syntactic Features for Detecting Content-Dense Texts in News},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Content-dense news report important factual information about an event in direct, succinct manner. Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80%.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {179–219},
numpages = {41}
}

@article{10.5555/3207692.3207696,
author = {Airiau, St\'{e}phane and Bonzon, Elise and Endriss, Ulle and Maudet, Nicolas and Rossit, Julien},
title = {Rationalisation of Profiles of Abstract Argumentation Frameworks: Characterisation and Complexity},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Different agents may have different points of view. Following a popular approach in the artificial intelligence literature, this can be modelled by means of different abstract argumentation frameworks, each consisting of a set of arguments the agent is contemplating and a binary attack-relation between them. A question arising in this context is whether the diversity of views observed in such a profile of argumentation frameworks is consistent with the assumption that every individual argumentation framework is induced by a combination of, first, some basic factual attack-relation between the arguments and, second, the personal preferences of the agent concerned regarding the moral or social values the arguments under scrutiny relate to. We treat this question of rationalisability of a profile as an algorithmic problem and identify tractable and intractable cases. In doing so, we distinguish different constraints on admissible rationalisations, e.g., concerning the types of preferences used or the number of distinct values involved. We also distinguish two different semantics for rationalisability, which differ in the assumptions made on how agents treat attacks between arguments they do not report. This research agenda, bringing together ideas from abstract argumentation and social choice, is useful for understanding what types of profiles can reasonably be expected to occur in a multiagent system.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {149–177},
numpages = {29}
}

@article{10.5555/3207692.3207695,
author = {Oveisi, Mehrdad and Delgrande, James P. and Pelletier, Francis Jeffry and Popowich, Fred},
title = {Kernel Contraction and Base Dependence},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {The AGM paradigm of belief change studies the dynamics of belief states in light of new information. Finding, or even approximating, those beliefs that are dependent on or relevant to a change is valuable because, for example, it can narrow the set of beliefs considered during belief change operations. A strong intuition in this area is captured by G\"{a}rdenfors's preservation criterion (GPC), which suggests that formulas independent of a belief change should remain intact. GPC thus allows one to build dependence relations that are linked with belief change. Such dependence relations can in turn be used as a theoretical benchmark against which to evaluate other approximate dependence or relevance relations. Fari\~{n}as and Herzig axiomatize a dependence relation with respect to a belief set, and, based on GPC, they characterize the correspondence between AGM contraction functions and dependence relations. In this paper, we introduce base dependence as a relation between formulas with respect to a belief base, and prove a more general characterization that shows the correspondence between kernel contraction and base dependence. At this level of generalization, different types of base dependence emerge, which we show to be a result of possible redundancy in the belief base. We further show that one of these relations that emerge, strong base dependence, is parallel to saturated kernel contraction. We then prove that our latter characterization is a reversible generalization of Fari\~{n}as and Herzig's characterization. That is, in the special case when the underlying belief base is deductively closed (i.e., it is a belief set), strong base dependence reduces to dependence, and so do their respective characterizations. Finally, an intriguing feature of Fari\~{n}as and Herzig's formalism is that it meets other criteria for dependence, namely, Keynes's conjunction criterion for dependence (CCD) and G\"{a}rdenfors's conjunction criterion for independence (CCI). We prove that our base dependence formalism also meets these criteria. Even more interestingly, we offer a more specific criterion that implies both CCD and CCI, and show our base dependence formalism also meets this new criterion.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {97–148},
numpages = {52}
}

@article{10.5555/3207692.3207694,
author = {Gebser, Martin and Maratea, Marco and Ricca, Francesco},
title = {The Sixth Answer Set Programming Competition},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Answer Set Programming (ASP) is a well-known paradigm of declarative programming with roots in logic programming and non-monotonic reasoning. Similar to other closely related problem-solving technologies, such as SAT/SMT, QBF, Planning and Scheduling, advancements in ASP solving are assessed in competition events. In this paper, we report about the design and results of the Sixth ASP Competition, which was jointly organized by the University of Calabria (Italy), Aalto University (Finland), and the University of Genoa (Italy), in affiliation with the 13th International Conference on Logic Programming and Non-Monotonic Reasoning. This edition maintained some of the design decisions introduced in 2014, e.g., the conception of sub-tracks, the scoring scheme, and the adherence to a fixed modeling language in order to push the adoption of the ASP-Core-2 standard. On the other hand, it featured also some novelties, like a benchmark selection stage classifying instances according to their empirical hardness, and a "Marathon" track where the top-performing systems are given more time for solving hard benchmarks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {41–95},
numpages = {55}
}

@article{10.5555/3207692.3207693,
author = {Wallner, Johannes P. and Niskanen, Andreas and J\"{a}rvisalo, Matti},
title = {Complexity Results and Algorithms for Extension Enforcement in Abstract Argumentation},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Argumentation is an active area of modern artificial intelligence (AI) research, with connections to a range of fields, from computational complexity theory and knowledge representation and reasoning to philosophy and social sciences, as well as application-oriented work in domains such as legal reasoning, multi-agent systems, and decision support. Argumentation frameworks (AFs) of abstract argumentation have become the graph-based formal model of choice for many approaches to argumentation in AI, with semantics defining sets of jointly acceptable arguments, i.e., extensions. Understanding the dynamics of AFs has been recently recognized as an important topic in the study of argumentation in AI. In this work, we focus on the so-called extension enforcement problem in abstract argumentation as a recently proposed form of argumentation dynamics. We provide a nearly complete computational complexity map of argument-fixed extension enforcement under various major AF semantics, with results ranging from polynomial-time algorithms to completeness for the second level of the polynomial hierarchy. Complementing the complexity results, we propose algorithms for NP-hard extension enforcement based on constraint optimization under the maximum satisfiability (MaxSAT) paradigm. Going beyond NP, we propose novel MaxSAT-based counterexample-guided abstraction refinement procedures for the second-level complete problems and present empirical results on a prototype system constituting the first approach to extension enforcement in its generality.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–40},
numpages = {40}
}

@article{10.5555/3176788.3176807,
author = {Gent, Ian P. and Jefferson, Christopher and Nightingale, Peter},
title = {Complexity of N-Queens Completion},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {The n-Queens problem is to place n chess queens on an n by n chessboard so that no two queens are on the same row, column or diagonal. The n-Queens Completion problem is a variant, dating to 1850, in which some queens are already placed and the solver is asked to place the rest, if possible. We show that n-Queens Completion is both NP-Complete and #P-Complete. A corollary is that any non-attacking arrangement of queens can be included as a part of a solution to a larger n-Queens problem. We introduce generators of random instances for n-Queens Completion and the closely related Blocked n-Queens and Excluded Diagonals Problem. We describe three solvers for these problems, and empirically analyse the hardness of randomly generated instances. For Blocked n-Queens and the Excluded Diagonals Problem, we show the existence of a phase transition associated with hard instances as has been seen in other NP-Complete problems, but a natural generator for n-Queens Completion did not generate consistently hard instances. The significance of this work is that the n-Queens problem has been very widely used as a benchmark in Artificial Intelligence, but conclusions on it are often disputable because of the simple complexity of the decision problem. Our results give alternative benchmarks which are hard theoretically and empirically, but for which solving techniques designed for n-Queens need minimal or no change.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {815–848},
numpages = {34}
}

@article{10.5555/3176788.3176806,
author = {Allen, Thomas E. and Goldsmith, Judy and Justice, Hayden Elizabeth and Mattei, Nicholas and Raines, Kayla},
title = {Uniform Random Generation and Dominance Testing for CP-Nets},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {The generation of preferences represented as CP-nets for experiments and empirical testing has typically been done in an ad hoc manner that may have introduced a large statistical bias in previous experimental work. We present novel polynomial-time algorithms for generating CP-nets with n nodes and maximum in-degree c uniformly at random. We extend this result to several statistical cultures commonly used in the social choice and preference reasoning literature. A CP-net is composed of both a graph and underlying cp-statements; our algorithm is the first to provably generate both the graph structure and cp-statements, and hence the underlying preference orders themselves, uniformly at random. We have released this code as a free and open source project. We use the uniform generation algorithm to investigate the maximum and expected flipping lengths, i.e., the maximum length over all outcomes o1 and o2, of a minimal proof that o1 is preferred to o2. Using our new statistical evidence, we conjecture that, for CP-nets with binary variables and complete conditional preference tables, the expected flipping length is polynomial in the number of preference variables. This has positive implications for the usability of CP-nets as compact preference models.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {771–813},
numpages = {43}
}

@article{10.5555/3176788.3176805,
author = {Sonu, Ekhlas and Chen, Yingke and Doshi, Prashant},
title = {Decision-Theoretic Planning under Anonymity in Agent Populations},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We study the problem of self-interested planning under uncertainty in settings shared with more than a thousand other agents, each of which plans at its own individual level. We refer to such large numbers of agents as an agent population. The decision-theoretic formalism of interactive partially observable Markov decision process (I-POMDP) is used to model the agent's self-interested planning. The first contribution of this article is a method for drastically scaling the finitely-nested I-POMDP to certain agent populations for the first time. Our method exploits two types of structure that is often exhibited by agent populations - anonymity and context-specific independence. We present a variant called the many-agent I-POMDP that models both these types of structure to plan efficiently under uncertainty in multiagent settings. In particular, the complexity of the belief update and solution in the many-agent I-POMDP is polynomial in the number of agents compared with the exponential growth that challenges the original framework.While exploiting structure helps mitigate the curse of many agents, the well-known curse of history that afflicts I-POMDPs continues to challenge scalability in terms of the planning horizon. The second contribution of this article is an application of the branch-and-bound scheme to reduce the exponential growth of the search tree for look ahead. For this, we introduce new fast-computing upper and lower bounds for the exact value function of the many-agent I-POMDP. This speeds up the look-ahead computations without trading off optimality, and reduces both memory and run time complexity. The third contribution is a comprehensive empirical evaluation of the methods on three new problems domains - policing large protests, controlling traffic congestion at a busy intersection, and improving the AI for the popular Clash of Clans multiplayer game. We demonstrate the feasibility of exact self-interested planning in these large problems, and that our methods for speeding up the planning are effective. Altogether, these contributions represent a principled and significant advance toward moving self-interested planning under uncertainty to real-world applications.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {725–770},
numpages = {46}
}

@article{10.5555/3176788.3176804,
author = {Davis, Ernest},
title = {Logical Formalizations of Commonsense Reasoning: A Survey},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Commonsense reasoning is in principle a central problem in artificial intelligence, but it is a very difficult one. One approach that has been pursued since the earliest days of the field has been to encode commonsense knowledge as statements in a logic-based representation language and to implement commonsense reasoning as some form of logical inference. This paper surveys the use of logic-based representations of commonsense knowledge in artificial intelligence research.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {651–723},
numpages = {73}
}

@article{10.5555/3176788.3176803,
author = {Wah, Elaine and Wright, Mason and Wellman, Michael P.},
title = {Welfare Effects of Market Making in Continuous Double Auctions},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We investigate the effects of market making on market performance, focusing on allocative efficiency as well as gains from trade accrued by background traders. We employ empirical simulation-based methods to evaluate heuristic strategies for market makers as well as background investors in a variety of complex trading environments. Our market model incorporates private and common valuation elements, with dynamic fundamental value and asymmetric information. In this context, we compare the surplus achieved by background traders in strategic equilibrium, with and without a market maker. Our findings indicate that the presence of the market maker strongly tends to increase total welfare across various environments. Market-maker profit may or may not exceed the welfare gain, thus the effect on background-investor surplus is ambiguous. We find that market making tends to benefit investors in relatively thin markets, and situations where background traders are impatient, due to limited trading opportunities. The presence of additional market makers increases these benefits, as competition drives the market makers to provide liquidity at lower price spreads. A thorough sensitivity analysis indicates that these results are robust to reasonable changes in model parameters.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {613–650},
numpages = {38}
}

@article{10.5555/3176788.3176802,
author = {Hunter, Anthony and Thimm, Matthias},
title = {Probabilistic Reasoning with Abstract Argumentation Frameworks},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Abstract argumentation offers an appealing way of representing and evaluating arguments and counterarguments. This approach can be enhanced by considering probability assignments on arguments, allowing for a quantitative treatment of formal argumentation. In this paper, we regard the assignment as denoting the degree of belief that an agent has in an argument being acceptable. While there are various interpretations of this, an example is how it could be applied to a deductive argument. Here, the degree of belief that an agent has in an argument being acceptable is a combination of the degree to which it believes the premises, the claim, and the derivation of the claim from the premises. We consider constraints on these probability assignments, inspired by crisp notions from classical abstract argumentation frameworks and discuss the issue of probabilistic reasoning with abstract argumentation frameworks. Moreover, we consider the scenario when assessments on the probabilities of a subset of the arguments are given and the probabilities of the remaining arguments have to be derived, taking both the topology of the argumentation framework and principles of probabilistic reasoning into account. We generalise this scenario by also considering inconsistent assessments, i.e., assessments that contradict the topology of the argumentation framework. Building on approaches to inconsistency measurement, we present a general framework to measure the amount of conict of these assessments and provide a method for inconsistency-tolerant reasoning.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {565–611},
numpages = {47}
}

@article{10.5555/3176788.3176801,
author = {Hew, Patrick Chisan},
title = {The Length of Shortest Vertex Paths in Binary Occupancy Grids Compared to Shortest R-Constrained Ones},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We study the problem of finding a short path from a start to a goal within a two-dimensional continuous and isotropic terrain that has been discretized into an array of accessible and blocked cells. A classic approach obtains a grid path where each step is along the edge of an accessible cell or diagonally across one. Grid paths su_er from 'digitization bias' - even if two locations have line-of-sight, the minimum travelling cost between them can be greater than the distance along the line-of-sight. In a vertex path, steps are allowed from a cell corner to any other cell corner if they have line-of-sight. While the 'digitization bias' is smaller, shortest vertex paths are impractical to find by brute force. Recent research has thus turned to methods for finding short (but not necessarily shortest) vertex paths. To establish the methods' potential utility, we calculate upper bounds on the difference in length between the shortest vertex paths versus the shortest r-constrained ones where an r-constrained path consists of line segments that each traverse at most r rows and at most r columns of cells. The difference in length reduces as r increases - indeed the shortest vertex paths are at most 1 percent shorter than the shortest 4-constrained ones. This article will be useful to developers and users of short(est) vertex paths algorithms who want to trade path length for improved runtimes in a predictable manner.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {543–563},
numpages = {21}
}

@article{10.5555/3176788.3176800,
author = {Ramakrishnan, Ramya and Zhang, Chongjie and Shah, Julie},
title = {Perturbation Training for Human-Robot Teams},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {In this work, we design and evaluate a computational learning model that enables a human-robot team to co-develop joint strategies for performing novel tasks that require coordination. The joint strategies are learned through "perturbation training," a human team-training strategy that requires team members to practice variations of a given task to help their team generalize to new variants of that task. We formally define the problem of human-robot perturbation training and develop and evaluate the first end-to-end framework for such training, which incorporates a multi-agent transfer learning algorithm, human-robot co-learning framework and communication protocol. Our transfer learning algorithm, Adaptive Perturbation Training (AdaPT), is a hybrid of transfer and reinforcement learning techniques that learns quickly and robustly for new task variants. We empirically validate the benefits of AdaPT through comparison to other hybrid reinforcement and transfer learning techniques aimed at transferring knowledge from multiple source tasks to a single target task.We also demonstrate that AdaPT's rapid learning supports live interaction between a person and a robot, during which the human-robot team trains to achieve a high level of performance for new task variants. We augment AdaPT with a co-learning framework and a computational bi-directional communication protocol so that the robot can co-train with a person during live interaction. Results from large-scale human subject experiments (n=48) indicate that AdaPT enables an agent to learn in a manner compatible with a human's own learning process, and that a robot undergoing perturbation training with a human results in a high level of team performance. Finally, we demonstrate that human-robot training using AdaPT in a simulation environment produces effective performance for a team incorporating an embodied robot partner.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {495–541},
numpages = {47}
}

@article{10.5555/3176788.3176799,
author = {Cai, Shaowei and Lin, Jinkun and Luo, Chuan},
title = {Finding a Small Vertex Cover in Massive Sparse Graphs: Construct, Local Search, and Preprocess},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {The problem of finding a minimum vertex cover (MinVC) in a graph is a well known NP-hard combinatorial optimization problem of great importance in theory and practice. Due to its NP-hardness, there has been much interest in developing heuristic algorithms for finding a small vertex cover in reasonable time. Previously, heuristic algorithms for MinVC have focused on solving graphs of relatively small size, and they are not suitable for solving massive graphs as they usually have high-complexity heuristics. This paper explores techniques for solving MinVC in very large scale real-world graphs, including a construction algorithm, a local search algorithm and a preprocessing algorithm. Both the construction and search algorithms are based on low-complexity heuristics, and we combine them to develop a heuristic algorithm for MinVC called FastVC. Experimental results on a broad range of real-world massive graphs show that, our algorithms are very fast and have better performance than previous heuristic algorithms for MinVC. We also develop a preprocessing algorithm to simplify graphs for MinVC algorithms. By applying the preprocessing algorithm to local search algorithms, we obtain two efficient MinVC solvers called NuMVC2+p and FastVC2+p, which show further improvement on the massive graphs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {463–494},
numpages = {32}
}

@article{10.5555/3176788.3176798,
author = {Li, Yuqian and Conitzer, Vincent},
title = {Game-Theoretic Question Selection for Tests},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Conventionally, the questions on a test are assumed to be kept secret from test takers until the test. However, for tests that are taken on a large scale, particularly asynchronously, this is very hard to achieve. For example, TOEFL iBT and driver's license test questions are easily found online. This also appears likely to become an issue for Massive Open Online Courses (MOOCs, as offered for example by Coursera, Udacity, and edX). Specifically, the test result may not reflect the true ability of a test taker if questions are leaked beforehand.In this paper, we take the loss of confidentiality as a fact. Even so, not all hope is lost as the test taker can memorize only a limited set of questions' answers, and the tester can randomize which questions to let appear on the test. We model this as a Stackelberg game, where the tester commits to a mixed strategy and the follower responds. Informally, the goal of the tester is to best reveal the true ability of a test taker, while the test taker tries to maximize the test result (pass probability or score). We provide an exponential-size linear program formulation that computes the optimal test strategy, prove several NP-hardness results on computing optimal test strategies in general, and give efficient algorithms for special cases (scored tests and single-question tests). Experiments are also provided for those proposed algorithms to show their scalability and the increase of the tester's utility relative to that of the uniform-at-random strategy. The increase is quite significant when questions have some correlation--for example, when a test taker who can solve a harder question can always solve easier questions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {437–462},
numpages = {26}
}

@article{10.5555/3176788.3176797,
author = {Otten, Lars and Dechter, Rina},
title = {AND/OR Branch-and-Bound on a Computational Grid},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We present a parallel AND/OR Branch-and-Bound scheme that uses the power of a computational grid to push the boundaries of feasibility for combinatorial optimization. Two variants of the scheme are described, one of which aims to use machine learning techniques for parallel load balancing. In-depth analysis identifies two inherent sources of parallel search space redundancies that, together with general parallel execution overhead, can impede parallelization and render the problem far from embarrassingly parallel. We conduct extensive empirical evaluation on hundreds of CPUs, the first of its kind, with overall positive results. In a significant number of cases parallel speedup is close to the theoretical maximum and we are able to solve many very complex problem instances orders of magnitude faster than before; yet analysis of certain results also serves to demonstrate the inherent limitations of the approach due to the aforementioned redundancies.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {351–435},
numpages = {85}
}

@article{10.5555/3176788.3176796,
author = {Tassa, Tamir and Grinshpoun, Tal and Zivan, Roie},
title = {Privacy Preserving Implementation of the Max-Sum Algorithm and Its Variants},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {One of the basic motivations for solving DCOPs is maintaining agents' privacy. Thus, researchers have evaluated the privacy loss of DCOP algorithms and defined corresponding notions of privacy preservation for secured DCOP algorithms. However, no secured protocol was proposed for MAX-SUM, which is among the most studied DCOP algorithms. As part of the ongoing effort of designing secure DCOP algorithms, we propose P-MAX-SUM, the first private algorithm that is based on Max-Sum. The proposed algorithm has multiple agents preforming the role of each node in the factor graph, on which the MAX-SUM algorithm operates. P-MAX-SUM preserves three types of privacy: topology privacy, constraint privacy, and assignment/decision privacy. By allowing a single call to a trusted coordinator, P-MAX-SUM also preserves agent privacy. The two main cryptographic means that enable this privacy preservation are secret sharing and homomorphic encryption. In addition, we design privacy-preserving implementations of four variants of Max-Sum. We conclude by analyzing the price of privacy in terns of runtime overhead, both theoretically and by extensive experimentation.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {311–349},
numpages = {39}
}

@article{10.5555/3176788.3176795,
author = {Farina, Gabriele and Gatti, Nicola},
title = {Adopting the Cascade Model in Ad Auctions: Efficiency Bounds and Truthful Algorithmic Mechanisms},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Sponsored Search Auctions (SSAs) are one of the most successful applications of microeconomic mechanisms, with a revenue of about $72 billion in the US alone in 2016. However, the problem of designing the best economic mechanism for sponsored search auctions is far from being solved, and, given the amount at stake, it is no surprise that it has received growing attention over the past few years. The most common auction mechanism for SSAs is the Generalized Second Price (GSP). However, the GSP is known not to be truthful: the agents participating in the auction might have an incentive to report false values, generating economic inefficiency and suboptimal revenues in turn. Superior, efficient truthful mechanisms, such as the Vickrey-Clarke-Groves (VCG) auction, are well known in the literature. However, while the VCG auction is currently adopted for the strictly related scenario of contextual advertising, e.g., by Google and Facebook, companies are reluctant to extend it to SSAs, fearing prohibitive switching costs. Other than truthfulness, two issues are of paramount importance in designing effective SSAs. First, the choice of the user model; not only does an accurate user model better target ads to users, it also is a critical factor in reducing the inefficiency of the mechanism. Often an antagonist to this, the second issue is the running time of the mechanism, given the performance pressure these mechanisms undertake in real-world applications. In our work, we argue in favor of adopting the VCG mechanism based on the cascade model with ad/position externalities (APDC-VCG). Our study includes both the derivation of inefficiency bounds and the design and the experimental evaluation of exact and approximate algorithms.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {265–310},
numpages = {46}
}

@article{10.5555/3176788.3176794,
author = {Ahmed, Asrar and Varakantham, Pradeep and Lowalekar, Meghna and Adulyasak, Yossiri and Jaillet, Patrick},
title = {Sampling Based Approaches for Minimizing Regret in Uncertain Markov Decision Processes (MDPs)},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Markov Decision Processes (MDPs) are an effective model to represent decision processes in the presence of transitional uncertainty and reward tradeoffs. However, due to the difficulty in exactly specifying the transition and reward functions in MDPs, researchers have proposed uncertain MDP models and robustness objectives in solving those models. Most approaches for computing robust policies have focused on the computation of maximin policies which maximize the value in the worst case amongst all realisations of uncertainty. Given the overly conservative nature of maximin policies, recent work has proposed minimax regret as an ideal alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only and they are also limited in their scalability. Therefore, we provide a general model of uncertain MDPs that considers uncertainty over both transition and reward functions. Furthermore, we also consider dependence of the uncertainty across different states and decision epochs. We also provide a mixed integer linear program formulation for minimizing regret given a set of samples of the transition and reward functions in the uncertain MDP. In addition, we provide two myopic variants of regret, namely Cumulative Expected Myopic Regret (CEMR) and One Step Regret (OSR) that can be optimized in a scalable manner. Specifically, we provide dynamic programming and policy iteration based algorithms to optimize CEMR and OSR respectively. Finally, to demonstrate the effectiveness of our approaches, we provide comparisons on two benchmark problems from literature. We observe that optimizing the myopic variants of regret, OSR and CEMR are better than directly optimizing the regret.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {229–264},
numpages = {36}
}

@article{10.5555/3176788.3176793,
author = {Stein, Sebastian and Gerding, Enrico H. and Nedea, Adrian and Rosenfeld, Avi and Jennings, Nicholas R.},
title = {Market Interfaces for Electric Vehicle Charging},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We consider settings where owners of electric vehicles (EVs) participate in a market mechanism to charge their vehicles. Existing work on such mechanisms has typically assumed that participants are fully rational and can report their preferences accurately via some interface to the mechanism or to a software agent participating on their behalf. However, this may not be reasonable in settings with non-expert human end-users. Thus, our overarching aim in this paper is to determine experimentally if a fully expressive market interface that enables accurate preference reports is suitable for the EV charging domain, or, alternatively, if a simpler, restricted interface that reduces the space of possible options is preferable. In doing this, we measure the performance of an interface both in terms of how it helps participants maximise their utility and how it affects deliberation time. Our secondary objective is to contrast two different types of restricted interfaces that vary in how they restrict the space of preferences that can be reported. To enable this analysis, we develop a novel game that replicates key features of an abstract EV charging scenario. In two experiments with over 300 users, we show that restricting the users' preferences significantly reduces the time they spend deliberating (by up to half in some cases). An extensive usability survey confirms that this restriction is furthermore associated with a lower perceived cognitive burden on the users. More surprisingly, at the same time, using restricted interfaces leads to an increase in the users' performance compared to the fully expressive interface (by up to 70%). We also show that some restricted interfaces have the desirable effect of reducing the energy consumption of their users by up to 20% while achieving the same utility as other interfaces. Finally, we find that a reinforcement learning agent displays similar performance trends to human users, enabling a novel methodology for evaluating market interfaces.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {175–227},
numpages = {53}
}

@article{10.5555/3176788.3176792,
author = {Bredereck, Robert and Chen, Jiehua and Niedermeier, Rolf and Walsh, Toby},
title = {Parliamentary Voting Procedures: Agenda Control, Manipulation, and Uncertainty},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {We study computational problems for two popular parliamentary voting procedures: the amendment procedure and the successive procedure. They work in multiple stages where the result of each stage may influence the result of the next stage. Both procedures proceed according to a given linear order of the alternatives, an agenda. We obtain the following results for both voting procedures: On the one hand, deciding whether one can make a specific alternative win by reporting insincere preferences by the fewest number of voters, the Coalitional Manipulation problem, or whether there is a suitable ordering of the agenda, the Agenda Control problem, takes polynomial time. On the other hand, our experimental studies with real-world data indicate that most preference profiles cannot be manipulated by only few voters and a successful agenda control is typically impossible. If the voters' preferences are incomplete, then deciding whether an alternative can possibly win is NP-hard for both procedures. Whilst deciding whether an alternative necessarily wins is coNP-hard for the amendment procedure, it is polynomial-time solvable for the successive procedure.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {133–173},
numpages = {41}
}

@article{10.5555/3176788.3176791,
author = {Chen, Yi-Chun and Wheeler, Tim A. and Kochenderfer, Mykel J.},
title = {Learning Discrete Bayesian Networks from Continuous Data},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Learning Bayesian networks from raw data can help provide insights into the relationships between variables. While real data often contains a mixture of discrete and continuous-valued variables, many Bayesian network structure learning algorithms assume all random variables are discrete. Thus, continuous variables are often discretized when learning a Bayesian network. However, the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the established minimum description length algorithm. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {103–132},
numpages = {30}
}

@article{10.5555/3176788.3176790,
author = {Roughgarden, Tim and Syrgkanis, Vasilis and Tardos, \'{E}va},
title = {The Price of Anarchy in Auctions},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {This survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings. This theory complements traditional economic techniques, which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings.We highlight three user-friendly analytical tools: smoothness-type inequalities, which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies; extension theorems, which extend such guarantees to randomized strategies, no-regret learning outcomes, and incomplete-information settings; and composition theorems, which extend such guarantees from simpler to more complex auctions. Combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {59–101},
numpages = {43}
}

@article{10.5555/3176788.3176789,
author = {Motzek, Alexander and M\"{o}ller, Ralf},
title = {Indirect Causes in Dynamic Bayesian Networks Revisited},
year = {2017},
issue_date = {May 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {59},
number = {1},
issn = {1076-9757},
abstract = {Modeling causal dependencies often demands cycles at a coarse-grained temporal scale. If Bayesian networks are to be used for modeling uncertainties, cycles are eliminated with dynamic Bayesian networks, spreading indirect dependencies over time and enforcing an infinitesimal resolution of time. Without a "causal design," i.e., without anticipating indirect influences appropriately in time, we argue that such networks return spurious results. By identifying activator random variables, we propose activator dynamic Bayesian networks (ADBNs) which are able to rapidly adapt to contexts under a causal use of time, anticipating indirect influences on a solid mathematical basis using familiar Bayesian network semantics. ADBNs are well-defined dynamic probabilistic graphical models allowing one to model cyclic dependencies from local and causal perspectives while preserving a classical, familiar calculus and classically known algorithms, without introducing any overhead in modeling or inference.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–58},
numpages = {58}
}

@article{10.5555/3176764.3176787,
author = {Ghooshchi, Nina Ghanbari and Namazi, Majid and Newton, M. A. Hakim and Sattar, Abdul},
title = {Encoding Domain Transitions for Constraint-Based Planning},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {We describe a constraint-based automated planner named Transition Constraints for Parallel Planning (TCPP). TCPP constructs its constraint model from a redefined version of the domain transition graphs (DTG) of a given planning problem. TCPP encodes state transitions in the redefined DTGs by using table constraints with cells containing don't cares or wild cards. TCPP uses Minion the constraint solver to solve the constraint model and returns a parallel plan. We empirically compare TCPP with the other state-of-theart constraint-based parallel planner PaP2. PaP2 encodes action successions in the finite state automata (FSA) as table constraints with cells containing sets of values. PaP2 uses SICStus Prolog as its constraint solver. We also improve PaP2 by using don't cares and mutex constraints. Our experiments on a number of standard classical planning benchmark domains demonstrate TCPP's efficiency over the original PaP2 running on SICStus Prolog and our reconstructed and enhanced versions of PaP2 running on Minion.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {905–966},
numpages = {62}
}

@article{10.5555/3176764.3176786,
author = {Miller, Tim and Pfau, Jens and Sonenberg, Liz and Kashima, Yoshihisa},
title = {Logics of Common Ground},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {According to Clark's seminal work on common ground and grounding, participants collaborating in a joint activity rely on their shared information, known as common ground, to perform that activity successfully, and continually align and augment this information during their collaboration. Similarly, teams of human and artificial agents require common ground to successfully participate in joint activities. Indeed, without appropriate information being shared, using agent autonomy to reduce the workload on humans may actually increase workload as the humans seek to understand why the agents are behaving as they are. While many researchers have identified the importance of common ground in artificial intelligence, there is no precise definition of common ground on which to build the foundational aspects of multi-agent collaboration. In this paper, building on previously-defined modal logics of belief, we present logic definitions for four different types of common ground. We define modal logics for three existing notions of common ground and introduce a new notion of common ground, called salient common ground. Salient common ground captures the common ground of a group participating in an activity and is based on the common ground that arises from that activity as well as on the common ground they shared prior to the activity. We show that the four definitions share some properties, and our analysis suggests possible refinements of the existing informal and semi-formal definitions.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {859–904},
numpages = {46}
}

@article{10.5555/3176764.3176785,
author = {Abseher, Michael and Musliu, Nysret and Woltran, Stefan},
title = {Improving the Efficiency of Dynamic Programming on Tree Decompositions via Machine Learning},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Dynamic Programming (DP) over tree decompositions is a well-established method to solve problems - that are in general NP-hard - efficiently for instances of small treewidth. Experience shows that (i) heuristically computing a tree decomposition has negligible runtime compared to the DP step; and (ii) DP algorithms exhibit a high variance in runtime when using different tree decompositions; in fact, given an instance of the problem at hand, even decompositions of the same width might yield extremely diverging runtimes. We thus propose here a novel and general method that is based on selection of the best decomposition from an available pool of heuristically generated ones. For this purpose, we require machine learning techniques that provide automated selection based on features of the decomposition rather than on the actual problem instance. Thus, one main contribution of this work is to propose novel features for tree decompositions. Moreover, we report on extensive experiments in different problem domains which show a significant speedup when choosing the tree decomposition according to this concept over simply using an arbitrary one of the same width.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {829–858},
numpages = {30}
}

@article{10.5555/3176764.3176784,
author = {Anshelevich, Elliot and Postl, John},
title = {Randomized Social Choice Functions under Metric Preferences},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {We determine the quality of randomized social choice algorithms in a setting in which the agents have metric preferences: every agent has a cost for each alternative, and these costs form a metric. We assume that these costs are unknown to the algorithms (and possibly even to the agents themselves), which means we cannot simply select the optimal alternative, i.e. the alternative that minimizes the total agent cost (or median agent cost). However, we do assume that the agents know their ordinal preferences that are induced by the metric space. We examine randomized social choice functions that require only this ordinal information and select an alternative that is good in expectation with respect to the costs from the metric. To quantify how good a randomized social choice function is, we bound the distortion, which is the worst-case ratio between the expected cost of the alternative selected and the cost of the optimal alternative. We provide new distortion bounds for a variety of randomized algorithms, for both general metrics and for important special cases. Our results show a sizable improvement in distortion over deterministic algorithms.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {797–827},
numpages = {31}
}

@article{10.5555/3176764.3176783,
author = {Shperberg, Shahaf S. and Shimony, Solomon Eyal},
title = {Some Properties of Batch Value of Information in the Selection Problem},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Given a set of items of unknown utility, we need to select one with a utility as high as possible ("the selection problem"). Measurements (possibly noisy) of item values prior to selection are allowed, at a known cost. The goal is to optimize the overall sequential decision process of measurements and selection.Value of information (VOI) is a well-known scheme for selecting measurements, but the intractability of the problem typically leads to using myopic VOI estimates. Other schemes have also been proposed, some with approximation guarantees, based on submodularity criteria. However, it was observed that the VOI is not submodular in general. In this paper we examine theoretical properties of VOI for the selection problem, and identify cases of submodularity and supermodularity. We suggest how to use these properties to compute approximately optimal measurement batch policies, with an example based on a "wine selection problem".},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {777–796},
numpages = {20}
}

@article{10.5555/3176764.3176782,
author = {Bauters, Kim and McAreavey, Kevin and Liu, Weiru and Hong, Jun and Godo, Llu\'{\i}s},
title = {Managing Different Sources of Uncertainty in a BDI Framework in a Principled Way with Tractable Fragments},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {The Belief-Desire-Intention (BDI) architecture is a practical approach for modelling large-scale intelligent systems. In the BDI setting, a complex system is represented as a network of interacting agents - or components - each one modelled based on its beliefs, desires and intentions. However, current BDI implementations are not well-suited for modelling more realistic intelligent systems which operate in environments pervaded by different types of uncertainty. Furthermore, existing approaches for dealing with uncertainty typically do not offer syntactical or tractable ways of reasoning about uncertainty. This complicates their integration with BDI implementations, which heavily rely on fast and reactive decisions. In this paper, we advance the state-of-the-art w.r.t. handling different types of uncertainty in BDI agents. The contributions of this paper are, first, a new way of modelling the beliefs of an agent as a set of epistemic states. Each epistemic state can use a distinct underlying uncertainty theory and revision strategy, and commensurability between epistemic states is achieved through a stratification approach. Second, we present a novel syntactic approach to revising beliefs given unreliable input. We prove that this syntactic approach agrees with the semantic definition, and we identify expressive fragments that are particularly useful for resource-bounded agents. Third, we introduce full operational semantics that extend Can, a popular semantics for BDI, to establish how reasoning about uncertainty can be tightly integrated into the BDI framework. Fourth, we provide comprehensive experimental results to highlight the usefulness and feasibility of our approach, and explain how the generic epistemic state can be instantiated into various representations.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {731–775},
numpages = {45}
}

@article{10.5555/3176764.3176781,
author = {Zhou, Hao and Zhang, Yue and Cheng, Chuan and Huang, Shujian and Dai, Xinyu and Chen, Jiajun},
title = {A Neural Probabilistic Structured-Prediction Method for Transition-Based Natural Language Processing},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {We propose a neural probabilistic structured-prediction method for transition-based natural language processing, which integrates beam search and contrastive learning. The method uses a global optimization model, which can leverage arbitrary features over nonlocal context. Beam search is used for efficient heuristic decoding, and contrastive learning is performed for adjusting the model according to search errors. When evaluated on both chunking and dependency parsing tasks, the proposed method achieves significant accuracy improvements over the locally normalized greedy baseline on the two tasks, respectively.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {703–729},
numpages = {27}
}

@article{10.5555/3176764.3176780,
author = {Onta\~{n}\'{o}n, Santiago},
title = {Combinatorial Multi-Armed Bandits for Real-Time Strategy Games},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Games with large branching factors pose a significant challenge for game tree search algorithms. In this paper, we address this problem with a sampling strategy for Monte Carlo Tree Search (MCTS) algorithms called na\"{\i}ve sampling, based on a variant of the Multiarmed Bandit problem called Combinatorial Multi-armed Bandits (CMAB). We analyze the theoretical properties of several variants of na\"{\i}ve sampling, and empirically compare it against the other existing strategies in the literature for CMABs. We then evaluate these strategies in the context of real-time strategy (RTS) games, a genre of computer games characterized by their very large branching factors. Our results show that as the branching factor grows, na\"{\i}ve sampling outperforms the other sampling strategies.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {665–702},
numpages = {38}
}

@article{10.5555/3176764.3176779,
author = {Gluz, Jo\~{a}o and Jaques, Patricia A.},
title = {A Probabilistic Formalization of the Appraisal for the OCC Event-Based Emotions},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {This article presents a logical formalization of the emotional appraisal theory, i.e., it formalizes the cognitive process of evaluation that elicits an emotion. This formalization is psychologically grounded on the OCC cognitive model of emotions. More specifically, we are interested in event-based emotions, i.e., emotions that are elicited by the evaluation of the consequences of an event that either happened or will happen. The formal modelling presented here is based on the AfPL Probabilistic Logic, a BDI-like probabilistic modal logic, which allows our model to verify whether the variables that determine the elicitation of emotions achieved the necessary threshold or not. The proposed logical formalization aims at addressing how the emotions are elicited by the agent cognitive mental states (desires, beliefs and intentions), and how to represent the intensity of the emotions. These are important initial points in the investigation of the dynamic interaction among emotions and other mental states.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {627–664},
numpages = {38}
}

@article{10.5555/3176764.3176778,
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
title = {Explicit Document Modeling through Weighted Multiple-Instance Learning},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Representing documents is a crucial component in many NLP tasks, for instance predicting aspect ratings in reviews. Previous methods for this task treat documents globally, and do not acknowledge that target categories are often assigned by their authors with generally no indication of the specific sentences that motivate them. To address this issue, we adopt a weakly supervised learning model, which jointly learns to focus on relevant parts of a document according to the context along with a classifier for the target categories. Derived from the weighted multiple-instance regression (MIR) framework, the model learns decomposable document vectors for each individual category and thus overcomes the representational bottleneck in previous methods due to a fixed-length document vector. During prediction, the estimated relevance or saliency weights explicitly capture the contribution of each sentence to the predicted rating, thus offering an explanation of the rating. Our model achieves state-of-the-art performance on multi-aspect sentiment analysis, improving over several baselines. Moreover, the predicted saliency weights are close to human estimates obtained by crowdsourcing, and increase the performance of lexical and topical features for review segmentation and summarization.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {591–626},
numpages = {36}
}

@article{10.5555/3176764.3176777,
author = {Tran, Tony T. and Vaquero, Tiago and Nejat, Goldie and Beck, J. Christopher},
title = {Robots in Retirement Homes: Applying off-the-Shelf Planning and Scheduling to a Team of Assistive Robots},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {This paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. The models proposed make use of standard techniques and solvers developed in AI planning and scheduling, with two primary motivations. First, to find a planning and scheduling solution that we can deploy in our real-world application. Second, to evaluate planning and scheduling technology in terms of the "model-and-solve" functionality that forms a major research goal in both domain-independent planning and constraint programming. Seven variations of our application are studied using the following three technologies: PDDL-based planning, time-line planning and scheduling, and constraint-based scheduling. The variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. We evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. PDDL-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. It would be misleading to conclude that constraint programming is "better" than PDDL-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. Nonetheless, we believe our investigation is valuable for AI planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of AI planning and scheduling for similar robotics problems. In particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523–590},
numpages = {68}
}

@article{10.5555/3176764.3176776,
author = {Lai, Yong and Liu, Dayou and Yin, Minghao},
title = {New Canonical Representations by Augmenting OBDDs with Conjunctive Decomposition},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {We identify two families of canonical knowledge compilation languages. Both families augment ROBDD with conjunctive decomposition bounded by an integer i ranging from 0 to ∞. In the former, the decomposition is finest and the decision respects a chain C of variables, while both the decomposition and decision of the latter respect a tree τ of variables. In particular, these two families cover three existing languages ROBDD, ROBDD with as many implied literals as possible, and AND/OR BDD. We demonstrate that each language in the first family is complete, while each one in the second family is incomplete with expressivity that does not decrease with incremental i. We also demonstrate that the succinctness does not decrease from the i-th language in the second family to the i-th language in the first family, and then to the (i+1)-th language in the first family. For the operating efficiency, on the one hand, we show that the two families of languages support a rich class of tractable logical operations, and particularly the tractability of each language in the second family is not less than that of ROBDD; and on the other hand, we introduce a new time efficiency criterion called rapidity which reflects the idea that exponential operations may be preferable if the language can be exponentially more succinct, and we demonstrate that the rapidity of each operation does not decrease from the i-th language in the second family to the i-th language in the first family, and then to the (i + 1)-th language in the first family. Furthermore, we develop a compiler for the last language in the first family (i = ∞). Empirical results show that the compiler significantly advances the compiling efficiency of canonical representations. In fact, its compiling efficiency is comparable with that of the state-of-the-art compilers of non-canonical representations. We also provide a compiler for the i-th language in the first family by translating the last language in the first family into the i-th language (i &lt; ∞). Empirical results show that we can sometimes use the i-th language instead of the last language without any obvious loss of space efficiency.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {453–521},
numpages = {69}
}

@article{10.5555/3176764.3176775,
author = {Aleksandrowicz, Gadi and Chockler, Hana and Halpern, Joseph Y. and Ivrii, Alexander},
title = {The Computational Complexity of Structure-Based Causality},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Halpern and Pearl introduced a definition of actual causality; Eiter and Lukasiewicz showed that computing whether X = x is a cause of Y = y is NP-complete in binary models (where all variables can take on only two values) and Σ2P-complete in general models. In the final version of their paper, Halpern and Pearl slightly modified the definition of actual cause, in order to deal with problems pointed out by Hopkins and Pearl. As we show, this modification has a nontrivial impact on the complexity of computing whether X = x is a cause of Y = y. To characterize the complexity, a new family DkP, k = 1, 2, 3, . . ., of complexity classes is introduced, which generalizes the class DP introduced by Papadimitriou and Yannakakis (DP is just D1P). We show that the complexity of computing causality under the updated definition is D2P-complete.Chockler and Halpern extended the definition of causality by introducing notions of responsibility and blame, and characterized the complexity of determining the degree of responsibility and blame using the original definition of causality. Here, we completely characterize the complexity using the updated definition of causality. In contrast to the results on causality, we show that moving to the updated definition does not result in a difference in the complexity of computing responsibility and blame.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {431–451},
numpages = {21}
}

@article{10.5555/3176764.3176774,
author = {Ghosh, Supriyo and Varakantham, Pradeep and Adulyasak, Yossiri and Jaillet, Patrick},
title = {Dynamic Repositioning to Reduce Lost Demand in Bike Sharing Systems},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Bike Sharing Systems (BSSs) are widely adopted in major cities of the world due to concerns associated with extensive private vehicle usage, namely, increased carbon emissions, traffic congestion and usage of non-renewable resources. In a BSS, base stations are strategically placed throughout a city and each station is stocked with a pre-determined number of bikes at the beginning of the day. Customers hire the bikes from one station and return them at another station. Due to unpredictable movements of customers hiring bikes, there is either congestion (more than required) or starvation (fewer than required) of bikes at base stations. Existing data has shown that congestion/starvation is a common phenomenon that leads to a large number of unsatisfied customers resulting in a significant loss in customer demand. In order to tackle this problem, we propose an optimisation formulation to reposition bikes using vehicles while also considering the routes for vehicles and future expected demand. Furthermore, we contribute two approaches that rely on decomposability in the problem (bike repositioning and vehicle routing) and aggregation of base stations to reduce the computation time significantly. Finally, we demonstrate the utility of our approach by comparing against two benchmark approaches on two real-world data sets of bike sharing systems. These approaches are evaluated using a simulation where the movements of customers are generated from real-world data sets.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {387–430},
numpages = {44}
}

@article{10.5555/3176764.3176773,
author = {Bodirsky, Manuel and Jonsson, Peter},
title = {A Model-Theoretic View on Qualitative Constraint Reasoning},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Qualitative reasoning formalisms are an active research topic in artificial intelligence. In this survey we present a model-theoretic perspective on qualitative constraint reasoning and explain some of the basic concepts and results in an accessible way. In particular, we discuss the significance of ω-categoricity for qualitative reasoning, of primitive positive interpretations for complexity analysis, and of Datalog as a unifying language for describing local consistency algorithms.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {339–385},
numpages = {47}
}

@article{10.5555/3176764.3176772,
author = {Erd\'{e}lyi, G\'{a}bor and Lackner, Martin and Pfandler, Andreas},
title = {Computational Aspects of Nearly Single-Peaked Electorates},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Manipulation, bribery, and control are well-studied ways of changing the outcome of an election. Many voting rules are, in the general case, computationally resistant to some of these manipulative actions. However when restricted to single-peaked electorates, these rules suddenly become easy to manipulate. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra studied the computational complexity of strategic behavior in nearly single-peaked electorates. These are electorates that are not single-peaked but close to it according to some distance measure.In this paper we introduce several new distance measures regarding single-peakedness. We prove that determining whether a given profile is nearly single-peaked is NP-complete in many cases. For one case we present a polynomial-time algorithm. In case the singlepeaked axis is given, we show that determining the distance is always possible in polynomial time. Furthermore, we explore the relations between the new notions introduced in this paper and existing notions from the literature.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {297–337},
numpages = {41}
}

@article{10.5555/3176764.3176771,
author = {Wang, Yiyuan and Cai, Shaowei and Yin, Minghao},
title = {Local Search for Minimum Weight Dominating Set with Two-Level Configuration Checking and Frequency Based Scoring Function},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {The Minimum Weight Dominating Set (MWDS) problem is an important generalization of the Minimum Dominating Set (MDS) problem with extensive applications. This paper proposes a new local search algorithm for the MWDS problem, which is based on two new ideas. The first idea is a heuristic called two-level configuration checking (CC2), which is a new variant of a recent powerful configuration checking strategy (CC) for effectively avoiding the recent search paths. The second idea is a novel scoring function based on the frequency of being uncovered of vertices. Our algorithm is called CC2FS, according to the names of the two ideas. The experimental results show that, CC2FS performs much better than some state-of-the-art algorithms in terms of solution quality on a broad range of MWDS benchmarks.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {267–295},
numpages = {29}
}

@article{10.5555/3176764.3176770,
author = {Ye, Nan and Somani, Adhiraj and Hsu, David and Lee, Wee Sun},
title = {DESPOT: Online POMDP Planning with Regularization},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {The partially observable Markov decision process (POMDP) provides a principled general framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the "curse of dimensionality" and the "curse of history". To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the "execution" of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for real-time vehicle control. The source code for the algorithm is available online.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {231–266},
numpages = {36}
}

@article{10.5555/3176764.3176769,
author = {Cussens, James and J\'{a}rvisalo, Matti and Korhonen, Janne H. and Bartlett, Mark},
title = {Bayesian Network Structure Learning with Integer Programming: Polytopes, Facets and Complexity},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {The challenging task of learning structures of probabilistic graphical models is an important problem within modern AI research. Recent years have witnessed several major algorithmic advances in structure learning for Bayesian networks--arguably the most central class of graphical models--especially in what is known as the score-based setting. A successful generic approach to optimal Bayesian network structure learning (BNSL), based on integer programming (IP), is implemented in the gobnilp system. Despite the recent algorithmic advances, current understanding of foundational aspects underlying the IP based approach to BNSL is still somewhat lacking. Understanding fundamental aspects of cutting planes and the related separation problem is important not only from a purely theoretical perspective, but also since it holds out the promise of further improving the efficiency of state-of-the-art approaches to solving BNSL exactly. In this paper, we make several theoretical contributions towards these goals: (i) we study the computational complexity of the separation problem, proving that the problem is NP-hard; (ii) we formalise and analyse the relationship between three key polytopes underlying the IP-based approach to BNSL; (iii) we study the facets of the three polytopes both from the theoretical and practical perspective, providing, via exhaustive computation, a complete enumeration of facets for low-dimensional family-variable polytopes; and, furthermore, (iv) we establish a tight connection of the BNSL problem to the acyclic subgraph problem.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {185–229},
numpages = {45}
}

@article{10.5555/3176764.3176768,
author = {Kurata, Ryoji and Hamada, Naoto and Iwasaki, Atsushi and Yokoo, Makoto},
title = {Controlled School Choice with Soft Bounds and Overlapping Types},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {School choice programs are implemented to give students/parents an opportunity to choose the public school the students attend. Controlled school choice programs need to provide choices for students/parents while maintaining distributional constraints on the composition of students, typically in terms of socioeconomic status. Previous works show that setting soft-bounds, which exibly change the priorities of students based on their types, is more appropriate than setting hard-bounds, which strictly limit the number of accepted students for each type. We consider a case where soft-bounds are imposed and one student can belong to multiple types, e.g., "financially-distressed" and "minority" types. We first show that when we apply a model that is a straightforward extension of an existing model for disjoint types, there is a chance that no stable matching exists. Thus we propose an alternative model and an alternative stability definition, where a school has reserved seats for each type. We show that a stable matching is guaranteed to exist in this model and develop a mechanism called Deferred Acceptance for Overlapping Types (DA-OT). The DA-OT mechanism is strategy-proof and obtains the student-optimal matching within all stable matchings. Furthermore, we introduce an extended model that can handle both type-specific ceilings and oors and propose a extended mechanism DA-OT* to handle the extended model. Computer simulation results illustrate that DA-OT outperforms an artificial cap mechanism where we set a hard-bound for each type in each school. DA-OT* can achieve stability in the extended model without sacrificing students' welfare.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {153–184},
numpages = {32}
}

@article{10.1613/jair.5282,
author = {Caragiannis, Ioannis and Nath, Swaprava and Procaccia, Ariel D. and Shah, Nisarg},
title = {Subset Selection via Implicit Utilitarian Voting},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.5282},
doi = {10.1613/jair.5282},
abstract = {How should one aggregate ordinal preferences expressed by voters into a measurably superior social choice? A well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings, and seeks voting rules that approximately maximize utilitarian social welfare. We extend this approach to the design of rules that select a subset of alternatives. We derive analytical bounds on the performance of optimal (deterministic as well as randomized) rules in terms of two measures, distortion and regret. Empirical results show that regret-based rules are more compelling than distortion-based rules, leading us to focus on developing a scalable implementation for the optimal (deterministic) regret-based rule. Our methods underlie the design and implementation of RoboVote.org, a not-for-profit website that helps users make group decisions via AI-driven voting methods.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {123–152},
numpages = {30}
}

@article{10.1613/jair.5249,
author = {Asai, Masataro and Fukunaga, Alex},
title = {Tie-Breaking Strategies for Cost-Optimal Best First Search},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.5249},
doi = {10.1613/jair.5249},
abstract = {Best-first search algorithms such as A* need to apply tie-breaking strategies in order to decide which node to expand when multiple search nodes have the same evaluation score. We investigate and improve tie-breaking strategies for cost-optimal search using A*. We first experimentally analyze the performance of common tie-breaking strategies that break ties according to the heuristic value of the nodes. We find that the tie-breaking strategy has a significant impact on search algorithm performance when there are 0-cost operators that induce large plateau regions in the search space. Based on this, we develop two new classes of tie-breaking strategies. We first propose a depth diversification strategy which breaks ties according to the distance from the entrance to the plateau, and then show that this new strategy significantly outperforms standard strategies on domains with 0-cost actions. Next, we propose a new framework for interpreting A* search as a series of satisficing searches within plateaus consisting of nodes with the same f-cost. Based on this framework, we investigate a second, new class of tie-breaking strategy, a multi-heuristic tie-breaking strategy which embeds inadmissible, distance-to-go variations of various heuristics within an admissible search. This is shown to further improve the performance in combination with the depth metric.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {67–121},
numpages = {55}
}

@article{10.1613/jair.5222,
author = {Guti\'{e}rrez-Basulto, V\'{\i}ctor and Jung, Jean Christoph and Lutz, Carsten and Schr\"{o}der, Lutz},
title = {Probabilistic Description Logics for Subjective Uncertainty},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.5222},
doi = {10.1613/jair.5222},
abstract = {We propose a family of probabilistic description logics (DLs) that are derived in a principled way from Halpern's probabilistic first-order logic. The resulting probabilistic DLs have a two-dimensional semantics similar to temporal DLs and are well-suited for representing subjective probabilities. We carry out a detailed study of reasoning in the new family of logics, concentrating on probabilistic extensions of the DLs ALC and EL, and showing that the complexity ranges from PTime via ExpTime and 2ExpTime to undecidable.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–66},
numpages = {66}
}

@article{10.5555/3176748.3176763,
author = {Ramchurn, Sarvapali D. and Huynh, Trung Dong and Wu, Feng and Ikuno, Yuki and Flann, Jack and Moreau, Luc and Fischer, Joel E. and Jiang, Wenchao and Rodden, Tom and Simpson, Edwin and Reece, Steven and Roberts, Stephen and Jennings, Nicholas R.},
title = {A Disaster Response System Based on Human-Agent Collectives},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Major natural or man-made disasters such as Hurricane Katrina or the 9/11 terror attacks pose significant challenges for emergency responders. First, they have to develop an understanding of the unfolding event either using their own resources or through third-parties such as the local population and agencies. Second, based on the information gathered, they need to deploy their teams in a flexible manner, ensuring that each team performs tasks in the most effective way. Third, given the dynamic nature of a disaster space, and the uncertainties involved in performing rescue missions, information about the disaster space and the actors within it needs to be managed to ensure that responders are always acting on up-to-date and trusted information. Against this background, this paper proposes a novel disaster response system called HAC-ER. Thus HAC-ER interweaves humans and agents, both robotic and software, in social relationships that augment their individual and collective capabilities. To design HAC-ER, we involved end-users including both experts and volunteers in a several participatory design workshops, lab studies, and field trials of increasingly advanced prototypes of individual components of HAC-ER as well as the overall system. This process generated a number of new quantitative and qualitative results but also raised a number of new research questions. HAC-ER thus demonstrates how such Human-Agent Collectives (HACs) can address key challenges in disaster response. Specifically, we show how HAC-ER utilises crowd-sourcing combined with machine learning to obtain most important situational awareness from large streams of reports posted by members of the public and trusted organisations. We then show how this information can inform human-agent teams in coordinating multi-UAV deployments, as well as task planning for responders on the ground. Finally, HAC-ER incorporates an infrastructure and the associated intelligence for tracking and utilising the provenance of information shared across the entire system to ensure its accountability. We individually validate each of these elements of HAC-ER and show how they perform against standard (non-HAC) baselines and also elaborate on the evaluation of the overall system.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {661–708},
numpages = {48}
}

@article{10.5555/3176748.3176762,
author = {Grinshpoun, Tal and Tassa, Tamir},
title = {P-SyncBB: A Privacy Preserving Branch and Bound DCOP Algorithm},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Distributed constraint optimization problems enable the representation of many combinatorial problems that are distributed by nature. An important motivation for such problems is to preserve the privacy of the participating agents during the solving process. The present paper introduces a novel privacy-preserving branch and bound algorithm for this purpose. The proposed algorithm, P-SyncBB, preserves constraint, topology and decision privacy. The algorithm requires secure solutions to several multi-party computation problems. Consequently, appropriate novel secure protocols are devised and analyzed. An extensive experimental evaluation on different benchmarks, problem sizes, and constraint densities shows that P-SyncBB exhibits superior performance to other privacy-preserving complete DCOP algorithms.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {621–660},
numpages = {40}
}

@article{10.5555/3176748.3176761,
author = {Lev, Omer and Rosenschein, Jeffrey S.},
title = {Convergence of Iterative Scoring Rules},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {In multiagent systems, social choice functions can help aggregate the distinct preferences that agents have over alternatives, enabling them to settle on a single choice. Despite the basic manipulability of all reasonable voting systems, it would still be desirable to find ways to reach plausible outcomes, which are stable states, i.e., a situation where no agent would wish to change its vote. One possibility is an iterative process in which, after everyone initially votes, participants may change their votes, one voter at a time. This technique, explored in previous work, converges to a Nash equilibrium when Plurality voting is used, along with a tie-breaking rule that chooses a winner according to a linear order of preferences over candidates.In this paper, we both consider limitations of the iterative voting method, as well as expanding upon it. We demonstrate the significance of tie-breaking rules, showing that no iterative scoring rule converges for all tie-breaking. However, using a restricted tie-breaking rule (such as the linear order rule used in previous work) does not by itself ensure convergence. We prove that in addition to plurality, the veto voting rule converges as well using a linear order tie-breaking rule. However, we show that these two voting rules are the only scoring rules that converge, regardless of tie-breaking mechanism.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {573–591},
numpages = {19}
}

@article{10.5555/3176748.3176760,
author = {Shehu, Amarda and Plaku, Erion},
title = {A Survey of Computational Treatments of Biomolecules by Robotics-Inspired Methods Modeling Equilibrium Structure and Dynamics},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {More than fifty years of research in molecular biology have demonstrated that the ability of small and large molecules to interact with one another and propagate the cellular processes in the living cell lies in the ability of these molecules to assume and switch between specific structures under physiological conditions. Elucidating biomolecular structure and dynamics at equilibrium is therefore fundamental to furthering our understanding of biological function, molecular mechanisms in the cell, our own biology, disease, and disease treatments. By now, there is a wealth of methods designed to elucidate biomolecular structure and dynamics contributed from diverse scientific communities. In this survey, we focus on recent methods contributed from the Robotics community that promise to address outstanding challenges regarding the disparate length and time scales that characterize dynamic molecular processes in the cell. In particular, we survey robotics-inspired methods designed to obtain efficient representations of structure spaces of molecules in isolation or in assemblies for the purpose of characterizing equilibrium structure and dynamics. While an exhaustive review is an impossible endeavor, this survey balances the description of important algorithmic contributions with a critical discussion of outstanding computational challenges. The objective is to spur further research to address outstanding challenges in modeling equilibrium biomolecular structure and dynamics.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {509–572},
numpages = {64}
}

@article{10.5555/3176748.3176759,
author = {G\"{u}nay, Akin and Liu, Yang and Zhang, Jie},
title = {PROMOCA: Probabilistic Modeling and Analysis of Agents in Commitment Protocols},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Social commitment protocols regulate interactions of agents in multiagent systems. Several methods have been developed to analyze properties of commitment protocols. However, analysis of an agent's behavior in a commitment protocol, which should take into account the agent's goals and beliefs, has received less attention. In this paper we present ProMoca framework to address this issue. Firstly, we develop an expressive formal language to model agents with respect to their commitments. Our language provides dedicated elements to define commitment protocols, and model agents in terms of their goals, behaviors, and beliefs. Furthermore, our language provides probabilistic and non-deterministic elements to model uncertainty in agents' beliefs. Secondly, we identify two essential properties of an agent with respect to a commitment protocol, namely compliance and goal satisfaction. We formalize these properties using a probabilistic variant of linear temporal logic. Thirdly, we adapt a probabilistic model checking algorithm to automatically analyze compliance and goal satisfaction properties. Finally, we present empirical results about efficiency and scalability of ProMoca.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {465–508},
numpages = {44}
}

@article{10.5555/3176748.3176758,
author = {Malapert, Arnaud and R\'{e}gin, Jean-Charles and Rezgui, Mohamed},
title = {Embarrassingly Parallel Search in Constraint Programming},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {We introduce an Embarrassingly Parallel Search (EPS) method for solving constraint problems in parallel, and we show that this method matches or even outperforms state-of-the-art algorithms on a number of problems using various computing infrastructures. EPS is a simple method in which a master decomposes the problem into many disjoint subproblems which are then solved independently by workers. Our approach has three advantages: it is an efficient method; it involves almost no communication or synchronization between workers; and its implementation is made easy because the master and the workers rely on an underlying constraint solver, but does not require to modify it. This paper describes the method, and its applications to various constraint problems (satisfaction, enumeration, optimization). We show that our method can be adapted to different underlying solvers (Gecode, Choco2, OR-tools) on different computing infrastructures (multi-core, data centers, cloud computing). The experiments cover unsatisfiable, enumeration and optimization problems, but do not cover first solution search because it makes the results hard to analyze. The same variability can be observed for optimization problems, but at a lesser extent because the optimality proof is required. EPS offers good average performance, and matches or outperforms other available parallel implementations of Gecode as well as some solvers portfolios. Moreover, we perform an in-depth analysis of the various factors that make this approach efficient as well as the anomalies that can occur. Last, we show that the decomposition is a key component for efficiency and load balancing.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {421–464},
numpages = {44}
}

@article{10.5555/3176748.3176757,
author = {Goldberg, Yoav},
title = {A Primer on Neural Network Models for Natural Language Processing},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {345–420},
numpages = {76}
}

@article{10.5555/3176748.3176756,
author = {Sturtevant, Nathan R. and Bulitko, Vadim},
title = {Scrubbing during Learning in Real-Time Heuristic Search},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Real-time agent-centered heuristic search is a well-studied problem where an agent that can only reason locally about the world must travel to a goal location using bounded computation and memory at each step. Many algorithms have been proposed for this problem and theoretical results have also been derived for the worst-case performance with simple examples demonstrating worst-case performance in practice. Lower bounds, however, have not been widely studied. In this paper we study best-case performance more generally and derive theoretical lower bounds for reaching the goal using LRTA*, a canonical example of a real-time agent-centered heuristic search algorithm. The results show that, given some reasonable restrictions on the state space and the heuristic function, the number of steps an LRTA*-like algorithm requires to reach the goal will grow asymptotically faster than the state space, resulting in "scrubbing" where the agent repeatedly visits the same state. We then show that while the asymptotic analysis does not hold for more complex realtime search algorithms, experimental results suggest that it is still descriptive of practical performance.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {307–343},
numpages = {37}
}

@article{10.5555/3176748.3176755,
author = {Wilt, Christopher and Ruml, Wheeler},
title = {Effective Heuristics for Suboptimal Best-First Search},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Suboptimal heuristic search algorithms such as weighted A* and greedy best-first search are widely used to solve problems for which guaranteed optimal solutions are too expensive to obtain. These algorithms crucially rely on a heuristic function to guide their search. However, most research on building heuristics addresses optimal solving. In this paper, we illustrate how established wisdom for constructing heuristics for optimal search can fail when considering suboptimal search. We consider the behavior of greedy best-first search in detail and we test several hypotheses for predicting when a heuristic will be effective for it. Our results suggest that a predictive characteristic is a heuristic's goal distance rank correlation (GDRC), a robust measure of whether it orders nodes according to distance to a goal. We demonstrate that GDRC can be used to automatically construct abstraction-based heuristics for greedy best-first search that are more effective than those built by methods oriented toward optimal search. These results reinforce the point that suboptimal search deserves sustained attention and specialized methods of its own.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {273–306},
numpages = {34}
}

@article{10.5555/3176748.3176754,
author = {Steinmetz, Marcel and Hoffmann, J\"{o}rg and Buffet, Olivier},
title = {Goal Probability Analysis in MDP Probabilistic Planning: Exploring and Enhancing the State of the Art},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Unavoidable dead-ends are common in many probabilistic planning problems, e.g. when actions may fail or when operating under resource constraints. An important objective in such settings is MaxProb, determining the maximal probability with which the goal can be reached, and a policy achieving that probability. Yet algorithms for MaxProb probabilistic planning are severely under-explored, to the extent that there is scant evidence of what the empirical state of the art actually is. We close this gap with a comprehensive empirical analysis. We design and explore a large space of heuristic search algorithms, systematizing known algorithms and contributing several new algorithm variants. We consider MaxProb, as well as weaker objectives that we baptize AtLeastProb (requiring to achieve a given goal probabilty threshold) and ApproxProb (requiring to compute the maximum goal probability up to a given accuracy). We explore both the general case where there may be 0-reward cycles, and the practically relevant special case of acyclic planning, such as planning with a limited action-cost budget. We design suitable termination criteria, search algorithm variants, dead-end pruning methods using classical planning heuristics, and node selection strategies. We design a benchmark suite comprising more than 1000 instances adapted from the IPPC, resource-constrained planning, and simulated penetration testing. Our evaluation clarifies the state of the art, characterizes the behavior of a wide range of heuristic search algorithms, and demonstrates significant benefits of our new algorithm variants.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {229–271},
numpages = {43}
}

@article{10.5555/3176748.3176753,
author = {Parisi, Simone and Pirotta, Matteo and Restelli, Marcello},
title = {Multi-Objective Reinforcement Learning through Continuous Pareto Manifold Approximation},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Many real-world control applications, from economics to robotics, are characterized by the presence of multiple conflicting objectives. In these problems, the standard concept of optimality is replaced by Pareto-optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi-objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. In this paper, we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the Pareto frontier in multi-objective Markov Decision Problems (MOMDPs). Differently from previous policy gradient algorithms, where n optimization routines are executed to have n solutions, our approach performs a single gradient ascent run, generating at each step an improved continuous approximation of the Pareto frontier. The idea is to optimize the parameters of a function defining a manifold in the policy parameters space, so that the corresponding image in the objectives space gets as close as possible to the true Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two problems, a linear-quadratic Gaussian regulator and a water reservoir control task.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {187–227},
numpages = {41}
}

@article{10.5555/3176748.3176752,
author = {Fern\'{a}ndez, Alejandro Moreo and Esuli, Andrea and Sebastiani, Fabrizio},
title = {Lightweight Random Indexing for Polylingual Text Classification},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Multilingual Text Classification (MLTC) is a text classification task in which documents are written each in one among a set L of natural languages, and in which all documents must be classified under the same classification scheme, irrespective of language. There are two main variants of MLTC, namely Cross-Lingual Text Classification (CLTC) and Polylingual Text Classification (PLTC). In PLTC, which is the focus of this paper, we assume (differently from CLTC) that for each language in L there is a representative set of training documents; PLTC consists of improving the accuracy of each of the |L| monolingual classifiers by also leveraging the training documents written in the other (|L| - 1) languages. The obvious solution, consisting of generating a single polylingual classifier from the juxtaposed monolingual vector spaces, is usually infeasible, since the dimensionality of the resulting vector space is roughly |L| times that of a monolingual one, and is thus often unmanageable. As a response, the use of machine translation tools or multilingual dictionaries has been proposed. However, these resources are not always available, or are not always free to use.One machine-translation-free and dictionary-free method that, to the best of our knowledge, has never been applied to PLTC before, is Random Indexing (RI). We analyse RI in terms of space and time efficiency, and propose a particular configuration of it (that we dub Lightweight Random Indexing - LRI). By running experiments on two well known public benchmarks, Reuters RCV1/RCV2 (a comparable corpus) and JRC-Acquis (a parallel one), we show LRI to outperform (both in terms of effectiveness and efficiency) a number of previously proposed machine-translation-free and dictionary-free PLTC methods that we use as baselines.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {151–185},
numpages = {35}
}

@article{10.5555/3176748.3176751,
author = {Muise, Christian and Beck, J. Christopher and McIlraith, Sheila A.},
title = {Optimal Partial-Order Plan Relaxation via MaxSAT},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Partial-order plans (POPs) are attractive because of their least-commitment nature, which provides enhanced plan flexibility at execution time relative to sequential plans. Current research on automated plan generation focuses on producing sequential plans, despite the appeal of POPs. In this paper we examine POP generation by relaxing or modifying the action orderings of a sequential plan to optimize for plan criteria that promote flexibility. Our approach relies on a novel partial weighted MaxSAT encoding of a sequential plan that supports the minimization of deordering or reordering of actions. Using a similar technique, we further demonstrate how to remove redundant actions from the plan, and how to combine this criterion with the objective of maximizing a POP's flexibility. Our partial weighted MaxSAT encoding allows us to compute a POP from a sequential plan effectively. We compare the efficiency of our approach to previous methods for POP generation via sequential-plan relaxation. Our results show that while an existing heuristic approach consistently produces the optimal deordering of a sequential plan, our approach has greater flexibility when we consider reordering the actions in the plan while also providing a guarantee of optimality. We also investigate and confirm the accuracy of the standard flex metric typically used to predict the true flexibility of a POP as measured by the number of linearizations it represents.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {113–149},
numpages = {37}
}

@article{10.5555/3176748.3176750,
author = {Martiny, Karsten and M\"{o}ller, Ralf},
title = {PDT Logic: A Probabilistic Doxastic Temporal Logic for Reasoning about Beliefs in Multi-Agent Systems},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {We present Probabilistic Doxastic Temporal (PDT) Logic, a formalism to represent and reason about probabilistic beliefs and their temporal evolution in multi-agent systems. This formalism enables the quantification of agents' beliefs through probability intervals and incorporates an explicit notion of time. We discuss how over time agents dynamically change their beliefs in facts, temporal rules, and other agents' beliefs with respect to any new information they receive. We introduce an appropriate formal semantics for PDT Logic and show that it is decidable. Alternative options of specifying problems in PDT Logic are possible. For these problem specifications, we develop different satisfiability checking algorithms and provide complexity results for the respective decision problems. The use of probability intervals enables a formal representation of probabilistic knowledge without enforcing (possibly incorrect) exact probability values. By incorporating an explicit notion of time, PDT Logic provides enriched possibilities to represent and reason about temporal relations.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {39–112},
numpages = {74}
}

@article{10.5555/3176748.3176749,
author = {Villa, Simone and Stella, Fabio},
title = {Learning Continuous Time Bayesian Networks in Non-Stationary Domains},
year = {2016},
issue_date = {September 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {57},
number = {1},
issn = {1076-9757},
abstract = {Non-stationary continuous time Bayesian networks are introduced. They allow the parents set of each node to change over continuous time. Three settings are developed for learning non-stationary continuous time Bayesian networks from data: known transition times, known number of epochs and unknown number of epochs. A score function for each setting is derived and the corresponding learning algorithm is developed. A set of numerical experiments on synthetic data is used to compare the effectiveness of non-stationary continuous time Bayesian networks to that of non-stationary dynamic Bayesian networks. Furthermore, the performance achieved by non-stationary continuous time Bayesian networks is compared to that achieved by state-of-the-art algorithms on four real-world datasets, namely drosophila, saccharomyces cerevisiae, songbird and macroeconomics.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–37},
numpages = {37}
}

@article{10.5555/3013589.3013608,
author = {Du, Heshan and Alechina, Natasha},
title = {Qualitative Spatial Logics for Buffered Geometries},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {This paper describes a series of new qualitative spatial logics for checking consistency of sameAs and partOf matches between spatial objects from different geospatial datasets, especially from crowd-sourced datasets. Since geometries in crowd-sourced data are usually not very accurate or precise, we buffer geometries by a margin of error or a level of tolerance σ ∈ R≥0, and define spatial relations for buffered geometries. The spatial logics formalize the notions of 'buffered equal' (intuitively corresponding to 'possibly sameAs'), 'buffered part of' ('possibly partOf'), 'near' ('possibly connected') and 'far' ('definitely disconnected'). A sound and complete axiomatisation of each logic is provided with respect to models based on metric spaces. For each of the logics, the satisfiability problem is shown to be NP-complete. Finally, we briefl y describe how the logics are used in a system for generating and debugging matches between spatial objects, and report positive experimental evaluation results for the system.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {693–745},
numpages = {53}
}

@article{10.5555/3013589.3013607,
author = {Cenamor, Isabel and De La Rosa, Tom\'{a}s and Fern\'{a}ndez, Fernando},
title = {The IBaCoP Planning System: Instance-Based Configured Portfolios},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Sequential planning portfolios are very powerful in exploiting the complementary strength of different automated planners. The main challenge of a portfolio planner is to define which base planners to run, to assign the running time for each planner and to decide in what order they should be carried out to optimize a planning metric. Portfolio configurations are usually derived empirically from training benchmarks and remain fixed for an evaluation phase. In this work, we create a per-instance configurable portfolio, which is able to adapt itself to every planning task. The proposed system pre-selects a group of candidate planners using a Pareto-dominance filtering approach and then it decides which planners to include and the time assigned according to predictive models. These models estimate whether a base planner will be able to solve the given problem and, if so, how long it will take. We define different portfolio strategies to combine the knowledge generated by the models. The experimental evaluation shows that the resulting portfolios provide an improvement when compared with non-informed strategies. One of the proposed portfolios was the winner of the Sequential Satisficing Track of the International Planning Competition held in 2014.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {657–691},
numpages = {35}
}

@article{10.5555/3013589.3013606,
author = {Deagustini, Cristhian Ariel D. and Mart\'{\i}nez, Mar\'{\i}a Vanina and Falappa, Marcelo A. and Simari, Guillermo R.},
title = {Datalog± Ontology Consolidation},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Knowledge bases in the form of ontologies are receiving increasing attention as they allow to clearly represent both the available knowledge, which includes the knowledge in itself and the constraints imposed to it by the domain or the users. In particular, Datalog± ontologies are attractive because of their property of decidability and the possibility of dealing with the massive amounts of data in real world environments; however, as it is the case with many other ontological languages, their application in collaborative environments often lead to inconsistency related issues. In this paper we introduce the notion of incoherence regarding Datalog± ontologies, in terms of satisfiability of sets of constraints, and show how under specific conditions incoherence leads to inconsistent Datalog± ontologies. The main contribution of this work is a novel approach to restore both consistency and coherence in Datalog± ontologies. The proposed approach is based on kernel contraction and restoration is performed by the application of incision functions that select formulas to delete. Nevertheless, instead of working over minimal incoherent/inconsistent sets encountered in the ontologies, our operators produce incisions over non-minimal structures called clusters. We present a construction for consolidation operators, along with the properties expected to be satisfied by them. Finally, we establish the relation between the construction and the properties by means of a representation theorem. Although this proposal is presented for Datalog± ontologies consolidation, these operators can be applied to other types of ontological languages, such as Description Logics, making them apt to be used in collaborative environments like the Semantic Web.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {613–656},
numpages = {44}
}

@article{10.5555/3013589.3013605,
author = {Aziz, Haris and Cahan, Casey and Gretton, Charles and Kilby, Philip and Mattei, Nicholas and Walsh, Toby},
title = {A Study of Proxies for Shapley Allocations of Transport Costs},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {We survey existing rules of thumb, propose novel methods, and comprehensively evaluate a number of solutions to the problem of calculating the cost to serve each location in a single-vehicle transport setting. Cost to serve analysis has applications both strategically and operationally in transportation settings. The problem is formally modeled as the traveling salesperson game (TSG), a cooperative transferable utility game in which agents correspond to locations in a traveling salesperson problem (TSP). The total cost to serve all locations in the TSP is the length of an optimal tour. An allocation divides the total cost among individual locations, thus providing the cost to serve each of them. As one of the most important normative division schemes in cooperative games, the Shapley value gives a principled and fair allocation for a broad variety of games including the TSG. We consider a number of direct and sampling-based procedures for calculating the Shapley value, and prove that approximating the Shapley value of the TSG within a constant factor is NP-hard. Treating the Shapley value as an ideal baseline allocation, we survey six proxies for it that are each relatively easy to compute. Some of these proxies are rules of thumb and some are procedures international delivery companies use(d) as cost allocation methods. We perform an experimental evaluation using synthetic Euclidean games as well as games derived from real-world tours calculated for scenarios involving fast-moving goods; where deliveries are made on a road network every day. We explore several computationally tractable allocation techniques that are good proxies for the Shapley value in problem instances of a size and complexity that is commercially relevant.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {573–611},
numpages = {39}
}

@article{10.5555/3013589.3013604,
author = {Hern\'{a}ndez, Carlos and Baier, Jorge A. and As\'{\i}n, Roberto},
title = {Time-Bounded Best-First Search for Reversible and Non-Reversible Search Graphs},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Time-Bounded A* is a real-time, single-agent, deterministic search algorithm that expands states of a graph in the same order as A* does, but that unlike A* interleaves search and action execution. Known to outperform state-of-the-art real-time search algorithms based on Korf's Learning Real-Time A* (LRTA*) in some benchmarks, it has not been studied in detail and is sometimes not considered as a "true" real-time search algorithm since it fails in non-reversible problems even it the goal is still reachable from the current state. In this paper we propose and study Time-Bounded Best-First Search (TB(BFS)) a straightforward generalization of the time-bounded approach to any best-first search algorithm. Furthermore, we propose Restarting Time-Bounded Weighted A* (TBR (WA*)), an algorithm that deals more adequately with non-reversible search graphs, eliminating "backtracking moves" and incorporating search restarts and heuristic learning. In nonreversible problems we prove that TB(BFS) terminates and we deduce cost bounds for the solutions returned by Time-Bounded Weighted A* (TB(WA*)), an instance of TB(BFS). Furthermore, we prove TBR (WA*), under reasonable conditions, terminates. We evaluate TB(WA) in both grid pathfinding and the 15-puzzle. In addition, we evaluate TBR (WA*) on the racetrack problem. We compare our algorithms to LSS-LRTWA*, a variant of LRTA* that can exploit lookahead search and a weighted heuristic. A general observation is that the performance of both TB(WA*) and TBR (WA*) improves as the weight parameter is increased. In addition, our time-bounded algorithms almost always outperform LSS-LRTWA* by a significant margin.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {547–571},
numpages = {25}
}

@article{10.5555/3013589.3013603,
author = {Venanzi, Matteo and Guiver, John and Kohli, Pushmeet and Jennings, Nicholas R.},
title = {Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Many aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. In this work we introduce a new time-sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task's duration compared to state-of-the-art methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {517–545},
numpages = {29}
}

@article{10.5555/3013589.3013602,
author = {Eiter, Thomas and Fink, Michael and Stepanova, Daria},
title = {Computing Repairs of Inconsistent DL-Programs over EL Ontologies},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Description Logic (DL) ontologies and non-monotonic rules are two prominent Knowledge Representation (KR) formalisms with complementary features that are essential for various applications. Nonmonotonic Description Logic (DL) programs combine these formalisms thus providing support for rule-based reasoning on top of DL ontologies using a well-defined query interface represented by so-called DL-atoms. Unfortunately, interaction of the rules and the ontology may incur inconsistencies such that a DL-program lacks answer sets (i.e., models), and thus yields no information. This issue is addressed by recently defined repair answer sets, for computing which an effective practical algorithm was proposed for DL-LiteA ontologies that reduces a repair computation to constraint matching based on so-called support sets. However, the algorithm exploits particular features of DL-LiteA and can not be readily applied to repairing DL-programs over other prominent DLs like EL. Compared to DL-LiteA, in EL support sets may neither be small nor only few support sets might exist, and completeness of the algorithm may need to be given up when the support information is bounded. We thus provide an approach for computing repairs for DL-programs over EL ontologies based on partial (incomplete) support families. The latter are constructed using datalog query rewriting techniques as well as ontology approximation based on logical difference between EL-terminologies. We show how the maximal size and number of support sets for a given DL-atom can be estimated by analyzing the properties of a support hypergraph, which characterizes a relevant set of TBox axioms needed for query derivation. We present a declarative implementation of the repair approach and experimentally evaluate it on a set of benchmark problems; the promising results witness practical feasibility of our repair approach.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {463–515},
numpages = {53}
}

@article{10.5555/3013589.3013601,
author = {Chen, Xujin and Hu, Xiaodong and Liu, Tie-Yan and Ma, Weidong and Qin, Tao and Tang, Pingzhong and Wang, Changjun and Zheng, Bo},
title = {Efficient Mechanism Design for Online Scheduling},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {This paper concerns the mechanism design for online scheduling in a strategic setting. In this setting, each job is owned by a self-interested agent who may misreport the release time, deadline, length, and value of her job, while we need to determine not only the schedule of the jobs, but also the payment of each agent. We focus on the design of incentive compatible (IC) mechanisms, and study the maximization of social welfare (i.e., the aggregated value of completed jobs) by competitive analysis. We first derive two lower bounds on the competitive ratio of any deterministic IC mechanism to characterize the landscape of our research: one bound is 5, which holds for equal-length jobs; the other bound is κ/ln κ + 1-o(1), which holds for unequal-length jobs, where κ is the maximum ratio between lengths of any two jobs. We then propose a deterministic IC mechanism and show that such a simple mechanism works very well for two models: (1) In the preemption-restart model, the mechanism can achieve the optimal competitive ratio of 5 for equal-length jobs and a near optimal ratio of (1/(1-ε)2 + o(1))κ/ln κ for unequal-length jobs, where 0 &lt; ε &lt; 1 is a small constant; (2) In the preemption-resume model, the mechanism can achieve the optimal competitive ratio of 5 for equal-length jobs and a near optimal competitive ratio (within factor 2) for unequal-length jobs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {429–461},
numpages = {33}
}

@article{10.5555/3013589.3013600,
author = {Zhang, Xiaowang and Van Den Bussche, Jan and Picalausa, Fran\c{c}ois},
title = {On the Satisfiability Problem for SPARQL Patterns},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {The satisfiability problem for SPARQL 1.0 patterns is undecidable in general, since the relational algebra can be emulated using such patterns. The goal of this paper is to delineate the boundary of decidability of satisfiability in terms of the constraints allowed in filter conditions. The classes of constraints considered are bound-constraints, negated bound-constraints, equalities, nonequalities, constant-equalities, and constant-nonequalities. The main result of the paper can be summarized by saying that, as soon as inconsistent filter conditions can be formed, satisfiability is undecidable. The key insight in each case is to find a way to emulate the set difference operation. Undecidability can then be obtained from a known undecidability result for the algebra of binary relations with union, composition, and set difference. When no inconsistent filter conditions can be formed, satisfiability is decidable by syntactic checks on bound variables and on the use of literals. Although the problem is shown to be NP-complete, it is experimentally shown that the checks can be implemented efficiently in practice. The paper also points out that satisfiability for the so-called 'well-designed' patterns can be decided by a check on bound variables and a check for inconsistent filter conditions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {403–428},
numpages = {26}
}

@article{10.5555/3013589.3013599,
author = {Savicky, Petr and Ku\v{c}era, Petr},
title = {Generating Models of a Matched Formula with a Polynomial Delay},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {A matched formula is a CNF formula whose incidence graph admits a matching which matches a distinct variable to every clause. Such a formula is always satisfiable. Matched formulas are used, for example, in the area of parametrized complexity. We prove that the problem of counting the number of the models (satisfying assignments) of a matched formula is #P-complete. On the other hand, we define a class of formulas generalizing the matched formulas and prove that for a formula in this class one can choose in polynomial time a variable suitable for splitting the tree for the search of the models of the formula. As a consequence, the models of a formula from this class, in particular of any matched formula, can be generated sequentially with a delay polynomial in the size of the input. On the other hand, we prove that this task cannot be performed efficiently for linearly satisfiable formulas, which is a generalization of matched formulas containing the class considered above.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {379–402},
numpages = {24}
}

@article{10.5555/3013589.3013598,
author = {Zhuang, Zhiqiang and Wang, Zhe and Wang, Kewen and Qi, Guilin},
title = {DL-Lite Contraction and Revision},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Two essential tasks in managing description logic knowledge bases are eliminating problematic axioms and incorporating newly formed ones. Such elimination and incorporation are formalised as the operations of contraction and revision in belief change. In this paper, we deal with contraction and revision for the DL-Lite family through a model-theoretic approach. Standard description logic semantics yields an infinite number of models for DL-Lite knowledge bases, thus it is difficult to develop algorithms for contraction and revision that involve DL models. The key to our approach is the introduction of an alternative semantics called type semantics which can replace the standard semantics in characterising the standard inference tasks of DL-Lite. Type semantics has several advantages over the standard one. It is more succinct and importantly, with a finite signature, the semantics always yields a finite number of models. We then define model-based contraction and revision functions for DL-Lite knowledge bases under type semantics and provide representation theorems for them. Finally, the finiteness and succinctness of type semantics allow us to develop tractable algorithms for instantiating the functions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {329–378},
numpages = {50}
}

@article{10.5555/3013589.3013597,
author = {Fickert, Maximilian and Hoffmann, J\"{o}rg and Steinmetz, Marcel},
title = {Combining the Delete Relaxation with Critical-Path Heuristics: A Direct Characterization},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Recent work has shown how to improve delete relaxation heuristics by computing relaxed plans, i. e., the hFF heuristic, in a compiled planning task ΠC which represents a given set C of fact conjunctions explicitly. While this compilation view of such partial delete relaxation is simple and elegant, its meaning with respect to the original planning task is opaque, and the size of ΠC grows exponentially in |C|. We herein provide a direct characterization, without compilation, making explicit how the approach arises from a combination of the delete-relaxation with critical-path heuristics. Designing equations characterizing a novel view on h+ on the one hand, and a generalized version hC of hm on the other hand, we show that h+(ΠC) can be characterized in terms of a combined hC+ equation. This naturally generalizes the standard delete-relaxation framework: understanding that framework as a relaxation over singleton facts as atomic subgoals, one can refine the relaxation by using the conjunctions C as atomic subgoals instead. Thanks to this explicit view, we identify the precise source of complexity in hFF(ΠC), namely maximization of sets of supported atomic subgoals during relaxed plan extraction, which is easy for singleton-fact subgoals but is NP-complete in the general case. Approximating that problem greedily, we obtain a polynomial-time hCFF version of hFF(ΠC), superseding the ΠC compilation, and superseding the modified ΠceC compilation which achieves the same complexity reduction but at an information loss. Experiments on IPC benchmarks show that these theoretical advantages can translate into empirical ones.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {269–327},
numpages = {59}
}

@article{10.5555/3013589.3013596,
author = {Xu, Zenglin and Zhe, Shandian and Qi, Yuan and Yu, Peng},
title = {Association Discovery and Diagnosis of Alzheimer's Disease with Bayesian Multiview Learning},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {The analysis and diagnosis of Alzheimer's disease (AD) can be based on genetic variations, e.g., single nucleotide polymorphisms (SNPs) and phenotypic traits, e.g., Magnetic Resonance Imaging (MRI) features. We consider two important and related tasks: i) to select genetic and phenotypical markers for AD diagnosis and ii) to identify associations between genetic and phenotypical data. While previous studies treat these two tasks separately, they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. Here we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels; in return, the disease status can guide the discovery of relationships between data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. Moreover, to take advantage of the linkage disequilibrium (LD) measuring the non-random association of alleles, we incorporate a graph Laplacian type of prior in the model. To learn the model from data, we develop an efficient variational inference algorithm. Analysis on an imaging genetics dataset for the study of Alzheimer's Disease (AD) indicates that our model identifies biologically meaningful associations between genetic variations and MRI features, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {247–268},
numpages = {22}
}

@article{10.5555/3013589.3013595,
author = {Grooters, Diana and Prakken, Henry},
title = {Two Aspects of Relevance in Structured Argumentation: Minimality and Paraconsistency},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {This paper studies two issues concerning relevance in structured argumentation in the context of the ASPIC+ framework, arising from the combined use of strict and defeasible inference rules. One issue arises if the strict inference rules correspond to classical logic. A longstanding problem is how the trivialising effect of the classical Ex Falso principle can be avoided while satisfying consistency and closure postulates. In this paper, this problem is solved by disallowing chaining of strict rules, resulting in a variant of the ASPIC+ framework called ASPIC*, and then disallowing the application of strict rules to inconsistent sets of formulas. Thus in effect Rescher &amp; Manor's paraconsistent notion of weak consequence is embedded in ASPIC*.Another issue is minimality of arguments. If arguments can apply defeasible inference rules, then they cannot be required to have subset-minimal premises, since defeasible rules based on more information may well make an argument stronger. In this paper instead minimality is required of applications of strict rules throughout an argument. It is shown that under some plausible assumptions this does not affect the set of conclusions. In addition, circular arguments are in the new ASPIC* framework excluded in a way that satisfies closure and consistency postulates and that generates finitary argumentation frameworks if the knowledge base and set of defeasible rules are finite. For the latter result the exclusion of chaining of strict rules is essential.Finally, the combined results of this paper are shown to be a proper extension of classical-logic argumentation with preferences and defeasible rules.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {197–245},
numpages = {49}
}

@article{10.5555/3013589.3013594,
author = {Kawaguchi, Kenji and Maruyama, Yu and Zheng, Xiaoyu},
title = {Global Continuous Optimization with Error Bound and Fast Convergence},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {This paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable. Such a difficult optimization problem arises in many real-world applications, such as parameter tuning in machine learning, engineering design problem, and planning with a complex physics simulator. This paper proposes a new global optimization algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both fast convergence in practice and finite-time error bound in theory. The advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions. Further, we modify the LOGO algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound. We apply the proposed planning method to accident management of a nuclear power plant. The result of the application study demonstrates the practical utility of our method.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {153–195},
numpages = {43}
}

@article{10.5555/3013589.3013593,
author = {Azimi, Javad and Fern, Xiaoli Z. and Fern, Alan},
title = {Budgeted Optimization with Constrained Experiments},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Motivated by a real-world problem, we study a novel budgeted optimization problem where the goal is to optimize an unknown function f(undefined) given a budget by requesting a sequence of samples from the function. In our setting, however, evaluating the function at precisely specified points is not practically possible due to prohibitive costs. Instead, we can only request constrained experiments. A constrained experiment, denoted by Q, specifies a subset of the input space for the experimenter to sample the function from. The outcome of Q includes a sampled experiment x, and its function output f(x). Importantly, as the constraints of Q become looser, the cost of fulfilling the request decreases, but the uncertainty about the location x increases. Our goal is to manage this trade-off by selecting a set of constrained experiments that best optimize f(undefined) within the budget. We study this problem in two different settings, the non-sequential (or batch) setting where a set of constrained experiments is selected at once, and the sequential setting where experiments are selected one at a time. We evaluate our proposed methods for both settings using synthetic and real functions. The experimental results demonstrate the efficacy of the proposed methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {119–152},
numpages = {34}
}

@article{10.5555/3013589.3013592,
author = {Harabor, Daniel and Grastien, Alban and \"{O}z, Dindar and Aksakalli, Vural},
title = {Optimal Any-Angle Pathfinding in Practice},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Any-angle pathfinding is a fundamental problem in robotics and computer games. The goal is to find a shortest path between a pair of points on a grid map such that the path is not artificially constrained to the points of the grid. Prior research has focused on approximate online solutions. A number of exact methods exist but they all require super-linear space and pre-processing time. In this study, we describe Anya: a new and optimal any-angle pathfinding algorithm. Where other works find approximate any-angle paths by searching over individual points from the grid, Anya finds optimal paths by searching over sets of states represented as intervals. Each interval is identified on-the-fly. From each interval Anya selects a single representative point that it uses to compute an admissible cost estimate for the entire set. Anya always returns an optimal path if one exists. Moreover it does so without any offine pre-processing or the introduction of additional memory overheads. In a range of empirical comparisons we show that Anya is competitive with several recent (sub-optimal) online and pre-processing based techniques and is up to an order of magnitude faster than the most common benchmark algorithm, a grid-based implementation of A.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {89–118},
numpages = {30}
}

@article{10.5555/3013589.3013591,
author = {Taghizadeh, Nasrin and Faili, Hesham},
title = {Automatic Wordnet Development for Low-Resource Languages Using Cross-Lingual WSD},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Wordnets are an effective resource for natural language processing and information retrieval, especially for semantic processing and meaning related tasks. So far, wordnets have been constructed for many languages. However, the automatic development of wordnets for low-resource languages has not been well studied. In this paper, an Expectation-Maximization algorithm is used to create high quality and large scale wordnets for poor-resource languages. The proposed method benefits from possessing cross-lingual word sense disambiguation and develops a wordnet by only using a bi-lingual dictionary and a monolingual corpus. The proposed method has been executed with Persian language and the resulting wordnet has been evaluated through several experiments. The results show that the induced wordnet has a precision score of 90% and a recall score of 35%.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {61–87},
numpages = {27}
}

@article{10.5555/3013589.3013590,
author = {Baader, Franz and Bienvenu, Meghyn and Lutz, Carsten and Wolter, Frank},
title = {Query and Predicate Emptiness in Ontology-Based Data Access},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {In ontology-based data access (OBDA), database querying is enriched with an ontology that provides domain knowledge and additional vocabulary for query formulation. We identify query emptiness and predicate emptiness as two central reasoning services in this context. Query emptiness asks whether a given query has an empty answer over all databases formulated in a given vocabulary. Predicate emptiness is defined analogously, but quantifies universally over all queries that contain a given predicate. In this paper, we determine the computational complexity of query emptiness and predicate emptiness in the EL, DL-Lite, and ALC-families of description logics, investigate the connection to ontology modules, and perform a practical case study to evaluate the new reasoning services.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–59},
numpages = {59}
}

@article{10.5555/3013558.3013588,
author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
title = {Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian Networks},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Dynamic Bayesian networks (DBNs) are a general model for stochastic processes with partially observed states. Belief filtering in DBNs is the task of inferring the belief state (i.e. the probability distribution over process states) based on incomplete and noisy observations. This can be a hard problem in complex processes with large state spaces. In this article, we explore the idea of accelerating the filtering task by automatically exploiting causality in the process. We consider a specific type of causal relation, called passivity, which pertains to how state variables cause changes in other variables. We present the Passivity-based Selective Belief Filtering (PSBF) method, which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors. PSBF produces exact belief states under certain assumptions and approximate belief states otherwise, where the approximation error is bounded by the degree of uncertainty in the process. We show empirically, in synthetic processes with varying sizes and degrees of passivity, that PSBF is faster than several alternative methods while achieving competitive accuracy. Furthermore, we demonstrate how passivity occurs naturally in a complex system such as a multi-robot warehouse, and how PSBF can exploit this to accelerate the filtering task.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1135–1178},
numpages = {44}
}

@article{10.5555/3013558.3013587,
author = {Le, Tuan M. V. and Lauw, Hady W.},
title = {Semantic Visualization with Neighborhood Graph Regularization},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Visualization of high-dimensional data, such as text documents, is useful to map out the similarities among various data points. In the high-dimensional space, documents are commonly represented as bags of words, with dimensionality equal to the vocabulary size. Classical approaches to document visualization directly reduce this into visualizable two or three dimensions. Recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling. While aiming for a good fit between the model parameters and the observed data, previous approaches have not considered the local consistency among data instances. We consider the problem of semantic visualization by jointly modeling topics and visualization on the intrinsic document manifold, modeled using a neighborhood graph. Each document has both a topic distribution and visualization coordinate. Specifically, we propose an unsupervised probabilistic model, called Semafore, which aims to preserve the manifold in the lower-dimensional spaces through a neighborhood regularization framework designed for the semantic visualization task. To validate the efficacy of SEMAFORE, our comprehensive experiments on a number of real-life text datasets of news articles and Web pages show that the proposed methods outperform the state-of-the-art baselines on objective evaluation metrics.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1091–1133},
numpages = {43}
}

@article{10.5555/3013558.3013586,
author = {Liu, Hanxiao and Ma, Wanli and Yang, Yiming and Carbonell, Jaime},
title = {Learning Concept Graphs from Online Educational Data},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {This paper addresses an open challenge in educational data mining, i.e., the problem of automatically mapping online courses from different providers (universities, MOOCs, etc.) onto a universal space of concepts, and predicting latent prerequisite dependencies (directed links) among both concepts and courses. We propose a novel approach for inference within and across course-level and concept-level directed graphs. In the training phase, our system projects partially observed course-level prerequisite links onto directed concept-level links; in the testing phase, the induced concept-level links are used to infer the unknown course-level prerequisite links. Whereas courses may be specific to one institution, concepts are shared across different providers. The bi-directional mappings enable our system to perform interlingua-style transfer learning, e.g. treating the concept graph as the interlingua and transferring the prerequisite relations across universities via the interlingua. Experiments on our newly collected datasets of courses from MIT, Caltech, Princeton and CMU show promising results.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1059–1090},
numpages = {32}
}

@article{10.5555/3013558.3013585,
author = {Baskaya, Osman and Jurgens, David},
title = {Semi-Supervised Learning with Induced Word Senses for State of the Art Word Sense Disambiguation},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Word Sense Disambiguation (WSD) aims to determine the meaning of a word in context, and successful approaches are known to benefit many applications in Natural Language Processing. Although supervised learning has been shown to provide superior WSD performance, current sense-annotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words. While unsupervised techniques have been proposed to overcome this data sparsity problem, such techniques have not outperformed supervised methods. In this paper, we propose a new approach to building semi-supervised WSD systems that combines a small amount of sense-annotated data with information from Word Sense Induction, a fully-unsupervised technique that automatically learns the different senses of a word based on how it is used. In three experiments, we show how sense induction models may be effectively combined to ultimately produce high-performance semi-supervised WSD systems that exceed the performance of state-of-the-art supervised WSD techniques trained on the same sense-annotated data. We anticipate that our results and released software will also benefit evaluation practices for sense induction systems and those working in low-resource languages by demonstrating how to quickly produce accurate WSD systems with minimal annotation effort.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1025–1058},
numpages = {34}
}

@article{10.5555/3013558.3013584,
author = {Guo, Jiang and Che, Wanxiang and Yarowsky, David and Wang, Haifeng and Liu, Ting},
title = {A Distributed Representation-Based Framework for Cross-Lingual Transfer Parsing},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {This paper investigates the problem of cross-lingual transfer parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g., English). Existing model transfer approaches typically don't include lexical features, which are not transferable across languages. In this paper, we bridge the lexical feature gap by using distributed feature representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and nonlexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is flexible enough to incorporate additional useful features such as cross-lingual word clusters. Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly outperforms state-of-the-art delexicalized models augmented with projected cluster features on identical data. Finally, we demonstrate that our models can be further boosted with minimal supervision (e.g., 100 annotated sentences) from target languages, which is of great significance for practical usage.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {995–1023},
numpages = {29}
}

@article{10.5555/3013558.3013583,
author = {Vulic, Ivan and Moens, Marie-Francine},
title = {Bilingual Distributed Word Representations from Document-Aligned Comparable Data},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {We propose a new model for learning bilingual word representations from nonparallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {953–994},
numpages = {42}
}

@article{10.5555/3013558.3013582,
author = {Flerova, Natalia and Marinescu, Radu and Dechter, Rina},
title = {Searching for the M Best Solutions in Graphical Models},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {The paper focuses on finding the m best solutions to combinatorial optimization problems using best-first or depth-first branch and bound search. Specifically, we present a new algorithm m-A*, extending the well-known A* to the m-best task, and for the first time prove that all its desirable properties, including soundness, completeness and optimal efficiency, are maintained. Since best-first algorithms require extensive memory, we also extend the memory-efficient depth-first branch and bound to the m-best task.We adapt both algorithms to optimization tasks over graphical models (e.g., Weighted CSP and MPE in Bayesian networks), provide complexity analysis and an empirical evaluation. Our experiments confirm theory that the best-first approach is largely superior when memory is available, but depth-first branch and bound is more robust. We also show that our algorithms are competitive with related schemes recently developed for the m-best task.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {889–952},
numpages = {64}
}

@article{10.5555/3013558.3013581,
author = {Jannach, Dietmar and Schmitz, Thomas and Shchekotykhin, Kostyantyn},
title = {Parallel Model-Based Diagnosis on Multi-Core Computers},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Model-Based Diagnosis (MBD) is a principled and domain-independent way of analyzing why a system under examination is not behaving as expected. Given an abstract description (model) of the system's components and their behavior when functioning normally, MBD techniques rely on observations about the actual system behavior to reason about possible causes when there are discrepancies between the expected and observed behavior. Due to its generality, MBD has been successfully applied in a variety of application domains over the last decades.In many application domains of MBD, testing different hypotheses about the reasons for a failure can be computationally costly, e.g., because complex simulations of the system behavior have to be performed. In this work, we therefore propose different schemes of parallelizing the diagnostic reasoning process in order to better exploit the capabilities of modern multi-core computers. We propose and systematically evaluate parallelization schemes for Reiter's hitting set algorithm for finding all or a few leading minimal diagnoses using two different con flict detection techniques. Furthermore, we perform initial experiments for a basic depth-first search strategy to assess the potential of parallelization when searching for one single diagnosis. Finally, we test the effects of parallelizing "direct encodings" of the diagnosis problem in a constraint solver.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {835–887},
numpages = {53}
}

@article{10.5555/3013558.3013580,
author = {Fang, Zhiwen and Li, Chu-Min and Xu, Ke},
title = {An Exact Algorithm Based on MaxSAT Reasoning for the Maximum Weight Clique Problem},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Recently, MaxSAT reasoning is shown very effective in computing a tight upper bound for a Maximum Clique (MC) of a (unweighted) graph. In this paper, we apply MaxSAT reasoning to compute a tight upper bound for a Maximum Weight Clique (MWC) of a wighted graph. We first study three usual encodings of MWC into weighted partial MaxSAT dealing with hard clauses, which must be satisfied in all solutions, and soft clauses, which are weighted and can be falsified. The drawbacks of these encodings motivate us to propose an encoding of MWC into a special weighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding and dedicated for upper bounding an MWC, in which both soft clauses and literals in soft clauses are weighted. An optimal solution of the LW MaxSAT instance gives an upper bound for an MWC, instead of an optimal solution for MWC. We then introduce two notions called the Top-k literal failed clause and the Top-k empty clause to extend classical MaxSAT reasoning techniques, as well as two sound transformation rules to transform an LW MaxSAT instance. Successive transformations of an LW MaxSAT instance driven by MaxSAT reasoning give a tight upper bound for the encoded MWC. The approach is implemented in a branch-and-bound algorithm called MWCLQ. Experimental evaluations on the broadly used DIMACS benchmark, BHOSLIB benchmark, random graphs and the benchmark from the winner determination problem show that our approach allows MWCLQ to reduce the search space significantly and to solve MWC instances effectively. Consequently, MWCLQ outperforms state-of-the-art exact algorithms on the vast majority of instances. Moreover, it is surprisingly effective in solving hard and dense instances.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {799–833},
numpages = {35}
}

@article{10.5555/3013558.3013579,
author = {Parisi, Francesco and Grant, John},
title = {Knowledge Representation in Probabilistic Spatio-Temporal Knowledge Bases},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {We represent knowledge as integrity constraints in a formalization of probabilistic spatio-temporal knowledge bases. We start by defining the syntax and semantics of a formalization called PST knowledge bases. This definition generalizes an earlier version, called SPOT, which is a declarative framework for the representation and processing of probabilistic spatio-temporal data where probability is represented as an interval because the exact value is unknown. We augment the previous definition by adding a type of non-atomic formula that expresses integrity constraints. The result is a highly expressive formalism for knowledge representation dealing with probabilistic spatio-temporal data. We obtain complexity results both for checking the consistency of PST knowledge bases and for answering queries in PST knowledge bases, and also specify tractable cases. All the domains in the PST framework are finite, but we extend our results also to arbitrarily large finite domains.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {743–798},
numpages = {56}
}

@article{10.5555/3013558.3013578,
author = {Garc\'{\i}a-Dur\'{a}n, Alberto and Bordes, Antoine and Usunier, Nicolas and Grandvalet, Yves},
title = {Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {This paper tackles the problem of endogenous link prediction for knowledge base completion. Knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose TATEC, a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {715–742},
numpages = {28}
}

@article{10.5555/3013558.3013577,
author = {De Nijs, Roderick and Landsiedel, Christian and Wollher, Dirk and Buss, Martin},
title = {Quadratization and Roof Duality of Markov Logic Networks},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {This article discusses the quadratization of Markov Logic Networks, which enables efficient approximate MAP computation by means of maximum ows. The procedure relies on a pseudo-Boolean representation of the model, and allows handling models of any order. The employed pseudo-Boolean representation can be used to identify problems that are guaranteed to be solvable in low polynomial-time. Results on common benchmark problems show that the proposed approach finds optimal assignments for most variables in excellent computational time and approximate solutions that match the quality of ILP-based solvers.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {685–714},
numpages = {30}
}

@article{10.5555/3013558.3013576,
author = {Zhu, Xiaoyuan and Yuan, Changhe},
title = {Exact Algorithms for MRE Inference},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Most Relevant Explanation (MRE) is an inference task in Bayesian networks that finds the most relevant partial instantiation of target variables as an explanation for given evidence by maximizing the Generalized Bayes Factor (GBF). No exact MRE algorithm has been developed previously except exhaustive search. This paper fills the void by introducing two Breadth-First Branch-and-Bound (BFBnB) algorithms for solving MRE based on novel upper bounds of GBF. One upper bound is created by decomposing the computation of GBF using a target blanket decomposition of evidence variables. The other upper bound improves the first bound in two ways. One is to split the target blankets that are too large by converting auxiliary nodes into pseudo-targets so as to scale to large problems. The other is to perform summations instead of maximizations on some of the target variables in each target blanket. Our empirical evaluations show that the proposed BFBnB algorithms make exact MRE inference tractable in Bayesian networks that could not be solved previously.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {653–683},
numpages = {31}
}

@article{10.5555/3013558.3013575,
author = {Bredereck, Robert and Faliszewski, Piotr and Niedermeier, Rolf and Talmon, Nimrod},
title = {Large-Scale Election Campaigns: Combinatorial Shift Bribery},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {We study the complexity of a combinatorial variant of the SHIFT BRIBERY problem in elections. In the standard SHIFT BRIBERY problem, we are given an election where each voter has a preference order over the set of candidates and where an outside agent, the briber, can pay each voter to rank the briber's favorite candidate a given number of positions higher. The goal is to ensure the victory of the briber's preferred candidate. The combinatorial variant of the problem, introduced in this paper, models settings where it is possible to affect the position of the preferred candidate in multiple votes, either positively or negatively, with a single bribery action. This variant of the problem is particularly interesting in the context of large-scale campaign management problems (which, from the technical side, are modeled as bribery problems). We show that, in general, the combinatorial variant of the problem is highly intractable; specifically, NP-hard, hard in the parameterized sense, and hard to approximate. Nevertheless, we provide parameterized algorithms and approximation algorithms for natural restricted cases.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {603–652},
numpages = {50}
}

@article{10.5555/3013558.3013574,
author = {Brandt, Felix and Geist, Christian},
title = {Finding Strategyproof Social Choice Functions via SAT Solving},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {A promising direction in computational social choice is to address research problems using computer-aided proving techniques. In particular with SAT solvers, this approach has been shown to be viable not only for proving classic impossibility theorems such as Arrow's Theorem but also for finding new impossibilities in the context of preference extensions. In this paper, we demonstrate that these computer-aided techniques can also be applied to improve our understanding of strategyproof irresolute social choice functions. These functions, however, require a more evolved encoding as otherwise the search space rapidly becomes much too large. Our contribution is two-fold: We present an efficient encoding for translating such problems to SAT and leverage this encoding to prove new results about strategyproofness with respect to Kelly's and Fishburn's preference extensions. For example, we show that no Pareto-optimal majoritarian social choice function satisfies Fishburn-strategyproofness. Furthermore, we explain how human-readable proofs of such results can be extracted from minimal unsatisfiable cores of the corresponding SAT formulas.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {565–602},
numpages = {38}
}

@article{10.5555/3013558.3013573,
author = {Romero, Ana Armas and Kaminski, Mark and Grau, Bernardo Cuenca and Horrocks, Ian},
title = {Module Extraction in Expressive Ontology Languages via Datalog Reasoning},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Module extraction is the task of computing a (preferably small) fragment M of an ontology O that preserves a class of entailments over a signature of interest Σ. Extracting modules of minimal size is well-known to be computationally hard, and often algorithmically infeasible, especially for highly expressive ontology languages. Thus, practical techniques typically rely on approximations, where M provably captures the relevant entailments, but is not guaranteed to be minimal. Existing approximations ensure that M preserves all second-order entailments of O w.r.t. Σ, which is a stronger condition than is required in many applications, and may lead to unnecessarily large modules in practice. In this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog. Our approach generalises existing approximations in an elegant way. More importantly, it allows extraction of modules that are tailored to preserve only specific kinds of entailments, and thus are often significantly smaller. Our evaluation on a wide range of ontologies confirms the feasibility and benefits of our approach in practice.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {499–564},
numpages = {66}
}

@article{10.5555/3013558.3013572,
author = {Dibangoye, Jilles Steeve and Amato, Christopher and Buffet, Olivier and Charpillet, Fran\c{c}ois},
title = {Optimally Solving Dec-POMDPs as Continuous-State MDPs},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally (NEXP-Complete). As a new way of solving these problems, we introduce the idea of transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function. This approach makes use of the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP methods to be used for the first time. To provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent domains, without losing the ability to converge to an optimal solution. In particular, we introduce a feature-based heuristic search value iteration (FB-HSVI) algorithm that relies on feature-based compact representations, point-based updates and efficient action selection. A theoretical analysis demonstrates that FB-HSVI terminates in finite time with an optimal solution. We include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {443–497},
numpages = {55}
}

@article{10.5555/3013558.3013571,
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
title = {Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {409–442},
numpages = {34}
}

@article{10.5555/3013558.3013570,
author = {Culotta, Aron and Ravi, Nirmal Kumar and Cutler, Jennifer},
title = {Predicting Twitter User Demographics Using Distant Supervision from Website Traffic Data},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Understanding the demographics of users of online social networks has important applications for health, marketing, and public messaging. Whereas most prior approaches rely on a supervised learning approach, in which individual users are labeled with demographics for training, we instead create a distantly labeled dataset by collecting audience measurement data for 1,500 websites (e.g., 50% of visitors to gizmodo.com are estimated to have a bachelor's degree). We then fit a regression model to predict these demographics from information about the followers of each website on Twitter. Using patterns derived both from textual content and the social network of each user, our final model produces an average held-out correlation of .77 across seven different variables (age, gender, education, ethnicity, income, parental status, and political preference). We then apply this model to classify individual Twitter users by ethnicity, gender, and political preference, finding performance that is surprisingly competitive with a fully supervised approach.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {389–408},
numpages = {20}
}

@article{10.5555/3013558.3013569,
author = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and De Freitas, Nando},
title = {Bayesian Optimization in a Billion Dimensions via Random Embeddings},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {361–387},
numpages = {27}
}

@article{10.5555/3013558.3013568,
author = {Ho, Chien-Ju and Slivkins, Aleksandrs and Vaughan, Jennifer Wortman},
title = {Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of "bonus" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {317–359},
numpages = {43}
}

@article{10.5555/3013558.3013567,
author = {Rupnik, Jan and Muhi\v{c}, Andrej and Leban, Gregor and \v{S}kraba, Primo\v{z} and Fortuna, Bla\v{z} and Grobelnik, Marko},
title = {News across Languages - Cross-Lingual Document Similarity and Event Tracking},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {In today's world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {283–316},
numpages = {34}
}

@article{10.5555/3013558.3013566,
author = {Khwileh, Ahmad and Ganguly, Debasis and Jones, Gareth J. F.},
title = {Utilisation of Metadata Fields and Query Expansion in Cross-Lingual Search of User-Generated Internet Video},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Recent years have seen significant efforts in the area of Cross Language Information Retrieval (CLIR) for text retrieval. This work initially focused on formally published content, but more recently research has begun to concentrate on CLIR for informal social media content. However, despite the current expansion in online multimedia archives, there has been little work on CLIR for this content. While there has been some limited work on Cross-Language Video Retrieval (CLVR) for professional videos, such as documentaries or TV news broadcasts, there has to date, been no significant investigation of CLVR for the rapidly growing archives of informal user generated (UGC) content. Key differences between such UGC and professionally produced content are the nature and structure of the textual UGC metadata associated with it, as well as the form and quality of the content itself. In this setting, retrieval effectiveness may not only suffer from translation errors common to all CLIR tasks, but also recognition errors associated with the automatic speech recognition (ASR) systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated user-created metadata for each video. This work proposes and evaluates techniques to improve CLIR effectiveness of such noisy UGC content. Our experimental investigation shows that different sources of evidence, e.g. the content from different fields of the structured metadata, significantly affect CLIR effectiveness. Results from our experiments also show that each metadata field has a varying robustness to query expansion (QE) and hence can have a negative impact on the CLIR effectiveness. Our work proposes a novel adaptive QE technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving CLIR effectiveness for UGC content.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {249–281},
numpages = {33}
}

@article{10.5555/3013558.3013565,
author = {Tiedemann, J\"{o}rg and Agic, \v{Z}eljko},
title = {Synthetic Treebanking for Cross-Lingual Dependency Parsing},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {How do we parse the languages for which no treebanks are available? This contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. We outline the benefits, and indicate the drawbacks of the current major approaches. We emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. We present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. We further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {209–248},
numpages = {40}
}

@article{10.5555/3013558.3013564,
author = {Helou, Mamoun Abu and Palmonari, Matteo and Jarrar, Mustafa},
title = {Effectiveness of Automatic Translations for Cross-Lingual Ontology Mapping},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Accessing or integrating data lexicalized in different languages is a challenge. Multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages. In this paper we present a large-scale study on the effectiveness of automatic translations to support two key cross-lingual ontology mapping tasks: the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment. We conduct our experiments using four different large gold standards, each one consisting of a pair of mapped wordnets, to cover four different families of languages. We categorize concepts based on their lexicalization (type of words, synonym richness, position in a subconcept graph) and analyze their distributions in the gold standards. Leveraging this categorization, we measure several aspects of translation effectiveness, such as word-translation correctness, word sense coverage, synset and synonym coverage. Finally, we thoroughly discuss several findings of our study, which we believe are helpful for the design of more sophisticated cross-lingual mapping algorithms.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {165–208},
numpages = {44}
}

@article{10.5555/3013558.3013563,
author = {Fern\'{a}ndez, Alejandro Moreo and Esuli, Andrea and Sebastiani, Fabrizio},
title = {Distributional Correspondence Indexing for Cross-Lingual and Cross-Domain Sentiment Classification},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Domain Adaptation (DA) techniques aim at enabling machine learning methods learn effective classifiers for a "target" domain when the only available training data belongs to a different "source" domain. In this paper we present the Distributional Correspondence Indexing (DCI) method for domain adaptation in sentiment classification. DCI derives term representations in a vector space common to both domains where each dimension re ects its distributional correspondence to a pivot, i.e., to a highly predictive term that behaves similarly across domains. Term correspondence is quantified by means of a distributional correspondence function (DCF). We propose a number of efficient DCFs that are motivated by the distributional hypothesis, i.e., the hypothesis according to which terms with similar meaning tend to have similar distributions in text. Experiments show that DCI obtains better performance than current state-of-the-art techniques for cross-lingual and cross-domain sentiment classification. DCI also brings about a significantly reduced computational cost, and requires a smaller amount of human intervention. As a final contribution, we discuss a more challenging formulation of the domain adaptation problem, in which both the cross-domain and cross-lingual dimensions are tackled simultaneously.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {131–163},
numpages = {33}
}

@article{10.5555/3013558.3013562,
author = {Mohammad, Saif M. and Salameh, Mohammad and Kiritchenko, Svetlana},
title = {How Translation Alters Sentiment},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Sentiment analysis research has predominantly been on English texts. Thus there exist many sentiment resources for English, but less so for other languages. Approaches to improve sentiment analysis in a resource-poor focus language include: (a) translate the focus language text into a resource-rich language such as English, and apply a powerful English sentiment analysis system on the text, and (b) translate resources such as sentiment labeled corpora and sentiment lexicons from English into the focus language, and use them as additional resources in the focus-language sentiment analysis system. In this paper we systematically examine both options. We use Arabic social media posts as stand-in for the focus language text. We show that sentiment analysis of English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We show that Arabic sentiment analysis systems benefit from the use of automatically translated English sentiment lexicons. We also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text. This is especially relevant for building better automatic translation systems. In the process, we create a state-of-the-art Arabic sentiment analysis system, a new dialectal Arabic sentiment lexicon, and the first Arabic-English parallel corpus that is independently annotated for sentiment by Arabic and English speakers.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {95–130},
numpages = {36}
}

@article{10.5555/3013558.3013561,
author = {Tsvetkov, Yulia and Dyer, Chris},
title = {Cross-Lingual Bridges with Models of Lexical Borrowing},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a "donor" language to a "recipient" language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and--in contrast to cognate relationships--borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili's vocabulary is borrowed from the unrelated language Arabic). In this work, we develop a model of morpho-phonological transformations across languages. Its features are based on universal constraints from Optimality Theory (OT), and we show that compared to several standard--but linguistically more na\"{\i}ve--baselines, our OT-inspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages. We demonstrate applications of the lexical borrowing model in machine translation, using resource-rich donor language to obtain translations of out-of-vocabulary loanwords in a lower resource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {63–93},
numpages = {31}
}

@article{10.5555/3013558.3013560,
author = {S\'{a}nchez-Cartagena, V\'{\i}ctor M. and P\'{e}rez-Ortiz, Juan Antonio and S\'{a}nchez-Mart\'{\i}nez, Felipe},
title = {Integrating Rules and Dictionaries from Shallow-Transfer Machine Translation into Phrase-Based Statistical Machine Translation},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {We describe a hybridisation strategy whose objective is to integrate linguistic resources from shallow-transfer rule-based machine translation (RBMT) into phrase-based statistical machine translation (PBSMT). It basically consists of enriching the phrase table of a PBSMT system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallow-transfer RBMT system. This new strategy takes advantage of how the linguistic resources are used by the RBMT system to segment the source-language sentences to be translated, and overcomes the limitations of existing hybrid approaches that treat the RBMT systems as a black box. Experimental results confirm that our approach delivers translations of higher quality than existing ones, and that it is specially useful when the parallel corpus available for training the SMT system is small or when translating out-of-domain texts that are well covered by the RBMT dictionaries. A combination of this approach with a recently proposed unsupervised shallow-transfer rule inference algorithm results in a significantly greater translation quality than that of a baseline PBSMT; in this case, the only hand-crafted resource used are the dictionaries commonly used in RBMT. Moreover, the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with hand-crafted rules.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {17–61},
numpages = {45}
}

@article{10.5555/3013558.3013559,
author = {Costa-juss\`{a}, Marta R. and Bangalore, Srinivas and Lambert, Patrik and M\`{a}rquez, Llu\'{\i}s and Montiel-Ponsoda, Elena},
title = {Introduction to the Special Issue on Cross-Language Algorithms and Applications},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {With the increasingly global nature of our everyday interactions, the need for multilingual technologies to support efficient and effective information access and communication cannot be overemphasized. Computational modeling of language has been the focus of Natural Language Processing, a subdiscipline of Artificial Intelligence. One of the current challenges for this discipline is to design methodologies and algorithms that are cross-language in order to create multilingual technologies rapidly. The goal of this JAIR special issue on Cross-Language Algorithms and Applications (CLAA) is to present leading research in this area, with emphasis on developing unifying themes that could lead to the development of the science of multi- and cross-lingualism. In this introduction, we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included. The selected papers cover a broad range of cross-lingual technologies including machine translation, domain and language adaptation for sentiment analysis, cross-language lexical resources, dependency parsing, information retrieval and knowledge representation. We anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of cross-lingual natural language processing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–15},
numpages = {15}
}

@article{10.5555/2910557.2910573,
author = {Imai, Tatsuya and Fukunaga, Alex},
title = {On a Practical, Integer-Linear Programming Model for Delete-Free Tasks and Its Use as a Heuristic for Cost-Optimal Planning},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {We propose a new integer-linear programming model for the delete relaxation in cost-optimal planning. While a straightforward IP for the delete relaxation is impractical, our enhanced model incorporates variable reduction techniques based on landmarks, relevance-based constraints, dominated action elimination, immediate action application, and inverse action constraints, resulting in an IP that can be used to directly solve delete-free planning problems. We show that our IP model is competitive with previous state-of-the-art solvers for delete-free problems. The LP-relaxation of the IP model is often a very good approximation to the IP, providing an approach to approximating the optimal value of the delete-free task that is complementary to the well-known LM-cut heuristic. We also show that constraints that partially consider delete effects can be added to our IP/LP models. We embed the new IP/LP models into a forward-search based planner, and show that the performance of the resulting planner on standard IPC benchmarks is comparable with the state-of-the-art for cost-optimal planning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {631–677},
numpages = {47}
}

@article{10.5555/2910557.2910572,
author = {Strasser, Ben and Botea, Adi and Harabor, Daniel},
title = {Compressing Optimal Paths with Run Length Encoding},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {We introduce a novel approach to Compressed Path Databases, space efficient oracles used to very quickly identify the first edge on a shortest path. Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (RLE) on the contents of the matrix. One variant of our implemented system was, by a convincing margin, the fastest entry in the 2014 Grid-Based Path Planning Competition.We give a first tractability analysis for the compression scheme used by our algorithm. We study the complexity of computing a database of minimum size for general directed and undirected graphs. We find that in both cases the problem is NP-complete. We also show that, for graphs which can be decomposed along articulation points, the problem can be decomposed into independent parts, with a corresponding reduction in its level of difficulty. In particular, this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {593–629},
numpages = {37}
}

@article{10.5555/2910557.2910571,
author = {Steigmiller, Andreas and Glimm, Birte},
title = {Pay-as-You-Go Description Logic Reasoning by Coupling Tableau and Saturation Procedures},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Nowadays, saturation-based reasoners for the OWL EL profile of the Web Ontology Language are able to handle large ontologies such as SNOMED very efficiently. However, it is currently unclear how saturation-based reasoning procedures can be extended to very expressive Description Logics such as SROIQ--the logical underpinning of the current and second iteration of the Web Ontology Language. Tableau-based procedures, on the other hand, are not limited to specific Description Logic languages or OWL profiles, but even highly optimised tableau-based reasoners might not be efficient enough to handle large ontologies such as SNOMED. In this paper, we present an approach for tightly coupling tableau - and saturation-based procedures that we implement in the OWL DL reasoner Konclude. Our detailed evaluation shows that this combination significantly improves the reasoning performance for a wide range of ontologies.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {535–592},
numpages = {58}
}

@article{10.5555/2910557.2910570,
author = {Aziz, Haris and Brill, Markus and Fischer, Felix and Harrenstein, Paul and Lang, J\'{e}r\^{o}me and Seedig, Hans Georg},
title = {Possible and Necessary Winners of Partial Tournaments},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {We study the problem of computing possible and necessary winners for partially specified weighted and unweighted tournaments. This problem arises naturally in elections with incompletely specified votes, partially completed sports competitions, and more generally in any scenario where the outcome of some pairwise comparisons is not yet fully known. We specifically consider a number of well-known solution concepts--including the uncovered set, Borda, ranked pairs, and maximin--and show that for most of them, possible and necessary winners can be identified in polynomial time. These positive algorithmic results stand in sharp contrast to earlier results concerning possible and necessary winners given partially specified preference profiles.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {493–534},
numpages = {42}
}

@article{10.5555/2910557.2910569,
author = {Halpern, Joseph Y.},
title = {Weighted Regret-Based Likelihood: A New Approach to Describing Uncertainty},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Recently, Halpern and Leung suggested representing uncertainty by a set of weighted probability measures, and suggested a way of making decisions based on this representation of uncertainty: maximizing weighted regret. Their paper does not answer an apparently simpler question: what it means, according to this representation of uncertainty, for an event E to be more likely than an event E′. In this paper, a notion of comparative likelihood when uncertainty is represented by a set of weighted probability measures is defined. It generalizes the ordering defined by probability (and by lower probability) in a natural way; a generalization of upper probability can also be defined. A complete axiomatic characterization of this notion of regret-based likelihood is given.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {471–492},
numpages = {22}
}

@article{10.5555/2910557.2910568,
author = {Rochlin, Igor and Sarne, David},
title = {Constraining Information Sharing to Improve Cooperative Information Gathering},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {This paper considers the problem of cooperation between self-interested agents in acquiring better information regarding the nature of the different options and opportunities available to them. By sharing individual findings with others, the agents can potentially achieve a substantial improvement in overall and individual expected benefits. Unfortunately, it is well known that with self-interested agents equilibrium considerations often dictate solutions that are far from the fully cooperative ones, hence the agents do not manage to fully exploit the potential benefits encapsulated in such cooperation. In this paper we introduce, analyze and demonstrate the benefit of five methods aiming to improve cooperative information gathering. Common to all five that they constrain and limit the information sharing process. Nevertheless, the decrease in benefit due to the limited sharing is outweighed by the resulting substantial improvement in the equilibrium individual information gathering strategies. The equilibrium analysis given in the paper, which, in itself is an important contribution to the study of cooperation between self-interested agents, enables demonstrating that for a wide range of settings an improved individual expected benefit is achieved for all agents when applying each of the five methods.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {437–469},
numpages = {33}
}

@article{10.5555/2910557.2910567,
author = {Siddiqui, Fazlul Hasan and Haslum, Patrik},
title = {Continuing Plan Quality Optimisation},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Finding high quality plans for large planning problems is hard. Although some current anytime planners are often able to improve plans quickly, they tend to reach a limit at which the plans produced are still very far from the best possible, but these planners fail to find any further improvement, even when given several hours of runtime.We present an approach to continuing plan quality optimisation at larger time scales, and its implementation in a system called BDPO2. Key to this approach is a decomposition into subproblems of improving parts of the current best plan. The decomposition is based on block deordering, a form of plan deordering which identifies hierarchical plan structure. BDPO2 can be seen as an application of the large neighbourhood search (LNS) local search strategy to planning, where the neighbourhood of a plan is defined by replacing one or more subplans with improved subplans. On-line learning is also used to adapt the strategy for selecting subplans and subplanners over the course of plan optimisation.Even starting from the best plans found by other means, BDPO2 is able to continue improving plan quality, often producing better plans than other anytime planners when all are given enough runtime. The best results, however, are achieved by a combination of different techniques working together.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {369–435},
numpages = {67}
}

@article{10.5555/2910557.2910566,
author = {Zhou, Yujiao and Grau, Bernardo Cuenca and Nenov, Yavor and Kaminski, Mark and Horrocks, Ian},
title = {PAGOdA: Pay-as-You-Go Ontology Query Answering Using a Datalog Reasoner},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Answering conjunctive queries over ontology-enriched datasets is a core reasoning task for many applications. Query answering is, however, computationally very expensive, which has led to the development of query answering procedures that sacrifice either expressive power of the ontology language, or the completeness of query answers in order to improve scalability. In this paper, we describe a hybrid approach to query answering over OWL 2 ontologies that combines a datalog reasoner with a fully-fledged OWL 2 reasoner in order to provide scalable 'pay-as-you-go' performance. The key feature of our approach is that it delegates the bulk of the computation to the datalog reasoner and resorts to expensive OWL 2 reasoning only as necessary to fully answer the query. Furthermore, although our main goal is to efficiently answer queries over OWL 2 ontologies and data, our technical results are very general and our approach is applicable to first-order knowledge representation languages that can be captured by rules allowing for existential quantification and disjunction in the head; our only assumption is the availability of a datalog reasoner and a fully-fledged reasoner for the language of interest, both of which are used as 'black boxes'. We have implemented our techniques in the PAGOdA system, which combines the datalog reasoner RDFox and the OWL 2 reasoner HermiT. Our extensive evaluation shows that PAGOdA succeeds in providing scalable pay-as-you-go query answering for a wide range of OWL 2 ontologies, datasets and queries.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {309–367},
numpages = {59}
}

@article{10.5555/2910557.2910565,
author = {Mossakowski, Till and Moratz, Reinhard},
title = {Relations between Spatial Calculi about Directions and Orientations},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Qualitative spatial descriptions characterize essential properties of spatial objects or configurations by relying on relative comparisons rather than measuring. Typically, in qualitative approaches only relatively coarse distinctions between configurations are made. Qualitative spatial knowledge can be used to represent incomplete and underdetermined knowledge in a systematic way. This is especially useful if the task is to describe features of classes of configurations rather than individual configurations.Although reasoning with them is generally NP-hard (even ∃IR-complete), relative directions are important because they play a key role in human spatial descriptions and there are several approaches how to represent them using qualitative methods. In these approaches directions between spatial locations can be expressed as constraints over infinite domains, e.g. the Euclidean plane. The theory of relation algebras has been successfully applied to this field. Viewing relation algebras as universal algebras and applying and modifying standard tools from universal algebra in this work, we (re)define notions of qualitative constraint calculus, of homomorphism between calculi, and of quotient of calculi. Based on this method we derive important properties for spatial calculi from corresponding properties of related calculi. From a conceptual point of view these formal mappings between calculi are a means to translate between different granularities.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {277–308},
numpages = {32}
}

@article{10.5555/2910557.2910564,
author = {Kalech, Meir and Reches, Shulamit},
title = {Decision Making with Dynamic Uncertain Events},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {When to make a decision is a key question in decision making problems characterized by uncertainty. In this paper we deal with decision making in environments where information arrives dynamically. We address the tradeoff between waiting and stopping strategies. On the one hand, waiting to obtain more information reduces uncertainty, but it comes with a cost. Stopping and making a decision based on an expected utility reduces the cost of waiting, but the decision is based on uncertain information. We propose an optimal algorithm and two approximation algorithms. We prove that one approximation is optimistic - waits at least as long as the optimal algorithm, while the other is pessimistic - stops not later than the optimal algorithm. We evaluate our algorithms theoretically and empirically and show that the quality of the decision in both approximations is near-optimal and much faster than the optimal algorithm. Also, we can conclude from the experiments that the cost function is a key factor to chose the most effective algorithm.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {233–275},
numpages = {43}
}

@article{10.5555/2910557.2910563,
author = {Strass, Hannes},
title = {Expressiveness of Two-Valued Semantics for Abstract Dialectical Frameworks},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {We analyse the expressiveness of Brewka and Woltran's abstract dialectical frameworks for two-valued semantics. By expressiveness we mean the ability to encode a desired set of two-valued interpretations over a given propositional vocabulary A using only atoms from A. We also compare ADFs' expressiveness with that of (the two-valued semantics of) abstract argumentation frameworks, normal logic programs and propositional logic. While the computational complexity of the two-valued model existence problem for all these languages is (almost) the same, we show that the languages form a neat hierarchy with respect to their expressiveness. We then demonstrate that this hierarchy collapses once we allow to introduce a linear number of new vocabulary elements. We finally also analyse and compare the representational succinctness of ADFs (for two-valued model semantics), that is, their capability to represent two-valued interpretation sets in a space-efficient manner.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {193–231},
numpages = {39}
}

@article{10.5555/2910557.2910562,
author = {Formiga, Llu\'{\i}s and Barr\'{o}n-Cede\~{n}o, Alberto and M\`{a}rquez, Llu\'{\i}s and Henr\'{\i}quez, Carlos A. and Mari\~{n}o, Jos\'{e} B.},
title = {Leveraging Online User Feedback to Improve Statistical Machine Translation},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {In this article we present a three-step methodology for dynamically improving a statistical machine translation (SMT) system by incorporating human feedback in the form of free edits on the system translations. We target at feedback provided by casual users, which is typically error-prone. Thus, we first propose a filtering step to automatically identify the better user-edited translations and discard the useless ones. A second step produces a pivot-based alignment between source and user-edited sentences, focusing on the errors made by the system. Finally, a third step produces a new translation model and combines it linearly with the one from the original system. We perform a thorough evaluation on a real-world dataset collected from the Reverso.net translation service and show that every step in our methodology contributes significantly to improve a general purpose SMT system. Interestingly, the quality improvement is not only due to the increase of lexical coverage, but to a better lexical selection, reordering, and morphology. Finally, we show the robustness of the methodology by applying it to a different scenario, in which the new examples come from an automatically Web-crawled parallel corpus. Using exactly the same architecture and models provides again a significant improvement of the translation quality of a general purpose baseline SMT system.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {159–192},
numpages = {34}
}

@article{10.5555/2910557.2910561,
author = {Kiesel, Scott and Burns, Ethan and Ruml, Wheeler},
title = {Achieving Goals Quickly Using Real-Time Search: Experimental Results in Video Games},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {In real-time domains such as video games, planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must return the next action for the agent to execute. We explore the use of real-time heuristic search in two benchmark domains inspired by video games. Unlike classic benchmarks such as grid pathfinding and the sliding tile puzzle, these new domains feature exogenous change and directed state space graphs. We consider the setting in which planning and acting are concurrent and we use the natural objective of minimizing goal achievement time. Using both the classic benchmarks and the new domains, we investigate several enhancements to a leading real-time search algorithm, LSS-LRTA*. We show experimentally that 1) it is better to plan after each action or to use a dynamically sized lookahead, 2) A*-based lookahead can cause undesirable actions to be selected, and 3) on-line de-biasing of the heuristic can lead to improved performance. We hope this work encourages future research on applying real-time search in dynamic domains.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {123–158},
numpages = {36}
}

@article{10.5555/2910557.2910560,
author = {Izquierdo, Rub\'{e}n and Su\'{a}rez, Armando and Rigau, German},
title = {Word vs. Class-Based Word Sense Disambiguation},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {As empirically demonstrated by the Word Sense Disambiguation (WSD) tasks of the last SensEval/SemEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed. Many authors argue that one possible reason could be the use of inappropriate sets of word meanings. In particular, WordNet has been used as a defacto standard repository of word meanings in most of these tasks. Thus, instead of using the word senses defined in WordNet, some approaches have derived semantic classes representing groups of word senses. However, the meanings represented by WordNet have been only used for WSD at a very fine-grained sense level or at a very coarse-grained semantic class level (also called SuperSenses). We suspect that an appropriate level of abstraction could be on between both levels. The contributions of this paper are manifold. First, we propose a simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal Word-Net meanings. Second, we empirically demonstrate that our automatically derived semantic classes outperform classical approaches based on word senses and more coarse-grained sense groupings. Third, we also demonstrate that our supervised WSD system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples. Finally, we also demonstrate the robustness of our supervised semantic class-based WSD system when tested on out of domain corpus.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {83–122},
numpages = {40}
}

@article{10.5555/2910557.2910559,
author = {S\ae{}ther, Sigve Hortemo and Telle, Jan Arne and Vatshelle, Martin},
title = {Solving #SAT and MaxSAT by Dynamic Programming},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {We look at dynamic programming algorithms for propositional model counting, also called #SAT, and MaxSAT. Tools from graph structure theory, in particular treewidth, have been used to successfully identify tractable cases in many subfields of AI, including SAT, Constraint Satisfaction Problems (CSP), Bayesian reasoning, and planning. In this paper we attack #SAT and MaxSAT using similar, but more modern, graph structure tools. The tractable cases will include formulas whose class of incidence graphs have not only unbounded treewidth but also unbounded clique-width. We show that our algorithms extend all previous results for MaxSAT and #SAT achieved by dynamic programming along structural decompositions of the incidence graph of the input formula. We present some limited experimental results, comparing implementations of our algorithms to state-of-the-art #SAT and MaxSAT solvers, as a proof of concept that warrants further research.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {59–82},
numpages = {24}
}

@article{10.5555/2910557.2910558,
author = {Bar-Haim, Roy and Dagan, Ido and Berant, Jonathan},
title = {Knowledge-Based Textual Inference via Parse-Tree Transformations},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Textual inference is an important component in many applications for understanding natural language. Classical approaches to textual inference rely on logical representations for meaning, which may be regarded as "external" to the natural language itself. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe an inference formalism that operates directly on language-based structures, particularly syntactic parse trees. New trees are generated by applying inference rules, which provide a unified representation for varying types of inferences. We use manual and automatic methods to generate these rules, which cover generic linguistic structures as well as specific lexical-based inferences. We also present a novel packed data-structure and a corresponding inference algorithm that allows efficient implementation of this formalism. We proved the correctness of the new algorithm and established its efficiency analytically and empirically. The utility of our approach was illustrated on two tasks: unsupervised relation extraction from a large corpus, and the Recognizing Textual Entailment (RTE) benchmarks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–57},
numpages = {57}
}

@article{10.5555/2831071.2831089,
author = {Hunter, Aaron and Delgrande, James P.},
title = {Belief Change with Uncertain Action Histories},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {We consider the iterated belief change that occurs following an alternating sequence of actions and observations. At each instant, an agent has beliefs about the actions that have occurred as well as beliefs about the resulting state of the world. We represent such problems by a sequence of ranking functions, so an agent assigns a quantitative plausibility value to every action and every state at each point in time. The resulting formalism is able to represent fallible belief, erroneous perception, exogenous actions, and failed actions. We illustrate that our framework is a generalization of several existing approaches to belief change, and it appropriately captures the non-elementary interaction between belief update and belief revision.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {779–824},
numpages = {46}
}

@article{10.5555/2831071.2831088,
author = {Lindauer, Marius and Hoos, Holger H. and Hutter, Frank and Schaub, Torsten},
title = {AutoFolio: An Automatically Configured Algorithm Selector},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Algorithm selection (AS) techniques - which involve choosing from a set of algorithms the one expected to solve a given problem instance most efficiently - have substantially improved the state of the art in solving many prominent AI problems, such as SAT, CSP, ASP, MAXSAT and QBF. Although several AS procedures have been introduced, not too surprisingly, none of them dominates all others across all AS scenarios. Furthermore, these procedures have parameters whose optimal values vary across AS scenarios. This holds specifically for the machine learning techniques that form the core of current AS procedures, and for their hyperparameters. Therefore, to successfully apply AS to new problems, algorithms and benchmark sets, two questions need to be answered: (i) how to select an AS approach and (ii) how to set its parameters effectively. We address both of these problems simultaneously by using automated algorithm configuration. Specifically, we demonstrate that we can automatically configure CLASPFOLIO 2, which implements a large variety of different AS approaches and their respective parameters in a single, highly-parameterized algorithm framework. Our approach, dubbed AUTOFOLIO, allows researchers and practitioners across a broad range of applications to exploit the combined power of many different AS methods. We demonstrate AUTOFOLIO can significantly improve the performance of CLASPFOLIO 2 on 8 out of the 13 scenarios from the Algorithm Selection Library, leads to new state-of-the-art algorithm selectors for 7 of these scenarios, and matches state-of-the-art performance (statistically) on all other scenarios. Compared to the best single algorithm for each AS scenario, AUTOFOLIO achieves average speedup factors between 1:3 and 15:4.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {745–778},
numpages = {34}
}

@article{10.5555/2831071.2831087,
author = {Krysta, Piotr and Telelis, Orestis and Ventre, Carmine},
title = {Mechanisms for Multi-Unit Combinatorial Auctions with a Few Distinct Goods},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {We design and analyze deterministic truthful approximation mechanisms for multiunit Combinatorial Auctions involving only a constant number of distinct goods, each in arbitrary limited supply. Prospective buyers (bidders) have preferences over multisets of items, i.e., for more than one unit per distinct good. Our objective is to determine allocations of multisets that maximize the Social Welfare. Our main results are for multiminded and submodular bidders. In the first setting each bidder has a positive value for being allocated one multiset from a prespecified demand set of alternatives. In the second setting each bidder is associated to a submodular valuation function that defines his value for the multiset he is allocated. For multi-minded bidders, we design a truthful FPTAS that fully optimizes the Social Welfare, while violating the supply constraints on goods within factor (1+ε), for any fixed ε &gt; 0 (i.e., the approximation applies to the constraints and not to the Social Welfare). This result is best possible, in that full optimization is impossible without violating the supply constraints. For submodular bidders, we obtain a PTAS that approximates the optimum Social Welfare within factor (1 + ε), for any fixed ε &gt; 0, without violating the supply constraints. This result is best possible as well. Our allocation algorithms are Maximal-in-Range and yield truthful mechanisms, when paired with Vickrey-Clarke-Groves payments.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {721–744},
numpages = {24}
}

@article{10.5555/2831071.2831086,
author = {Kwisthout, Johan},
title = {Tree-Width and the Computational Complexity of Map Approximations in Bayesian Networks},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The problem of finding the most probable explanation to a designated set of variables given partial evidence (the MAP problem) is a notoriously intractable problem in Bayesian networks, both to compute exactly and to approximate. It is known, both from theoretical considerations and from practical experience, that low tree-width is typically an essential prerequisite to efficient exact computations in Bayesian networks. In this paper we investigate whether the same holds for approximating MAP. We define four notions of approximating MAP (by value, structure, rank, and expectation) and argue that all of them are intractable in general. We prove that efficient value-approximations, structure-approximations, and rank-approximations of MAP instances with high tree-width will violate the Exponential Time Hypothesis. In contrast, we show that MAP can sometimes be efficiently expectation-approximated, even in instances with high tree-width, if the most probable explanation has a high probability. We introduce the complexity class FERT, analogous to the class FPT, to capture this notion of fixed-parameter expectation-approximability. We suggest a road-map to future research that yields fixed-parameter tractable results for expectation-approximate MAP, even in graphs with high tree-width.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {699–720},
numpages = {22}
}

@article{10.5555/2831071.2831085,
author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
title = {Evolutionary Dynamics of Multi-Agent Learning: A Survey},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {659–697},
numpages = {39}
}

@article{10.5555/2831071.2831084,
author = {Funke, Stefan and Nusser, Andr\'{e} and Storandt, Sabine},
title = {Placement of Loading Stations for Electric Vehicles: No Detours Necessary!},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Compared to conventional cars, electric vehicles (EVs) still suffer from considerably shorter cruising ranges. Combined with the sparsity of battery loading stations, the complete transition to E-mobility still seems a long way to go. In this paper, we consider the problem of placing as few loading stations as possible so that on any shortest path there are sufficiently many not to run out of energy. We show how to model this problem and introduce heuristics which provide close-to-optimal solutions even in large road networks.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {633–658},
numpages = {26}
}

@article{10.5555/2831071.2831083,
author = {Rankooh, Masood Feyzbakhsh and Ghassem-Sani, Gholamreza},
title = {ITSAT: An Efficient Sat-Based Temporal Planner},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Planning as satisfiability is known as an efficient approach to deal with many types of planning problems. However, this approach has not been competitive with the state-space based methods in temporal planning. This paper describes ITSAT as an efficient SAT-based (satisfiability based) temporal planner capable of temporally expressive planning. The novelty of ITSAT lies in the way it handles temporal constraints of given problems without getting involved in the difficulties of introducing continuous variables into the corresponding satisfiability problems. We also show how, as in SAT-based classical planning, carefully devised preprocessing and encoding schemata can considerably improve the efficiency of SAT-based temporal planning. We present two preprocessing methods for mutex relation extraction and action compression. We also show that the separation of causal and temporal reasoning enables us to employ compact encodings that are based on the concept of parallel execution semantics. Although such encodings have been shown to be quite effective in classical planning, ITSAT is the first temporal planner utilizing this type of encoding. Our empirical results show that not only does ITSAT outperform the state-of-the-art temporally expressive planners, it is also competitive with the fast temporal planners that cannot handle required concurrency.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {541–632},
numpages = {92}
}

@article{10.5555/2831071.2831082,
author = {Ginsberg, Matthew L.},
title = {Satisfiability and Systematicity},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {We introduce a new notion of systematicity for satisfiability algorithms with restarts, saying that an algorithm is strongly systematic if it is systematic independent of restart policy but weakly systematic if it is systematic for some restart policies but not others. We show that existing satisfiability engines are generally only weakly systematic, and describe FLEX, a strongly systematic algorithm that uses an amount of memory polynomial in the size of the problem. On large number factoring problems, FLEX appears to outperform weakly systematic approaches.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {497–540},
numpages = {44}
}

@article{10.5555/2831071.2831081,
author = {Brandt, Felix and Brill, Markus and Hemaspaandra, Edith and Hemaspaandra, Lane A.},
title = {Bypassing Combinatorial Protections: Polynomial-Time Algorithms for Single-Peaked Electorates},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {For many election systems, bribery (and related) attacks have been shown NP-hard using constructions on combinatorially rich structures such as partitions and covers. This paper shows that for voters who follow the most central political-science model of electorates-- single-peaked preferences--those hardness protections vanish. By using single-peaked preferences to simplify combinatorial covering challenges, we for the first time show that NP-hard bribery problems--including those for Kemeny and Llull elections--fall to polynomial time for single-peaked electorates. By using single-peaked preferences to simplify combinatorial partition challenges, we for the first time show that NP-hard partition-of-voters problems fall to polynomial time for single-peaked electorates. We show that for single-peaked electorates, the winner problems for Dodgson and Kemeny elections, though Θ2p-complete in the general case, fall to polynomial time. And we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {439–496},
numpages = {58}
}

@article{10.5555/2831071.2831080,
author = {Mann, Timothy A. and Mannor, Shie and Precup, Doina},
title = {Approximate Value Iteration with Temporally Extended Actions},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Temporally extended actions have proven useful for reinforcement learning, but their duration also makes them valuable for efficient planning. The options framework provides a concrete way to implement and reason about temporally extended actions. Existing literature has demonstrated the value of planning with options empirically, but there is a lack of theoretical analysis formalizing when planning with options is more efficient than planning with primitive actions. We provide a general analysis of the convergence rate of a popular Approximate Value Iteration (AVI) algorithm called Fitted Value Iteration (FVI) with options. Our analysis reveals that longer duration options and a pessimistic estimate of the value function both lead to faster convergence. Furthermore, options can improve convergence even when they are suboptimal and sparsely distributed throughout the state-space. Next we consider the problem of generating useful options for planning based on a subset of landmark states. This suggests a new algorithm, Landmark-based AVI (LAVI), that represents the value function only at the landmark states. We analyze both FVI and LAVI using the proposed landmark-based options and compare the two algorithms. Our experimental results in three different domains demonstrate the key properties from the analysis. Our theoretical and experimental results demonstrate that options can play an important role in AVI by decreasing approximation error and inducing fast convergence.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {375–438},
numpages = {64}
}

@article{10.5555/2831071.2831079,
author = {Bienvenu, Meghyn and Ortiz, Magdalena and \v{S}imkus, Mantas},
title = {Regular Path Queries in Lightweight Description Logics: Complexity and Algorithms},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Conjunctive regular path queries are an expressive extension of the well-known class of conjunctive queries. Such queries have been extensively studied in the (graph) database community, since they support a controlled form of recursion and enable sophisticated path navigation. Somewhat surprisingly, there has been little work aimed at using such queries in the context of description logic (DL) knowledge bases, particularly for the lightweight DLs that are considered best suited for data-intensive applications. This paper aims to bridge this gap by providing algorithms and tight complexity bounds for answering two-way conjunctive regular path queries over DL knowledge bases formulated in lightweight DLs of the DL-Lite and EL families. Our results demonstrate that in data complexity, the cost of moving to this richer query language is as low as one could wish for: the problem is NL-complete for DL-Lite and P-complete for EL. The combined complexity of query answering increases from NP- to PSPACE-complete, but for two-way regular path queries (without conjunction), we show that query answering is tractable even with respect to combined complexity. Our results reveal two-way conjunctive regular path queries as a promising language for querying data enriched by ontologies formulated in DLs of the DL-Lite and EL families or the corresponding OWL 2 QL and EL profiles.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {315–374},
numpages = {60}
}

@article{10.5555/2831071.2831078,
author = {Figueira, Diego and Figueira, Santiago and Areces, Carlos},
title = {Model Theory of XPath on Data Trees. Part I: Bisimulation and Characterization},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {We investigate model theoretic properties of XPath with data (in)equality tests over the class of data trees, i.e., the class of trees where each node contains a label from a finite alphabet and a data value from an infinite domain.We provide notions of (bi)simulations for XPath logics containing the child, parent, ancestor and descendant axes to navigate the tree. We show that these notions precisely characterize the equivalence relation associated with each logic. We study formula complexity measures consisting of the number of nested axes and nested subformulas in a formula; these notions are akin to the notion of quantifier rank in first-order logic. We show characterization results for fine grained notions of equivalence and (bi)simulation that take into account these complexity measures. We also prove that positive fragments of these logics correspond to the formulas preserved under (non-symmetric) simulations. We show that the logic including the child axis is equivalent to the fragment of first-order logic invariant under the corresponding notion of bisimulation. If upward navigation is allowed the characterization fails but a weaker result can still be established. These results hold both over the class of possibly infinite data trees and over the class of finite data trees.Besides their intrinsic theoretical value, we argue that bisimulations are useful tools to prove (non)expressivity results for the logics studied here, and we substantiate this claim with examples.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {271–314},
numpages = {44}
}

@article{10.5555/2831071.2831077,
author = {Kumar, Akshat and Zilberstein, Shlomo and Toussaint, Marc},
title = {Probabilistic Inference Techniques for Scalable Multiagent Decision Making},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Decentralized POMDPs provide an expressive framework for multiagent sequential decision making. However, the complexity of these models--NEXP-Complete even for two agents--has limited their scalability. We present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine learning. We show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic Bayesian networks (DBNs). This planning-as-inference approach paves the way for the application of efficient inference techniques in DBNs to multiagent decision making. To further improve scalability, we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents. Specifically, we show that the necessary inference within the expectation-maximization framework can be decomposed into processes that often involve a small subset of agents, thereby facilitating scalability. We further show that a number of existing multiagent planning models satisfy these conditions. Experiments on large planning benchmarks confirm the benefits of our approach in terms of runtime and scalability with respect to existing techniques.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {223–270},
numpages = {48}
}

@article{10.5555/2831071.2831076,
author = {Espl\`{a}-Gomis, Miquel and S\'{a}nchez-Mart\'{\i}nez, Felipe and Forcada, Mikel L.},
title = {Using Machine Translation to Provide Target-Language Edit Hints in Computer Aided Translation Based on Translation Memories},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {This paper explores the use of general-purpose machine translation (MT) in assisting the users of computer-aided translation (CAT) systems based on translation memory (TM) to identify the target words in the translation proposals that need to be changed (either replaced or removed) or kept unedited, a task we term as word-keeping recommendation. MT is used as a black box to align source and target sub-segments on the fly in the translation units (TUs) suggested to the user. Source-language (SL) and target-language (TL) segments in the matching TUs are segmented into overlapping sub-segments of variable length and machine-translated into the TL and the SL, respectively. The bilingual subsegments obtained and the matching between the SL segment in the TU and the segment to be translated are employed to build the features that are then used by a binary classifier to determine the target words to be changed and those to be kept unedited. In this approach, MT results are never presented to the translator. Two approaches are presented in this work: one using a word-keeping recommendation system which can be trained on the TM used with the CAT system, and a more basic approach which does not require any training.Experiments are conducted by simulating the translation of texts in several language pairs with corpora belonging to different domains and using three different MT systems. We compare the performance obtained to that of previous works that have used statistical word alignment for word-keeping recommendation, and show that the MT-based approaches presented in this paper are more accurate in most scenarios. In particular, our results confirm that the MT-based approaches are better than the alignment-based approach when using models trained on out-of-domain TMs. Additional experiments were also performed to check how dependent the MT-based recommender is on the language pair and MT system used for training. These experiments confirm a high degree of reusability of the recommendation models across various MT systems, but a low level of reusability across language pairs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {169–222},
numpages = {54}
}

@article{10.5555/2831071.2831075,
author = {Heule, Marijn and J\"{a}rvisalo, Matti and Lonsing, Florian and Seidl, Martina and Biere, Armin},
title = {Clause Elimination for SAT and QSAT},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The famous archetypical NP-complete problem of Boolean satisfiability (SAT) and its PSPACE-complete generalization of quantified Boolean satisfiability (QSAT) have become central declarative programming paradigms through which real-world instances of various computationally hard problems can be efficiently solved. This success has been achieved through several breakthroughs in practical implementations of decision procedures for SAT and QSAT, that is, in SAT and QSAT solvers. Here, simplification techniques for conjunctive normal form (CNF) for SAT and for prenex conjunctive normal form (PCNF) for QSAT--the standard input formats of SAT and QSAT solvers--have recently proven very effective in increasing solver efficiency when applied before (i.e., in preprocessing) or during (i.e., in inprocessing) satisfiability search.In this article, we develop and analyze clause elimination procedures for pre- and inprocessing. Clause elimination procedures form a family of (P)CNF formula simplification techniques which remove clauses that have specific (in practice polynomial-time) redundancy properties while maintaining the satisfiability status of the formulas. Extending known procedures such as tautology, subsumption, and blocked clause elimination, we introduce novel elimination procedures based on asymmetric variants of these techniques, and also develop a novel family of so-called covered clause elimination procedures, as well as natural liftings of the CNF-level procedures to PCNF. We analyze the considered clause elimination procedures from various perspectives. Furthermore, for the variants not preserving logical equivalence under clause elimination, we show how to reconstruct solutions to original CNFs from satisfying assignments to simplified CNFs, which is important for practical applications for the procedures. Complementing the more theoretical analysis, we present results on an empirical evaluation on the practical importance of the clause elimination procedures in terms of the effect on solver runtimes on standard real-world application benchmarks. It turns out that the importance of applying the clause elimination procedures developed in this work is empirically emphasized in the context of state-of-the-art QSAT solving.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {127–168},
numpages = {42}
}

@article{10.5555/2831071.2831074,
author = {Grossi, Davide and Lorini, Emiliano and Schwarzentruber, Fran\c{c}ois},
title = {The Ceteris Paribus Structure of Logics of Game Forms},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The article introduces a ceteris paribus modal logic, called CP, interpreted on the equivalence classes induced by finite sets of propositional atoms. This logic is studied and then used to embed three logics of strategic interaction, namely atemporal STIT, the coalition logic of propositional control (CL--PC) and the starless fragment of the dynamic logic of propositional assignments (DL--PA). The embeddings highlight a common ceteris paribus structure underpinning the key operators of all these apparently very different logics and show, we argue, remarkable similarities behind some of the most in uential formalisms for reasoning about strategic interaction.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {91–126},
numpages = {36}
}

@article{10.5555/2831071.2831073,
author = {Dubba, Krishna S. R. and Cohn, Anthony G. and Hogg, David C. and Bhatt, Mehul and Dylla, Frank},
title = {Learning Relational Event Models from Video},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {Event models obtained automatically from video can be used in applications ranging from abnormal event detection to content based video retrieval. When multiple agents are involved in the events, characterizing events naturally suggests encoding interactions as relations. Learning event models from this kind of relational spatio-temporal data using relational learning techniques such as Inductive Logic Programming (ILP) hold promise, but have not been successfully applied to very large datasets which result from video data. In this paper, we present a novel framework remind (Relational Event Model INDuction) for supervised relational learning of event models from large video datasets using ILP. Efficiency is achieved through the learning from interpretations setting and using a typing system that exploits the type hierarchy of objects in a domain. The use of types also helps prevent over generalization. Furthermore, we also present a type-refining operator and prove that it is optimal. The learned models can be used for recognizing events from previously unseen videos. We also present an extension to the framework by integrating an abduction step that improves the learning performance when there is noise in the input data. The experimental results on several hours of video data from two challenging real world domains (an airport domain and a physical action verbs domain) suggest that the techniques are suitable to real world scenarios.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {41–90},
numpages = {50}
}

@article{10.5555/2831071.2831072,
author = {Shivaswamy, Pannaga and Joachims, Thorsten},
title = {Coactive Learning},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. Interactions in the Coactive Learning model take the following form: at each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking); the user responds by correcting the system if necessary, providing a slightly improved - but not necessarily optimal - object as feedback. We argue that such preference feedback can be inferred in large quantity from observable user behavior (e.g., clicks in web search), unlike the optimal feedback required in the expert model or the cardinal valuations required for bandit learning. Despite the relaxed requirements for the feedback, we show that it is possible to adapt many existing online learning algorithms to the coactive framework. In particular, we provide algorithms that achieve O(1/√T) average regret in terms of cardinal utility, even though the learning algorithm never observes cardinal utility values directly. We also provide an algorithm with O(log(T)/T) average regret in the case of λ-strongly convex loss functions. An extensive empirical study demonstrates the applicability of our model and algorithms on a movie recommendation task, as well as ranking for web search.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–40},
numpages = {40}
}

@article{10.5555/2831407.2831421,
author = {Yu, Haonan and Siddharth, N. and Barbu, Andrei and Siskind, Jeffrey Mark},
title = {A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {We present an approach to simultaneously reasoning about a video clip and an entire natural-language sentence. The compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicate-argument relations. We demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) affect the meaning of a sentence and how it is grounded in video. We exploit this methodology in three ways. In the first, a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted, even when the clip depicts multiple similar simultaneous events. In the second, a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip. In the third, a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned. We learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to. The learned meaning representations are shown to be intelligible to humans.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {601–713},
numpages = {113}
}

@article{10.5555/2831407.2831420,
author = {Dao-Tran, Minh and Eiter, Thomas and Fink, Michael and Krennwallner, Thomas},
title = {Distributed Evaluation of Nonmonotonic Multi-Context Systems},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {Multi-context Systems (MCSs) are a formalism for systems consisting of knowledge bases (possibly heterogeneous and non-monotonic) that are interlinked via bridge rules, where the global system semantics emerges from the local semantics of the knowledge bases (also called "contexts") in an equilibrium. While MCSs and related formalisms are inherently targeted for distributed settings, no truly distributed algorithms for their evaluation were available. We address this shortcoming and present a suite of such algorithms which includes a basic algorithm DMCS, an advanced version DMCSOPT that exploits topology-based optimizations, and a streaming algorithm DMCS-STREAMING that computes equilibria in packages of bounded size. The algorithms behave quite differently in several respects, as experienced in thorough experimental evaluation of a system prototype. From the experimental results, we derive a guideline for choosing the appropriate algorithm and running mode in particular situations, determined by the parameter settings.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {543–600},
numpages = {58}
}

@article{10.5555/2831407.2831419,
author = {Faliszewski, Piotr and Hemaspaandra, Edith and Hemaspaandra, Lane A.},
title = {Weighted Electoral Control},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {Although manipulation and bribery have been extensively studied under weighted voting, there has been almost no work done on election control under weighted voting. This is unfortunate, since weighted voting appears in many important natural settings. In this paper, we study the complexity of controlling the outcome of weighted elections through adding and deleting voters. We obtain polynomial-time algorithms, NP-completeness results, and for many NP-complete cases, approximation algorithms. In particular, for scoring rules we completely characterize the complexity of weighted voter control. Our work shows that for quite a few important cases, either polynomial-time exact algorithms or polynomial-time approximation algorithms exist.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {507–542},
numpages = {36}
}

@article{10.5555/2831407.2831418,
author = {Irissappane, Athirai A. and Zhang, Jie},
title = {A Case-Based Reasoning Framework to Choose Trust Models for Different E-Marketplace Environments},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {The performance of trust models highly depend on the characteristics of the environments where they are applied. Thus, it becomes challenging to choose a suitable trust model for a given e-marketplace environment, especially when ground truth about the agent (buyer and seller) behavior is unknown (called unknown environment). We propose a case-based reasoning framework to choose suitable trust models for unknown environments, based on the intuition that if a trust model performs well in one environment, it will do so in another similar environment. Firstly, we build a case base with a number of simulated environments (with known ground truth) along with the trust models most suitable for each of them. Given an unknown environment, case-based retrieval algorithms retrieve the most similar case(s), and the trust model of the most similar case(s) is chosen as the most suitable model for the unknown environment. Evaluation results confirm the effectiveness of our framework in choosing suitable trust models for different e-marketplace environments.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {477–505},
numpages = {29}
}

@article{10.5555/2831407.2831417,
author = {De Marneffe, Marie-Catherine and Recasens, Marta and Potts, Christopher},
title = {Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models, leading to increased performance. To realize these gains, we build a classifier for predicting the singleton/coreferent distinction. The model's feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation, modality, and attitude predication) with existing results about the benefits of "surface" (part-of-speech and n-gram-based) features for coreference resolution. The model is effective in its own right, and the feature representations help to identify the anchor phrases in bridging anaphora as well. Furthermore, incorporating the model into two very different state-of-the-art coreference resolution systems, one rule-based and the other learning-based, yields significant performance improvements.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {445–475},
numpages = {31}
}

@article{10.5555/2831407.2831416,
author = {Roijers, Diederik M. and Whiteson, Shimon and Oliehoek, Frans A.},
title = {Computing Convex Coverage Sets for Faster Multi-Objective Coordination},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {In this article, we propose new algorithms for multi-objective coordination graphs (MO-CoGs). Key to the efficiency of these algorithms is that they compute a convex coverage set (CCS) instead of a Pareto coverage set (PCS). Not only is a CCS a sufficient solution set for a large class of problems, it also has important characteristics that facilitate more efficient solutions. We propose two main algorithms for computing a CCS in MO-CoGs. Convex multi-objective variable elimination (CMOVE) computes a CCS by performing a series of agent eliminations, which can be seen as solving a series of local multi-objective subproblems. Variable elimination linear support (VELS) iteratively identifies the single weight vector w that can lead to the maximal possible improvement on a partial CCS and calls variable elimination to solve a scalarized instance of the problem for w. VELS is faster than CMOVE for small and medium numbers of objectives and can compute an ε-approximate CCS in a fraction of the runtime. In addition, we propose variants of these methods that employ AND/OR tree search instead of variable elimination to achieve memory efficiency. We analyze the runtime and space complexities of these methods, prove their correctness, and compare them empirically against a naive baseline and an existing PCS method, both in terms of memory-usage and runtime. Our results show that, by focusing on the CCS, these methods achieve much better scalability in the number of agents than the current state of the art.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {399–443},
numpages = {45}
}

@article{10.5555/2831407.2831415,
author = {Kim, Been and Chacha, Caleb M. and Shah, Julie A.},
title = {Inferring Team Task Plans from Human Meetings: A Generative Modeling Approach with Logic-Based Prior},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {We aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains such as military field operations and disaster response. Deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. A human operator then translates the agreed-upon plan into machine instructions for the robots. We present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. Our hybrid approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans, enabling us to overcome the challenge of performing inference over a large solution space with only a small amount of noisy data from the team planning session. We validate the algorithm through human subject experimentations and show that it is able to infer a human team's final plan with 86% accuracy on average. We also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a PR2 robot. To the best of our knowledge, this is the first work to integrate a logical planning technique within a generative model to perform plan inference.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {361–398},
numpages = {38}
}

@article{10.5555/2831407.2831414,
author = {Xue, Shan and Fern, Alan and Sheldon, Daniel},
title = {Scheduling Conservation Designs for Maximum Flexibility via Network Cascade Optimization},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {One approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread. Unfortunately, an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally. This raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control. In this paper, we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization. In particular, we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible. Such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way. We develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread. Our solution approach is based on reducing the stochastic problem to a novel variant of the directed Steiner tree problem, which we call the set-weighted directed Steiner graph problem. We show that this problem is computationally hard, motivating the development of a primal-dual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution. We evaluate the approach on both real and synthetic conservation data with a standard population spread model. The algorithm is shown to produce near optimal results and is much more scalable than more generic off-the-shelf optimizers. Finally, we evaluate a variant of the algorithm to explore the trade-offs between budget savings and population growth.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {331–360},
numpages = {30}
}

@article{10.5555/2831407.2831413,
author = {Liberatore, Paolo},
title = {Revision by History},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {This article proposes a solution to the problem of obtaining plausibility information, which is necessary to perform belief revision: given a sequence of revisions, together with their results, derive a possible initial order that has generated them; this is different from the usual assumption of starting from an all-equal initial order and modifying it by a sequence of revisions. Four semantics for iterated revision are considered: natural, restrained, lexicographic and reinforcement. For each, a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved. Complexity is proved coNP complete in all cases but one (reinforcement revision with unbounded sequence length).},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {287–329},
numpages = {43}
}

@article{10.5555/2831407.2831412,
author = {De Cat, Broes and Denecker, Marc and Stuckey, Peter and Bruynooghe, Maurice},
title = {Lazy Model Expansion: Interleaving Grounding with Search},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {Finding satisfying assignments for the variables involved in a set of constraints can be cast as a (bounded) model generation problem: search for (bounded) models of a theory in some logic. The state-of-the-art approach for bounded model generation for rich knowledge representation languages like Answer Set Programming (ASP) and FO(undefined) and a CSP modeling language such as Zinc, is ground-and-solve: reduce the theory to a ground or propositional one and apply a search algorithm to the resulting theory.An important bottleneck is the blow-up of the size of the theory caused by the grounding phase. Lazily grounding the theory during search is a way to overcome this bottleneck. We present a theoretical framework and an implementation in the context of the FO(undefined) knowledge representation language. Instead of grounding all parts of a theory, justifications are derived for some parts of it. Given a partial assignment for the grounded part of the theory and valid justifications for the formulas of the non-grounded part, the justifications provide a recipe to construct a complete assignment that satisfies the non-grounded part. When a justification for a particular formula becomes invalid during search, a new one is derived; if that fails, the formula is split in a part to be grounded and a part that can be justified. Experimental results illustrate the power and generality of this approach.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {235–286},
numpages = {52}
}

@article{10.5555/2831407.2831411,
author = {De Haan, Ronald and Kanj, Iyad and Szeider, Stefan},
title = {On the Subexponential-Time Complexity of CSP},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {Not all NP-complete problems share the same practical hardness with respect to exact computation. Whereas some NP-complete problems are amenable to efficient computational methods, others are yet to show any such sign. It becomes a major challenge to develop a theoretical framework that is more fine-grained than the theory of NP-completeness, and that can explain the distinction between the exact complexities of various NP-complete problems. This distinction is highly relevant for constraint satisfaction problems under natural restrictions, where various shades of hardness can be observed in practice.Acknowledging the NP-hardness of such problems, one has to look beyond polynomial time computation. The theory of subexponential-time complexity provides such a framework, and has been enjoying increasing popularity in complexity theory. An instance of the constraint satisfaction problem with n variables over a domain of d values can be solved by brute-force in dn steps (omitting a polynomial factor). In this paper we study the existence of subexponential-time algorithms, that is, algorithms running in do(n) steps, for various natural restrictions of the constraint satisfaction problem. We consider both the constraint satisfaction problem in which all the constraints are given extensionally as tables, and that in which all the constraints are given intensionally in the form of global constraints. We provide tight characterizations of the subexponential-time complexity of the aforementioned problems with respect to several natural structural parameters, which allows us to draw a detailed landscape of the subexponential-time complexity of the constraint satisfaction problem. Our analysis provides fundamental results indicating whether and when one can significantly improve on the brute-force search approach for solving the constraint satisfaction problem.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {203–234},
numpages = {32}
}

@article{10.5555/2831407.2831410,
author = {Wiener, Yair and El-Yaniv, Ran},
title = {Agnostic Pointwise-Competitive Selective Classification},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {A pointwise competitive classifier from class F is required to classify identically to the best classifier in hindsight from F. For noisy, agnostic settings we present a strategy for learning pointwise-competitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice. For some interesting hypothesis classes and families of distributions, the measure of this rejected region is shown to be diminishing at rate β1 undefined O((polylog(m) undefined log(1/δ)/m)beta;2/2), with high probability, where m is the sample size, δ is the standard confidence parameter, and β1, β2 are smoothness parameters of a Bernstein type condition of the associated excess loss class (related to F and the 0/1 loss). Exact implementation of the proposed learning strategy is dependent on an ERM oracle that is hard to compute in the agnostic case. We thus consider a heuristic approximation procedure that is based on SVMs, and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {179–201},
numpages = {23}
}

@article{10.5555/2831407.2831409,
author = {Domshlak, Carmel and Mirkis, Vitaly},
title = {Deterministic Oversubscription Planning as Heuristic Search: Abstractions and Reformulations},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {While in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible, the objective in deterministic oversubscription planning (OSP) is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost. Although numerous applications in various fields share the latter objective, no substantial algorithmic advances have been made in deterministic OSP. Tracing the key sources of progress in classical planning, we identify a severe lack of effective domain-independent approximations for OSP.With our focus here on optimal planning, our goal is to bridge this gap. Two classes of approximation techniques have been found especially useful in the context of optimal classical planning: those based on state-space abstractions and those based on logical landmarks for goal reachability. The question we study here is whether some similar-in-spirit, yet possibly mathematically different, approximation techniques can be developed for OSP. In the context of abstractions, we define the notion of additive abstractions for OSP, study the complexity of deriving effective abstractions from a rich space of hypotheses, and reveal some substantial, empirically relevant islands of tractability. In the context of landmarks, we show how standard goal-reachability landmarks of certain classical planning tasks can be compiled into the OSP task of interest, resulting in an equivalent OSP task with a lower cost allowance, and thus with a smaller search space. Our empirical evaluation confirms the effectiveness of the proposed techniques, and opens a wide gate for further developments in oversubscription planning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {97–169},
numpages = {73}
}

@article{10.5555/2831407.2831408,
author = {De Cooman, Gert and De Bock, Jasper and Diniz, M\'{a}rcio Alves},
title = {Coherent Predictive Inference under Exchangeability with Imprecise Probabilities},
year = {2015},
issue_date = {January 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {52},
number = {1},
issn = {1076-9757},
abstract = {Coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles. In a context that does not allow for indecision, this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities. If we do allow for indecision, this leads to a more general foundation for coherent (imprecise-)probabilistic inference. In this framework, and for a given finite category set, coherent predictive inference under exchangeability can be represented using Bernstein coherent cones of multivariate polynomials on the simplex generated by this category set. This is a powerful generalisation of de Finetti's Representation Theorem allowing for both imprecision and indecision.We define an inference system as a map that associates a Bernstein coherent cone of polynomials with every finite category set. Many inference principles encountered in the literature can then be interpreted, and represented mathematically, as restrictions on such maps. We discuss, as particular examples, two important inference principles: representation insensitivity--a strengthened version of Walley's representation invariance--and specificity. We show that there is an infinity of inference systems that satisfy these two principles, amongst which we discuss in particular the skeptically cautious inference system, the inference systems corresponding to (a modified version of) Walley and Bernard's Imprecise Dirichlet Multinomial Models (IDMM), the skeptical IDMM inference systems, and the Haldane inference system. We also prove that the latter produces the same posterior inferences as would be obtained using Haldane's improper prior, implying that there is an infinity of proper priors that produce the same coherent posterior inferences as Haldane's improper one. Finally, we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the IDMM inference systems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–95},
numpages = {95}
}

@article{10.5555/2750423.2750446,
author = {Bo\v{s}ansk\'{y}, Branislav and Kiekintveld, Christopher and Lis\'{y}, Viliam and P\v{e}chou\v{c}ek, Michal},
title = {An Exact Double-Oracle Algorithm for Zero-Sum Extensive-Form Games with Imperfect Information},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Developing scalable solution algorithms is one of the central problems in computational game theory. We present an iterative algorithm for computing an exact Nash equilibrium for two-player zero-sum extensive-form games with imperfect information. Our approach combines two key elements: (1) the compact sequence-form representation of extensive- form games and (2) the algorithmic framework of double-oracle methods. The main idea of our algorithm is to restrict the game by allowing the players to play only selected sequences of available actions. After solving the restricted game, new sequences are added by finding best responses to the current solution using fast algorithms.We experimentally evaluate our algorithm on a set of games inspired by patrolling scenarios, board, and card games. The results show significant runtime improvements in games admitting an equilibrium with small support, and substantial improvement in memory use even on games with large support. The improvement in memory use is particularly important because it allows our algorithm to solve much larger game instances than existing linear programming methods.Our main contributions include (1) a generic sequence-form double-oracle algorithm for solving zero-sum extensive-form games; (2) fast methods for maintaining a valid restricted game model when adding new sequences; (3) a search algorithm and pruning methods for computing best-response sequences; (4) theoretical guarantees about the convergence of the algorithm to a Nash equilibrium; (5) experimental analysis of our algorithm on several games, including an approximate version of the algorithm.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {829–866},
numpages = {38}
}

@article{10.5555/2750423.2750445,
author = {Zilli, Davide and Parson, Oliver and Merrett, Geoff V. and Rogers, Alex},
title = {A Hidden Markov Model-Based Acoustic Cicada Detector for Crowdsourced Smartphone Biodiversity Monitoring},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {In recent years, the field of computational sustainability has striven to apply artificial intelligence techniques to solve ecological and environmental problems. In ecology, a key issue for the safeguarding of our planet is the monitoring of biodiversity. Automated acoustic recognition of species aims to provide a cost-effective method for biodiversity monitoring. This is particularly appealing for detecting endangered animals with a distinctive call, such as the New Forest cicada. To this end, we pursue a crowdsourcing approach, whereby the millions of visitors to the New Forest, where this insect was historically found, will help to monitor its presence by means of a smartphone app that can detect its mating call. Existing research in the field of acoustic insect detection has typically focused upon the classification of recordings collected from fixed field microphones. Such approaches segment a lengthy audio recording into individual segments of insect activity, which are independently classified using cepstral coefficients extracted from the recording as features. This paper reports on a contrasting approach, whereby we use crowdsourcing to collect recordings via a smartphone app, and present an immediate feedback to the users as to whether an insect has been found. Our classification approach does not remove silent parts of the recording via segmentation, but instead uses the temporal patterns throughout each recording to classify the insects present. We show that our approach can successfully discriminate between the call of the New Forest cicada and similar insects found in the New Forest, and is robust to common types of environment noise. A large scale trial deployment of our smartphone app collected over 6000 reports of insect activity from over 1000 users. Despite the cicada not having been rediscovered in the New Forest, the effectiveness of this approach was confirmed for both the detection algorithm, which successfully identified the same cicada through the app in countries where the same species is still present, and of the crowdsourcing methodology, which collected a vast number of recordings and involved thousands of contributors.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {805–827},
numpages = {23}
}

@article{10.5555/2750423.2750444,
author = {Kissmann, Peter and Hoffmann, J\"{o}rg},
title = {BDD Ordering Heuristics for Classical Planning},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Symbolic search using binary decision diagrams (BDDs) can often save large amounts of memory due to its concise representation of state sets. A decisive factor for this method's success is the chosen variable ordering. Generally speaking, it is plausible that dependent variables should be brought close together in order to reduce BDD sizes. In planning, variable dependencies are typically captured by means of causal graphs, and in preceding work these were taken as the basis for finding BDD variable orderings. Starting from the observation that the two concepts of "dependency" are actually quite different, we introduce a framework for assessing the strength of variable ordering heuristics in sub-classes of planning. It turns out that, even for extremely simple planning tasks, causal graph based variable orders may be exponentially worse than optimal.Experimental results on a wide range of variable ordering variants corroborate our theoretical findings. Furthermore, we show that dynamic reordering is much more effective at reducing BDD size, but it is not cost-effective due to a prohibitive runtime overhead. We exhibit the potential of middle-ground techniques, running dynamic reordering until simple stopping criteria hold.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {779–804},
numpages = {26}
}

@article{10.5555/2750423.2750443,
author = {Shelton, Christian R. and Ciardo, Gianfranco},
title = {Tutorial on Structured Continuous-Time Markov Processes},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {A continuous-time Markov process (CTMP) is a collection of variables indexed by a continuous quantity, time. It obeys the Markov property that the distribution over a future variable is independent of past variables given the state at the present time. We introduce continuous-time Markov process representations and algorithms for filtering, smoothing, expected sufficient statistics calculations, and model estimation, assuming no prior knowledge of continuous-time processes but some basic knowledge of probability and statistics. We begin by describing "flat" or unstructured Markov processes and then move to structured Markov processes (those arising from state spaces consisting of assignments to variables) including Kronecker, decision-diagram, and continuous-time Bayesian network representations. We provide the first connection between decision-diagrams and continuous- time Bayesian networks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {725–778},
numpages = {54}
}

@article{10.5555/2750423.2750442,
author = {\v{C}epek, Ond\v{r}ej and Gursk\'{y}, \v{S}tefan and Ku\v{c}era, Petr},
title = {On Minimum Representations of Matched Formulas},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {A Boolean formula in conjunctive normal form (CNF) is called matched if the system of sets of variables which appear in individual clauses has a system of distinct representatives. Each matched CNF is trivially satisfiable (each clause can be satisfied by its representative variable). Another property which is easy to see, is that the class of matched CNFs is not closed under partial assignment of truth values to variables. This latter property leads to a fact (proved here) that given two matched CNFs it is co-NP complete to decide whether they are logically equivalent. The construction in this proof leads to another result: a much shorter and simpler proof of Σ2p-completeness of Boolean minimization for matched CNFs. The main result of this paper deals with the structure of clause minimum CNFs. We prove here that if a Boolean function f admits a representation by a matched CNF then every clause minimum CNF representation of f is matched.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {707–724},
numpages = {18}
}

@article{10.5555/2750423.2750441,
author = {Stefanoni, Giorgio and Motik, Boris and Kr\"{o}tzsch, Markus and Rudolph, Sebastian},
title = {The Complexity of Answering Conjunctive and Navigational Queries over OWL 2 EL Knowledge Bases},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {OWL 2 EL is a popular ontology language that supports role inclusions--axioms of the form S1...Sn ⊆ S that capture compositional properties of roles. Role inclusions closely correspond to context-free grammars, which was used to show that answering conjunctive queries (CQs) over OWL 2 EL knowledge bases with unrestricted role inclusions is undecidable. However, OWL 2 EL inherits from OWL 2 DL the syntactic regularity restriction on role inclusions, which ensures that role chains implying a particular role can be described using a finite automaton (FA). This is sufficient to ensure decidability of CQ answering; however, the FAs can be worst-case exponential in size so the known approaches do not provide a tight upper complexity bound.In this paper, we solve this open problem and show that answering CQs over OWL 2 EL knowledge bases is PSpace-complete in combined complexity (i.e., the complexity measured in the total size of the input). To this end, we use a novel encoding of regular role inclusions using bounded-stack pushdown automata--that is, FAs extended with a stack of bounded size. Apart from theoretical interest, our encoding can be used in practical tableau algorithms to avoid the exponential blowup due to role inclusions. In addition, we sharpen the lower complexity bound and show that the problem is PSPACE-hard even if we consider only role inclusions as part of the input (i.e., the query and all other parts of the knowledge base are fxed). Finally, we turn our attention to navigational queries over OWL 2 EL knowledge bases, and we show that answering positive, converse-free conjunctive graph XPath queries is PSPACE-complete as well; this is interesting since allowing the converse operator in queries is known to make the problem ExpTime-hard. Thus, in this paper we present several important contributions to the landscape of the complexity of answering expressive queries over description logic knowledge bases.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {645–705},
numpages = {61}
}

@article{10.5555/2750423.2750440,
author = {Nguyen, Phong and Hilario, Melanie and Kalousis, Alexandros},
title = {Using Meta-Mining to Support Data Mining Workflow Planning and Optimization},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Knowledge Discovery in Databases is a complex process that involves many different data processing and learning operators. Today's Knowledge Discovery Support Systems can contain several hundred operators. A major challenge is to assist the user in designing workflows which are not only valid but also - ideally - optimize some performance measure associated with the user goal. In this paper we present such a system. The system relies on a meta-mining module which analyses past data mining experiments and extracts meta-mining models which associate dataset characteristics with workflow descriptors in view of workflow performance optimization. The meta-mining model is used within a data mining workflow planner, to guide the planner during the workflow planning. We learn the meta-mining models using a similarity learning approach, and extract the workflow descriptors by mining the workflows for generalized relational patterns accounting also for domain knowledge provided by a data mining ontology. We evaluate the quality of the data mining workflows that the system produces on a collection of real world datasets coming from biology and show that it produces workflows that are significantly better than alternative methods that can only do workflow selection and not planning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {605–644},
numpages = {40}
}

@article{10.5555/2750423.2750439,
author = {Kash, Ian and Procaccia, Ariel D. and Shah, Nisarg},
title = {No Agent Left behind: Dynamic Fair Division of Multiple Resources},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents. While in reality agents are not all present in the system simultaneously, previous work has studied static settings where all relevant information is known upfront. Our goal is to better understand the dynamic setting. On the conceptual level, we develop a dynamic model of fair division, and propose desirable axiomatic properties for dynamic resource allocation mechanisms. On the technical level, we construct two novel mechanisms that provably satisfy some of these properties, and analyze their performance using real data. We believe that our work informs the design of superior multiagent systems, and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {579–603},
numpages = {25}
}

@article{10.5555/2750423.2750438,
author = {Cohen, David and Crampton, Jason and Gagarin, Andrei and Gutin, Gregory and Jones, Mark},
title = {Iterative Plan Construction for the Workflow Satisfiability Problem},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {The Workflow Satisfiability Problem (WSP) is a problem of practical interest that arises whenever tasks need to be performed by authorized users, subject to constraints defined by business rules. We are required to decide whether there exists a plan - an assignment of tasks to authorized users - such that all constraints are satisfied. It is natural to see the WSP as a subclass of the Constraint Satisfaction Problem (CSP) in which the variables are tasks and the domain is the set of users. What makes the WSP distinctive is that the number of tasks is usually very small compared to the number of users, so it is appropriate to ask for which constraint languages the WSP is fixed-parameter tractable (FPT), parameterized by the number of tasks.This novel approach to the WSP, using techniques from CSP, has enabled us to design a generic algorithm which is FPT for several families of workflow constraints considered in the literature. Furthermore, we prove that the union of FPT languages remains FPT if they satisfy a simple compatibility condition. Lastly, we identify a new FPT constraint language, user-independent constraints, that includes many of the constraints of interest in business processing systems. We demonstrate that our generic algorithm has provably optimal running time O*(2k log k), for this language, where k is the number of tasks.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {555–577},
numpages = {23}
}

@article{10.5555/2750423.2750437,
author = {L\'{o}pez-Ortiz, Alejandro and Angelopoulos, Spyros and Hamel, Ang\`{e}le M.},
title = {Optimal Scheduling of Contract Algorithms for Anytime Problem-Solving},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {A contract algorithm is an algorithm which is given, as part of the input, a specified amount of allowable computation time. The algorithm must then complete its execution within the allotted time. An interruptible algorithm, in contrast, can be interrupted at an arbitrary point in time, at which point it must report its currently best solution. It is known that contract algorithms can simulate interruptible algorithms using iterative deepening techniques. This simulation is done at a penalty in the performance of the solution, as measured by the so-called acceleration ratio.In this paper we give matching (i.e., optimal) upper and lower bounds for the acceleration ratio under such a simulation. We assume the most general setting in which n problem instances must be solved by means of scheduling executions of contract algorithms in m identical parallel processors. This resolves an open conjecture of Bernstein, Finkelstein, and Zilberstein who gave an optimal schedule under the restricted setting of round robin and length-increasing schedules, but whose optimality in the general unrestricted case remained open.Lastly, we show how to evaluate the average acceleration ratio of the class of exponential strategies in the setting of n problem instances and m parallel processors. This is a broad class of schedules that tend to be either optimal or near-optimal, for several variants of the basic problem.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {533–554},
numpages = {22}
}

@article{10.5555/2750423.2750436,
author = {Cohn, Anthony G. and Li, Sanjiang and Liu, Weiming and Renz, Jochen},
title = {Reasoning about Topological and Cardinal Direction Relations between 2-Dimensional Spatial Objects},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Increasing the expressiveness of qualitative spatial calculi is an essential step towards meeting the requirements of applications. This can be achieved by combining existing calculi in a way that we can express spatial information using relations from multiple calculi. The great challenge is to develop reasoning algorithms that are correct and complete when reasoning over the combined information. Previous work has mainly studied cases where the interaction between the combined calculi was small, or where one of the two calculi was very simple. In this paper we tackle the important combination of topological and directional information for extended spatial objects. We combine some of the best known calculi in qualitative spatial reasoning, the RCC8 algebra for representing topological information, and the Rectangle Algebra (RA) and the Cardinal Direction Calculus (CDC) for directional information. We consider two different interpretations of the RCC8 algebra, one uses a weak connectedness relation, the other uses a strong connectedness relation. In both interpretations, we show that reasoning with topological and directional information is decidable and remains in NP. Our computational complexity results unveil the significant differences between RA and CDC, and that between weak and strong RCC8 models. Take the combination of basic RCC8 and basic CDC constraints as an example: we show that the consistency problem is in P only when we use the strong RCC8 algebra and explicitly know the corresponding basic RA constraints.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {493–532},
numpages = {40}
}

@article{10.5555/2750423.2750435,
author = {De Wilde, Boris and Ter Mors, Adriaan W. and Witteveen, Cees},
title = {Push and Rotate: A Complete Multi-Agent Pathfinding Algorithm},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Multi-agent Pathfinding is a relevant problem in a wide range of domains, for example in robotics and video games research. Formally, the problem considers a graph consisting of vertices and edges, and a set of agents occupying vertices. An agent can only move to an unoccupied, neighbouring vertex, and the problem of finding the minimal sequence of moves to transfer each agent from its start location to its destination is an NP-hard problem.We present Push and Rotate, a new algorithm that is complete for Multi-agent Pathfinding problems in which there are at least two empty vertices. Push and Rotate first divides the graph into subgraphs within which it is possible for agents to reach any position of the subgraph, and then uses the simple push, swap, and rotate operations to find a solution; a post-processing algorithm is also presented that eliminates redundant moves. Push and Rotate can be seen as extending Luna and Bekris's Push and Swap algorithm, which we showed to be incomplete in a previous publication.In our experiments we compare our approach with the Push and Swap, MAPP, and Bibox algorithms. The latter algorithm is restricted to a smaller class of instances as it requires biconnected graphs, but can nevertheless be considered state of the art due to its strong performance. Our experiments show that Push and Swap suffers from incompleteness, MAPP is generally not competitive with Push and Rotate, and Bibox is better than Push and Rotate on randomly generated biconnected instances, while Push and Rotate performs better on grids.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {443–492},
numpages = {50}
}

@article{10.5555/2750423.2750434,
author = {Cai, Shaowei and Luo, Chuan and Su, Kaile},
title = {Scoring Functions Based on Second Level Score for K-SAT with Long Clauses},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {It is widely acknowledged that stochastic local search (SLS) algorithms can efficiently find models for satisfiable instances of the satisfiability (SAT) problem, especially for random k-SAT instances. However, compared to random 3-SAT instances where SLS algorithms have shown great success, random k-SAT instances with long clauses remain very difficult. Recently, the notion of second level score, denoted as score2, was proposed for improving SLS algorithms on long-clause SAT instances, and was first used in the powerful CCASat solver as a tie breaker.In this paper, we propose three new scoring functions based on score2. Despite their simplicity, these functions are very effective for solving random k-SAT with long clauses. The first function combines score and score2, and the second one additionally integrates the diversification property age. These two functions are used in developing a new SLS algorithm called CScoreSAT. Experimental results on large random 5-SAT and 7-SAT instances near phase transition show that CScoreSAT significantly outperforms previous SLS solvers. However, CScoreSAT cannot rival its competitors on random k-SAT instances at phase transition. We improve CScoreSAT for such instances by another scoring function which combines score2 with age. The resulting algorithm HScoreSAT exhibits state-of-the-art performance on random k-SAT (k &gt; 3) instances at phase transition. We also study the computation of score2, including its implementation and computational complexity.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {413–441},
numpages = {29}
}

@article{10.5555/2750423.2750433,
author = {Metodi, Amit and Stern, Roni and Kalech, Meir and Codish, Michael},
title = {A Novel SAT-Based Approach to Model Based Diagnosis},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {This paper introduces a novel encoding of Model Based Diagnosis (MBD) to Boolean Satisfaction (SAT) focusing on minimal cardinality diagnosis. The encoding is based on a combination of sophisticated MBD preprocessing algorithms and the application of a SAT compiler which optimizes the encoding to provide more succinct CNF representations than obtained with previous works. Experimental evidence indicates that our approach is superior to all published algorithms for minimal cardinality MBD. In particular, we can determine, for the first time, minimal cardinality diagnoses for the entire standard ISCAS-85 and 74XXX benchmarks. Our results open the way to improve the state-of-the-art on a range of similar MBD problems.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {377–411},
numpages = {35}
}

@article{10.5555/2750423.2750432,
author = {Belardinelli, Francesco and Lomuscio, Alessio and Patrizi, Fabio},
title = {Verification of Agent-Based Artifact Systems},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts. Artifacts consist of data and lifecycles, accounting respectively for the relational structure of the artifacts' states and their possible evolutions over time. In this paper we put forward artifact-centric multi-agent systems, a novel formalisation of artifact systems in the context of multi-agent systems operating on them. Differently from the usual process-based models of services, we give a semantics that explicitly accounts for the data structures on which artifact systems are defined.We study the model checking problem for artifact-centric multi-agent systems against specifications expressed in a quantified version of temporal-epistemic logic expressing the knowledge of the agents in the exchange. We begin by noting that the problem is undecidable in general. We identify a noteworthy class of systems that admit bisimilar, finite abstractions. It follows that we can verify these systems by investigating their finite abstractions; we also show that the corresponding model checking problem is EXPSPACE-complete. We then introduce artifact-centric programs, compact and declarative representations of the programs governing both the artifact system and the agents. We show that, while these in principle generate infinite-state systems, under natural conditions their verification problem can be solved on finite abstractions that can be effectively computed from the programs. We exemplify the theoretical results here pursued through a mainstream procurement scenario from the artifact systems literature.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {333–376},
numpages = {44}
}

@article{10.5555/2750423.2750431,
author = {Nissim, Raz and Brafman, Ronen},
title = {Distributed Heuristic Forward Search for Multi-Agent Planning},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {This paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal. Two main approaches have recently been proposed to solve this type of prob- lem - one is based on reduction to distributed constraint satisfaction, and the other on partial-order planning techniques. In classical single-agent planning, constraint-based and partial-order planning techniques are currently dominated by heuristic forward search. The question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacy-preserving classical multi-agent planning. Our work provides a positive answer to this question in the form of a general approach to distributed state-space search in which each agent performs only the part of the state expansion relevant to it. The resulting algorithms are simple and efficient - outperforming previous algorithms by orders of magnitude - while offering similar flexibility to that of forward-search algorithms for single-agent planning. Furthermore, one particular variant of our general approach yields a distributed version of the a* algorithm that is the first cost-optimal distributed algorithm for privacy-preserving planning.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {293–332},
numpages = {40}
}

@article{10.5555/2750423.2750430,
author = {B\"{a}ckstr\"{o}m, Christer and Jonsson, Anders and Jonsson, Peter},
title = {Automaton Plans},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Macros have long been used in planning to represent subsequences of operators. Macros can be used in place of individual operators during search, sometimes reducing the effort required to find a plan to the goal. Another use of macros is to compactly represent long plans. In this paper we introduce a novel solution concept called automaton plans in which plans are represented using hierarchies of automata. Automaton plans can be viewed as an extension of macros that enables parameterization and branching. We provide several examples that illustrate how automaton plans can be useful, both as a compact representation of exponentially long plans and as an alternative to sequential solutions in benchmark domains such as LOGISTICS and GRID. We also compare automaton plans to other compact plan representations from the literature, and find that automaton plans are strictly more expressive than macros, but strictly less expressive than HTNs and certain representations allowing efficient sequential access to the operators of the plan.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {255–291},
numpages = {37}
}

@article{10.5555/2750423.2750429,
author = {Zhuang, Zhiqiang and Pagnucco, Maurice},
title = {Entrenchment-Based Horn Contraction},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {The AGM framework is the benchmark approach in belief change. Since the framework assumes an underlying logic containing classical Propositional Logic, it can not be applied to systems with a logic weaker than Propositional Logic. To remedy this limitation, several researchers have studied AGM-style contraction and revision under the Horn fragment of Propositional Logic (i.e., Horn logic). In this paper, we contribute to this line of research by investigating the Horn version of the AGM entrenchment-based contraction. The study is challenging as the construction of entrenchment-based contraction refers to arbitrary disjunctions which are not expressible under Horn logic. In order to adapt the construction to Horn logic, we make use of a Horn approximation technique called Horn strengthening. We provide a representation theorem for the newly constructed contraction which we refer to as entrenchment-based Horn contraction. Ideally, contractions defined under Horn logic (i.e., Horn contractions) should be as rational as AGM contraction. We propose the notion of Horn equivalence which intuitively captures the equivalence between Horn contraction and AGM contraction. We show that, under this notion, entrenchment-based Horn contraction is equivalent to a restricted form of entrenchment-based contraction.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {227–254},
numpages = {28}
}

@article{10.5555/2750423.2750428,
author = {Adiga, Abhijin and Kuhlman, Chris J. and Mortveit, Henning S. and Vullikanti, Anil Kumar S.},
title = {Sensitivity of Diffusion Dynamics to Network Uncertainty},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Simple diffusion processes on networks have been used to model, analyze and predict diverse phenomena such as spread of diseases, information and memes. More often than not, the underlying network data is noisy and sampled. This prompts the following natural question: how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure?In this paper, we consider two popular diffusion models: Independent cascade (IC) model and Linear threshold (LT) model. We study how the expected number of vertices that are influenced/infected, for particular initial conditions, are affected by network perturbations. Through rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results. (1) For the IC model, we characterize the sensitivity to network perturbation in terms of the critical probability for phase transition of the network. We find that the expected number of infections is quite stable, unless the transmission probability is close to the critical probability. (2) We show that the standard LT model with uniform edge weights is relatively stable under network perturbations. (3) We study these sensitivity questions using extensive simulations on diverse real world networks and find that our theoretical predictions for both models match the observations quite closely. (4) Experimentally, the transient behavior, i.e., the time series of the number of infections, in both models appears to be more sensitive to network perturbations.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {207–226},
numpages = {20}
}

@article{10.5555/2750423.2750427,
author = {Feldman, Zohar and Domshlak, Carmel},
title = {Simple Regret Optimization in Online Planning for Markov Decision Processes},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {We consider online planning in Markov decision processes (MDPs). In online planning, the agent focuses on its current state only, deliberates about the set of possible policies from that state onwards and, when interrupted, uses the outcome of that exploratory deliberation to choose what action to perform next. Formally, the performance of algorithms for online planning is assessed in terms of simple regret, the agent's expected performance loss when the chosen action, rather than an optimal one, is followed.To date, state-of-the-art algorithms for online planning in general MDPs are either best effort, or guarantee only polynomial-rate reduction of simple regret over time. Here we introduce a new Monte-Carlo tree search algorithm, BRUE, that guarantees exponential- rate and smooth reduction of simple regret. At a high level, BRUE is based on a simple yet non-standard state-space sampling scheme, MCTS2e, in which different parts of each sample are dedicated to different exploratory objectives. We further extend BRUE with a variant of "learning by forgetting." The resulting parametrized algorithm, BRUE(α), exhibits even more attractive formal guarantees than BRUE. Our empirical evaluation shows that both BRUE and its generalization, BRUE(α), are also very effective in practice and compare favorably to the state-of-the-art.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {165–205},
numpages = {41}
}

@article{10.5555/2750423.2750426,
author = {Woodsend, Kristian and Lapata, Mirella},
title = {Text Rewriting Improves Semantic Role Labeling},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Large-scale annotated corpora are a prerequisite to developing high-performance NLP systems. Such corpora are expensive to produce, limited in size, often demanding linguistic expertise. In this paper we use text rewriting as a means of increasing the amount of labeled data available for model training. Our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels. We apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the CoNLL-2009 benchmark dataset.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {133–164},
numpages = {32}
}

@article{10.5555/2750423.2750425,
author = {Winikoff, Michael and Cranefield, Stephen},
title = {On the Testability of BDI Agent Systems},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Before deploying a software system we need to assure ourselves (and stakeholders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this "obvious intuition" in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be. Specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {71–131},
numpages = {61}
}

@article{10.5555/2750423.2750424,
author = {Micalizio, Roberto and Torasso, Pietro},
title = {Cooperative Monitoring to Diagnose Multiagent Plans},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {Diagnosing the execution of a Multiagent Plan (MAP) means identifying and explaining action failures (i.e., actions that did not reach their expected effects). Current approaches to MAP diagnosis are substantially centralized, and assume that action failures are independent of each other.In this paper, the diagnosis of MAPs, executed in a dynamic and partially observable environment, is addressed in a fully distributed and asynchronous way; in addition, action failures are no longer assumed as independent of each other.The paper presents a novel methodology, named Cooperative Weak-Committed Monitoring (CWCM), enabling agents to cooperate while monitoring their own actions. Cooperation helps the agents to cope with very scarcely observable environments: what an agent cannot observe directly can be acquired from other agents. CWCM exploits nondeterministic action models to carry out two main tasks: detecting action failures and building trajectory-sets (i.e., structures representing the knowledge an agent has about the environment in the recent past). Relying on trajectory-sets, each agent is able to explain its own action failures in terms of exogenous events that have occurred during the execution of the actions themselves. To cope with dependent failures, CWCM is coupled with a diagnostic engine that distinguishes between primary and secondary action failures.An experimental analysis demonstrates that the CWCM methodology, together with the proposed diagnostic inferences, are effective in identifying and explaining action failures even in scenarios where the system observability is significantly reduced.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–70},
numpages = {70}
}

@article{10.5555/2693068.2693092,
author = {Bonet, Blai and Geffner, Hector},
title = {Belief Tracking for Planning with Sensing: Width, Complexity and Approximations},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of belief tracking in a planning setting where states are valuations over a set of variables that are partially observable, and beliefs stand for the sets of states that are possible. While the problem is intractable in the worst case, it has been recently shown that in deterministic conformant and contingent problems, belief tracking is exponential in a width parameter that is often bounded and small. In this work, we extend these results in two ways. First, we introduce a width notion that applies to non-deterministic problems as well, develop a factored belief tracking algorithm that is exponential in the problem width, and show how it applies to existing benchmarks. Second, we introduce a meaningful, powerful, and sound approximation scheme, beam tracking, that is exponential in a smaller parameter, the problem causal width, and has much broader applicability. We illustrate the value of this algorithm over large instances of problems such as Battleship, Minesweeper, and Wumpus, where it yields state-of-the-art performance in real-time.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {923–970},
numpages = {48}
}

@article{10.5555/2693068.2693091,
author = {Veit, Andreas and Xu, Ying and Zheng, Ronghuo and Chakraborty, Nilanjan and Sycara, Katia},
title = {Demand Side Energy Management via Multiagent Coordination in Consumer Cooperatives},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {A key challenge in creating a sustainable and energy-efficient society is to make consumer demand adaptive to the supply of energy, especially to the renewable supply. In this article, we propose a partially-centralized organization of consumers (or agents), namely, a consumer cooperative that purchases electricity from the market. In the cooperative, a central coordinator buys the electricity for the whole group. The technical challenge is that consumers make their own demand decisions, based on their private demand constraints and preferences, which they do not share with the coordinator or other agents. We propose a novel multiagent coordination algorithm, to shape the energy demand of the cooperative. To coordinate individual consumers under incomplete information, the coordinator determines virtual price signals that it sends to the consumers to induce them to shift their demands when required. We prove that this algorithm converges to the central optimal solution and minimizes the electric energy cost of the cooperative. Additionally, we present results on the time complexity of the iterative algorithm and its implications for agents' incentive compatibility. Furthermore, we perform simulations based on real world consumption data to (a) characterize the convergence properties of our algorithm and (b) understand the effect of differing demand characteristics of participants as well as of different price functions on the cost reduction. The results show that the convergence time scales linearly with the agent population size and length of the optimization horizon. Finally, we observe that as participants' flexibility of shifting their demands increases, cost reduction increases and that the cost reduction is not sensitive to variation in consumption patterns of the consumers.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {885–922},
numpages = {38}
}

@article{10.5555/2693068.2693090,
author = {Zick, Yair and Markakis, Evangelos and Elkind, Edith},
title = {Arbitration and Stability in Cooperative Games with Overlapping Coalitions},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Overlapping Coalition Formation (OCF) games, introduced by Chalkiadakis, Elkind, Markakis, Polukarov and Jennings in 2010, are cooperative games where players can simultaneously participate in several coalitions. Capturing the notion of stability in OCF games is a difficult task: deviating players may continue to contribute resources to joint projects with non-deviators, and the crucial question is what payoffs the deviators expect to receive from such projects. Chalkiadakis et al. introduce three stability concepts for OCF games--the conservative core, the refined core, and the optimistic core--that are based on different answers to this question. In this paper, we propose a unified framework for the study of stability in the OCF setting, which encompasses the stability concepts considered by Chalkiadakis et al. as well as a wide variety of alternative stability concepts. Our approach is based on the notion of arbitration functions, which determine the payoff obtained by the deviators, given their deviation and the current allocation of resources. We provide a characterization of stable outcomes under arbitration. We then conduct an in-depth study of four types of arbitration functions, which correspond to four notions of the core; these include the three notions of the core considered by Chalkiadakis et al. Our results complement those of Chalkiadakis et al. and answer questions left open by their work. In particular, we show that OCF games with the conservative arbitration function are essentially equivalent to non-OCF games, by relating the conservative core of an OCF game to the core of a non-overlapping cooperative game, and use this result to obtain a strictly weaker sufficient condition for conservative core non-emptiness than the one given by Chalkiadakis et al.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {847–884},
numpages = {38}
}

@article{10.5555/2693068.2693089,
author = {Thayasivam, Uthayasanker and Doshi, Prashant},
title = {Speeding up Iterative Ontology Alignment Using Block-Coordinate Descent},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {In domains such as biomedicine, ontologies are prominently utilized for annotating data. Consequently, aligning ontologies facilitates integrating data. Several algorithms exist for automatically aligning ontologies with diverse levels of performance. As alignment applications evolve and exhibit online run time constraints, performing the alignment in a reasonable amount of time without compromising the quality of the alignment is a crucial challenge. A large class of alignment algorithms is iterative and often consumes more time than others in delivering solutions of high quality. We present a novel and general approach for speeding up the multivariable optimization process utilized by these algorithms. Specifically, we use the technique of block-coordinate descent (BCD), which exploits the subdimensions of the alignment problem identified using a partitioning scheme. We integrate this approach into multiple well-known alignment algorithms and show that the enhanced algorithms generate similar or improved alignments in significantly less time on a comprehensive testbed of ontology pairs. Because BCD does not overly constrain how we partition or order the parts, we vary the partitioning and ordering schemes in order to empirically determine the best schemes for each of the selected algorithms. As biomedicine represents a key application domain for ontologies, we introduce a comprehensive biomedical ontology testbed for the community in order to evaluate alignment algorithms. Because biomedical ontologies tend to be large, default iterative techniques find it difficult to produce a good quality alignment within a reasonable amount of time. We align a significant number of ontology pairs from this testbed using BCD-enhanced algorithms. Our contributions represent an important step toward making a significant class of alignment techniques computationally feasible.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {805–845},
numpages = {41}
}

@article{10.5555/2693068.2693088,
author = {Barreto, Andr\'{e} M. S. and Pineau, Joelle and Precup, Doina},
title = {Policy Iteration Based on Stochastic Factorization},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {When a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix that retains some fundamental characteristics of the original. Since the derived matrix can be much smaller than its precursor, this property can be exploited to create a compact version of a Markov decision process (MDP), and hence to reduce the computational cost of dynamic programming. Building on this idea, this paper presents an approximate policy iteration algorithm called policy iteration based on stochastic factorization, or PISF for short. In terms of computational complexity, PISF replaces standard policy iteration's cubic dependence on the size of the MDP with a function that grows only linearly with the number of states in the model. The proposed algorithm also enjoys nice theoretical properties: it always terminates after a finite number of iterations and returns a decision policy whose performance only depends on the quality of the stochastic factorization. In particular, if the approximation error in the factorization is sufficiently small, PISF computes the optimal value function of the MDP. The paper also discusses practical ways of factoring an MDP and illustrates the usefulness of the proposed algorithm with an application involving a large-scale decision problem of real economical interest.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {763–803},
numpages = {41}
}

@article{10.5555/2693068.2693087,
author = {Kiritchenko, Svetlana and Zhu, Xiaodan and Mohammad, Saif M.},
title = {Sentiment Analysis of Short Informal Texts},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {We describe a state-of-the-art sentiment analysis system that detects (a) the sentiment of short informal textual messages such as tweets and SMS (message-level task) and (b) the sentiment of a word or a phrase within a message (term-level task). The system is based on a supervised statistical text classification approach leveraging a variety of surface-form, semantic, and sentiment features. The sentiment features are primarily derived from novel high-coverage tweet-specific sentiment lexicons. These lexicons are automatically generated from tweets with sentiment-word hashtags and from tweets with emoticons. To adequately capture the sentiment of words in negated contexts, a separate sentiment lexicon is generated for negated words.The system ranked first in the SemEval-2013 shared task 'Sentiment Analysis in Twitter' (Task 2), obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. Post-competition improvements boost the performance to an F-score of 70.45 (message-level task) and 89.50 (term-level task). The system also obtains state-of-the-art performance on two additional datasets: the SemEval-2013 SMS test set and a corpus of movie review excerpts. The ablation experiments demonstrate that the use of the automatically generated lexicons results in performance gains of up to 6.5 absolute percentage points.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {723–762},
numpages = {40}
}

@article{10.5555/2693068.2693086,
author = {Bergman, David and Cire, Andre A. and Van Hoeve, Willem-Jan},
title = {MDD Propagation for Sequence Constraints},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {We study propagation for the Sequence constraint in the context of constraint programming based on limited-width MDDs. Our first contribution is proving that establishing MDD-consistency for Sequence is NP-hard. Yet, we also show that this task is fixed parameter tractable with respect to the length of the sub-sequences. In addition, we propose a partial filtering algorithm that relies on a specific decomposition of the constraint and a novel extension of MDD filtering to node domains. We experimentally evaluate the performance of our proposed filtering algorithm, and demonstrate that the strength of the MDD propagation increases as the maximum width is increased. In particular, MDD propagation can outperform conventional domain propagation for Sequence by reducing the search tree size and solving time by several orders of magnitude. Similar improvements are observed with respect to the current best MDD approach that applies the decomposition of Sequence into Among constraints.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {697–722},
numpages = {26}
}

@article{10.5555/2693068.2693085,
author = {Gerevini, Alfonso Emilio and Saetti, Alessandro and Vallati, Mauro},
title = {Planning through Automatic Portfolio Configuration: The PbP Approach},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {In the field of domain-independent planning, several powerful planners implementing different techniques have been developed. However, no one of these systems outperforms all others in every known benchmark domain. In this work, we propose a multiplanner approach that automatically configures a portfolio of planning techniques for each given domain. The configuration process for a given domain uses a set of training instances to: (i) compute and analyze some alternative sets of macro-actions for each planner in the portfolio identifying a (possibly empty) useful set, (ii) select a cluster of planners, each one with the identified useful set of macro-actions, that is expected to perform best, and (iii) derive some additional information for configuring the execution scheduling of the selected planners at planning time. The resulting planning system, called PbP (Portfolio-based Planner), has two variants focusing on speed and plan quality. Different versions of PbP entered and won the learning track of the sixth and seventh International Planning Competitions. In this paper, we experimentally analyze PbP considering planning speed and plan quality in depth. We provide a collection of results that help to understand PbP's behavior, and demonstrate the effectiveness of our approach to configuring a portfolio of planners with macro-actions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {639–696},
numpages = {58}
}

@article{10.5555/2693068.2693084,
author = {Mau\'{a}, Denis Deratani and De Campos, Cassio Polpo and Benavoli, Alessio and Antonucci, Alessandro},
title = {Probabilistic Inference in Credal Networks: New Complexity Results},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Credal networks are graph-based statistical models whose parameters take values in a set, instead of being sharply specified as in traditional statistical models (e.g., Bayesian networks). The computational complexity of inferences on such models depends on the irrelevance/independence concept adopted. In this paper, we study inferential complexity under the concepts of epistemic irrelevance and strong independence. We show that inferences under strong independence are NP-hard even in trees with binary variables except for a single ternary one. We prove that under epistemic irrelevance the polynomial-time complexity of inferences in credal trees is not likely to extend to more general models (e.g., singly connected topologies). These results clearly distinguish networks that admit efficient inferences and those where inferences are most likely hard, and settle several open questions regarding their computational complexity. We show that these results remain valid even if we disallow the use of zero probabilities. We also show that the computation of bounds on the probability of the future state in a hidden Markov model is the same whether we assume epistemic irrelevance or strong independence, and we prove a similar result for inference in naive Bayes structures. These inferential equivalences are important for practitioners, as hidden Markov models and naive Bayes structures are used in real applications of imprecise probability.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {603–637},
numpages = {35}
}

@article{10.5555/2693068.2693083,
author = {Rey, Anja and Rothe, J\"{o}rg},
title = {False-Name Manipulation in Weighted Voting Games is Hard for Probabilistic Polynomial Time},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {False-name manipulation refers to the question of whether a player in a weighted voting game can increase her power by splitting into several players and distributing her weight among these false identities. Relatedly, the beneficial merging problem asks whether a coalition of players can increase their power in a weighted voting game by merging their weights. For the problems of whether merging or splitting players in weighted voting games is beneficial in terms of the Shapley-Shubik and the normalized Banzhaf index, merely NP-hardness lower bounds are known, leaving the question about their exact complexity open. For the Shapley-Shubik and the probabilistic Banzhaf index, we raise these lower bounds to hardness for PP, "probabilistic polynomial time," a class considered to be by far a larger class than NP. For both power indices, we provide matching upper bounds for beneficial merging and, whenever the new players' weights are given, also for beneficial splitting, thus resolving previous conjectures in the affirmative. Relatedly, we consider the beneficial annexation problem, asking whether a single player can increase her power by taking over other players' weights. It is known that annexation is never disadvantageous for the Shapley-Shubik index, and that beneficial annexation is NP-hard for the normalized Banzhaf index. We show that annexation is never disadvantageous for the probabilistic Banzhaf index either, and for both the Shapley-Shubik index and the probabilistic Banzhaf index we show that it is NP-complete to decide whether annexing another player is advantageous. Moreover, we propose a general framework for merging and splitting that can be applied to different classes and representations of games.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {573–601},
numpages = {29}
}

@article{10.5555/2693068.2693082,
author = {Terekhov, Daria and Tran, Tony T. and Down, Douglas G. and Beck, J. Christopher},
title = {Integrating Queueing Theory and Scheduling for Dynamic Scheduling Problems},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Dynamic scheduling problems consist of both challenging combinatorics, as found in classical scheduling problems, and stochastics due to uncertainty about the arrival times, resource requirements, and processing times of jobs. To address these two challenges, we investigate the integration of queueing theory and scheduling. The former reasons about long-run stochastic system characteristics, whereas the latter typically deals with short-term combinatorics. We investigate two simple problems to isolate the core differences and potential synergies between the two approaches: a two-machine dynamic flowshop and a flexible queueing network. We show for the first time that stability, a fundamental characteristic in queueing theory, can be applied to approaches that periodically solve combinatorial scheduling problems. We empirically demonstrate that for a dynamic flowshop, the use of combinatorial reasoning has little impact on schedule quality beyond queueing approaches. In contrast, for the more complicated flexible queueing network, a novel algorithm that combines long-term guidance from queueing theory with short-term combinatorial decision making outperforms all other tested approaches. To our knowledge, this is the first time that such a hybrid of queueing theory and scheduling techniques has been proposed and evaluated.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {535–572},
numpages = {38}
}

@article{10.5555/2693068.2693081,
author = {Keyder, Emil and Hoffmann, J\"{o}rg and Haslum, Patrik},
title = {Improving Delete Relaxation Heuristics through Explicitly Represented Conjunctions},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Heuristic functions based on the delete relaxation compute upper and lower bounds on the optimal delete-relaxation heuristic h+, and are of paramount importance in both optimal and satisficing planning. Here we introduce a principled and flexible technique for improving h+, by augmenting delete-relaxed planning tasks with a limited amount of delete information. This is done by introducing special fluents that explicitly represent conjunctions of fluents in the original planning task, rendering h+ the perfect heuristic h* in the limit. Previous work has introduced a method in which the growth of the task is potentially exponential in the number of conjunctions introduced. We formulate an alternative technique relying on conditional effects, limiting the growth of the task to be linear in this number. We show that this method still renders h+ the perfect heuristic h* in the limit. We propose techniques to find an informative set of conjunctions to be introduced in different settings, and analyze and extend existing methods for lower-bounding and upperbounding h+ in the presence of conditional effects. We evaluate the resulting heuristic functions empirically on a set of IPC benchmarks, and show that they are sometimes much more informative than standard delete-relaxation heuristics.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {487–533},
numpages = {47}
}

@article{10.5555/2693068.2693080,
author = {Cooper, Martin C. and Maris, Fr\'{e}d\'{e}ric and R\'{e}gnier, Pierre},
title = {Monotone Temporal Planning: Tractability, Extensions and Applications},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {This paper describes a polynomially-solvable class of temporal planning problems. Polynomiality follows from two assumptions. Firstly, by supposing that each sub-goal fluent can be established by at most one action, we can quickly determine which actions are necessary in any plan. Secondly, the monotonicity of sub-goal fluents allows us to express planning as an instance of STP≠ (Simple Temporal Problem with difference constraints). This class includes temporally-expressive problems requiring the concurrent execution of actions, with potential applications in the chemical, pharmaceutical and construction industries.We also show that any (temporal) planning problem has a monotone relaxation which can lead to the polynomial-time detection of its unsolvability in certain cases. Indeed we show that our relaxation is orthogonal to relaxations based on the ignore-deletes approach used in classical planning since it preserves deletes and can also exploit temporal information.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {447–485},
numpages = {39}
}

@article{10.5555/2693068.2693079,
author = {Bredereck, Robert and Chen, Jiehua and Hartung, Sepp and Kratsch, Stefan and Niedermeier, Rolf and Such\'{y}, Ond\v{r}ej and Woeginger, Gerhard J.},
title = {A Multivariate Complexity Analysis of Lobbying in Multiple Referenda},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Assume that each of n voters may or may not approve each of m issues. If an agent (the lobby) may influence up to k voters, then the central question of the NP-hard LOBBYING problem is whether the lobby can choose the voters to be influenced so that as a result each issue gets a majority of approvals. This problem can be modeled as a simple matrix modification problem: Can one replace k rows of a binary n\texttimes{}m-matrix by k all-1 rows such that each column in the resulting matrix has a majority of 1s? Significantly extending on previous work that showed parameterized intractability (W[2]-completeness) with respect to the number k of modified rows, we study how natural parameters such as n, m, k, or the "maximum number of 1s missing for any column to have a majority of 1s" (referred to as "gap value g") govern the computational complexity of LOBBYING. Among other results, we prove that LOBBYING is fixed-parameter tractable for parameter m and provide a greedy logarithmic-factor approximation algorithm which solves LOBBYING even optimally if m ≤ 4. We also show empirically that this greedy algorithm performs well on general instances. As a further key result, we prove that LOBBYING is LOGSNP-complete for constant values g ≥ 1, thus providing a first natural complete problem from voting for this complexity class of limited nondeterminism.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {409–446},
numpages = {38}
}

@article{10.5555/2693068.2693078,
author = {Doppa, Janardhan Rao and Fern, Alan and Tadepalli, Prasad},
title = {HC-Search: A Learning Framework for Search-Based Structured Prediction},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Structured prediction is the problem of learning a function that maps structured inputs to structured outputs. Prototypical examples of structured prediction include part-of-speech tagging and semantic segmentation of images. Inspired by the recent successes of search-based structured prediction, we introduce a new framework for structured prediction called HC-Search. Given a structured input, the framework uses a search procedure guided by a learned heuristic H to uncover high quality candidate outputs and then employs a separate learned cost function C to select a final prediction among those outputs. The overall loss of this prediction architecture decomposes into the loss due to H not leading to high quality outputs, and the loss due to C not selecting the best among the generated outputs. Guided by this decomposition, we minimize the overall loss in a greedy stagewise manner by first training H to quickly uncover high quality outputs via imitation learning, and then training C to correctly rank the outputs generated via H according to their true losses. Importantly, this training procedure is sensitive to the particular loss function of interest and the time-bound allowed for predictions. Experiments on several benchmark domains show that our approach significantly outperforms several state-of-the-art methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {369–407},
numpages = {39}
}

@article{10.5555/2693068.2693077,
author = {Fave, Francesco M. Delle and Jiang, Albert Xin and Yin, Zhengyu and Zhang, Chao and Tambe, Milind and Kraus, Sarit and Sullivan, John P.},
title = {Game-Theoretic Security Patrolling with Dynamic Execution Uncertainty and a Case Study on a Real Transit System},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Attacker-Defender Stackelberg security games (SSGs) have emerged as an important research area in multi-agent systems. However, existing SSGs models yield fixed, static, schedules which fail in dynamic domains where defenders face execution uncertainty, i.e., in domains where defenders may face unanticipated disruptions of their schedules. A concrete example is an application involving checking fares on trains, where a defender's schedule is frequently interrupted by fare evaders, making static schedules useless.To address this shortcoming, this paper provides four main contributions. First, we present a novel general Bayesian Stackelberg game model for security resource allocation in dynamic uncertain domains. In this new model, execution uncertainty is handled by using a Markov decision process (MDP) for generating defender policies. Second, we study the problem of computing a Stackelberg equilibrium for this game and exploit problem structure to reduce it to a polynomial-sized optimization problem. Shifting to evaluation, our third contribution shows, in simulation, that our MDP-based policies overcome the failures of previous SSG algorithms. In so doing, we can now build a complete system, that enables handling of schedule interruptions and, consequently, to conduct some of the first controlled experiments on SSGs in the field. Hence, as our final contribution, we present results from a real-world experiment on Metro trains in Los Angeles validating our MDP-based model, and most importantly, concretely measuring the benefits of SSGs for security resource allocation.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {321–367},
numpages = {47}
}

@article{10.5555/2693068.2693076,
author = {Suda, Martin},
title = {Property Directed Reachability for Automated Planning},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Property Directed Reachability (PDR) is a very promising recent method for deciding reachability in symbolically represented transition systems. While originally conceived as a model checking algorithm for hardware circuits, it has already been successfully applied in several other areas. This paper is the first investigation of PDR from the perspective of automated planning.Similarly to the planning as satisfiability paradigm, PDR draws its strength from internally employing an efficient SAT-solver. We show that most standard encoding schemes of planning into SAT can be directly used to turn PDR into a planning algorithm. As a non-obvious alternative, we propose to replace the SAT-solver inside PDR by a planning-specific procedure implementing the same interface. This SAT-solver free variant is not only more efficient, but offers additional insights and opportunities for further improvements. An experimental comparison to the state of the art planners finds it highly competitive, solving most problems on several domains.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {265–319},
numpages = {55}
}

@article{10.5555/2693068.2693075,
author = {Rivera, Nicol\'{a}s and Illanes, Le\'{o}n and Baier, Jorge A. and Hern\'{a}ndez, Carlos},
title = {Reconnection with the Ideal Tree: A New Approach to Real-Time Search},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Many applications, ranging from video games to dynamic robotics, require solving single-agent, deterministic search problems in partially known environments under very tight time constraints. Real-Time Heuristic Search (RTHS) algorithms are specifically designed for those applications. As a subroutine, most of them invoke a standard, but bounded, search algorithm that searches for the goal. In this paper we present FRIT, a simple approach for single-agent deterministic search problems under tight constraints and partially known environments that unlike traditional RTHS does not search for the goal but rather searches for a path that connects the current state with a so-called ideal tree T. When the agent observes that an arc in the tree cannot be traversed in the actual environment, it removes such an arc from T and then carries out a reconnection search whose objective is to find a path between the current state and any node in T. The reconnection search is done using an algorithm that is passed as a parameter to FRIT. If such a parameter is an RTHS algorithm, then the resulting algorithm can be an RTHS algorithm. We show, in addition, that FRIT may be fed with a (bounded) complete blind-search algorithm. We evaluate our approach over grid pathfinding benchmarks including game maps and mazes. Our results show that FRIT, used with RTAA*, a standard RTHS algorithm, outperforms RTAA* significantly; by one order of magnitude under tight time constraints. In addition, FRIT(daRTAA*) substantially outperforms daRTAA*, a state-of-the-art RTHS algorithm, usually obtaining solutions 50% cheaper on average when performing the same search effort. Finally, FRIT(BFS), i.e., FRIT using breadth-first-search, obtains best-quality solutions when time is limited compared to Adaptive A* and Repeated A*. Finally we show that Bug2, a pathfinding-specific navigation algorithm, outperforms FRIT(BFS) when planning time is extremely limited, but when given more time, the situation reverses.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {235–264},
numpages = {30}
}

@article{10.5555/2693068.2693074,
author = {De Bock, Jasper and De Cooman, Gert},
title = {An Efficient Algorithm for Estimating State Sequences in Imprecise Hidden Markov Models},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {We present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden Markov models (iHMMs). The uncertainty linking one state to the next, and that linking a state to its output, is represented by a set of probability mass functions instead of a single such mass function. We consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence, associated with a gain function that is the indicator of the state sequence. This corresponds to and generalises finding the state sequence with the highest posterior probability in (precise-probabilistic) HMMs, thereby making our algorithm a generalisation of the one by Viterbi. We argue that the computational complexity of our algorithm is at worst quadratic in the length of the iHMM, cubic in the number of states, and essentially linear in the number of maximal state sequences. An important feature of our imprecise approach is that there may be more than one maximal sequence, typically in those instances where its precise-probabilistic counterpart is sensitive to the choice of prior. For binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters. We also present an application in optical character recognition, demonstrating that our algorithm can be usefully applied to robustify the inferences made by its precise-probabilistic counterpart.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {189–233},
numpages = {45}
}

@article{10.5555/2693068.2693073,
author = {Goldenberg, Meir and Felner, Ariel and Stern, Roni and Sharon, Guni and Sturtevant, Nathan and Holte, Robert C. and Schaeffer, Jonathan},
title = {Enhanced Partial Expansion A*},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {When solving instances of problem domains that feature a large branching factor, A* may generate a large number of nodes whose cost is greater than the cost of the optimal solution. We designate such nodes as surplus. Generating surplus nodes and adding them to the OPEN list may dominate both time and memory of the search. A recently introduced variant of A* called Partial Expansion A* (PEA*) deals with the memory aspect of this problem. When expanding a node n, PEA* generates all of its children and puts into OPEN only the children with f = f(n). n is reinserted in the OPEN list with the f-cost of the best discarded child. This guarantees that surplus nodes are not inserted into OPEN.In this paper, we present a novel variant of A* called Enhanced Partial Expansion A* (EPEA*) that advances the idea of PEA* to address the time aspect. Given a priori domain-and heuristic-specific knowledge, EPEA* generates only the nodes with f = f(n). Although EPEA* is not always applicable or practical, we study several variants of EPEA*, which make it applicable to a large number of domains and heuristics. In particular, the ideas of EPEA* are applicable to IDA* and to the domains where pattern databases are traditionally used. Experimental studies show significant improvements in run-time and memory performance for several standard benchmark applications. We provide several theoretical studies to facilitate an understanding of the new algorithm.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {141–187},
numpages = {47}
}

@article{10.5555/2693068.2693072,
author = {De Keijzer, Bart and Klos, Tomas B. and Zhang, Yingqian},
title = {Finding Optimal Solutions for Voting Game Design Problems},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {In many circumstances where multiple agents need to make a joint decision, voting is used to aggregate the agents' preferences. Each agent's vote carries a weight, and if the sum of the weights of the agents in favor of some outcome is larger than or equal to a given quota, then this outcome is decided upon. The distribution of weights leads to a certain distribution of power. Several 'power indices' have been proposed to measure such power. In the so-called inverse problem, we are given a target distribution of power, and are asked to come up with a game--in the form of a quota, plus an assignment of weights to the players--whose power distribution is as close as possible to the target distribution (according to some specified distance measure).Here we study solution approaches for the larger class of voting game design (VGD) problems, one of which is the inverse problem. In the general VGD problem, the goal is to find a voting game (with a given number of players) that optimizes some function over these games. In the inverse problem, for example, we look for a weighted voting game that minimizes the distance between the distribution of power among the players and a given target distribution of power (according to a given distance measure).Our goal is to find algorithms that solve voting game design problems exactly, and we approach this goal by enumerating all games in the class of games of interest. We first present a doubly exponential algorithm for enumerating the set of simple games. We then improve on this algorithm for the class of weighted voting games and obtain a quadratic exponential (i.e., 2O(n2)) algorithm for enumerating them. We show that this improved algorithm runs in output-polynomial time, making it the fastest possible enumeration algorithm up to a polynomial factor. Finally, we propose an exact anytime-algorithm that runs in exponential time for the power index weighted voting game design problem (the 'inverse problem').We implement this algorithm to find a weighted voting game with a normalized Banzhaf power distribution closest to a target power index, and perform experiments to obtain some insights about the set of weighted voting games. We remark that our algorithm is applicable to optimizing any exponential-time computable function, the distance of the normalized Banzhaf index to a target power index is merely taken as an example.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {105–140},
numpages = {36}
}

@article{10.5555/2693068.2693071,
author = {Fern, Alan and Natarajan, Sriraam and Judah, Kshitij and Tadepalli, Prasad},
title = {A Decision-Theoretic Model of Assistance},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {There is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores. In this paper, we formulate the problem of intelligent assistance in a decision-theoretic framework, and present both theoretical and empirical results. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection for HGMDPs is PSPACE-complete even for deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), which are sufficient for modeling many real-world problems. We show classes of HAMDPs for which efficient algorithms are possible. More interestingly, for general HAMDPs we show that a simple myopic policy achieves a near optimal regret, compared to an oracle assistant that knows the agent's goal. We then introduce more sophisticated versions of this policy for the general case of HGMDPs that we combine with a novel approach for quickly learning about the agent being assisted. We evaluate our approach in two game-like computer environments where human subjects perform tasks, and in a real-world domain of providing assistance during folder navigation in a computer desktop environment. The results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {71–104},
numpages = {34}
}

@article{10.5555/2693068.2693070,
author = {Wang, Yisong and Zhang, Yan and Zhou, Yi and Zhang, Mingyi},
title = {Knowledge Forgetting in Answer Set Programming},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {The ability of discarding or hiding irrelevant information has been recognized as an important feature for knowledge based systems, including answer set programming. The notion of strong equivalence in answer set programming plays an important role for different problems as it gives rise to a substitution principle and amounts to knowledge equivalence of logic programs. In this paper, we uniformly propose a semantic knowledge forgetting, called HT-and FLP-forgetting, for logic programs under stable model and FLP-stable model semantics, respectively. Our proposed knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables. Thus it preserves strong equivalence in the sense that strongly equivalent logic programs will remain strongly equivalent after forgetting the same variables. We show that this semantic forgetting result is always expressible; and we prove a representation theorem stating that the HT-and FLP-forgetting can be precisely characterized by Zhang-Zhou's four forgetting postulates under the HT-and FLP-model semantics, respectively. We also reveal underlying connections between the proposed forgetting and the forgetting of propositional logic, and provide complexity results for decision problems in relation to the forgetting. An application of the proposed forgetting is also considered in a conflict solving scenario.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {31–70},
numpages = {40}
}

@article{10.5555/2693068.2693069,
author = {Zhang, Min and Xiao, Xinyan and Xiong, Deyi and Liu, Qun},
title = {Topic-Based Dissimilarity and Sensitivity Models for Translation Rule Selection},
year = {2014},
issue_date = {May 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {50},
number = {1},
issn = {1076-9757},
abstract = {Translation rule selection is a task of selecting appropriate translation rules for an ambiguous source-language segment. As translation ambiguities are pervasive in statistical machine translation, we introduce two topic-based models for translation rule selection which incorporates global topic information into translation disambiguation. We associate each synchronous translation rule with source- and target-side topic distributions. With these topic distributions, we propose a topic dissimilarity model to select desirable (less dissimilar) rules by imposing penalties for rules with a large value of dissimilarity of their topic distributions to those of given documents. In order to encourage the use of non-topic specific translation rules, we also present a topic sensitivity model to balance translation rule selection between generic rules and topic-specific rules. Furthermore, we project target-side topic distributions onto the source-side topic model space so that we can benefit from topic information of both the source and target language. We integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrase-based machine translation for synchronous translation rule selection. Experiments show that our topic-based translation rule selection model can substantially improve translation quality.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–30},
numpages = {30}
}

@article{10.5555/2655713.2655734,
author = {Garc\'{\i}a, Norberto Fern\'{a}ndez and Fisteus, Jes\'{u}s Arias and Fern\'{a}ndez, Luis S\'{a}nchez},
title = {Comparative Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia Systems},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {In recent years, the task of automatically linking pieces of text (anchors) mentioned in a document to Wikipedia articles that represent the meaning of these anchors has received extensive research attention. Typically, link-to-Wikipedia systems try to find a set of Wikipedia articles that are candidates to represent the meaning of the anchor and, later, rank these candidates to select the most appropriate one. In this ranking process the systems rely on context information obtained from the document where the anchor is mentioned and/or from Wikipedia. In this paper we center our attention in the use of Wikipedia links as context information. In particular, we offer a review of several candidate ranking approaches in the state-of-the-art that rely on Wikipedia link information. In addition, we provide a comparative empirical evaluation of the different approaches on five different corpora: the TAC 2010 corpus and four corpora built from actual Wikipedia articles and news items.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {733–773},
numpages = {41}
}

@article{10.5555/2655713.2655733,
author = {Carden, Stephen},
title = {Convergence of a Q-Learning Variant for Continuous States and Actions},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a reinforcement learning algorithm for solving in finite horizon Markov Decision Processes under the expected total discounted reward criterion when both the state and action spaces are continuous. This algorithm is based onWatkins' Q-learning, but uses Nadaraya-Watson kernel smoothing to generalize knowledge to unvisited states. As expected, continuity conditions must be imposed on the mean rewards and transition probabilities. Using results from kernel regression theory, this algorithm is proven capable of producing a Q-value function estimate that is uniformly within an arbitrary tolerance of the true Q-value function with probability one. The algorithm is then applied to an example problem to empirically show convergence as well.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {705–731},
numpages = {27}
}

@article{10.5555/2655713.2655732,
author = {Bonet, Maria Luisa and Buss, Sam and Johannsen, Jan},
title = {Improved Separations of Regular Resolution from Clause Learning Proof Systems},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {This paper studies the relationship between resolution and conflict driven clause learning (CDCL) without restarts, and refutes some conjectured possible separations. We prove that the guarded, xor-ified pebbling tautology clauses, which Urquhart proved are hard for regular resolution, as well as the guarded graph tautology clauses of Alekhnovich, Johannsen, Pitassi, and Urquhart have polynomial size pool resolution refutations that use only input lemmas as learned clauses. For the latter set of clauses, we extend this to prove that a CDCL search without restarts can refute these clauses in polynomial time, provided it makes the right choices for decision literals and clause learning. This holds even if the CDCL search is required to greedily process conflicts arising from unit propagation. This refutes the conjecture that the guarded graph tautology clauses or the guarded xorified pebbling tautology clauses can be used to separate CDCL without restarts from general resolution. Together with subsequent results by Buss and Ko\l{}odziejczyk, this means we lack any good conjectures about how to establish the exact logical strength of conflictdriven clause learning without restarts.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {669–703},
numpages = {35}
}

@article{10.5555/2655713.2655731,
author = {Nofal, Samer and Atkinson, Katie and Dunne, Paul E.},
title = {Algorithms for Argumentation Semantics: Labeling Attacks as a Generalization of Labeling Arguments},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {A Dung argumentation framework (AF) is a pair (A,R): A is a set of abstract arguments and R ⊆ A\texttimes{}A is a binary relation, so-called the attack relation, for capturing the conflicting arguments. "Labeling" based algorithms for enumerating extensions (i.e. sets of acceptable arguments) have been set out such that arguments (i.e. elements of A) are the only subject for labeling. In this paper we present implemented algorithms for listing extensions by labeling attacks (i.e. elements of R) along with arguments. Specifically, these algorithms are concerned with enumerating all extensions of an AF under a number of argumentation semantics: preferred, stable, complete, semi stable, stage, ideal and grounded. Our algorithms have impact, in particular, on enumerating extensions of AF-extended models that allow attacks on attacks. To demonstrate this impact, we instantiate our algorithms for an example of such models: namely argumentation frameworks with recursive attacks (AFRA), thereby we end up with unified algorithms that enumerate extensions of any AF/AFRA.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {635–668},
numpages = {34}
}

@article{10.5555/2655713.2655730,
author = {Chen, Suming and Choi, Arthur and Darwiche, Adnan},
title = {Algorithms and Applications for the Same-Decision Probability},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {When making decisions under uncertainty, the optimal choices are often difficult to discern, especially if not enough information has been gathered. Two key questions in this regard relate to whether one should stop the information gathering process and commit to a decision (stopping criterion), and if not, what information to gather next (selection criterion). In this paper, we show that the recently introduced notion, Same-Decision Probability (SDP), can be useful as both a stopping and a selection criterion, as it can provide additional insight and allow for robust decision making in a variety of scenarios. This query has been shown to be highly intractable, being PPPP-complete, and is exemplary of a class of queries which correspond to the computation of certain expectations. We propose the first exact algorithm for computing the SDP, and demonstrate its effectiveness on several real and synthetic networks. Finally, we present new complexity results, such as the complexity of computing the SDP on models with a Naive Bayes structure. Additionally, we prove that computing the non-myopic value of information is complete for the same complexity class as computing the SDP},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {601–633},
numpages = {33}
}

@article{10.5555/2655713.2655729,
author = {Wu, Yu (Ledell) and Austrin, Per and Pitassi, Toniann and Liu, David},
title = {Inapproximability of Treewidth, One-Shot Pebbling, and Related Layout Problems},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Graphical models, such as Bayesian Networks and Markov networks play an important role in artificial intelligence and machine learning. Inference is a central problem to be solved on these networks. This, and other problems on these graph models are often known to be hard to solve in general, but tractable on graphs with bounded Treewidth. Therefore, finding or approximating the Treewidth of a graph is a fundamental problem related to inference in graphical models. In this paper, we study the approximability of a number of graph problems: Treewidth and Pathwidth of graphs, Minimum Fill-In, One-Shot Black (and Black-White) pebbling costs of directed acyclic graphs, and a variety of different graph layout problems such as Minimum Cut Linear Arrangement and Interval Graph Completion. We show that, assuming the recently introduced Small Set Expansion Conjecture, all of these problems are NP-hard to approximate to within any constant factor in polynomial time.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {569–600},
numpages = {32}
}

@article{10.5555/2655713.2655728,
author = {Hoki, Kunihito and Kaneko, Tomoyuki},
title = {Large-Scale Optimization for Evaluation Functions with Minimax Search},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a new method, Minimax Tree Optimization (MMTO), to learn a heuristic evaluation function of a practical alpha-beta search program. The evaluation function may be a linear or non-linear combination of weighted features, and the weights are the parameters to be optimized. To control the search results so that the move decisions agree with the game records of human experts, a well-modeled objective function to be minimized is designed. Moreover, a numerical iterative method is used to find local minima of the objective function, and more than forty million parameters are adjusted by using a small number of hyper parameters. This method was applied to shogi, a major variant of chess in which the evaluation function must handle a larger state space than in chess. Experimental results show that the large-scale optimization of the evaluation function improves the playing strength of shogi programs, and the new method performs significantly better than other methods. Implementation of the new method in our shogi program Bonanza made substantial contributions to the program's first-place finish in the 2013 World Computer Shogi Championship. Additionally, we present preliminary evidence of broader applicability of our method to other two-player games such as chess.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {527–568},
numpages = {42}
}

@article{10.5555/2655713.2655727,
author = {Yang, Pei and Gao, Wei},
title = {Information-Theoretic Multi-View Domain Adaptation: A Theoretical and Empirical Study},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Multi-view learning aims to improve classification performance by leveraging the consistency among different views of data. The incorporation of multiple views was paid little attention in the studies of domain adaptation, where the view consistency based on source data is largely violated in the target domain due to the distribution gap between different domain data. In this paper, we leverage multiple views for cross-domain document classification. The central idea is to strengthen the views' consistency on target data by identifying the associations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) using a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated features across domains, which boosts the consistency between document clusterings that are based on the respective word and link views. Moreover, we demonstrate that IMAM can always find the document clustering with the minimal disagreement rate to the overlap of viewbased clusterings. We provide both theoretical and empirical justifications of the proposed method. Our experiments show that IMAM significantly outperforms traditional multi-view algorithm cotraining, the co-training-based adaptation algorithm CODA, the single-view transfer model CoCC and the large-margin-based multi-view transfer model MVTL-LM.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {501–525},
numpages = {25}
}

@article{10.5555/2655713.2655726,
author = {Han, Bo and Cook, Paul and Baldwin, Timothy},
title = {Text-Based Twitter User Geolocation Prediction},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Geographical location is vital to geospatial applications like local search and event detection. In this paper, we investigate and improve on the task of text-based geolocation prediction of Twitter users. Previous studies on this topic have typically assumed that geographical references (e.g., gazetteer terms, dialectal words) in a text are indicative of its author's location. However, these references are often buried in informal, ungrammatical, and multilingual data, and are therefore non-trivial to identify and exploit. We present an integrated geolocation prediction framework and investigate what factors impact on prediction accuracy. First, we evaluate a range of feature selection methods to obtain "location indicative words". We then evaluate the impact of nongeotagged tweets, language, and user-declared metadata on geolocation prediction. In addition, we evaluate the impact of temporal variance on model generalisation, and discuss how users differ in terms of their geolocatability.We achieve state-of-the-art results for the text-based Twitter user geolocation task, and also provide the most extensive exploration of the task to date. Our findings provide valuable insights into the design of robust, practical text-based geolocation prediction systems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {451–500},
numpages = {50}
}

@article{10.5555/2655713.2655725,
author = {Greco, Gianluigi and Scarcello, Francesco},
title = {Mechanisms for Fair Allocation Problems: No-Punishment Payment Rules in Verifiable Settings},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Mechanism design is considered in the context of fair allocations of indivisible goods with monetary compensation, by focusing on problems where agents' declarations on allocated goods can be verified before payments are performed. A setting is considered where verification might be subject to errors, so that payments have to be awarded under the presumption of innocence, as incorrect declared values do not necessarily mean manipulation attempts by the agents. Within this setting, a mechanism is designed that is shown to be truthful, efficient, and budget-balanced. Moreover, agents' utilities are fairly determined by the Shapley value of suitable coalitional games, and enjoy highly desirable properties such as equal treatment of equals, envy-freeness, and a stronger one called individual-optimality. In particular, the latter property guarantees that, for every agent, her/his utility is the maximum possible one over any alternative optimal allocation.The computational complexity of the proposed mechanism is also studied. It turns out that it is #P-complete so that, to deal with applications with many agents involved, two polynomial-time randomized variants are also proposed: one that is still truthful and efficient, and which is approximately budget-balanced with high probability, and another one that is truthful in expectation, while still budget-balanced and efficient.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {403–449},
numpages = {47}
}

@article{10.5555/2655713.2655724,
author = {Belle, Vaishak and Lakemeyer, Gerhard},
title = {Multiagent Only Knowing in Dynamic Systems},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {The idea of "only knowing" a collection of sentences, as proposed by Levesque, has been previously shown to be very useful in characterizing knowledge-based agents: in terms of a specification, a precise and perspicuous account of the beliefs and non-beliefs is obtained in a monotonic setting. Levesque's logic is based on a first-order modal language with quantifying-in, thus allowing for de re versus de dicto distinctions, among other things. However, the logic and its recent dynamic extension only deal with the case of a single agent. In this work, we propose a first-order multiagent framework with knowledge, actions, sensing and only knowing, that is shown to inherit all the features of the single agent version. Most significantly, we prove reduction theorems by means of which reasoning about knowledge and actions in the framework simplifies to non-epistemic, non-dynamic reasoning about the initial situation.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {363–402},
numpages = {40}
}

@article{10.5555/2655713.2655723,
author = {Cigler, Ludek and Faltings, Boi},
title = {Symmetric Subgame-Perfect Equilibria in Resource Allocation},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {We analyze symmetric protocols to rationally coordinate on an asymmetric, efficient allocation in an infinitely repeated N-agent, C-resource allocation problems, where the resources are all homogeneous. Bhaskar proposed one way to achieve this in 2-agent, 1- resource games: Agents start by symmetrically randomizing their actions, and as soon as they each choose different actions, they start to follow a potentially asymmetric "convention" that prescribes their actions from then on. We extend the concept of convention to the general case of infinitely repeated resource allocation games with N agents and C resources. We show that for any convention, there exists a symmetric subgameperfect equilibrium which implements it. We present two conventions: bourgeois, where agents stick to the first allocation; and market, where agents pay for the use of resources, and observe a global coordination signal which allows them to alternate between different allocations. We define price of anonymity of a convention as a ratio between the maximum social payoff of any (asymmetric) strategy profile and the expected social payoff of the subgame-perfect equilibrium which implements the convention. We show that while the price of anonymity of the bourgeois convention is infinite, the market convention decreases this price by reducing the conflict between the agents.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {323–361},
numpages = {39}
}

@article{10.5555/2655713.2655722,
author = {Eiter, Thomas and Fink, Michael and Krennwallner, Thomas and Redl, Christoph and Sch\"{u}ller, Peter},
title = {Efficient HEX-Program Evaluation Based on Unfounded Sets},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {HEX-programs extend logic programs under the answer set semantics with external computations through external atoms. As reasoning from ground Horn programs with nonmonotonic external atoms of polynomial complexity is already on the second level of the polynomial hierarchy, minimality checking of answer set candidates needs special attention. To this end, we present an approach based on unfounded sets as a generalization of related techniques for ASP programs. The unfounded set detection is expressed as a propositional SAT problem, for which we provide two different encodings and optimizations to them. We then integrate our approach into a previously developed evaluation framework for HEX-programs, which is enriched by additional learning techniques that aim at avoiding the reconstruction of the same or related unfounded sets. Furthermore, we provide a syntactic criterion that allows one to skip the minimality check in many cases. An experimental evaluation shows that the new approach significantly decreases runtime.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {269},
numpages = {1}
}

@article{10.5555/2655713.2655721,
author = {Berrar, Daniel},
title = {An Empirical Evaluation of Ranking Measures with Respect to Robustness to Noise},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Ranking measures play an important role in model evaluation and selection. Using both synthetic and real-world data sets, we investigate how different types and levels of noise affect the area under the ROC curve (AUC), the area under the ROC convex hull, the scored AUC, the Kolmogorov-Smirnov statistic, and the H-measure. In our experiments, the AUC was, overall, the most robust among these measures, thereby reinvigorating it as a reliable metric despite its well-known deficiencies. This paper also introduces a novel ranking measure, which is remarkably robust to noise yet conceptually simple.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {241–267},
numpages = {27}
}

@article{10.5555/2655713.2655720,
author = {Apt, Krzysztof R. and Sch\"{a}fer, Guido},
title = {Selfishness Level of Strategic Games},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {We introduce a new measure of the discrepancy in strategic games between the social welfare in a Nash equilibrium and in a social optimum, that we call selfishness level. It is the smallest fraction of the social welfare that needs to be offered to each player to achieve that a social optimum is realized in a pure Nash equilibrium. The selfishness level is unrelated to the price of stability and the price of anarchy and is invariant under positive linear transformations of the payoff functions. Also, it naturally applies to other solution concepts and other forms of games.We study the selfishness level of several well-known strategic games. This allows us to quantify the implicit tension within a game between players' individual interests and the impact of their decisions on the society as a whole. Our analyses reveal that the selfishness level often provides a deeper understanding of the characteristics of the underlying game that influence the players' willingness to cooperate.In particular, the selfishness level of finite ordinal potential games is finite, while that of weakly acyclic games can be infinite. We derive explicit bounds on the selfishness level of fair cost sharing games and linear congestion games, which depend on specific parameters of the underlying game but are independent of the number of players. Further, we show that the selfishness level of the n-players Prisoner's Dilemma is c/(b(n-1)-c), where b and c are the benefit and cost for cooperation, respectively, that of the n-players public goods game is (1 - c/n)/(c - 1), where c is the public good multiplier, and that of the Traveler's Dilemma game is 1/2 (b - 1), where b is the bonus. Finally, the selfishness level of Cournot competition (an example of an infinite ordinal potential game), Tragedy of the Commons, and Bertrand competition is infinite.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {207–240},
numpages = {34}
}

@article{10.5555/2655713.2655719,
author = {Schiffel, Stephan and Thielscher, Michael},
title = {Representing and Reasoning about the Rules of General Games with Imperfect Information},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {A general game player is a system that can play previously unknown games just by being given their rules. For this purpose, the Game Description Language (GDL) has been developed as a high-level knowledge representation formalism to communicate game rules to players. In this paper, we address a fundamental limitation of state-of-the-art methods and systems for General Game Playing, namely, their being confined to deterministic games with complete information about the game state. We develop a simple yet expressive extension of standard GDL that allows for formalising the rules of arbitrary finite, n-player games with randomness and incomplete state knowledge. In the second part of the paper, we address the intricate reasoning challenge for general game-playing systems that comes with the new description language. We develop a full embedding of extended GDL into the Situation Calculus augmented by Scherl and Levesque's knowledge fluent. We formally prove that this provides a sound and complete reasoning method for players' knowledge about game states as well as about the knowledge of the other players.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {171–206},
numpages = {36}
}

@article{10.5555/2655713.2655718,
author = {Halpern, Joseph Y. and Moses, Yoram},
title = {A Procedural Characterization of Solution Concepts in Games},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {We show how game-theoretic solution concepts such as Nash equilibrium, correlated equilibrium, rationalizability, and sequential equilibrium can be given a uniform definition in terms of a knowledge-based program with counterfactual semantics. In a precise sense, this program can be viewed as providing a procedural characterization of rationality.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {143–170},
numpages = {28}
}

@article{10.5555/2655713.2655717,
author = {Crandall, Jacob W.},
title = {Towards Minimizing Disappointment in Repeated Games},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of learning in repeated games against arbitrary associates. Specifically, we study the ability of expert algorithms to quickly learn effective strategies in repeated games, towards the ultimate goal of learning near-optimal behavior against any arbitrary associate within only a handful of interactions. Our contribution is three-fold. First, we advocate a new metric, called disappointment, for evaluating expert algorithms in repeated games. Unlike minimizing traditional notions of regret, minimizing disappointment in repeated games is equivalent to maximizing payoffs. Unfortunately, eliminating disappointment is impossible to guarantee in general. However, it is possible for an expert algorithm to quickly achieve low disappointment against many known classes of algorithms in many games. Second, we show that popular existing expert algorithms often fail to achieve low disappointment against a variety of associates, particularly in early rounds of the game. Finally, we describe a new meta-algorithm that can be applied to existing expert algorithms to substantially reduce disappointment in many two-player repeated games when associates follow various static, reinforcement learning, and expert algorithms.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {111–142},
numpages = {32}
}

@article{10.5555/2655713.2655716,
author = {Dung, Phan Minh and Thang, Phan Minh},
title = {Closure and Consistency in Logic-Associated Argumentation},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Properties like logical closure and consistency are important properties in any logical reasoning system. Caminada and Amgoud showed that not every logic-based argument system satisfies these relevant properties. But under conditions like closure under contraposition or transposition of the monotonic part of the underlying logic, ASPIC-like systems satisfy these properties. In contrast, the logical closure and consistency properties are not well-understood for other well-known and widely applied systems like logic programming or assumption based argumentation. Though conditions like closure under contraposition or transposition seem intuitive in ASPIC-like systems, they rule out many sensible ASPIClike systems that satisfy both properties of closure and consistency.We present a new condition referred to as the self-contradiction axiom that guarantees the consistency property in both ASPIC-like and assumption-based systems and is implied by both properties of closure under contraposition or transposition. We develop a logicassociated abstract argumentation framework, by associating abstract argumentation with abstract logics to represent the conclusions of arguments. We show that logicassociated abstract argumentation frameworks capture ASPIC-like systems (without preferences) and assumption-based argumentation. We present two simple and natural properties of compactness and cohesion in logic-associated abstract argumentation frameworks and show that they capture the logical closure and consistency properties. We demonstrate that in both assumption-based argumentation and ASPIC-like systems, cohesion follows naturally from the self-contradiction axiom. We further give a translation from ASPIC-like systems (without preferences) into equivalent assumption-based systems that keeps the selfcontradiction axiom invariant.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {79–109},
numpages = {31}
}

@article{10.5555/2655713.2655715,
author = {Climent, Laura and Wallace, Richard J. and Salido, Miguel A. and Barber, Federico},
title = {Robustness and Stability in Constraint Programming under Dynamism and Uncertainty},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Many real life problems that can be solved by constraint programming, come from uncertain and dynamic environments. Because of the dynamism, the original problem may change over time, and thus the solution found for the original problem may become invalid. For this reason, dealing with such problems has become an important issue in the fields of constraint programming. In some cases, there is extant knowledge about the uncertain and dynamic environment. In other cases, this information is fragmentary or unknown. In this paper, we extend the concept of robustness and stability for Constraint Satisfaction Problems (CSPs) with ordered domains, where only limited assumptions need to be made as to possible changes. We present a search algorithm that searches for both robust and stable solutions for CSPs of this nature. It is well-known that meeting both criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic environments. We also present compelling evidence that our search algorithm outperforms other general-purpose algorithms for dynamic CSPs using random instances and benchmarks derived from real life problems.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {49–78},
numpages = {30}
}

@article{10.5555/2655713.2655714,
author = {Bruni, Elia and Tran, Nam Khanh and Baroni, Marco},
title = {Multimodal Distributional Semantics},
year = {2014},
issue_date = {January 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {49},
number = {1},
issn = {1076-9757},
abstract = {Distributional semantic models derive computational representations of word meaning from the patterns of co-occurrence of words in text. Such models have been a success story of computational linguistics, being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them. However, distributional models extract meaning information exclusively from text, which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge. We address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete "visual words" in images, so that the distributional representation of a word can be extended to also encompass its co-occurrence with the visual words of images it is associated with. We propose a flexible architecture to integrate text- and image-based distributional information, and we show in a set of empirical tests that our integrated model is superior to the purely text-based approach, and it provides somewhat complementary semantic information with respect to the latter.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–47},
numpages = {47}
}

@article{10.5555/2591248.2591271,
author = {Campeotto, Federico and Dal Pal\`{u}, Alessandro and Dovier, Agostino and Fioretto, Ferdinando and Pontelli, Enrico},
title = {A Constraint Solver for Flexible Protein Models},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {This paper proposes the formalization and implementation of a novel class of constraints aimed at modeling problems related to placement of multi-body systems in the 3-dimensional space. Each multi-body is a system composed of body elements, connected by joint relationships and constrained by geometric properties. The emphasis of this investigation is the use of multi-body systems to model native conformations of protein structures--where each body represents an entity of the protein (e.g., an amino acid, a small peptide) and the geometric constraints are related to the spatial properties of the composing atoms. The paper explores the use of the proposed class of constraints to support a variety of different structural analysis of proteins, such as loop modeling and structure prediction.The declarative nature of a constraint-based encoding provides elaboration tolerance and the ability to make use of any additional knowledge in the analysis studies. The filtering capabilities of the proposed constraints also allow to control the number of representative solutions that are withdrawn from the conformational space of the protein, by means of criteria driven by uniform distribution sampling principles. In this scenario it is possible to select the desired degree of precision and/or number of solutions. The filtering component automatically excludes configurations that violate the spatial and geometric properties of the composing multi-body system. The paper illustrates the implementation of a constraint solver based on the multi-body perspective and its empirical evaluation on protein structure analysis problems.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {953–1000},
numpages = {48}
}

@article{10.5555/2591248.2591270,
author = {Mossel, Elchanan and Procaccia, Ariel D. and R\'{a}cz, Mikl\'{o}s Z.},
title = {A Smooth Transition from Powerlessness to Absolute Power},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We study the phase transition of the coalitional manipulation problem for generalized scoring rules. Previously it has been shown that, under some conditions on the distribution of votes, if the number of manipulators is o (√n), where n is the number of voters, then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity, whereas if the number of manipulators is ω(√n), then the probability that a random profile is manipulable goes to one. Here we consider the critical window, where a coalition has size c√n, and we show that as c goes from zero to infinity, the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion, i.e., there is a smooth phase transition between the two regimes. This result analytically validates recent empirical results, and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {923–951},
numpages = {29}
}

@article{10.5555/2591248.2591269,
author = {Franconi, Enrico and Kerhet, Volha and Ngo, Nhung},
title = {Exact Query Reformulation over Databases with First-Order and Description Logics Ontologies},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We study a general framework for query rewriting in the presence of an arbitrary first-order logic ontology over a database signature. The framework supports deciding the existence of a safe-range first-order equivalent reformulation of a query in terms of the database signature, and if so, it provides an effective approach to construct the reformulation based on interpolation using standard theorem proving techniques (e.g., tableau). Since the reformulation is a safe-range formula, it is effectively executable as an SQL query. At the end, we present a non-trivial application of the framework with ontologies in the very expressive ALCHOIQ description logic, by providing effective means to compute safe-range first-order exact reformulations of queries.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {885–922},
numpages = {38}
}

@article{10.5555/2591248.2591268,
author = {Guez, Arthur and Silver, David and Dayan, Peter},
title = {Scalable and Efficient Bayes-Adaptive Reinforcement Learning Based on Monte-Carlo Tree Search},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Bayesian planning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, planning optimally in the face of uncertainty is notoriously taxing, since the search space is enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach avoids expensive applications of Bayes rule within the search tree by sampling models from current beliefs, and furthermore performs this sampling in a lazy manner. This enables it to outperform previous Bayesian model-based reinforcement learning algorithms by a significant margin on several well-known benchmark problems. As we show, our approach can even work in problems with an in finite state space that lie qualitatively out of reach of almost all previous work in Bayesian exploration.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {841–883},
numpages = {43}
}

@article{10.5555/2591248.2591267,
author = {Dhurandhar, Amit and Wang, Jun},
title = {Single Network Relational Transductive Learning},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Relational classification on a single connected network has been of particular interest in the machine learning and data mining communities in the last decade or so. This is mainly due to the explosion in popularity of social networking sites such as Facebook, LinkedIn and Google+ amongst others. In statistical relational learning, many techniques have been developed to address this problem, where we have a connected unweighted homogeneous/ heterogeneous graph that is partially labeled and the goal is to propagate the labels to the unlabeled nodes. In this paper, we provide a different perspective by enabling the effective use of graph transduction techniques for this problem. We thus exploit the strengths of this class of methods for relational learning problems. We accomplish this by providing a simple procedure for constructing a weight matrix that serves as input to a rich class of graph transduction techniques. Our procedure has multiple desirable properties. For example, the weights it assigns to edges between unlabeled nodes naturally relate to a measure of association commonly used in statistics, namely the Gamma test statistic. We further portray the efficacy of our approach on synthetic as well as real data, by comparing it with state-of-the-art relational learning algorithms, and graph transduction techniques with an adjacency matrix or a real valued weight matrix computed using available attributes as input. In these experiments we see that our approach consistently outperforms other approaches when the graph is sparsely labeled, and remains competitive with the best when the proportion of known labels increases.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {813–839},
numpages = {27}
}

@article{10.5555/2591248.2591266,
author = {Domshlak, Carmel and Nazarenko, Anton},
title = {The Complexity of Optimal Monotonic Planning: The Bad, the Good, and the Causal Graph},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {For almost two decades, monotonic, or "delete free," relaxation has been one of the key auxiliary tools in the practice of domain-independent deterministic planning. In the particular contexts of both satisficing and optimal planning, it underlies most state-of-theart heuristic functions. While satisficing planning for monotonic tasks is polynomial-time, optimal planning for monotonic tasks is NP-equivalent. Here we establish both negative and positive results on the complexity of some wide fragments of optimal monotonic planning, with the fragments being defined around the causal graph topology. Our results shed some light on the link between the complexity of general optimal planning and the complexity of optimal planning for the respective monotonic relaxations.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {783–812},
numpages = {30}
}

@article{10.5555/2591248.2591265,
author = {Xiao, Tong and Zhu, Jingbo},
title = {Unsupervised Sub-Tree Alignment for Tree-to-Tree Translation},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {This article presents a probabilistic sub-tree alignment model and its application to tree-to-tree machine translation. Unlike previous work, we do not resort to surface heuristics or expensive annotated data, but instead derive an unsupervised model to infer the syntactic correspondence between two languages. More importantly, the developed model is syntactically-motivated and does not rely on word alignments. As a by-product, our model outputs a sub-tree alignment matrix encoding a large number of diverse alignments between syntactic structures, from which machine translation systems can effciently extract translation rules that are often filtered out due to the errors in 1-best alignment. Experimental results show that the proposed approach outperforms three state-of-the-art baseline approaches in both alignment accuracy and grammar quality. When applied to machine translation, our approach yields a +1.0 BLEU improvement and a -0.9 TER reduction on the NIST machine translation evaluation corpora. With tree binarization and fuzzy decoding, it even outperforms a state-of-the-art hierarchical phrase-based system.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {733–782},
numpages = {50}
}

@article{10.5555/2591248.2591264,
author = {de la Cruz, Jos\'{e} Luis P\'{e}rez and Mandow, Lawrence and Machuca, Enrique},
title = {A Case of Pathology in Multiobjective Heuristic Search},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {This article considers the performance of the MOA* multiobjective search algorithm with heuristic information. It is shown that in certain cases blind search can be more efficient than perfectly informed search, in terms of both node and label expansions.A class of simple graph search problems is defined for which the number of nodes grows linearly with problem size and the number of nondominated labels grows quadratically. It is proved that for these problems the number of node expansions performed by blind MOA* grows linearly with problem size, while the number of such expansions performed with a perfectly informed heuristic grows quadratically. It is also proved that the number of label expansions grows quadratically in the blind case and cubically in the informed case.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {717–732},
numpages = {16}
}

@article{10.5555/2591248.2591263,
author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
title = {Generating Natural Language Descriptions from OWL Ontologies: The Natural OWL System},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We present Naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. Unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like Naturalowl, one can publish information in owl on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of Naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent linguistic resources are available, Naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {671–715},
numpages = {45}
}

@article{10.5555/2591248.2591262,
author = {Calvanese, Diego and Ortiz, Magdalena and \v{S}imkus, Mantas and Stefanoni, Giorgio},
title = {Reasoning about Explanations for Negative Query Answers in DL-Lite},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {In order to meet usability requirements, most logic-based applications provide explanation facilities for reasoning services. This holds also for Description Logics, where research has focused on the explanation of both TBox reasoning and, more recently, query answering. Besides explaining the presence of a tuple in a query answer, it is important to explain also why a given tuple is missing. We address the latter problem for instance and conjunctive query answering over DL-Lite ontologies by adopting abductive reasoning; that is, we look for additions to the ABox that force a given tuple to be in the result. As reasoning tasks we consider existence and recognition of an explanation, and relevance and necessity of a given assertion for an explanation. We characterize the computational complexity of these problems for arbitrary, subset minimal, and cardinality minimal explanations.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {635–669},
numpages = {35}
}

@article{10.5555/2591248.2591261,
author = {Fang, Fei and Jiang, Albert Xin and Tambe, Milind},
title = {Protecting Moving Targets with Multiple Mobile Resources},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {In recent years, Stackelberg Security Games have been successfully applied to solve resource allocation and scheduling problems in several security domains. However, previous work has mostly assumed that the targets are stationary relative to the defender and the attacker, leading to discrete game models with finite numbers of pure strategies. This paper in contrast focuses on protecting mobile targets that leads to a continuous set of strategies for the players. The problem is motivated by several real-world domains including protecting ferries with escort boats and protecting refugee supply lines. Our contributions include: (i) A new game model for multiple mobile defender resources and moving targets with a discretized strategy space for the defender and a continuous strategy space for the attacker. (ii) An efficient linear-programming-based solution that uses a compact representation for the defender's mixed strategy, while accurately modeling the attacker's continuous strategy using a novel sub-interval analysis method. (iii) Discussion and analysis of multiple heuristic methods for equilibrium refinement to improve robustness of defender's mixed strategy. (iv) Discussion of approaches to sample actual defender schedules from the defender's mixed strategy. (iv) Detailed experimental analysis of our algorithms in the ferry protection domain.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {583–634},
numpages = {52}
}

@article{10.5555/2591248.2591260,
author = {Fern\'{a}ndez, Jose David and Vico, Francisco},
title = {AI Methods in Algorithmic Composition: A Comprehensive Survey},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {513–582},
numpages = {70}
}

@article{10.5555/2591248.2591259,
author = {Delgrande, James P. and Wassermann, Renata},
title = {Horn Clause Contraction Functions},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {In classical, AGM-style belief change, it is assumed that the underlying logic contains classical propositional logic. This is clearly a limiting assumption, particularly in Artificial Intelligence. Consequently there has been recent interest in studying belief change in approaches where the full expressivity of classical propositional logic is not obtained. In this paper we investigate belief contraction in Horn knowledge bases. We point out that the obvious extension to the Horn case, involving Horn remainder sets as a starting point, is problematic. Not only do Horn remainder sets have undesirable properties, but also some desirable Horn contraction functions are not captured by this approach. For Horn belief set contraction, we develop an account in terms of a model-theoretic characterisation involving weak remainder sets. Maxichoice and partial meet Horn contraction is specified, and we show that the problems arising with earlier work are resolved by these approaches. As well, constructions of the specific operators and sets of postulates are provided, and representation results are obtained. We also examine Horn package contraction, or contraction by a set of formulas. Again, we give a construction and postulate set, linking them via a representation result. Last, we investigate the closely-related notion of forgetting in Horn clauses. This work is arguably interesting since Horn clauses have found widespread use in AI; as well, the results given here may potentially be extended to other areas which make use of Horn-like reasoning, such as logic programming, rule-based systems, and description logics. Finally, since Horn reasoning is weaker than classical reasoning, this work sheds light on the foundations of belief change.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {475–511},
numpages = {37}
}

@article{10.5555/2591248.2591258,
author = {Casini, Giovanni and Straccia, Umberto},
title = {Defeasible Inheritance-Based Description Logics},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Defeasible inheritance networks are a non-monotonic framework that deals with hierarchical knowledge. On the other hand, rational closure is acknowledged as a landmark of the preferential approach to non-monotonic reasoning. We will combine these two approaches and define a new non-monotonic closure operation for propositional knowledge bases that combines the advantages of both. Then we redefine such a procedure for Description Logics (DLs), a family of logics well-suited to model structured information. In both cases we will provide a simple reasoning method that is built on top of the classical entailment relation and, thus, is amenable of an implementation based on existing reasoners. Eventually, we evaluate our approach on well-known landmark test examples.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {415–473},
numpages = {59}
}

@article{10.5555/2591248.2591257,
author = {ten Cate, Balder and Franconi, Enrico and Seylan, undefinednan\c{c}},
title = {Beth Definability in Expressive Description Logics},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {The Beth definability property, a well-known property from classical logic, is investigated in the context of description logics: if a general L-TBox implicitly defines an L-concept in terms of a given signature, where L is a description logic, then does there always exist over this signature an explicit definition in L for the concept? This property has been studied before and used to optimize reasoning in description logics. In this paper a complete classification of Beth definability is provided for extensions of the basic description logic ALC with transitive roles, inverse roles, role hierarchies, and/or functionality restrictions, both on arbitrary and on finite structures. Moreover, we present a tableau-based algorithm which computes explicit definitions of at most double exponential size. This algorithm is optimal because it is also shown that the smallest explicit definition of an implicitly defined concept may be double exponentially long in the size of the input TBox. Finally, if explicit definitions are allowed to be expressed in first-order logic, then we show how to compute them in single exponential time.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {347–414},
numpages = {68}
}

@article{10.5555/2591248.2591256,
author = {Konstas, Ioannis and Lapata, Mirella},
title = {A Global Model for Concept-to-Text Generation},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection ("what to say") and surface realization ("how to say") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves results competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {305–346},
numpages = {42}
}

@article{10.5555/2591248.2591255,
author = {Kollia, Ilianna and Glimm, Birte},
title = {Optimizing SPARQL Query Answering over OWL Ontologies},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {The SPARQL query language is currently being extended by the World Wide Web Consortium (W3C) with so-called entailment regimes. An entailment regime defines how queries are evaluated under more expressive semantics than SPARQL's standard simple entailment, which is based on subgraph matching. The queries are very expressive since variables can occur within complex concepts and can also bind to concept or role names.In this paper, we describe a sound and complete algorithm for the OWL Direct Semantics entailment regime. We further propose several novel optimizations such as strategies for determining a good query execution order, query rewriting techniques, and show how specialized OWL reasoning tasks and the concept and role hierarchy can be used to reduce the query execution time. For determining a good execution order, we propose a cost-based model, where the costs are based on information about the instances of concepts and roles that are extracted from a model abstraction built by an OWL reasoner. We present two ordering strategies: a static and a dynamic one. For the dynamic case, we improve the performance by exploiting an individual clustering approach that allows for computing the cost functions based on one individual sample from a cluster.We provide a prototypical implementation and evaluate the efficiency of the proposed optimizations. Our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available. This changes, however, when the statistics are less accurate, e.g., due to nondeterministic reasoning decisions. For queries that go beyond conjunctive instance queries we observe an improvement of up to three orders of magnitude due to the proposed optimizations.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {253–303},
numpages = {51}
}

@article{10.5555/2591248.2591254,
author = {Gent, Ian P.},
title = {Optimal Implementation of Watched Literals and More General Techniques},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {I prove that an implementation technique for scanning lists in backtracking search algorithms is optimal. The result applies to a simple general framework, which I present: applications include watched literal unit propagation in SAT and a number of examples in constraint satisfaction. Techniques like watched literals are known to be highly space efficient and effective in practice. When implemented in the 'circular' approach described here, these techniques also have optimal run time per branch in big-O terms when amortized across a search tree. This also applies when multiple list elements must be found. The constant factor overhead of the worst case is only 2. Replacing the existing nonoptimal implementation of unit propagation in MiniSat speeds up propagation by 29%, though this is not enough to improve overall run time significantly.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {231–252},
numpages = {22}
}

@article{10.5555/2591248.2591253,
author = {Robu, Valentin and Gerding, Enrico H. and Stein, Sebastian and Parkes, David C. and Rogers, Alex and Jennings, Nicholas R.},
title = {An Online Mechanism for Multi-Unit Demand and Its Application to Plug-in Hybrid Electric Vehicle Charging},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We develop an online mechanism for the allocation of an expiring resource to a dynamic agent population. Each agent has a non-increasing marginal valuation function for the resource, and an upper limit on the number of units that can be allocated in any period. We propose two versions on a truthful allocation mechanism. Each modifies the decisions of a greedy online assignment algorithm by sometimes cancelling an allocation of resources. One version makes this modification immediately upon an allocation decision while a second waits until the point at which an agent departs the market. Adopting a prior-free framework, we show that the second approach has better worst-case allocative efficiency and is more scalable. On the other hand, the first approach (with immediate cancellation) may be easier in practice because it does not need to reclaim units previously allocated. We consider an application to recharging plug-in hybrid electric vehicles (PHEVs). Using data from a real-world trial of PHEVs in the UK, we demonstrate higher system performance than a fixed price system, performance comparable with a standard, but non-truthful scheduling heuristic, and the ability to support 50% more vehicles at the same fuel cost than a simple randomized policy.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {175–230},
numpages = {56}
}

@article{10.5555/2591248.2591252,
author = {Cal\`{\i}, Andrea and Gottlob, Georg and Kifer, Michael},
title = {Taming the Infinite Chase: Query Answering under Expressive Relational Constraints},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {The chase algorithm is a fundamental tool for query evaluation and for testing query containment under tuple-generating dependencies (TGDs) and equality-generating dependencies (EGDs). So far, most of the research on this topic has focused on cases where the chase procedure terminates. This paper introduces expressive classes of TGDs defined via syntactic restrictions: guarded TGDs (GTGDs) and weakly guarded sets of TGDs (WGT-GDs). For these classes, the chase procedure is not guaranteed to terminate and thus may have an infinite outcome. Nevertheless, we prove that the problems of conjunctive-query answering and query containment under such TGDs are decidable. We provide decision procedures and tight complexity bounds for these problems. Then we show how EGDs can be incorporated into our results by providing conditions under which EGDs do not harmfully interact with TGDs and do not affect the decidability and complexity of query answering. We show applications of the aforesaid classes of constraints to the problem of answering conjunctive queries in F-Logic Lite, an object-oriented ontology language, and in some tractable Description Logics.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {115–174},
numpages = {60}
}

@article{10.5555/2591248.2591251,
author = {Roijers, Diederik M. and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
title = {A Survey of Multi-Objective Sequential Decision-Making},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {67–113},
numpages = {47}
}

@article{10.5555/2591248.2591250,
author = {Yuan, Changhe and Malone, Brandon},
title = {Learning Optimal Bayesian Networks: A Shortest Path Perspective},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {In this paper, learning a Bayesian network structure that optimizes a scoring function for a given dataset is viewed as a shortest path problem in an implicit state-space search graph. This perspective highlights the importance of two research issues: the development of search strategies for solving the shortest path problem, and the design of heuristic functions for guiding the search. This paper introduces several techniques for addressing the issues. One is an A* search algorithm that learns an optimal Bayesian network structure by only searching the most promising part of the solution space. The others are mainly two heuristic functions. The first heuristic function represents a simple relaxation of the acyclicity constraint of a Bayesian network. Although admissible and consistent, the heuristic may introduce too much relaxation and result in a loose bound. The second heuristic function reduces the amount of relaxation by avoiding directed cycles within some groups of variables. Empirical results show that these methods constitute a promising approach to learning optimal Bayesian network structures.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {23–65},
numpages = {43}
}

@article{10.5555/2591248.2591249,
author = {Alabbas, Maytham and Ramsay, Allan},
title = {Natural Language Inference for Arabic Using Extended Tree Edit Distance with Subtrees},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Many natural language processing (NLP) applications require the computation of similarities between pairs of syntactic or semantic trees. Many researchers have used tree edit distance for this task, but this technique suffers from the drawback that it deals with single node operations only. We have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes. The extended algorithm with subtree operations, TED+ST, is more effective and flexible than the standard algorithm, especially for applications that pay attention to relations among nodes (e.g. in linguistic trees, deleting a modifier subtree should be cheaper than the sum of deleting its components individually). We describe the use of TED+ST for checking entailment between two Arabic text snippets. The preliminary results of using TED+ST were encouraging when compared with two string-based approaches and with the standard algorithm.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {1–22},
numpages = {22}
}

@article{10.5555/2566972.2566993,
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
title = {Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {853–899},
numpages = {47}
}

@article{10.5555/2566972.2566992,
author = {Mosurovic, Milenko and Krdzavac, Nenad and Graves, Henson and Zakharyaschev, Michael},
title = {A Decidable Extension of SROIQ with Complex Role Chains and Unions},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {We design a decidable extension of the description logic SROIQ underlying the Web Ontology Language OWL2. The new logic, called SR+OIQ, supports a controlled use of role axioms whose right-hand side may contain role chains or role unions. We give a tableau algorithm for checking concept satisfiability with respect to SR+OIQ ontologies and prove its soundness, completeness and termination.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {809–851},
numpages = {43}
}

@article{10.5555/2566972.2566991,
author = {Grau, Bernardo Cuenca and Horrocks, Ian and Kr\"{o}tzsch, Markus and Kupke, Clemens and Magka, Despoina and Motik, Boris and Wang, Zhe},
title = {Acyclicity Notions for Existential Rules and Their Application to Query Answering in Ontologies},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Answering conjunctive queries (CQs) over a set of facts extended with existential rules is a prominent problem in knowledge representation and databases. This problem can be solved using the chase algorithm, which extends the given set of facts with fresh facts in order to satisfy the rules. If the chase terminates, then CQs can be evaluated directly in the resulting set of facts. The chase, however, does not terminate necessarily, and checking whether the chase terminates on a given set of rules and facts is undecidable. Numerous acyclicity notions were proposed as sufficient conditions for chase termination. In this paper, we present two new acyclicity notions called model-faithful acyclicity (MFA) and model-summarising acyclicity (MSA). Furthermore, we investigate the landscape of the known acyclicity notions and establish a complete taxonomy of all notions known to us. Finally, we show that MFA and MSA generalise most of these notions.Existential rules are closely related to the Horn fragments of the OWL 2 ontology language; furthermore, several prominent OWL 2 reasoners implement CQ answering by using the chase to materialise all relevant facts. In order to avoid termination problems, many of these systems handle only the OWL 2 RL profile of OWL 2; furthermore, some systems go beyond OWL 2 RL, but without any termination guarantees. In this paper we also investigate whether various acyclicity notions can provide a principled and practical solution to these problems. On the theoretical side, we show that query answering for acyclic ontologies is of lower complexity than for general ontologies. On the practical side, we show that many of the commonly used OWL 2 ontologies are MSA, and that the number of facts obtained by materialisation is not too large. Our results thus suggest that principled development of materialisation-based OWL 2 reasoners is practically feasible.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {741–808},
numpages = {68}
}

@article{10.5555/2566972.2566990,
author = {Burns, Ethan and Ruml, Wheeler and Do, Minh B.},
title = {Heuristic Search When Time Matters},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In many applications of shortest-path algorithms, it is impractical to find a provably optimal solution; one can only hope to achieve an appropriate balance between search time and solution cost that respects the user's preferences. Preferences come in many forms; we consider utility functions that linearly trade-off search time and solution cost. Many natural utility functions can be expressed in this form. For example, when solution cost represents the makespan of a plan, equally weighting search time and plan makespan minimizes the time from the arrival of a goal until it is achieved. Current state-of-theart approaches to optimizing utility functions rely on anytime algorithms, and the use of extensive training data to compute a termination policy. We propose a more direct approach, called Bugsy, that incorporates the utility function directly into the search, obviating the need for a separate termination policy. We describe a new method based on off-line parameter tuning and a novel benchmark domain for planning under time pressure based on platform-style video games. We then present what we believe to be the first empirical study of applying anytime monitoring to heuristic search, and we compare it with our proposals. Our results suggest that the parameter tuning technique can give the best performance if a representative set of training instances is available. If not, then Bugsy is the algorithm of choice, as it performs well and does not require any off-line training. This work extends the tradition of research on metareasoning for search by illustrating the benefits of embedding lightweight reasoning about time into the search algorithm itself.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {697–740},
numpages = {44}
}

@article{10.5555/2566972.2566989,
author = {L\'{e}aut\'{e}, Thomas and Faltings, Boi},
title = {Protecting Privacy through Distributed Computation in Multi-Agent Decision Making},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {As large-scale theft of data from corporate servers is becoming increasingly common, it becomes interesting to examine alternatives to the paradigm of centralizing sensitive data into large databases. Instead, one could use cryptography and distributed computation so that sensitive data can be supplied and processed in encrypted form, and only the final result is made known. In this paper, we examine how such a paradigm can be used to implement constraint satisfaction, a technique that can solve a broad class of AI problems such as resource allocation, planning, scheduling, and diagnosis. Most previous work on privacy in constraint satisfaction only attempted to protect specific types of information, in particular the feasibility of particular combinations of decisions. We formalize and extend these restricted notions of privacy by introducing four types of private information, including the feasibility of decisions and the final decisions made, but also the identities of the participants and the topology of the problem. We present distributed algorithms that allow computing solutions to constraint satisfaction problems while maintaining these four types of privacy. We formally prove the privacy properties of these algorithms, and show experiments that compare their respective performance on benchmark problems.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {649–695},
numpages = {47}
}

@article{10.5555/2566972.2566988,
author = {Grinshpoun, Tal and Grubshtein, Alon and Zivan, Roie and Netzer, Arnon and Meisels, Amnon},
title = {Asymmetric Distributed Constraint Optimization Problems},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Distributed Constraint Optimization (DCOP) is a powerful framework for representing and solving distributed combinatorial problems, where the variables of the problem are owned by different agents. Many multi-agent problems include constraints that produce different gains (or costs) for the participating agents. Asymmetric gains of constrained agents cannot be naturally represented by the standard DCOP model.The present paper proposes a general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may have different valuations for constraints that they are involved in. The new framework bridges the gap between multi-agent problems which tend to have asymmetric structure and the standard symmetric DCOP model. The benefits of the proposed model over previous attempts to generalize the DCOP model are discussed and evaluated.Innovative algorithms that apply to the special properties of the proposed ADCOP model are presented in detail. These include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms (for standard DCOPs) which use alternative representations. Moreover, standard incomplete algorithms (i.e., local search algorithms) are inapplicable to the existing DCOP representations of asymmetric constraints and when they are applied to the new ADCOP framework they often fail to converge to a local optimum and yield poor results. The local search algorithms proposed in the present paper converge to high quality solutions. The experimental evidence that is presented reveals that the proposed local search algorithms for ADCOPs achieve high quality solutions while preserving a high level of privacy.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {613–647},
numpages = {35}
}

@article{10.5555/2566972.2566987,
author = {B\"{a}ckstr\"{o}m, Christer and Jonsson, Peter},
title = {A Refined View of Causal Graphs and Component Sizes: SP-Closed Graph Classes and Beyond},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {The causal graph of a planning instance is an important tool for planning both in practice and in theory. The theoretical studies of causal graphs have largely analysed the computational complexity of planning for instances where the causal graph has a certain structure, often in combination with other parameters like the domain size of the variables. Chen and Gim\'{e}nez ignored even the structure and considered only the size of the weakly connected components. They proved that planning is tractable if the components are bounded by a constant and otherwise intractable. Their intractability result was, however, conditioned by an assumption from parameterised complexity theory that has no known useful relationship with the standard complexity classes. We approach the same problem from the perspective of standard complexity classes, and prove that planning is NP-hard for classes with unbounded components under an additional restriction we refer to as SP-closed. We then argue that most NP-hardness theorems for causal graphs are difficult to apply and, thus, prove a more general result; even if the component sizes grow slowly and the class is not densely populated with graphs, planning still cannot be tractable unless the polynomial hierachy collapses. Both these results still hold when restricted to the class of acyclic causal graphs. We finally give a partial characterization of the borderline between NP-hard and NP-intermediate classes, giving further insight into the problem.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {575–611},
numpages = {37}
}

@article{10.5555/2566972.2566986,
author = {Joty, Shafiq and Carenini, Giuseppe and Ng, Raymond T.},
title = {Topic Segmentation and Labeling in Asynchronous Conversations},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {521–573},
numpages = {53}
}

@article{10.5555/2566972.2566985,
author = {Betzler, Nadja and Slinko, Arkadii and Uhlmann, Johannes},
title = {On the Computation of Fully Proportional Representation},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {We investigate two systems of fully proportional representation suggested by Chamberlin &amp; Courant and Monroe. Both systems assign a representative to each voter so that the "sum of misrepresentations" is minimized. The winner determination problem for both systems is known to be NP-hard, hence this work aims at investigating whether there are variants of the proposed rules and/or specific electorates for which these problems can be solved efficiently. As a variation of these rules, instead of minimizing the sum of misrepresentations, we considered minimizing the maximal misrepresentation introducing effectively two new rules. In the general case these "minimax" versions of classical rules appeared to be still NP-hard.We investigated the parameterized complexity of winner determination of the two classical and two new rules with respect to several parameters. Here we have a mixture of positive and negative results: e.g., we proved fixed-parameter tractability for the parameter the number of candidates but fixed-parameter intractability for the number of winners.For single-peaked electorates our results are overwhelmingly positive: we provide polynomial-time algorithms for most of the considered problems. The only rule that remains NP-hard for single-peaked electorates is the classical Monroe rule.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {475–519},
numpages = {45}
}

@article{10.5555/2566972.2566984,
author = {Cigler, Ludek and Faltings, Boi},
title = {Decentralized Anti-Coordination through Multi-Agent Learning},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {To achieve an optimal outcome in many situations, agents need to choose distinct actions from one another. This is the case notably in many resource allocation problems, where a single resource can only be used by one agent at a time. How shall a designer of a multi-agent system program its identical agents to behave each in a different way?From a game theoretic perspective, such situations lead to undesirable Nash equilibria. For example consider a resource allocation game in that two players compete for an exclusive access to a single resource. It has three Nash equilibria. The two pure-strategy NE are efficient, but not fair. The one mixed-strategy NE is fair, but not efficient. Aumann's notion of correlated equilibrium fixes this problem: It assumes a correlation device that suggests each agent an action to take.However, such a "smart" coordination device might not be available. We propose using a randomly chosen, "stupid" integer coordination signal. "Smart" agents learn which action they should use for each value of the coordination signal.We present a multi-agent learning algorithm that converges in polynomial number of steps to a correlated equilibrium of a channel allocation game, a variant of the resource allocation game. We show that the agents learn to play for each coordination signal value a randomly chosen pure-strategy Nash equilibrium of the game. Therefore, the outcome is an efficient correlated equilibrium. This CE becomes more fair as the number of the available coordination signal values increases.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {441–473},
numpages = {33}
}

@article{10.5555/2566972.2566983,
author = {Taghipour, Nima and Fierens, Daan and Davis, Jesse and Blockeel, Hendrik},
title = {Lifted Variable Elimination: Decoupling the Operators from the Constraint Language},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. In this article, we decouple lifted inference from the constraint language. We define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints' extension) rather than on the syntactic level, making them language-independent. As a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. We empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {393–439},
numpages = {47}
}

@article{10.5555/2566972.2566982,
author = {Faber, Wolfgang and Truszczy\'{n}ski, Miros\l{}aw and Woltran, Stefan},
title = {Strong Equivalence of Qualitative Optimization Problems},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {We introduce the framework of qualitative optimization problems (or, simply, optimization problems) to represent preference theories. The formalism uses separate modules to describe the space of outcomes to be compared (the generator) and the preferences on outcomes (the selector). We consider two types of optimization problems. They differ in the way the generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics. Under the latter interpretation of generators, optimization problems directly generalize answer-set optimization programs proposed previously. We study strong equivalence of optimization problems, which guarantees their interchangeability within any larger context. We characterize several versions of strong equivalence obtained by restricting the class of optimization problems that can be used as extensions and establish the complexity of associated reasoning tasks. Understanding strong equivalence is essential for modular representation of optimization problems and rewriting techniques to simplify them without changing their inherent properties.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {351–391},
numpages = {41}
}

@article{10.5555/2566972.2566981,
author = {Costa, Paulo and Botelho, Luis},
title = {Learning by Observation of Agent Software Images},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Learning by observation can be of key importance whenever agents sharing similar features want to learn from each other. This paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task. This is possible because the proposed architecture displays information that is essential for observation, making it possible for software agents to observe each other.The agent architecture supports a learning process that covers all aspects of learning by observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge and evaluating the agent's progress. The evaluation provides control over the decision to obtain new knowledge or apply the acquired knowledge to new problems.We combine two methods for learning from the observed information. The first one, the recall method, uses the sequence on which the actions were observed to solve new problems. The second one, the classification method, categorizes the information in the observed data and determines to which set of categories the new problems belong.Results show that agents are able to learn in conditions where common supervised learning algorithms fail, such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible. The results also show that our approach provides better results than other learning methods since it requires shorter learning periods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {313–349},
numpages = {37}
}

@article{10.5555/2566972.2566980,
author = {Bachrach, Yoram and Porat, Ely and Rosenschein, Jeffrey S.},
title = {Sharing Rewards in Cooperative Connectivity Games},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {We consider how selfish agents are likely to share revenues derived from maintaining connectivity between important network servers. We model a network where a failure of one node may disrupt communication between other nodes as a cooperative game called the vertex Connectivity Game (CG). In this game, each agent owns a vertex, and controls all the edges going to and from that vertex. A coalition of agents wins if it fully connects a certain subset of vertices in the graph, called the primary vertices.Power indices measure an agent's ability to affect the outcome of the game. We show that in our domain, such indices can be used to both determine the fair share of the revenues an agent is entitled to, and identify significant possible points of failure affecting the reliability of communication in the network. We show that in general graphs, calculating the Shapley and Banzhaf power indices is #P-complete, but suggest a polynomial algorithm for calculating them in trees.We also investigate finding stable payoff divisions of the revenues in CGs, captured by the game theoretic solution of the core, and its relaxations, the ε-core and least core. We show a polynomial algorithm for computing the core of a CG, but show that testing whether an imputation is in theε-core is coNP-complete. Finally, we show that for trees, it is possible to test for ε-core imputations in polynomial time.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {281–311},
numpages = {31}
}

@article{10.5555/2566972.2566979,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {253–279},
numpages = {27}
}

@article{10.5555/2566972.2566978,
author = {Tesauro, Gerald and Gondek, David C. and Lenchner, Jonathan and Fan, James and Prager, John M.},
title = {Analysis of Watson's Strategies for Playing Jeopardy!},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Major advances in Question Answering technology were needed for IBM Watson1 to play Jeopardy! at championship level - the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates. In addition, Jeopardy! features four types of decision making carrying great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., "buzz in." Using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a player's overall chances to win, when compared with simple "rule of thumb" strategies.This article presents our approach to developing Watson's game-playing strategies, comprising development of a faithful simulation model, and then using learning and Monte-Carlo methods within the simulator to optimize Watson's strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulator's predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes. We further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {205–251},
numpages = {47}
}

@article{10.5555/2566972.2566977,
author = {Mourad, Rapha\"{e}l and Sinoquet, Christine and Zhang, Nevin L. and Liu, Tengfei and Leray, Philippe},
title = {A Survey on Latent Tree Models and Applications},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {157–203},
numpages = {47}
}

@article{10.5555/2566972.2566976,
author = {Boerkoel, James C. and Durfee, Edmund H.},
title = {Distributed Reasoning for Multiagent Simple Temporal Problems},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {This research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity interdependencies outstrip people's cognitive capacity. We address the critical challenge of reasoning over individuals' interacting schedules to efficiently answer queries about how to meet scheduling goals while respecting individual privacy and autonomy to the extent possible. We formally define the Multiagent Simple Temporal Problem for naturally capturing and reasoning over the distributed but interconnected scheduling problems of multiple individuals. Our hypothesis is that combining bottom-up and top-down approaches will lead to effective solution techniques. In our bottom-up phase, an agent externalizes constraints that compactly summarize how its local subproblem affects other agents' subproblems, whereas in our top-down phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others'. We confirm this hypothesis by devising distributed algorithms that calculate summaries of the joint solution space for multiagent scheduling problems, without centralizing or otherwise redistributing the problems. The distributed algorithms permit concurrent execution to achieve significant speedup over the current art and also increase the level of privacy and independence in individual agent reasoning. These algorithms are most advantageous for problems where interactions between the agents are sparse compared to the complexity of agents' individual problems.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {95–156},
numpages = {62}
}

@article{10.5555/2566972.2566975,
author = {Vesic, Srdjan},
title = {Identifying the Class of Maxi-Consistent Operators in Argumentation},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Dung's abstract argumentation theory can be seen as a general framework for nonmonotonic reasoning. An important question is then: what is the class of logics that can be subsumed as instantiations of this theory? The goal of this paper is to identify and study the large class of logic-based instantiations of Dung's theory which correspond to the maxi-consistent operator, i.e. to the function which returns maximal consistent subsets of an inconsistent knowledge base. In other words, we study the class of instantiations where every extension of the argumentation system corresponds to exactly one maximal consistent subset of the knowledge base. We show that an attack relation belonging to this class must be conflict-dependent, must not be valid, must not be conflict-complete, must not be symmetric etc. Then, we show that some attack relations serve as lower or upper bounds of the class (e.g. if an attack relation contains canonical undercut then it is not a member of this class). By using our results, we show for all existing attack relations whether or not they belong to this class. We also define new attack relations which are members of this class. Finally, we interpret our results and discuss more general questions, like: what is the added value of argumentation in such a setting? We believe that this work is a first step towards achieving our long-term goal, which is to better understand the role of argumentation and, particularly, the expressivity of logic-based instantiations of Dung-style argumentation frameworks.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {71–93},
numpages = {23}
}

@article{10.5555/2566972.2566974,
author = {Bajestani, Maliheh Aramon and Beck, J. Christopher},
title = {Scheduling a Dynamic Aircraft Repair Shop with Limited Repair Resources},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {We address a dynamic repair shop scheduling problem in the context of military aircraft fleet management where the goal is to maintain a full complement of aircraft over the longterm. A number of flights, each with a requirement for a specific number and type of aircraft, are already scheduled over a long horizon. We need to assign aircraft to flights and schedule repair activities while considering the flights requirements, repair capacity, and aircraft failures. The number of aircraft awaiting repair dynamically changes over time due to failures and it is therefore necessary to rebuild the repair schedule online. To solve the problem, we view the dynamic repair shop as successive static repair scheduling sub-problems over shorter time periods. We propose a complete approach based on the logic-based Benders decomposition to solve the static sub-problems, and design different rescheduling policies to schedule the dynamic repair shop. Computational experiments demonstrate that the Benders model is able to find and prove optimal solutions on average four times faster than a mixed integer programming model. The rescheduling approach having both aspects of scheduling over a longer horizon and quickly adjusting the schedule increases aircraft available in the long term by 10% compared to the approaches having either one of the aspects alone.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {35–70},
numpages = {36}
}

@article{10.5555/2566972.2566973,
author = {Wang, Guangtao and Song, Qinbao and Sun, Heli and Zhang, Xueying and Xu, Baowen and Zhou, Yuming},
title = {A Feature Subset Selection Algorithm Automatic Recommendation Method},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {Many feature subset selection (FSS) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. At the same time, so far there is rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus, FSS algorithm automatic recommendation is very important and practically useful. In this paper, a meta learning based FSS algorithm automatic recommendation method is presented. The proposed method first identifies the data sets that are most similar to the one at hand by the k-nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. Then, it ranks all the candidate FSS algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. The proposed recommendation method is extensively tested on 115 real world data sets with 22 well-known and frequently-used different FSS algorithms for five representative classifiers. The results show the effectiveness of our proposed FSS algorithm recommendation method.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–34},
numpages = {34}
}

@article{10.5555/2512538.2512555,
author = {Cai, Shaowei and Su, Kaile and Luo, Chuan and Sattar, Abdul},
title = {NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial optimization problem of great importance in both theory and application. Local search has proved successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local search algorithms. First, they select a pair of vertices to exchange simultaneously, which is timeconsuming. Secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. The strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. These two strategies are used in designing a new MVC local search algorithm, which is referred to as NuMVC.We conduct extensive experimental studies on the standard benchmarks, namely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art heuristic algorithms show that NuMVC is at least competitive with the nearest competitor namely PLS on the DIMACS benchmark, and clearly dominates all competitors on the BHOSLIB benchmark. Also, experimental results indicate that NuMVC finds an optimal solution much faster than the current best exact algorithm for Maximum Clique on random instances as well as some structured ones. Moreover, we study the effectiveness of the two strategies and the run-time behaviour through experimental analysis.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {687–716},
numpages = {30}
}

@article{10.5555/2512538.2512554,
author = {Hariri, Babak Bagheri and Calvanese, Diego and Montali, Marco and De Giacomo, Giuseppe and De Masellis, Riccardo and Felli, Paolo},
title = {Description Logic Knowledge and Action Bases},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Description logic Knowledge and Action Bases (KAB) are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing new objects. We resort to a variant of DL-Lite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred. Actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the knowledge base (TBox and ABox), and effects are expressed in terms of new ABoxes. In this setting, we address verification of temporal properties expressed in a variant of first-order µ-calculus with quantification across states. Notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {651–686},
numpages = {36}
}

@article{10.5555/2512538.2512553,
author = {Michalak, Tomasz P. and Aadithya, Karthik V. and Szczepanski, Piotr L. and Ravindran, Balaraman and Jennings, Nicholas R.},
title = {Efficient Computation of the Shapley Value for Game-Theoretic Network Centrality},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {The Shapley value--probably the most important normative payoff division scheme in coalitional games--has recently been advocated as a useful measure of centrality in networks. However, although this approach has a variety of real-world applications (including social and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. To date, the only practicable approach to compute Shapley value-based centrality has been via Monte Carlo simulations which are computationally expensive and not guaranteed to give an exact answer. Against this background, this paper presents the first study of the computational aspects of the Shapley value for network centralities. Specifically, we develop exact analytical formulae for Shapley value-based centrality in both weighted and unweighted networks and develop efficient (polynomial time) and exact algorithms based on them. We empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of the Western States Power Grid and a collaboration network from the field of astrophysics) and demonstrate that they deliver significant speedups over the Monte Carlo approach. For instance, in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the Monte Carlo approximation, even if we allow for a generous 10% error margin for the latter method.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {607–650},
numpages = {44}
}

@article{10.5555/2512538.2512552,
author = {Wolpert, David H. and Bono, James W.},
title = {Predicting Behavior in Unstructured Bargaining with a Probability Distribution},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {In experimental tests of human behavior in unstructured bargaining games, typically many joint utility outcomes are found to occur, not just one. This suggests we predict the outcome of such a game as a probability distribution. This is in contrast to what is conventionally done (e.g, in the Nash bargaining solution), which is predict a single outcome. We show how to translate Nash's bargaining axioms to provide a distribution over outcomes rather than a single outcome. We then prove that a subset of those axioms forces the distribution over utility outcomes to be a power-law distribution. Unlike Nash's original result, our result holds even if the feasible set is finite. When the feasible set is convex and comprehensive, the mode of the power law distribution is the Harsanyi bargaining solution, and if we require symmetry it is the Nash bargaining solution. However, in general these modes of the joint utility distribution are not the experimentalist's Bayesoptimal predictions for the joint utility. Nor are the bargains corresponding to the modes of those joint utility distributions the modes of the distribution over bargains in general, since more than one bargain may result in the same joint utility. After introducing distributional bargaining solution concepts, we show how an external regulator can use them to optimally design an unstructured bargaining scenario. Throughout we demonstrate our analysis in computational experiments involving flight rerouting negotiations in the National Airspace System. We emphasize that while our results are formulated for unstructured bargaining, they can also be used to make predictions for noncooperative games where the modeler knows the utility functions of the players over possible outcomes of the game, but does not know the move spaces the players use to determine those outcomes.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {579–605},
numpages = {27}
}

@article{10.5555/2512538.2512551,
author = {Ono, Masahiro and Williams, Brian C. and Blackmore, Lars},
title = {Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a model-based planner called the Probabilistic Sulu Planner or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is "go to A through B within 30 minutes, with less than 0.001% probability of failure." We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {511–577},
numpages = {67}
}

@article{10.5555/2512538.2512550,
author = {Oliehoek, Frans A. and Spaan, Matthijs T. J. and Amato, Christopher and Whiteson, Shimon},
title = {Incremental Clustering and Expansion for Faster Optimal Planning in Decentralized POMDPs},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {This article presents the state-of-the-art in optimal solution methods for decentralized partially observable Markov decision processes (Dec-POMDPs), which are general models for collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative Bayesian games (CBGs), we describe several advances that greatly expand the range of Dec-POMDPs that can be solved optimally. First, we introduce lossless incremental clustering of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing optimality. Second, we introduce incremental expansion of nodes in the GMAA* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the node's depth. This is particularly beneficial when little clustering is possible. In addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec-POMDPs. We provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally, we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {449–509},
numpages = {61}
}

@article{10.5555/2512538.2512549,
author = {Snooke, Neal and Lee, Mark},
title = {Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and Effects Analysis},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {This paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. The modelling is split into two parts. Power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system. The power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time. The primary application is Failure Modes and Effects Analysis (FMEA) and a form of exaggeration reasoning is used, combined with an order of magnitude representation to derive the worst case failure modes. The novel aspects of the work are an order of magnitude(OM) qualitative network analyser to represent any power domain and topology, including multiple power sources, a feature that was not required for earlier specialised electrical versions of the approach. Secondly, the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations. The two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the FMEA task, while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models. We have used the method to support an automated FMEA system with examples of an aircraft fuel system and domestic a heating system discussed in this paper.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {413–447},
numpages = {35}
}

@article{10.5555/2512538.2512548,
author = {Coles, Amanda and Coles, Andrew and Fox, Maria and Long, Derek},
title = {A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in Planning},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Although the use of metric uents is fundamental to many practical planning problems, the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored. The most widely used heuristic is the relaxation of metric uents into interval-valued variables|an idea first proposed a decade ago. Other heuristics depend on domain encodings that supply additional information about fluents, such as capacity constraints or other resource-related annotations.A particular challenge to these approaches is in handling interactions between metric uents that represent exchange, such as the transformation of quantities of raw materials into quantities of processed goods, or trading of money for materials. The usual relaxation of metric fluents is often very poor in these situations, since it does not recognise that resources, once spent, are no longer available to be spent again.We present a heuristic for numeric planning problems building on the propositional relaxed planning graph, but using a mathematical program for numeric reasoning. We define a class of producer-consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program (MIP). This MIP is then combined with a metric Relaxed Planning Graph (RPG) heuristic to produce an integrated hybrid heuristic. The MIP tracks resource use more accurately than the usual relaxation, but relaxes the ordering of actions, while the RPG captures the causal propositional aspects of the problem. We discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the MIP. We show that encoding a limited subset of the propositional problem to augment the MIP can yield more accurate guidance, partly by exploiting structure such as propositional landmarks and propositional resources. Our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {343–412},
numpages = {70}
}

@article{10.5555/2512538.2512547,
author = {Metodi, Amit and Codish, Michael and Stuckey, Peter J.},
title = {Boolean Equi-Propagation for Concise and Efficient SAT Encodings of Combinatorial Problems},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {We present an approach to propagation-based SAT encoding of combinatorial problems, Boolean equi-propagation, where constraints are modeled as Boolean functions which propagate information about equalities between Boolean literals. This information is then applied to simplify the CNF encoding of the constraints. A key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger, and even complete, reasoning to detect equivalent literals in that fragment. Once detected, equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments. Equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to SAT-based finite domain constraint solving. We introduce a tool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of CNF encodings and subsequent speed-ups in SAT solving times.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {303–341},
numpages = {39}
}

@article{10.5555/2512538.2512546,
author = {Ordyniak, Sebastian and Szeider, Stefan},
title = {Parameterized Complexity Results for Exact Bayesian Network Structure Learning},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Bayesian network structure learning is the notoriously difficult problem of discovering a Bayesian network that optimally represents a given set of training data. In this paper we study the computational worst-case complexity of exact Bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure. The super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks. We introduce the directed super-structure as a natural generalization of its undirected counterpart. Our results apply to several variants of score-based Bayesian network structure learning where the score of a network decomposes into local scores of its nodes.Results: We show that exact Bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth, and in linear time if in addition the super-structure has bounded maximum degree. Furthermore, we show that if the directed super-structure is acyclic, then exact Bayesian network structure learning can be carried out in quadratic time. We complement these positive results with a number of hardness results. We show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard for "almost acyclic" directed super-structures. Furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search).},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {263–302},
numpages = {40}
}

@article{10.5555/2512538.2512545,
author = {G\"{o}rnitz, Nico and Kloft, Marius and Rieck, Konrad and Brefeld, Ulf},
title = {Toward Supervised Anomaly Detection},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. Our first contribution shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. We argue that semi-supervised anomaly detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. Although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we propose an active learning strategy to automatically filter candidates for labeling. In an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {235–262},
numpages = {28}
}

@article{10.5555/2512538.2512544,
author = {Zhao, Hai and Zhang, Xiaotian and Kit, Chunyu},
title = {Integrative Semantic Dependency Parsing via Efficient Large-Scale Feature Selection},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. We leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {203–233},
numpages = {31}
}

@article{10.5555/2512538.2512543,
author = {Qazvinian, Vahed and Radev, Dragomir R. and Mohammad, Saif M. and Dorr, Bonnie and Zajic, David and Whidby, Michael and Moon, Taesun},
title = {Generating Extractive Summaries of Scientific Paradigms},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scienti fic topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {165–201},
numpages = {37}
}

@article{10.5555/2512538.2512542,
author = {Guo, Mingyu and Markakis, Evangelos and Apt, Krzysztof R. and Conitzer, Vincent},
title = {Undominated Groves Mechanisms},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {The family of Groves mechanisms, which includes the well-known VCG mechanism (also known as the Clarke mechanism), is a family of efficient and strategy-proof mechanisms. Unfortunately, the Groves mechanisms are generally not budget balanced. That is, under such mechanisms, payments may flow into or out of the system of the agents, resulting in deficits or reduced utilities for the agents. We consider the following problem: within the family of Groves mechanisms, we want to identify mechanisms that give the agents the highest utilities, under the constraint that these mechanisms must never incur deficits.We adopt a prior-free approach. We introduce two general measures for comparing mechanisms in prior-free settings. We say that a non-deficit Groves mechanism M individually dominates another non-deficit Groves mechanism M' if for every type profile, every agent's utility under M is no less than that under M', and this holds with strict inequality for at least one type profile and one agent. We say that a non-deficit Groves mechanism M collectively dominates another non-deficit Groves mechanism M' if for every type profile, the agents' total utility under M is no less than that under M', and this holds with strict inequality for at least one type profile. The above definitions induce two partial orders on non-deficit Groves mechanisms. We study the maximal elements corresponding to these two partial orders, which we call the individually undominated mechanisms and the collectively undominated mechanisms, respectively.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {129–163},
numpages = {35}
}

@article{10.5555/2512538.2512541,
author = {Sauper, Christina and Barzilay, Regina},
title = {Automatic Aggregation by Joint Modeling of Aspects and Values},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {89–127},
numpages = {39}
}

@article{10.5555/2512538.2512540,
author = {Huang, Eric and Korf, Richard E.},
title = {Optimal Rectangle Packing: An Absolute Placement Approach},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap. Our rectangle packer chooses the x-coordinates of all the rectangles before any of the y-coordinates. We then transform the problem into a perfect-packing problem with no empty space by adding additional rectangles. To determine the y-coordinates, we branch on the different rectangles that can be placed in each empty position. Our packer allows us to extend the known solutions for a consecutive-square benchmark from 27 to 32 squares. We also introduce three new benchmarks, avoiding properties that make a benchmark easy, such as rectangles with shared dimensions. Our third benchmark consists of rectangles of increasingly high precision. To pack them efficiently, we limit the rectangles' coordinates and the bounding box dimensions to the set of subset sums of the rectangles' dimensions. Overall, our algorithms represent the current state-of-the-art for this problem, outperforming other algorithms by orders of magnitude, depending on the benchmark.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {47–87},
numpages = {41}
}

@article{10.5555/2512538.2512539,
author = {Nightingale, Peter and Gent, Ian P. and Jefferson, Christopher and Miguel, Ian},
title = {Short and Long Supports for Constraint Propagation},
year = {2013},
issue_date = {January 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {46},
number = {1},
issn = {1076-9757},
abstract = {Special-purpose constraint propagation algorithms frequently make implicit use of short supports | by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of an assignment that satisfies the constraint) for all other variables and values and save substantial work - but short supports have not been studied in their own right. The two main contributions of this paper are the identification of short supports as important for constraint propagation, and the introduction of HaggisGAC, an efficient and effective general purpose propagation algorithm for exploiting short supports. Given the complexity of HAGGISGAC, we present it as an optimised version of a simpler algorithm ShortGAC. Although experiments demonstrate the efficiency of ShortGAC compared with other general-purpose propagation algorithms where a compact set of short supports is available, we show theoretically and experimentally that HaggisGAC is even better. We also find that HaggisGAC performs better than GAC-Schema on full-length supports. We also introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use. All the proposed algorithms are excellent for propagating disjunctions of constraints. In all experiments with disjunctions we found our algorithms to be faster than Constructive Or and GAC-Schema by at least an order of magnitude, and up to three orders of magnitude.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–45},
numpages = {45}
}

@article{10.5555/2444851.2444870,
author = {Costa-juss\`{a}, Marta R. and Henr\'{\i}quez Q, Carlos A. and Banchs, Rafael E.},
title = {Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Although, Chinese and Spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. This paper focuses on investigating the state-of-the-art of Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one of the most popular approaches to machine translation. For this purpose, we report details of the available parallel corpus which are Basic Traveller Expressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we conduct experimental work with the largest of these three corpora to explore alternative Smt strategies by means of using a pivot language. Three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. As pivot language, we use either English, Arabic or French. Results show that, for a phrase-based Smt system, English is the best pivot language between Chinese and Spanish. We propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. The main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {761–780},
numpages = {20}
}

@article{10.5555/2444851.2444869,
author = {Bodirsky, Manuel and Hils, Martin},
title = {Tractable Set Constraints},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Many fundamental problems in artificial intelligence, knowledge representation, and verification involve reasoning about sets and relations between sets and can be modeled as set constraint satisfaction problems (set CSPs). Such problems are frequently intractable, but there are several important set CSPs that are known to be polynomial-time tractable. We introduce a large class of set CSPs that can be solved in quadratic time. Our class, which we call EI, contains all previously known tractable set CSPs, but also some new ones that are of crucial importance for example in description logics. The class of EI set constraints has an elegant universal-algebraic characterization, which we use to show that every set constraint language that properly contains all EI set constraints already has a finite sublanguage with an NP-hard constraint satisfaction problem.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {731–759},
numpages = {29}
}

@article{10.5555/2444851.2444868,
author = {Dinh, Hang and Dinh, Hieu and Michel, Laurent and Russell, Alexander},
title = {The Time Complexity of <i>A</i>* with Approximate Heuristics on Multiple-Solution Search Spaces},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {We study the behavior of the We then experimentally explore to what extent our rigorous upper bounds predict the behavior of },
journal = {J. Artif. Int. Res.},
month = sep,
pages = {685–729},
numpages = {45}
}

@article{10.5555/2444851.2444867,
author = {Radinsky, Kira and Davidovich, Sagie},
title = {Learning to Predict from Textual Data},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns. For generalization, the model uses a vast number of world knowledge ontologies. Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {641–684},
numpages = {44}
}

@article{10.5555/2444851.2444866,
author = {de Cooman, Gert and Miranda, Enrique},
title = {Irrelevant and Independent Natural Extension for Sets of Desirable Gambles},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {The results in this paper add useful tools to the theory of sets of desirable gambles, a growing toolbox for reasoning with partial probability assessments. We investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence. We provide formulas for the smallest such joint, called their independent natural extension, and study its main properties. The independent natural extension of maximal coherent sets of desirable gambles allows us to define the strong product of sets of desirable gambles. Finally, we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence. Having such a set of tools that are easily implemented in computer programs is clearly beneficial to fields, like AI, with a clear interest in coherent reasoning under uncertainty using general and robust uncertainty models that require no full specification.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {601–640},
numpages = {40}
}

@article{10.5555/2444851.2444865,
author = {Brafman, Ronen I. and Shani, Guy},
title = {Replanning in Domains with Partial Information and Sensing Actions},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Replanning via determinization is a recent, popular approach for online planning in MDPs. In this paper we adapt this idea to classical, non-stochastic domains with partial information and sensing actions, presenting a new planner: SDR (Sample, Determinize, Replan). At each step we generate a solution plan to a classical planning problem induced by the original problem. We execute this plan as long as it is safe to do so. When this is no longer the case, we replan. The classical planning problem we generate is based on the translation-based approach for conformant planning introduced by Palacios and Geffner. The state of the classical planning problem generated in this approach captures the belief state of the agent in the original problem. Unfortunately, when this method is applied to planning problems with sensing, it yields a non-deterministic planning problem that is typically very large. Our main contribution is the introduction of state sampling techniques for overcoming these two problems. In addition, we introduce a novel, lazy, regressionbased method for querying the agent's belief state during run-time. We provide a comprehensive experimental evaluation of the planner, showing that it scales better than the state-of-the-art CLG planner on existing benchmark problems, but also highlighting its weaknesses with new domains. We also discuss its theoretical guarantees.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {565–600},
numpages = {36}
}

@article{10.5555/2444851.2444864,
author = {Garc\'{\i}a, Javier and Fern\'{a}ndez, Fernando},
title = {Safe Exploration of State and Action Spaces in Reinforcement Learning},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {515–564},
numpages = {50}
}

@article{10.5555/2444851.2444863,
author = {Endriss, Ulle and Grandi, Umberto and Porello, Daniele},
title = {Complexity of Judgment Aggregation},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {We analyse the computational complexity of three problems in judgment aggregation: (1) computing a collective judgment from a profile of individual judgments (the winner determination problem); (2) deciding whether a given agent can influence the outcome of a judgment aggregation procedure in her favour by reporting insincere judgments (the strategic manipulation problem); and (3) deciding whether a given judgment aggregation scenario is guaranteed to result in a logically consistent outcome, independently from what the judgments supplied by the individuals are (the problem of the safety of the agenda). We provide results both for specific aggregation procedures (the quota rules, the premisebased procedure, and a distance-based procedure) and for classes of aggregation procedures characterised in terms of fundamental axioms.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {481–514},
numpages = {34}
}

@article{10.5555/2444851.2444862,
author = {Ab\'{\i}o, Ignasi and Nieuwenhuis, Robert and Oliveras, Albert and Rodr\'{\i}guez-Carbonell, Enric and Mayer-Eichberger, Valentin},
title = {A New Look at BDDs for Pseudo-Boolean Constraints},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Pseudo-Boolean constraints are omnipresent in practical applications, and thus a significant effort has been devoted to the development of good SAT encoding techniques for them. Some of these encodings first construct a Binary Decision Diagram (BDD) for the constraint, and then encode the BDD into a propositional formula. These BDD-based approaches have some important advantages, such as not being dependent on the size of the coefficients, or being able to share the same BDD for representing many constraints. We first focus on the size of the resulting BDDs, which was considered to be an open problem in our research community. We report on previous work where it was proved that there are Pseudo-Boolean constraints for which no polynomial BDD exists. We also give an alternative and simpler proof assuming that NP is different from Co-NP. More interestingly, here we also show how to overcome the possible exponential blowup of BDDs by coefficient decomposition. This allows us to give the first polynomial generalized arc-consistent ROBDD-based encoding for Pseudo-Boolean constraints. Finally, we focus on practical issues: we show how to efficiently construct such ROBDDs, how to encode them into SAT with only 2 clauses per node, and present experimental results that confirm that our approach is competitive with other encodings and state-of-the-art Pseudo-Boolean solvers.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {443–480},
numpages = {38}
}

@article{10.5555/2444851.2444861,
author = {Rossi, Ryan A. and McDowell, Luke K. and Aha, David W. and Neville, Jennifer},
title = {Transforming Graph Data for Statistical Relational Learning},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Relational data representations have become an increasingly important topic due to the recent proliferation of network datasets (e.g., social, biological, information networks) and a corresponding increase in the application of Statistical Relational Learning (SRL) algorithms to these domains. In this article, we examine and categorize techniques for transforming graph-based relational data to improve SRL algorithms. In particular, appropriate transformations of the nodes, links, and/or features of the data can dramatically affect the capabilities and results of SRL algorithms. We introduce an intuitive taxonomy for data representation transformations in relational domains that incorporates link transformation and node transformation as symmetric representation tasks. More specifically, the transformation tasks for both nodes and links include (i) predicting their existence, (ii) predicting their label or type, (iii) estimating their weight or importance, and (iv) systematically constructing their relevant features. We motivate our taxonomy through detailed examples and use it to survey competing approaches for each of these tasks. We also discuss general conditions for transforming links, nodes, and features. Finally, we highlight challenges that remain to be addressed.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {363–441},
numpages = {79}
}

@article{10.5555/2444851.2444860,
author = {Rush, Alexander M. and Collins, Michael},
title = {A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {305–362},
numpages = {58}
}

@article{10.5555/2444851.2444859,
author = {Gutierrez, Patricia and Meseguer, Pedro},
title = {Removing Redundant Messages in N-Ary BnB-ADOPT},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {This note considers how to modify BnB-ADOPT, a well-known algorithm for optimally solving distributed constraint optimization problems, with a double aim: (i) to avoid sending most of the redundant messages and (ii) to handle cost functions of any arity. Some of the messages exchanged by BnB-ADOPT turned out to be redundant. Removing most of the redundant messages increases substantially communication efficiency: the number of exchanged messages is -in most cases-at least three times fewer (keeping the other measures almost unchanged), and termination and optimality are maintained. On the other hand, handling n-ary cost functions was addressed in the original work, but the presence of thresholds makes their practical usage more complex. Both issues -removing most of the redundant messages and efficiently handling n-ary cost functions- can be combined, producing the new version BnB-ADOPT+. Experimentally, we show the benefits of this version over the original one.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {287–304},
numpages = {18}
}

@article{10.5555/2444851.2444858,
author = {Hoshino, Richard and Kawarabayashi, Ken-ichi},
title = {Generating Approximate Solutions to the Traveling Tournament Problem Using a Linear Distance Relaxation},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {In some domestic professional sports leagues, the home stadiums are located in cities connected by a common train line running in one direction. For these instances, we can incorporate this geographical information to determine optimal or nearly-optimal solutions to the We introduce the Linear Distance Traveling Tournament Problem (LD-TTP), and solve it for We conclude the paper by applying this linear distance relaxation to general (non-linear) },
journal = {J. Artif. Int. Res.},
month = sep,
pages = {257–286},
numpages = {30}
}

@article{10.5555/2444851.2444857,
author = {Grau, Bernardo Cuenca and Motik, Boris},
title = {Reasoning over Ontologies with Hidden Content: The Import-by-Query Approach},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {There is currently a growing interest in techniques for hiding parts of the signature of an ontology κ },
journal = {J. Artif. Int. Res.},
month = sep,
pages = {197–255},
numpages = {59}
}

@article{10.5555/2444851.2444856,
author = {Voice, Thomas and Polukarov, Maria and Jennings, Nicholas R.},
title = {Coalition Structure Generation over Graphs},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {We give the analysis of the computational complexity of coalition structure generation over graphs. Given an undirected graph },
journal = {J. Artif. Int. Res.},
month = sep,
pages = {165–196},
numpages = {32}
}

@article{10.5555/2444851.2444855,
author = {Mirroshandel, Seyed Abolghasem and Ghassem-Sani, Gholamreza},
title = {Towards Unsupervised Learning of Temporal Relations between Events},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of "one type of temporal relation per discourse", it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {125–163},
numpages = {39}
}

@article{10.5555/2444851.2444854,
author = {Vlaeminck, Hanne and Vennekens, Joost and Denecker, Marc and Bruynooghe, Maurice},
title = {An Approximative Inference Method for Solving ∃∀SO Satisfiability Problems},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {This paper considers the fragment ∃∀SO of second-order logic. Many interesting problems, such as conformant planning, can be naturally expressed as finite domain satisfiability problems of this logic. Such satisfiability problems are computationally hard (Σ },
journal = {J. Artif. Int. Res.},
month = sep,
pages = {79–124},
numpages = {46}
}

@article{10.5555/2444851.2444853,
author = {Cohen, David A. and Cooper, Martin C. and Creed, P\'{a}id\'{\i} and Marx, D\'{a}niel and Salamon, Andr\'{a}s Z.},
title = {The Tractability of CSP Classes Defined by Forbidden Patterns},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {The constraint satisfaction problem (CSP) is a general problem central to computer science and artificial intelligence. Although the CSP is NP-hard in general, considerable effort has been spent on identifying tractable subclasses. The main two approaches consider structural properties (restrictions on the hypergraph of constraint scopes) and relational properties (restrictions on the language of constraint relations). Recently, some authors have considered hybrid properties that restrict the constraint hypergraph and the relations simultaneously.Our key contribution is the novel concept of a CSP pattern and classes of problems defined by forbidden patterns (which can be viewed as forbidding generic sub-problems). We describe the theoretical framework which can be used to reason about classes of problems defined by forbidden patterns. We show that this framework generalises certain known hybrid tractable classes.Although we are not close to obtaining a complete characterisation concerning the tractability of general forbidden patterns, we prove a dichotomy in a special case: classes of problems that arise when we can only forbid binary negative patterns (generic subproblems in which only disallowed tuples are specified). In this case we show that all (finite sets of) forbidden patterns define either polynomial-time solvable or NP-complete classes of instances.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {47–78},
numpages = {32}
}

@article{10.5555/2444851.2444852,
author = {Belardinelli, F. and Lomuscio, A.},
title = {Interactions between Knowledge and Time in a First-Order Logic for Multi-Agent Systems: Completeness Results},
year = {2012},
issue_date = {September 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {45},
number = {1},
issn = {1076-9757},
abstract = {We investigate a class of first-order temporal-epistemic logics for reasoning about multiagent systems. We encode typical properties of systems including perfect recall, synchronicity, no learning, and having a unique initial state in terms of variants of quantified interpreted systems, a first-order extension of interpreted systems. We identify several monodic fragments of first-order temporal-epistemic logic and show their completeness with respect to their corresponding classes of quantified interpreted systems.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–45},
numpages = {45}
}

@article{10.5555/2387933.2387948,
author = {Domshlak, Carmel and Karpas, Erez and Markovitch, Shaul},
title = {Online Speedup Learning for Optimal Planning},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Domain-independent planning is one of the foundational areas in the field of Artificial Intelligence. A description of a planning task consists of an initial world state, a goal, and a set of actions for modifying the world state. The objective is to find a sequence of actions, that is, a plan, that transforms the initial world state into a goal state. In optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. A prominent approach to optimal planning these days is heuristic state-space search, guided by admissible heuristic functions. Numerous admissible heuristics have been developed, each with its own strengths and weaknesses, and it is well known that there is no single "best" heuristic for optimal planning in general. Thus, which heuristic to choose for a given planning task is a difficult question. This difficulty can be avoided by combining several heuristics, but that requires computing numerous heuristic estimates at each state, and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high. We present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits. Using an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach for learning a classifier with that decision rule as the target concept, and employ the learned classifier to decide which heuristic to compute at each state. We evaluate this technique empirically, and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {709–755},
numpages = {47}
}

@article{10.5555/2387933.2387947,
author = {Konev, Boris and Ludwig, Michel and Walther, Dirk and Wolter, Frank},
title = {The Logical Difference for the Lightweight Description Logic EL},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {We study a logic-based approach to versioning of ontologies. Under this view, ontologies provide answers to queries about some vocabulary of interest. The difference between two versions of an ontology is given by the set of queries that receive difierent answers. We investigate this approach for terminologies given in the description logic EL extended with role inclusions and domain and range restrictions for three distinct types of queries: subsumption, instance, and conjunctive queries. In all three cases, we present polynomialtime algorithms that decide whether two terminologies give the same answers to queries over a given vocabulary and compute a succinct representation of the difference if it is nonempty. We present an implementation, CEX2, of the developed algorithms for subsumption and instance queries and apply it to distinct versions of Snomed CT and the NCI ontology.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {633–708},
numpages = {76}
}

@article{10.5555/2387933.2387946,
author = {Hoffmann, J\"{o}rg and Weber, Ingo and Kraft, Frank Michael},
title = {SAP Speaks PDDL: Exploiting a Software-Engineering Model for Planning in Business Process Management},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Planning is concerned with the automated solution of action sequencing problems described in declarative languages giving the action preconditions and effects. One important application area for such technology is the creation of new processes in Business Process Management (BPM), which is essential in an ever more dynamic business environment. A major obstacle for the application of Planning in this area lies in the modeling. Obtaining a suitable model to plan with - ideally a description in PDDL, the most commonly used planning language - is often prohibitively complicated and/or costly. Our core observation in this work is that this problem can be ameliorated by leveraging synergies with model-based software development. Our application at SAP, one of the leading vendors of enterprise software, demonstrates that even one-to-one model re-use is possible.The model in question is called Status and Action Management (SAM). It describes the behavior of Business Objects (BO), i.e., large-scale data structures, at a level of abstraction corresponding to the language of business experts. SAM covers more than 400 kinds of BOs, each of which is described in terms of a set of status variables and how their values are required for, and affected by, processing steps (actions) that are atomic from a business perspective. SAM was developed by SAP as part of a major model-based software engineering effort. We show herein that one can use this same model for planning, thus obtaining a BPM planning application that incurs no modeling overhead at all.We compile SAM into a variant of PDDL, and adapt an off-the-shelf planner to solve this kind of problem. Thanks to the resulting technology, business experts may create new processes simply by specifying the desired behavior in terms of status variable value changes: effectively, by describing the process in their own language.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {587–632},
numpages = {46}
}

@article{10.5555/2387933.2387945,
author = {Turney, Peter D.},
title = {Domain and Function: A Dual-Space Model of Semantic Relations and Compositions},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {533–585},
numpages = {53}
}

@article{10.5555/2387933.2387944,
author = {Huang, Jonathan and Kapoor, Ashish and Guestrin, Carlos},
title = {Riffled Independence for Efficient Inference with Partial Rankings},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity -- users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information.Simultaneously addressing all of these challenges -- i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data -- is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based representations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {491–532},
numpages = {42}
}

@article{10.5555/2387933.2387943,
author = {Cooper, Martin C. and \v{Z}ivn\'{y}, Stanislav},
title = {Tractable Triangles and Cross-Free Convexity in Discrete Optimisation},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {The minimisation problem of a sum of unary and pairwise functions of discrete variables is a general NP-hard problem with wide applications such as computing MAP configurations in Markov Random Fields (MRF), minimising Gibbs energy, or solving binary Valued Constraint Satisfaction Problems (VCSPs).We study the computational complexity of classes of discrete optimisation problems given by allowing only certain types of costs in every triangle of variable-value assignments to three distinct variables. We show that for several computational problems, the only non-trivial tractable classes are the well known maximum matching problem and the recently discovered joint-winner property. Our results, apart from giving complete classifications in the studied cases, provide guidance in the search for hybrid tractable classes; that is, classes of problems that are not captured by restrictions on the functions (such as submodularity) or the structure of the problem graph (such as bounded treewidth).Furthermore, we introduce a class of problems with convex cardinality functions on cross-free sets of assignments. We prove that while imposing only one of the two conditions renders the problem NP-hard, the conjunction of the two gives rise to a novel tractable class satisfying the cross-free convexity property, which generalises the joint-winner property to problems of unbounded arity.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {455–490},
numpages = {36}
}

@article{10.5555/2387933.2387942,
author = {Albacete, Esperanza and Calle, Javier and Castro, Elena and Cuadra, Dolores},
title = {Semantic Similarity Measures Applied to an Ontology for Human-like Interaction},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {The focus of this paper is the calculation of similarity between two concepts from an ontology for a Human-Like Interaction system. In order to facilitate this calculation, a similarity function is proposed based on five dimensions (sort, compositional, essential, restrictive and descriptive) constituting the structure of ontological knowledge. The paper includes a proposal for computing a similarity function for each dimension of knowledge. Later on, the similarity values obtained are weighted and aggregated to obtain a global similarity measure. In order to calculate those weights associated to each dimension, four training methods have been proposed. The training methods differ in the element to fit: the user, concepts or pairs of concepts, and a hybrid approach. For evaluating the proposal, the knowledge base was fed from Word Net and extended by using a knowledge editing toolkit (Cognos). The evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects, both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {397–421},
numpages = {25}
}

@article{10.5555/2387933.2387941,
author = {Haslum, Patrik},
title = {Narrative Planning: Compilations to Classical Planning},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {A model of story generation recently proposed by Riedl and Young casts it as planning, with the additional condition that story characters behave intentionally. This means that characters have perceivable motivation for the actions they take. I show that this condition can be compiled away (in more ways than one) to produce a classical planning problem that can be solved by an off-the-shelf classical planner, more efficiently than by Riedl and Young's specialised planner.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {383–395},
numpages = {13}
}

@article{10.5555/2387933.2387940,
author = {Fox, Maria and Long, Derek and Magazzeni, Daniele},
title = {Plan-Based Policies for Efficient Multiple Battery Load Management},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Efficient use of multiple batteries is a practical problem with wide and growing application. The problem can be cast as a planning problem under uncertainty. We describe the approach we have adopted to modelling and solving this problem, seen as a Markov Decision Problem, building effective policies for battery switching in the face of stochastic load profiles.Our solution exploits and adapts several existing techniques: planning for deterministic mixed discrete-continuous problems and Monte Carlo sampling for policy learning. The paper describes the development of planning techniques to allow solution of the non-linear continuous dynamic models capturing the battery behaviours. This approach depends on carefully handled discretisation of the temporal dimension. The construction of policies is performed using a classification approach and this idea offers opportunities for wider exploitation in other problems. The approach and its generality are described in the paper.Application of the approach leads to construction of policies that, in simulation, significantly outperform those that are currently in use and the best published solutions to the battery management problem. We achieve solutions that achieve more than 99% efficiency in simulation compared with the theoretical limit and do so with far fewer battery switches than existing policies. Behaviour of physical batteries does not exactly match the simulated models for many reasons, so to confirm that our theoretical results can lead to real measured improvements in performance we also conduct and report experiments using a physical test system. These results demonstrate that we can obtain 5%-15% improvement in lifetimes in the case of a two battery system.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {335–382},
numpages = {48}
}

@article{10.5555/2387933.2387939,
author = {Ghosh, Priyankar and Sharma, Amit and Chakrabarti, P. P. and Dasgupta, Pallab},
title = {Algorithms for Generating Ordered Solutions for Explicit and/or Structures},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {We present algorithms for generating alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. The proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost. In this paper, we present two versions of the search algorithm - (a) an initial version of the best first search algorithm, ASG, which may present one solution more than once while generating the ordered solutions, and (b) another version, LASG, which avoids the construction of the duplicate solutions. The actual solutions can be reconstructed quickly from the implicit compact representation used. We have applied the methods on a few test domains, some of them are synthetic while the others are based on well known problems including the search space of the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding secondary structure of RNA. Experimental results show the efficacy of the proposed algorithms over the existing approach. Our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition, where the AND/OR structure is widely used for representing problems.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {275–333},
numpages = {59}
}

@article{10.5555/2387933.2387938,
author = {Mao, Wenji and Gratch, Jonathan},
title = {Modeling Social Causality and Responsibility Judgment in Multi-Agent Interactions},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Social causality is the inference an entity makes about the social behavior of other entities and self. Besides physical cause and effect, social causality involves reasoning about epistemic states of agents and coercive circumstances. Based on such inference, responsibility judgment is the process whereby one singles out individuals to assign responsibility, credit or blame for multi-agent activities. Social causality and responsibility judgment are a key aspect of social intelligence, and a model for them facilitates the design and development of a variety of multi-agent interactive systems. Based on psychological attribution theory, this paper presents a domain-independent computational model to automate social inference and judgment process according to an agent's causal knowledge and observations of interaction. We conduct experimental studies to empirically validate the computational model. The experimental results show that our model predicts human judgments of social attributions and makes inferences consistent with what most people do in their judgments. Therefore, the proposed model can be generically incorporated into an intelligent system to augment its social and cognitive functionality.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {223–273},
numpages = {51}
}

@article{10.5555/2387933.2387937,
author = {Nakov, Preslav and Ng, Hwee Tou},
title = {Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X1 and X2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian → English using Malay and for Spanish → English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary "real" training data by a factor of 2-5.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {179–222},
numpages = {44}
}

@article{10.5555/2387933.2387936,
author = {B\"{a}ckstr\"{o}m, Christer and Jonsson, Peter},
title = {Algorithms and Limits for Compact Plan Representations},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {Compact representations of objects is a common concept in computer science. Automated planning can be viewed as a case of this concept: a planning instance is a compact implicit representation of a graph and the problem is to find a path (a plan) in this graph. While the graphs themselves are represented compactly as planning instances, the paths are usually represented explicitly as sequences of actions. Some cases are known where the plans always have compact representations, for example, using macros. We show that these results do not extend to the general case, by proving a number of bounds for compact representations of plans under various criteria, like efficient sequential or random access of actions. In addition to this, we show that our results have consequences for what can be gained from reformulating planning into some other problem. As a contrast to this we also prove a number of positive results, demonstrating restricted cases where plans do have useful compact representations, as well as proving that macro plans have favourable access properties. Our results are finally discussed in relation to other relevant contexts.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {141–177},
numpages = {37}
}

@article{10.5555/2387933.2387935,
author = {Mau\'{a}, Denis Deratani and de Campos, Cassio Polpo and Zaffalon, Marco},
title = {Solving Limited Memory Influence Diagrams},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {We present a new algorithm for exactly solving decision making problems represented as influence diagrams. We do not require the usual assumptions of no forgetting and regularity; this allows us to solve problems with simultaneous decisions and limited information. The algorithm is empirically shown to outperform a state-of-the-art algorithm on randomly generated problems of up to 150 variables and 1064 solutions. We show that these problems are NP-hard even if the underlying graph structure of the problem has low treewidth and the variables take on a bounded number of states, and that they admit no provably good approximation if variables can take on an arbitrary number of states.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {97–140},
numpages = {44}
}

@article{10.5555/2387933.2387934,
author = {Coles, Amanda and Coles, Andrew and Fox, Maria and Long, Derek},
title = {COLIN: Planning with Continuous Linear Numeric Change},
year = {2012},
issue_date = {May 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {44},
number = {1},
issn = {1076-9757},
abstract = {In this paper we describe COLIN, a forward-chaining heuristic search planner, capable of reasoning with COntinuous LINear numeric change, in addition to the full temporal semantics of PDDL2.1. Through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning. COLIN combines FF-style forward chaining search, with the use of a Linear Program (LP) to check the consistency of the interacting temporal and numeric constraints at each state. The LP is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application. In addition, we develop an extension of the Temporal Relaxed Planning Graph heuristic of CRIKEY3, to support reasoning directly with continuous change. We extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action. Finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. To support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects. We present results for COLIN that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1–96},
numpages = {96}
}

@article{10.5555/2387915.2387932,
author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
title = {Learning to Win by Reading Manuals in a Monte-Carlo Framework},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {661–704},
numpages = {44}
}

@article{10.5555/2387915.2387931,
author = {Vasirani, Matteo and Ossowski, Sascha},
title = {A Market-Inspired Approach for Intersection Management in Urban Road Traffic Networks},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Traffic congestion in urban road networks is a costly problem that affects all major cities in developed countries. To tackle this problem, it is possible (i) to act on the supply side, increasing the number of roads or lanes in a network, (ii) to reduce the demand, restricting the access to urban areas at specific hours or to specific vehicles, or (iii) to improve the efficiency of the existing network, by means of a widespread use of so-called Intelligent Transportation Systems (ITS). In line with the recent advances in smart transportation management infrastructures, ITS has turned out to be a promising field of application for artificial intelligence techniques. In particular, multiagent systems seem to be the ideal candidates for the design and implementation of ITS. In fact, drivers can be naturally modelled as autonomous agents that interact with the transportation management infrastructure, thereby generating a large-scale, open, agent-based system. To regulate such a system and maintain a smooth and efficient flow of traffic, decentralised mechanisms for the management of the transportation infrastructure are needed.In this article we propose a distributed, market-inspired, mechanism for the management of a future urban road network, where intelligent autonomous vehicles, operated by software agents on behalf of their human owners, interact with the infrastructure in order to travel safely and efficiently through the road network. Building on the reservationbased intersection control model proposed by Dresner and Stone, we consider two different scenarios: one with a single intersection and one with a network of intersections. In the former, we analyse the performance of a novel policy based on combinatorial auctions for the allocation of reservations. In the latter, we analyse the impact that a traffic assignment strategy inspired by competitive markets has on the drivers' route choices. Finally we propose an adaptive management mechanism that integrates the auction-based traffic control policy with the competitive traffic assignment strategy.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {621–659},
numpages = {39}
}

@article{10.5555/2387915.2387930,
author = {Lee, Joohyung and Palla, Ravi},
title = {Reformulating the Situation Calculus and the Event Calculus in the General Theory of Stable Models and in Answer Set Programming},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Circumscription and logic programs under the stable model semantics are two wellknown nonmonotonic formalisms. The former has served as a basis of classical logic based action formalisms, such as the situation calculus, the event calculus and temporal action logics; the latter has served as a basis of a family of action languages, such as language A and several of its descendants. Based on the discovery that circumscription and the stable model semantics coincide on a class of canonical formulas, we reformulate the situation calculus and the event calculus in the general theory of stable models. We also present a translation that turns the reformulations further into answer set programs, so that efficient answer set solvers can be applied to compute the situation calculus and the event calculus.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {571–620},
numpages = {50}
}

@article{10.5555/2387915.2387929,
author = {Hern\'{a}ndez, Carlos and Baier, Jorge A.},
title = {Avoiding and Escaping Depressions in Real-Time Heuristic Search},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Heuristics used for solving hard real-time search problems have regions with depressions. Such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. Early real-time search algorithms, like LRTA*, easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times, which results in costly solutions. State-of-the-art real-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*'s mechanism to update the heuristic, resulting in improved performance. Those algorithms, however, do not guide search towards avoiding depressed regions. This paper presents depression avoidance, a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression. We propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. We implement these strategies on top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search algorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is to find a single solution by running the real-time search algorithm once, we show that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by one order of magnitude. Of the four new algorithms, daRTAA* produces the best solutions given a fixed deadline on the average time allowed per planning episode. We prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one exists, and converge to an optimal after a number of trials.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523–570},
numpages = {48}
}

@article{10.5555/2387915.2387928,
author = {Baum, Ji\v{r}\'{\i} and Nicholson, Ann E. and Dix, Trevor I.},
title = {Proximity-Based Non-Uniform Abstractions for Approximate Planning},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {In a deterministic world, a planning agent can be certain of the consequences of its planned sequence of actions. Not so, however, in dynamic, stochastic domains where Markov decision processes are commonly used. Unfortunately these suffer from the 'curse of dimensionality': if the state space is a Cartesian product of many small sets ('dimensions'), planning is exponential in the number of those dimensions.Our new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space. The resulting non-uniformity has strong implications, since the approximation is no longer Markovian, requiring the use of a modified planner. We also use a spatial and temporal proximity measure, which responds to continued planning as well as movement of the agent through the state space, to dynamically adapt the abstraction as planning progresses.We present qualitative and quantitative results across a range of experimental domains showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space. We assess and analyse the features of domains which our method can exploit.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {477–522},
numpages = {46}
}

@article{10.5555/2387915.2387927,
author = {Grau, Bernardo Cuenca and Motik, Boris and Stoilos, Giorgos and Horrocks, Ian},
title = {Completeness Guarantees for Incomplete Ontology Reasoners: Theory and Practice},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {To achieve scalability of query answering, the developers of Semantic Web applications are often forced to use incomplete OWL 2 reasoners, which fail to derive all answers for at least one query, ontology, and data set. The lack of completeness guarantees, however, may be unacceptable for applications in areas such as health care and defence, where missing answers can adversely affect the application's functionality. Furthermore, even if an application can tolerate some level of incompleteness, it is often advantageous to estimate how many and what kind of answers are being lost.In this paper, we present a novel logic-based framework that allows one to check whether a reasoner is complete for a given query Q and ontology T -- that is, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and an arbitrary data set A. Since ontologies and typical queries are often fixed at application design time, our approach allows application developers to check whether a reasoner known to be incomplete in general is actually complete for the kinds of input relevant for the application.We also present a technique that, given a query Q, an ontology T, and reasoners R1 and R2 that satisfy certain assumptions, can be used to determine whether, for each data set A, reasoner R1 computes more answers to Q w.r.t. T and A than reasoner R2. This allows application developers to select the reasoner that provides the highest degree of completeness for Q and T that is compatible with the application's scalability requirements.Our results thus provide a theoretical and practical foundation for the design of future ontology-based information systems that maximise scalability while minimising or even eliminating incompleteness of query answers.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {419–476},
numpages = {58}
}

@article{10.5555/2387915.2387926,
author = {S\'{a}nchez-Mart\'{\i}nez, Felipe and Carrasco, Rafael C. and Mart\'{\i}nez-Prieto, Miguel A. and Adiego, Joaqu\'{\i}n},
title = {Generalized Biwords for Bitext Compression and Translation Spotting},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Large bilingual parallel texts (also known as bitexts) are usually stored in a compressed form, and previous work has shown that they can be more efficiently compressed if the fact that the two texts are mutual translations is exploited. For example, a bitext can be seen as a sequence of biwords --pairs of parallel words with a high probability of cooccurrence-- that can be used as an intermediate representation in the compression process. However, the simple biword approach described in the literature can only exploit one-to-one word alignments and cannot tackle the reordering of words. We therefore introduce a generalization of biwords which can describe multi-word expressions and reorderings. We also describe some methods for the binary compression of generalized biword sequences, and compare their performance when different schemes are applied to the extraction of the biword sequence. In addition, we show that this generalization of biwords allows for the implementation of an efficient algorithm to look on the compressed bitext for words or text segments in one of the texts and retrieve their counterpart translations in the other text --an application usually referred to as translation spotting-- with only some minor modifications in the compression algorithm.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {389–418},
numpages = {30}
}

@article{10.5555/2387915.2387925,
author = {Planken, L\'{e}on and de Weerdt, Mathijs and van der Krogt, Roman},
title = {Computing All-Pairs Shortest Paths by Leveraging Low Treewidth},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {We present two new and efficient algorithms for computing all-pairs shortest paths. The algorithms operate on directed graphs with real (possibly negative) weights. They make use of directed path consistency along a vertex ordering d. Both algorithms run in O(n2wd) time, where wd is the graph width induced by this vertex ordering. For graphs of constant treewidth, this yields O(n2) time, which is optimal. On chordal graphs, the algorithms run in O(nm) time. In addition, we present a variant that exploits graph separators to arrive at a run time of O(nw2d + n2sd) on general graphs, where sd ≤ wd is the size of the largest minimal separator induced by the vertex ordering d. We show empirically that on both constructed and realistic benchmarks, in many cases the algorithms outperform Floyd-Warshall's as well as Johnson's algorithm, which represent the current state of the art with a run time of O(n3) and O(nm + n2 log n), respectively. Our algorithms can be used for spatial and temporal reasoning, such as for the Simple Temporal Problem, which underlines their relevance to the planning and scheduling community.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {353–388},
numpages = {36}
}

@article{10.5555/2387915.2387924,
author = {Jeavons, Peter and Petke, Justyna},
title = {Local Consistency and SAT-Solvers},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Local consistency techniques such as k-consistency are a key component of specialised solvers for constraint satisfaction problems. In this paper we show that the power of using k-consistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule, which we call negative-hyper-resolution, on the standard direct encoding of the problem into Boolean clauses. We also show that current clauselearning SAT-solvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size. We combine these two results to show that, without being explicitly designed to do so, current clause-learning SAT-solvers efficiently simulate k-consistency techniques, for all fixed values of k. We then give some experimental results to show that this feature allows clause-learning SAT-solvers to efficiently solve certain families of constraint problems which are challenging for conventional constraint-programming solvers.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {329–351},
numpages = {23}
}

@article{10.5555/2387915.2387923,
author = {Huang, Ruoyun and Chen, Yixin and Zhang, Weixiong},
title = {SAS+ Planning as Satisfiability},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Planning as satisfiability is a principal approach to planning with many eminent advantages. The existing planning as satisfiability techniques usually use encodings compiled from STRIPS. We introduce a novel SAT encoding scheme (SASE) based on the SAS+ formalism. The new scheme exploits the structural information in SAS+, resulting in an encoding that is both more compact and efficient for planning. We prove the correctness of the new encoding by establishing an isomorphism between the solution plans of SASE and that of STRIPS based encodings. We further analyze the transition variables newly introduced in SASE to explain why it accommodates modern SAT solving algorithms and improves performance. We give empirical statistical results to support our analysis. We also develop a number of techniques to further reduce the encoding size of SASE, and conduct experimental studies to show the strength of each individual technique. Finally, we report extensive experimental results to demonstrate significant improvements of SASE over the state-of-the-art STRIPS based encoding schemes in terms of both time and memory efficiency.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {293–328},
numpages = {36}
}

@article{10.5555/2387915.2387922,
author = {Lee, J. H. M. and Leung, K. L.},
title = {Consistency Techniques for Flow-Based Projection-Safe Global Cost Functions in Weighted Constraint Satisfaction},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Many combinatorial problems deal with preferences and violations, the goal of which is to find solutions with the minimum cost. Weighted constraint satisfaction is a framework for modeling such problems, which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments. Typical solution methods for weighted constraint satisfaction problems (WCSPs) are based on branch-and-bound search, which are made practical through the use of powerful consistency techniques such as AC*, FDAC*, EDAC* to deduce hidden cost information and value pruning during search. These techniques, however, are designed to be efficient only on binary and ternary cost functions which are represented in table form. In tackling many real-life problems, high arity (or global) cost functions are required. We investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions, which are often derived from hard global constraints from classical constraint satisfaction.The literature suggests some global cost functions can be represented as flow networks, and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time. We show that naive adoption of this flow-based algorithmic method for global cost functions can result in a stronger form of ?-inverse consistency. We further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of AC* and FDAC* for cost functions with more than two variables. Similar generalization for the stronger EDAC* is less straightforward. We reveal the oscillation problem when enforcing EDAC* on cost functions sharing more than one variable. To avoid oscillation, we propose a weak version of EDAC* and generalize it to weak EDGAC* for non-binary cost functions. Using various benchmarks involving the soft variants of hard global constraints ALLDIFFERENT, GCC, SAME, and REGULAR, empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach, both in terms of time and pruning.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {257–292},
numpages = {36}
}

@article{10.5555/2387915.2387921,
author = {Zeng, Yifeng and Doshi, Prashant},
title = {Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {We focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others' models, which changes as the agent acts and observes in the multiagent setting.As we may expect, solving I-DIDs is computationally hard. This is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. We present two methods for reducing the size of the model space and stemming its exponential growth. Both these methods involve aggregating individual models into equivalence classes. Our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. Exactly identifying the equivalences would require us to solve all models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving I-DIDs due to the equivalences.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {211–255},
numpages = {45}
}

@article{10.5555/2387915.2387920,
author = {Pesant, Gilles and Quimper, Claude-Guy and Zanarini, Alessandro},
title = {Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our proposals. We then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. Experimental results show that counting-based search generally outperforms other generic heuristics.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {173–210},
numpages = {38}
}

@article{10.5555/2387915.2387919,
author = {Flati, Tiziano and Navigli, Roberto},
title = {The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks. However, compared to monolingual computational lexicons like WordNet, bilingual dictionaries typically provide a lower amount of structured information such as lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. In this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary. The dictionary is represented as a graph, and cyclic patterns are sought in this graph to assign an appropriate sense tag to each translation in a lexical entry. Further, we use the algorithm's output to improve the quality of the dictionary itself, by suggesting accurate solutions to structural problems such as misalignments, partial alignments and missing entries. Finally, we successfully apply CQC to the task of synonym extraction.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {135–171},
numpages = {37}
}

@article{10.5555/2387915.2387918,
author = {Sadilek, Adam and Kautz, Henry},
title = {Location-Based Reasoning about Complex Multi-Agent Behavior},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. Further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of people's actions with high accuracy. We compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {87–133},
numpages = {47}
}

@article{10.5555/2387915.2387917,
author = {Fu, Na and Lau, Hoong Chuin and Varakantham, Pradeep and Xiao, Fei},
title = {Robust Local Search for Solving RCPSP/Max with Durational Uncertainty},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {Scheduling problems in manufacturing, logistics and project management have frequently been modeled using the framework of Resource Constrained Project Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the importance of these problems, providing scalable solution schedules for RCPSP/max problems is a topic of extensive research. However, all existing methods for solving RCPSP/max assume that durations of activities are known with certainty, an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability, weather changes, etc. lead to delays or advances in completion of activities. Thus, in this paper, our focus is on providing a scalable method for solving RCPSP/max problems with durational uncertainty. To that end, we introduce the robust local search method consisting of three key ideas: (a) Introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) Deriving the expression for robust makespan of an execution strategy based on decision rule approximations; and (c) A robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty. Furthermore, we also provide enhancements to local search that exploit temporal dependencies between activities. Our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {43–86},
numpages = {44}
}

@article{10.5555/2387915.2387916,
author = {Stulp, Freek and Fedrizzi, Andreas and M\"{o}senlechner, Lorenz and Beetz, Michael},
title = {Learning and Reasoning with Action-Related Places for Robust Mobile Manipulation},
year = {2012},
issue_date = {January 2012},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {43},
number = {1},
issn = {1076-9757},
abstract = {We propose the concept of Action-Related Place (ARPLACE) as a powerful and flexible representation of task-related place in the context of mobile manipulation. ARPlace represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when located there. ARPLACES are generated using a predictive model that is acquired through experience-based learning, and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated.When executing the task, rather than choosing one specific goal position based only on the initial knowledge about the task context, the robot instantiates an ARPLACE, and bases its decisions on this ARPLACE, which is updated as new information about the task becomes available. To show the advantages of this least-commitment approach, we present a transformational planner that reasons about ARPLACE in order to optimize symbolic plans. Our empirical evaluation demonstrates that using ARPLACE leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1–42},
numpages = {42}
}

@article{10.5555/2208436.2208460,
author = {Gabbay, Dov and Pearce, David and Valverde, Agust\'{\i}n},
title = {Interpolable Formulas in Equilibrium Logic and Answer Set Programming},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Interpolation is an important property of classical and many non-classical logics that has been shown to have interesting applications in computer science and AI. Here we study the Interpolation Property for the the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation. These results also yield a form of interpolation for ground logic programs under the answer sets semantics. For disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting. The first-order version of equilibrium logic has analogous Interpolation properties whenever the collection of equilibrium models is (first-order) definable. Since this is the case for so-called safe programs and theories, it applies to the usual situations that arise in practical answer set programming.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {917–943},
numpages = {27}
}

@article{10.5555/2208436.2208459,
author = {Agmon, Noa and Kaminka, Gal A and Kraus, Sarit},
title = {Multi-Robot Adversarial Patrolling: Facing a Full-Knowledge Opponent},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {The problem of adversarial multi-robot patrol has gained interest in recent years, mainly due to its immediate relevance to various security applications. In this problem, robots are required to repeatedly visit a target area in a way that maximizes their chances of detecting an adversary trying to penetrate through the patrol path. When facing a strong adversary that knows the patrol strategy of the robots, if the robots use a deterministic patrol algorithm, then in many cases it is easy for the adversary to penetrate undetected (in fact, in some of those cases the adversary can guarantee penetration). Therefore this paper presents a non-deterministic patrol framework for the robots. Assuming that the strong adversary will take advantage of its knowledge and try to penetrate through the patrol's weakest spot, hence an optimal algorithm is one that maximizes the chances of detection in that point. We therefore present a polynomial-time algorithm for determining an optimal patrol under the Markovian strategy assumption for the robots, such that the probability of detecting the adversary in the patrol's weakest spot is maximized. We build upon this framework and describe an optimal patrol strategy for several robotic models based on their movement abilities (directed or undirected) and sensing abilities (perfect or imperfect), and in different environment models - either patrol around a perimeter (closed polygon) or an open fence (open polyline).},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {887–916},
numpages = {30}
}

@article{10.5555/2208436.2208458,
author = {Ginsberg, Matthew L.},
title = {DR.FILL: Crosswords and an Implemented Solver for Singly Weighted CSPs},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {We describe Dr. Fill, a program that solves American-style crossword puzzles. From a technical perspective, Dr. Fill works by converting crosswords to weighted csps, and then using a variety of novel techniques to find a solution. These techniques include generally applicable heuristics for variable and value selection, a variant of limited discrepancy search, and postprocessing and partitioning ideas. Branch and bound is not used, as it was incompatible with postprocessing and was determined experimentally to be of little practical value. Dr. Fill's performance on crosswords from the American Crossword Puzzle Tournament suggests that it ranks among the top fifty or so crossword solvers in the world.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {851–886},
numpages = {36}
}

@article{10.5555/2208436.2208457,
author = {Wu, Jia-Hong and Kalyanam, Rajesh and Givan, Robert},
title = {Stochastic Enforced Hill-Climbing},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Enforced hill-climbing is an effective deterministic hill-climbing technique that deals with local optima using breadth-first search (a process called "basin flooding"). We propose and evaluate a stochastic generalization of enforced hill-climbing for online use in goal-oriented probabilistic planning problems. We assume a provided heuristic function estimating expected cost to the goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice. While breadth-first search is effective in exploring basins around local optima in deterministic problems, for stochastic problems we dynamically build and solve a heuristic-based Markov decision process (MDP) model of the basin in order to find a good escape policy exiting the local optimum. We note that building this model involves integrating the heuristic into the MDP problem because the local goal is to improve the heuristic.We evaluate our proposal in twenty-four recent probabilistic planning-competition benchmark domains and twelve probabilistically interesting problems from recent literature. For evaluation, we show that stochastic enforced hill-climbing (SEH) produces better policies than greedy heuristic following for value/cost functions derived in two very different ways: one type derived by using deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of Bellman-error features from domain-specific experience. Using the first type of heuristic, SEH is shown to generally outperform all planners from the first three international probabilistic planning competitions.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {815–850},
numpages = {36}
}

@article{10.5555/2208436.2208456,
author = {Vytelingum, Perukrishnen and Voice, Thomas D. and Ramchurn, Sarvapali D. and Rogers, Alex and Jennings, Nicholas R.},
title = {Theoretical and Practical Foundations of Large-Scale Agent-Based Micro-Storage in the Smart Grid},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {In this paper, we present a novel decentralised management technique that allows electricity micro-storage devices, deployed within individual homes as part of a smart electricity grid, to converge to profitable and efficient behaviours. Specifically, we propose the use of software agents, residing on the users' smart meters, to automate and optimise the charging cycle of micro-storage devices in the home to minimise its costs, and we present a study of both the theoretical underpinnings and the implications of a practical solution, of using software agents for such micro-storage management. First, by formalising the strategic choice each agent makes in deciding when to charge its battery, we develop a game-theoretic framework within which we can analyse the competitive equilibria of an electricity grid populated by such agents and hence predict the best consumption profile for that population given their battery properties and individual load profiles. Our framework also allows us to compute theoretical bounds on the amount of storage that will be adopted by the population. Second, to analyse the practical implications of micro-storage deployments in the grid, we present a novel algorithm that each agent can use to optimise its battery storage profile in order to minimise its owner's costs. This algorithm uses a learning strategy that allows it to adapt as the price of electricity changes in real-time, and we show that the adoption of these strategies results in the system converging to the theoretical equilibria. Finally, we empirically evaluate the adoption of our micro-storage management technique within a complex setting, based on the UK electricity market, where agents may have widely varying load profiles, battery types, and learning rates. In this case, our approach yields savings of up to 14% in energy cost for an average consumer using a storage device with a capacity of less than 4.5 kWh and up to a 7% reduction in carbon emissions resulting from electricity generation (with only domestic consumers adopting micro-storage and, commercial and industrial consumers not changing their demand). Moreover, corroborating our theoretical bound, an equilibrium is shown to exist where no more than 48% of households would wish to own storage devices and where social welfare would also be improved (yielding overall annual savings of nearly £1.5B).},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {765–813},
numpages = {49}
}

@article{10.5555/2208436.2208455,
author = {Bonatti, Piero A. and Faella, Marco and Sauro, Luigi},
title = {Defeasible Inclusions in Low-Complexity DLs},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Some of the applications of OWL and RDF (e.g. biomedical knowledge representation and semantic policy formulation) call for extensions of these languages with nonmonotonic constructs such as inheritance with overriding. Nonmonotonic description logics have been studied for many years, however no practical such knowledge representation languages exist, due to a combination of semantic difficulties and high computational complexity. Independently, low-complexity description logics such as DL-lite and EL have been introduced and incorporated in the OWL standard. Therefore, it is interesting to see whether the syntactic restrictions characterizing DL-lite and EL bring computational benefits to their nonmonotonic versions, too. In this paper we extensively investigate the computational complexity of Circumscription when knowledge bases are formulated in DL-liteR, EL, and fragments thereof. We identify fragments whose complexity ranges from P to the second level of the polynomial hierarchy, as well as fragments whose complexity raises to PSPACE and beyond.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {719–764},
numpages = {46}
}

@article{10.5555/2208436.2208454,
author = {Amig\'{o}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
title = {Combining Evaluation Metrics via the Unanimous Improvement Ratio and Its Application to Clustering Tasks},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted.Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {689–718},
numpages = {30}
}

@article{10.5555/2208436.2208453,
author = {Pe\~{n}a, Jose M.},
title = {Finding Consensus Bayesian Network Structures},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Suppose that multiple experts (or learning algorithms) provide us with alternative Bayesian network (BN) structures over a domain, and that we are interested in combining them into a single consensus BN structure. Specifically, we are interested in that the consensus BN structure only represents independences all the given BN structures agree upon and that it has as few parameters associated as possible. In this paper, we prove that there may exist several non-equivalent consensus BN structures and that finding one of them is NP-hard. Thus, we decide to resort to heuristics to find an approximated consensus BN structure. In this paper, we consider the heuristic proposed by Matzkevich and Abramson, which builds upon two algorithms, called Methods A and B, for efficiently deriving the minimal directed independence map of a BN structure relative to a given node ordering. Methods A and B are claimed to be correct although no proof is provided (a proof is just sketched). In this paper, we show that Methods A and B are not correct and propose a correction of them.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {661–687},
numpages = {27}
}

@article{10.5555/2208436.2208452,
author = {Conrad, Patrick R. and Williams, Brian C.},
title = {Drake: An Efficient Executive for Temporal Plans with Choice},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {This work presents Drake, a dynamic executive for temporal plans with choice. Dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events, improving the robustness of the agent. Prior work developed methods for dynamically dispatching Simple Temporal Networks, and further research enriched the expressiveness of the plans executives could handle, including discrete choices, which are the focus of this work. However, in some approaches to date, these additional choices induce significant storage or latency requirements to make flexible execution possible.Drake is designed to leverage the low latency made possible by a preprocessing step called compilation, while avoiding high memory costs through a compact representation. We leverage the concepts of labels and environments, taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to concisely record the implications of the discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance System, is distinguished by its focus on properties fundamental to temporal problems, and, more generally, weighted graph algorithms. In particular, the maintenance system focuses on maintaining a minimal representation of non-dominated constraints. We benchmark Drake's performance on random structured problems, and find that Drake reduces the size of the compiled representation by a factor of over 500 for large problems, while incurring only a modest increase in run-time latency, compared to prior work in compiled executives for temporal plans with discrete choices.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {607–659},
numpages = {53}
}

@article{10.5555/2208436.2208451,
author = {Ponsen, Marc and de Jong, Steven and Lanctot, Marc},
title = {Computing Approximate Nash Equilibria and Robust Best-Responses Using Sampling},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {This article discusses two contributions to decision-making in complex partially observable stochastic games. First, we apply two state-of-the-art search techniques that use Monte-Carlo sampling to the task of approximating a Nash-Equilibrium (NE) in such games, namely Monte-Carlo Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR). MCTS has been proven to approximate a NE in perfect-information games. We show that the algorithm quickly finds a reasonably strong strategy (but not a NE) in a complex imperfect information game, i.e. Poker. MCCFR on the other hand has theoretical NE convergence guarantees in such a game. We apply MCCFR for the first time in Poker. Based on our experiments, we may conclude that MCTS is a valid approach if one wants to learn reasonably strong strategies fast, whereas MCCFR is the better choice if the quality of the strategy is most important.Our second contribution relates to the observation that a NE is not a best response against players that are not playing a NE. We present Monte-Carlo Restricted Nash Response (MCRNR), a sample-based algorithm for the computation of restricted Nash strategies. These are robust bestresponse strategies that (1) exploit non-NE opponents more than playing a NE and (2) are not (overly) exploitable by other strategies. We combine the advantages of two state-of-the-art algorithms, i.e. MCCFR and Restricted Nash Response (RNR). MCRNR samples only relevant parts of the game tree. We show that MCRNR learns quicker than standard RNR in smaller games. Also we show in Poker that MCRNR learns robust best-response strategies fast, and that these strategies exploit opponents more than playing a NE does.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {575–605},
numpages = {31}
}

@article{10.5555/2208436.2208450,
author = {Elkind, Edith and Faliszewski, Piotr and Slinko, Arkadii},
title = {Cloning in Elections: Finding the Possible Winners},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {We consider the problem of manipulating elections by cloning candidates. In our model, a manipulator can replace each candidate c by several clones, i.e., new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates, ranked consecutively. The outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block. We formalize what it means for a cloning manipulation to be successful (which turns out to be a surprisingly delicate issue), and, for a number of common voting rules, characterize the preference profiles for which a successful cloning manipulation exists. We also consider the model where there is a cost associated with producing each clone, and study the complexity of finding a minimum-cost cloning manipulation. Finally, we compare cloning with two related problems: the problem of control by adding candidates and the problem of possible (co)winners when new alternatives can join.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {529–573},
numpages = {45}
}

@article{10.5555/2208436.2208449,
author = {Alviano, Mario and Calimeri, Francesco and Faber, Wolfgang and Leone, Nicola and Perri, Simona},
title = {Unfounded Sets and Well-Founded Semantics of Answer Set Programs with Aggregates},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Logic programs with aggregates (LPA) are one of the major linguistic extensions to Logic Programming (LP). In this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates (LPm,aA programs). In particular, we present a new notion of unfounded set for LPm,aA programs, which is a sound generalization of the original definition for standard (aggregate-free) LP. On this basis, we define a well-founded operator for LPm,aA programs, the fixpoint of which is called well-founded model (or well-founded semantics) for LPm,aA programs. The most important properties of unfounded sets and the well-founded semantics for standard LP are retained by this generalization, notably existence and uniqueness of the well-founded model, together with a strong relationship to the answer set semantics for LPm,aA programs. We show that one of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for a broader class of aggregates using approximating operators, coincides with the well-founded model as defined in this work on LPm,aA programs. We also discuss some complexity issues, most importantly we give a formal proof of tractable computation of the well-founded model for LPm,aA programs. Moreover, we prove that for general LPA programs, which may contain aggregates that are neither monotone nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial interpretations is coNP-complete. As a consequence, a well-founded semantics for general LPA programs that allows for tractable computation is unlikely to exist, which justifies the restriction on LPm,aA programs. Finally, we present a prototype system extending DLV, which supports the well-founded semantics for LPm,aA programs, at the time of writing the only implemented system that does so. Experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {487–527},
numpages = {41}
}

@article{10.5555/2208436.2208448,
author = {Golovin, Daniel and Krause, Andreas},
title = {Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse AI applications including management of sensing resources, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {427–486},
numpages = {60}
}

@article{10.5555/2208436.2208447,
author = {Gr\"{u}nwald, Peter D. and Halpern, Joseph Y.},
title = {Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {We consider how an agent should update her beliefs when her beliefs are represented by a set P of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. Our results emphasize the key role of the rectangularity condition of Epstein and Schneider.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {393–426},
numpages = {34}
}

@article{10.5555/2208436.2208446,
author = {Talvitie, Erik and Singh, Satinder},
title = {Learning to Make Predictions in Partially Observable Environments without a Generative Model},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {When faced with the problem of learning a model of a high-dimensional environment, a common approach is to limit the model to make only a restricted set of predictions, thereby simplifying the learning problem. These partial models may be directly useful for making decisions or may be combined together to form a more complete, structured model. However, in partially observable (non-Markov) environments, standard model-learning methods learn generative models, i.e. models that provide a probability distribution over all possible futures (such as POMDPs). It is not straightforward to restrict such models to make only certain predictions, and doing so does not always simplify the learning problem. In this paper we present prediction profile models: non-generative partial models for partially observable systems that make only a given set of predictions, and are therefore far simpler than generative models in some cases. We formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {353–392},
numpages = {40}
}

@article{10.5555/2208436.2208445,
author = {Yuan, Changhe and Lim, Heejin and Lu, Tsai-Ching},
title = {Most Relevant Explanation in Bayesian Networks},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. Existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). In this paper, we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the generalized Bayes factor (GBF) as the best explanation for the given evidence. Our study shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant target variables in forming its explanation. In particular, conditional Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing explanation, provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. As a result, MRE is able to automatically prune less relevant variables from its explanation. We also show that CBF is able to capture well the explaining-away phenomenon that is often represented in Bayesian networks. Moreover, we define two dominance relations between the candidate solutions and use the relations to generalize MRE to find a set of top explanations that is both diverse and representative. Case studies on several benchmark diagnostic Bayesian networks show that MRE is often able to find explanatory hypotheses that are not only precise but also concise.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {309–352},
numpages = {44}
}

@article{10.5555/2208436.2208444,
author = {Ribeiro, Ricardo and de Matos, David Martins},
title = {Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domainindependent. Thorough automatic evaluation shows that the method achieves state-of-the art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {275–308},
numpages = {34}
}

@article{10.5555/2208436.2208443,
author = {Santhanam, Ganesh Ram and Basu, Samik and Honavar, Vasant},
title = {Representing and Reasoning with Qualitative Preferences for Compositional Systems},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Many applications, e.g., Web service composition, complex system design, team formation, etc., rely on methods for identifying collections of objects or entities satisfying some functional requirement. Among the collections that satisfy the functional requirement, it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the non-functional properties of the collection.We develop a formalism that lets users express the relative importance among attributes and qualitative preferences over the valuations of each attribute. We define a dominance relation that allows us to compare collections of objects in terms of preferences over attributes of the objects that make up the collection. We establish some key properties of the dominance relation. In particular, we show that the dominance relation is a strict partial order when the intra-attribute preference relations are strict partial orders and the relative importance preference relation is an interval order.We provide algorithms that use this dominance relation to identify the set of most preferred collections. We show that under certain conditions, the algorithms are guaranteed to return only (sound), all (complete), or at least one (weakly complete) of the most preferred collections. We present results of simulation experiments comparing the proposed algorithms with respect to (a) the quality of solutions (number of most preferred solutions) produced by the algorithms, and (b) their performance and efficiency. We also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences, the dominance relation, and the algorithms.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {211–274},
numpages = {64}
}

@article{10.5555/2208436.2208442,
author = {Dai, Peng and Mausam and Weld, Daniel S. and Goldsmith, Judy},
title = {Topological Value Iteration Algorithms},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods use reachability analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm, topological value iteration (TVI), detects the structure of MDPs and backs up states based on topological sequences. It (1) divides an MDP into strongly-connected components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm, focused topological value iteration (FTVI), is an extension of TVI. FTVI restricts its attention to connected components that are relevant for solving the MDP. Specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows FTVI to find smaller connected components, thus running faster. We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI also significantly outperforms popular 'heuristically-informed' MDP algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of magnitude. Finally, we characterize the type of domains where FTVI excels -- suggesting a way to an informed choice of solver.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {181–209},
numpages = {29}
}

@article{10.5555/2208436.2208441,
author = {Lee, Joohyung and Meng, Yunsong},
title = {First-Order Stable Model Semantics and First-Order Loop Formulas},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Lin and Zhao's theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas, but this result does not fully carry over to the first-order case. We investigate the precise relationship between the first-order stable model semantics and first-order loop formulas, and study conditions under which the former can be represented by the latter. In order to facilitate the comparison, we extend the definition of a first-order loop formula which was limited to a nondisjunctive program, to a disjunctive program and to an arbitrary first-order theory. Based on the studied relationship we extend the syntax of a logic program with explicit quantifiers, which allows us to do reasoning involving non-Herbrand stable models using first-order reasoners. Such programs can be viewed as a special class of first-order theories under the stable model semantics, which yields more succinct loop formulas than the general language due to their restricted syntax.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {125–180},
numpages = {56}
}

@article{10.5555/2208436.2208440,
author = {Hoshino, Richard and Kawarabayashi, Ken-ichi},
title = {Scheduling Bipartite Tournaments to Minimize Total Travel Distance},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {In many professional sports leagues, teams from opposing leagues/conferences compete against one another, playing inter-league games. This is an example of a bipartite tournament. In this paper, we consider the problem of reducing the total travel distance of bipartite tournaments, by analyzing inter-league scheduling from the perspective of discrete optimization. This research has natural applications to sports scheduling, especially for leagues such as the National Basketball Association (NBA) where teams must travel long distances across North America to play all their games, thus consuming much time, money, and greenhouse gas emissions.We introduce the Bipartite Traveling Tournament Problem (BTTP), the interleague variant of the well-studied Traveling Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for small values of n, a distance-optimal inter-league schedule can be generated from an algorithm based on minimum-weight 4-cycle-covers. We apply our theoretical results to the 12-team Nippon Professional Baseball (NPB) league in Japan, producing a provably-optimal schedule requiring 42950 kilometres of total team travel, a 16% reduction compared to the actual distance traveled by these teams during the 2010 NPB season. We also develop a nearly-optimal inter-league tournament for the 30-team NBA league, just 3.8% higher than the trivial theoretical lower bound.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {91–124},
numpages = {34}
}

@article{10.5555/2208436.2208439,
author = {Wang, Ko-Hsin Cindy and Botea, Adi},
title = {MAPP: A Scalable Multi-Agent Path Planning Algorithm with Tractability and Completeness Guarantees},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Multi-agent path planning is a challenging problem with numerous real-life applications. Running a centralized search such as A* in the combined state space of all units is complete and cost-optimal, but scales poorly, as the state space size is exponential in the number of mobile units. Traditional decentralized approaches, such as FAR and WHCA*, are faster and more scalable, being based on problem decomposition. However, such methods are incomplete and provide no guarantees with respect to the running time or the solution quality. They are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance.We introduce MAPP, a tractable algorithm for multi-agent path planning on undirected graphs. We present a basic version and several extensions. They have low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions. Even though all algorithmic versions are incomplete in the general case, each provides formal guarantees on problems it can solve. For each version, we discuss the algorithm's completeness with respect to clearly defined subclasses of instances.Experiments were run on realistic game grid maps. MAPP solved 99.86% of all mobile units, which is 18-22% better than the percentage of FAR and WHCA*. MAPP marked 98.82% of all units as provably solvable during the first stage of plan computation. Parts of MAPP's computation can be re-used across instances on the same map. Speed-wise, MAPP is competitive or significantly faster than WHCA*, depending on whether MAPP performs all computations from scratch. When data that MAPP can re-use are preprocessed offline and readily available, MAPP is slower than the very fast FAR algorithm by a factor of 2.18 on average. MAPP's solutions are on average 20% longer than FAR's solutions and 7-31% longer than WHCA*'s solutions.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {55–90},
numpages = {36}
}

@article{10.5555/2208436.2208438,
author = {Booth, Richard and Meyer, Thomas and Varzinczak, Ivan and Wassermann, Renata},
title = {On the Link between Partial Meet, Kernel, and Infra Contraction and Its Application to Horn Logic},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Standard belief change assumes an underlying logic containing full classical propositional logic. However, there are good reasons for considering belief change in less expressive logics as well. In this paper we build on recent investigations by Delgrande on contraction for Horn logic. We show that the standard basic form of contraction, partial meet, is too strong in the Horn case. This result stands in contrast to Delgrande's conjecture that orderly maxichoice is the appropriate form of contraction for Horn logic. We then define a more appropriate notion of basic contraction for the Horn case, influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction. The main contribution of this work is a result which shows that the construction method for Horn contraction for belief sets based on our infra remainder sets corresponds exactly to Hansson's classical kernel contraction for belief sets, when restricted to Horn logic. This result is obtained via a detour through contraction for belief bases. We prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction. The use of belief bases to obtain this result provides evidence for the conjecture that Horn belief change is best viewed as a `hybrid' version of belief set change and belief base change. One of the consequences of the link with base contraction is the provision of a representation result for Horn contraction for belief sets in which a version of the Core-retainment postulate features.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {31–53},
numpages = {23}
}

@article{10.5555/2208436.2208437,
author = {Walsh, Toby},
title = {Where Are the Hard Manipulation Problems?},
year = {2011},
issue_date = {September 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {42},
number = {1},
issn = {1076-9757},
abstract = {Voting is a simple mechanism to combine together the preferences of multiple agents. Unfortunately, agents may try to manipulate the result by mis-reporting their preferences. One barrier that might exist to such manipulation is computational complexity. In particular, it has been shown that it is NP-hard to compute how to manipulate a number of different voting rules. However, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that manipulation may often be easy in practice. In this paper, we show that empirical studies are useful in improving our understanding of this issue. We consider two settings which represent the two types of complexity results that have been identified in this area: manipulation with unweighted votes by a single agent, and manipulation with weighted votes by a coalition of agents. In the first case, we consider Single Transferable Voting (STV), and in the second case, we consider veto voting. STV is one of the few voting rules used in practice where it is NP-hard to compute how a single agent can manipulate the result when votes are unweighted. It also appears one of the harder voting rules to manipulate since it involves multiple rounds. On the other hand, veto voting is one of the simplest representatives of voting rules where it is NP-hard to compute how a coalition of weighted agents can manipulate the result. In our experiments, we sample a number of distributions of votes including uniform, correlated and real world elections. In many of the elections in our experiments, it was easy to compute how to manipulate the result or to prove that manipulation was impossible. Even when we were able to identify a situation in which manipulation was hard to compute (e.g. when votes are highly correlated and the election is "hung"), we found that the computational difficulty of computing manipulations was somewhat precarious (e.g. with such "hung" elections, even a single uncorrelated voter was enough to make manipulation easy to compute).},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1–29},
numpages = {29}
}

@article{10.5555/2051237.2051253,
author = {Gra\c{c}a, Jo\~{a}o V. and Ganchev, Kuzman and Coheur, Lu\'{\i}sa and Pereira, Fernando and Taskar, Ben},
title = {Controlling Complexity in Part-of-Speech Induction},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {527–551},
numpages = {25}
}

@article{10.5555/2051237.2051252,
author = {Sturm, J\"{u}rgen and Stachniss, Cyrill and Burgard, Wolfram},
title = {A Probabilistic Framework for Learning Kinematic Models of Articulated Objects},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Robots operating in domestic environments generally need to interact with articulated objects, such as doors, cabinets, dishwashers or fridges. In this work, we present a novel, probabilistic framework for modeling articulated objects as kinematic graphs. Vertices in this graph correspond to object parts, while edges between them model their kinematic relationship. In particular, we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations. We furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks. We finally present how the learned models can be generalized to new and previously unseen objects. In various experiments using real robots with different camera systems as well as in simulation, we show that our approach is valid, accurate and efficient. Further, we demonstrate that our approach has a broad set of applications, in particular for the emerging fields of mobile manipulation and service robotics.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {477–526},
numpages = {50}
}

@article{10.5555/2051237.2051251,
author = {Dvo\v{r}\'{a}k, Wolfgang and Woltran, Stefan},
title = {On the Intertranslatability of Argumentation Semantics},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Translations between different nonmonotonic formalisms always have been an important topic in the field, in particular to understand the knowledge-representation capabilities those formalisms offer. We provide such an investigation in terms of different semantics proposed for abstract argumentation frameworks, a nonmonotonic yet simple formalism which received increasing interest within the last decade. Although the properties of these different semantics are nowadays well understood, there are no explicit results about intertranslatability. We provide such translations wrt. different properties and also give a few novel complexity results which underlie some negative results.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {445–475},
numpages = {31}
}

@article{10.5555/2051237.2051250,
author = {Gy\"{o}rgy, Andr\'{a}s and Kocsis, Levente},
title = {Efficient Multi-Start Strategies for Local Search Algorithms},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {Local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum. The common solution for this deficiency is to restart the algorithm when no progress is observed. Alternatively, one can start multiple instances of a local search algorithm, and allocate computational resources (in particular, processing time) to the instances depending on their behavior. Hence, a multi-start strategy has to decide (dynamically) when to allocate additional resources to a particular instance and when to start new instances. In this paper we propose multi-start strategies motivated by works on multi-armed bandit problems and Lipschitz optimization with an unknown constant. The strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant, and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant. Asymptotic bounds are given on the performance of the strategies. In particular, we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum. Experiments are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and kmeans as local search algorithms, and the results indicate that the proposed strategies work well in practice, and, in all cases studied, need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {407–444},
numpages = {38}
}

@article{10.5555/2051237.2051249,
author = {Lu, Xiaosong and Schwartz, Howard M. and Givigi, Sidney N.},
title = {Policy Invariance under Reward Transformations for General-Sum Stochastic Games},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We extend the potential-based shapingmethod fromMarkov decision processes to multiplayer general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. The property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {397–406},
numpages = {10}
}

