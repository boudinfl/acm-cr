@article{10.1145/3398191,
author = {Wang, Kun and Cui, Yanpeng and Hu, Jianwei and Zhao, Wei and Feng, Luming and Zhang, Yu},
title = {Cyberbullying Detection, Based on the FastText and Word Similarity Schemes},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3398191},
doi = {10.1145/3398191},
abstract = {With the ever-increasing population using online social networks (OSN) in their daily lives, cyberbullying behaviors have also been attracting more attention. Studies show that the cyberbullying adversely affects the mental health. This is especially more pronounced for teenagers. In order to reduce or even stop the cyberbullying, it is of significant importance in the natural language processing and machine learning communities to find an effective detection scheme. However, the existing detection methods are not well-developed for detecting the cyberbullying. In the present study, a new detection algorithm, based on the FastText and word similarity schemes is introduced. Moreover, characteristics of the cyberbullying from both lexical and syntactic aspects are studied. In fact, these aspects are utilized to detect the cyberbullying in social networks. Finally, the application of this method is studied in an example. It is concluded that the proposed algorithm can effectively improve the detection accuracy and recall rate of the cyberbullying detection.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3392048,
author = {Muthu, BalaAnand and CB, Sivaparthipan and Kumar, Priyan Malarvizhi and Kadry, Seifedine Nimer and Hsu, Ching-Hsien and Sanjuan, Oscar and Crespo, Ruben Gonzalez},
title = {A Framework for Extractive Text Summarization Based on Deep Learning Modified Neural Network Classifier},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3392048},
doi = {10.1145/3392048},
abstract = {On account of the exponential augmentation of documents on the internet, users need all the pertinent data at ?1? place with no hassle. Therefore, automatic text summarization (ATS) is needed to automate the procedure of summarizing text via extorting the salient details as of the documents. The goal is to propose an automatic, generic, in addition to extractive text summarization for a single document utilizing Deep Learning Modifier Neural Network (DLMNN) classifier for generating an adequately informative summary centered upon the entropy values. A proposed DLMNN framework comprises ?6? phases. In the initial phase, the input document is pre-processed which engages stop word removal, tokenization, along with stemming. Subsequently, the features are extorted as of the pre-processed data. Next, the most apposite features are selected employing the improved fruit fly optimization algorithm (IFFOA). The entropy value for every chosen feature is computed utilizing support as well as confident measure. Afterward, DLMNN classifier is utilized to classify these values into ?2? classes, a) highest entropy values and b) lowest entropy values. Lastly, the class that holds the highest entropy values are chosen besides, the informative sentences are selected as of the highest entropy values to form the last summary. Experimental outcomes are executed and the proposed DLMNN classifier?s performance is analyzed utilizing sensitivity, accuracy, recall, specificity, precision, and also f-measure. The proposed DLMNN provides the best outcomes amid all other techniques.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3398011,
author = {Sun, Jie and Wang, Ailing and Li, Leiming},
title = {An Analysis for Elements of Affecting the Establishment and Promotion of Micro-Business Trust in C2C Model under WeChat Circumstance},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3398011},
doi = {10.1145/3398011},
abstract = {The core of micro-business and consumer transactions is trust. Based on the Theory of Reasoned Action and Technology Acceptance Model, this paper discusses the factors of the establishment and promotion of micro-business trust from the trust orientation of consumer, the trust of WeChat businesses, and the trust of WeChat platform. Data were obtained by questionnaire, and SPSS software was used for data reliability and multiple regression analysis. It is concluded that all three levels have a significant positive impact on the establishment and promotion of C2C mode micro-business trust. The trust of WeChat businesses and the trust of WeChat platform have a greater influence on the establishment of micro-business trust.The trust orientation of consumer and the trust of WeChat businesses have a greater impact on the promotion of micro-business trust. Among them, the WeChat business trust level is the most important factor.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3394115,
author = {Liu, Bin and Lu, Yao},
title = {ROLE OF ADVANCED WEB BASED CONTENT MANAGEMENT SYSTEM AND ITS SIGNIFICANCE IN LIBRARIES MANAGEMENT SYSTEM},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3394115},
doi = {10.1145/3394115},
abstract = {Libraries are vaults of learning and the enormous development in computerized assets has constrained library experts to utilize different data innovation tools to oversee and render management to the clients in Chinese education and research sector. To accomplish more noteworthy proficiency in the quickly evolving economic condition, libraries are progressively searching for new standards to convey management to clients in their work areas to innovate their knowledge. Library gateways assume a significant job by making a web domain where clients can without much of a stretch access data. It offers benefits as well as improving insightful correspondence and research among the supporters of the library. This paper discuses the entry of Advanced Web based Content Management System (AW-CMS) using linked list based iterations for database management has been made here to clarify the different supervisionclients, so it mayimprove the successful data searchers and assess accurate data proficiently and reliability in an effective and efficient manner. It likewise features, how it attempts to meet client desires for data on interest in library management and its experimental validation in accordance with existing software systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3389010,
author = {List, Johann-Mattis and Sims, Nathaniel},
title = {Towards a Sustainable Handling of Inter-Linear-Glossed Text in Language Documentation},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389010},
doi = {10.1145/3389010},
abstract = {Efforts on language documentation have been increasing in the past. While the amount of digital data of the world's languages is increasing, only a small amount of the data is sustainable, since data reuse is often exacerbated by idiosyncratic formats and a negligence of standards that could help to increase the comparability of linguistic data. The sustainability problem is nicely reflected in the current practice of handling inter-linear-glossed text, one of the crucial resources produced in language documentation. Although large collections of glossed texts have been produced so far, the current practice of data handling greatly exacerbates the reuse of data. In order to address this problem, we propose a first framework for the computer-assisted, sustainable handling of inter-linear-glossed text resources. Building on recent standardization proposals for word lists and structural datasets, combined with state-of-the-art methods for automated sequence comparison in historical linguistics, we show how our workflow can be used to lift a collection of inter-linear-glossed Qiang texts (an endangered language spoken in Sichuan, China), and how the lifted data can assist linguists in their research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3394114,
author = {Shang, Rui and Li, Xia},
title = {Improved Heuristic Data Management and Protection Algorithm for Digital China Cultural Datasets},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3394114},
doi = {10.1145/3394114},
abstract = {In the present scenario sustainable management and protection of digital cultural data sets are considered as a significant area of research. In the recent past the protection and management of cultural data is facing several new challenged and opportunities. Though several researchers explored their work on managing and protecting cultural data, efficiently and reliability of the present data management algorithm seems to be more complicated due to its incompetence in managing data in optimized manner. This work presents an improved heuristic big data management algorithm for cultural datasets which is considered as a new discipline of digital cultural heritage specially established for strengthening strategic and interdisciplinary research. The scientific operation and management mechanism of digital protection of cultural heritage is experimentally validated and results shows promising outcomes.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3396167,
author = {Subburaman, Dhivya and Gandhi, Usha Devi},
title = {Study on Automated Approach to Generate Character Recognition for Handwritten and Historical Documents},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3396167},
doi = {10.1145/3396167},
abstract = {Script Recognition is the mechanism of automatic script analysis and recognition whereby intensive study has been carried out and a significant amount of papers on this problem have been released over the past. But there are still a few issues to be solved, particularly in Indian historical manuscripts. This literature examines the Script recognition with reference to multi-script document and different historical scripts such as Kurdish-Latin, Devanagari, Grantha, Arabic handwritten characters, Bangladesh, Devanagari and Gurumukhi, Ancient Chinese, Arabic, Nam Character, Greek, Nastalique Urdu, Georgian handwritten, Nandinagari, Hebrew which provide the course of study that focuses on the framework for script recogonition. And this review concentrates on scope of prediction, data set type, the methods used for data preprocessing, and measures of performance used for analysis. On the basis of this survey, Current research constraints have been recognized and future study specifications are emphasized in the area of modeling historical manuscripts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3392047,
author = {Krishna, Ravi and Mu, Norman and Keutzer, Kurt W},
title = {Applying Text Analytics to the Mind-Section Literature of the Tibetan Tradition of the Great Perfection},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3392047},
doi = {10.1145/3392047},
abstract = {Over the last decade, through a mixture of optical character recognition and manual input, there is now a growing corpus of Tibetan literature available as e-texts in Unicode format. With the creation of such a corpus, the techniques of text analytics that have been applied in the analysis of English and other modern languages may now be applied to Tibetan. In this work we narrow our focus to examine a modest portion of that literature, the Mind-section portion of the literature of the Tibetan tradition of the Great Perfection. Here we will use the lens of text analytics tools from machine learning to investigate a number of questions of interest to scholars of this and related traditions of the Great Perfection. It has been necessary for us to participate in all portions of this process: corpora identification and text edition selection, rendering the text as e-texts in Unicode using both Optical Character Recognition and manual entry, data cleaning and transformation, implementation of software for text analysis, and interpretation of results. For this reason, we hope this study can serve as something as a model for other low-resource languages that are just beginning to approach the problem of providing text analytics for their language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3389037,
author = {Saetia, Chanatip and Chalothorn, Tawunrat and Chuangsuwanich, Ekapol and Vateekul, Peerapon},
title = {Semi-Supervised Thai Sentence Segmentation Using Local and Distant Word Representations},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389037},
doi = {10.1145/3389037},
abstract = {A sentence is typically treated as the minimal syntactic unit used for extracting valuable information from a longer piece of text. However, in written Thai, there are no explicit sentence markers. We proposed a deep learning model for the task of sentence segmentation that includes three main contributions. First, we integrate n-gram embedding as a local representation to capture word groups near sentence boundaries. Second, to focus on the keywords of dependent clauses, we combine the model with a distant representation obtained from self-attention modules. Finally, due to the scarcity of labeled data, for which annotation is difficult and time-consuming, we also investigate and adapt Cross-View Training (CVT) as a semi-supervised learning technique, allowing us to utilize unlabeled data to improve the model representations. In the Thai sentence segmentation experiments, our model reduced the relative error by 7.4% and 10.5% compared with the baseline models on the Orchid and UGWC datasets, respectively. We also applied our model to the task of pronunciation recovery on the IWSLT English dataset. Our model outperformed the prior sequence tagging models, achieving a relative error reduction of 2.5%. Ablation studies revealed that utilizing n-gram presentations was the main contributing factor for Thai, while the semi-supervised training helped the most for English.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3397968,
author = {YANG, Xiao dong and Lin, Xiao xia},
title = {DESIGN AND DEVELOPMENT OF HEURISTIC UTILITY MANAGEMENT ALGORITHM FOR CHINESE LIBRARY MANAGEMENT SYSTEM},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2375-4699},
url = {https://doi.org/10.1145/3397968},
doi = {10.1145/3397968},
abstract = {Utility Management in library is the programmatic tool with the synthetic mental program ability along with Artificial intelligence capacities headed to manage high volume of books, articles and assignments which help to ease the manual significance of librarians. This computerized machine code helps librarians to deal with various databases of the library management system. This framework keeps the records of all the resource details in an optimized manner. It uses utility management software code with optimized search classifier that helps to deal with the resource of the library. In this work Heuristic Utility management Algorithm (HUMA) has been used to keep track of resources in the library using mathematical modeling and standardized programmatic computation on tags which relates the decode scanner to parse the input information. HUMA helps to reduce the manual routine work done by the librarians and it has been analyzed in this research with prominent survey outcomes based on experimental validation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
numpages = {1}
}

@article{10.1145/3407912,
author = {Tanwar, Ashwani and Majumder, Prasenjit},
title = {Translating Morphologically Rich Indian Languages under Zero-Resource Conditions},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3407912},
doi = {10.1145/3407912},
abstract = {This work presents an in-depth analysis of machine translations of morphologically-rich Indo-Aryan and Dravidian languages under zero-resource conditions. It focuses on Zero-Shot Systems for these languages and leverages transfer-learning by exploiting target-side monolingual corpora and parallel translations from other languages. These systems are compared with direct translations using the BLEU and TER metrics. Further, Zero-Shot Systems are used as pre-trained models for fine-tuning with real human-generated data taken in different proportions that range from 100 sentences to the entire training set. Performances of the Indo-Aryan and Dravidian languages are compared with a focus on their morphological complexity. The systems with a Dravidian source language performed much better and reached very near to the level of direct translations. This is observed likely due to morphological richness and complexity in the language, which in turn provided more room for transfer-learning in this case. A comparative analysis based on language families has been done. These systems were fine-tuned further, which in turn outperformed direct translations with just 500 parallel sentences for a Dravidian source language. However, systems with an Indo-Aryan source language showed similar performance after getting fine-tuned with 10,000 sentences.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {85},
numpages = {15},
keywords = {Zero-Shot Systems, Indo-Aryan, transfer-learning, low-resource, morphological complexity, Dravidian}
}

@article{10.1145/3402884,
author = {Ruan, Yu-Ping and Ling, Zhen-Hua and Zhu, Xiaodan},
title = {Condition-Transforming Variational Autoencoder for Generating Diverse Short Text Conversations},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3402884},
doi = {10.1145/3402884},
abstract = {In this article, conditional-transforming variational autoencoders (CTVAEs) are proposed for generating diverse short text conversations. In conditional variational autoencoders (CVAEs), the prior distribution of latent variable z follows a multivariate Gaussian distribution with mean and variance modulated by the input conditions. Previous work found that this distribution tended to become condition-independent in practical applications. Thus, this article designs CTVAEs to enhance the influence of conditions in CVAEs. In a CTVAE model, the latent variable z is sampled by performing a non-linear transformation on the combination of the input conditions and the samples from a condition-independent prior distribution N (0, I). In our experiments using a Chinese Sina Weibo dataset, the CTVAE model derives z samples for decoding with better condition-dependency than that of the CVAE model. The earth mover’s distance (EMD) between the distributions of the latent variable z at the training stage, and the testing stage is also reduced by using the CTVAE model. In subjective preference tests, our proposed CTVAE model performs significantly better than CVAE and sequence-to-sequence (Seq2Seq) models on generating diverse, informative, and topic-relevant responses.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {79},
numpages = {13},
keywords = {text generation, conversation, Neural network, variational autoencoder}
}

@article{10.1145/3412323,
author = {Yang, Qimeng and Yu, Long and Tian, Shengwei and Song, Jinmiao},
title = {Attention Mechanism for Uyghur Personal Pronouns Resolution},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3412323},
doi = {10.1145/3412323},
abstract = {Deep neural network models for Uyghur personal pronoun resolution learn semantic information for personal pronoun and antecedents, but tend to be short-sighted—they ignore the importance of each feature. In this article, we propose a Uyghur personal pronoun resolution model based on Attention mechanism, Convolutional neural networks and Gated recurrent unit (ATCG). Our model studies the grammatical structure and semantic features of Uyghur, and extracts 11 key features for Uyghur resolution task. Attention mechanism can focus on the importance of words in sentences. Gated Recurrent Unit (GRU) is applied in this model to achieve the interdependent features with long distance. The ATCG model effectively makes up for the shortcomings of relying only on the features of the content level and achieves better classification performance. Experimental results on Uyghur resolution dataset show that our model surpasses the state-of-the-art models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {83},
numpages = {13},
keywords = {Attention mechanism, Uyghur, anaphora resolution, GRU, CNN}
}

@article{10.1145/3404854,
author = {Badaro, Gilbert and Hajj, Hazem and Habash, Nizar},
title = {A Link Prediction Approach for Accurately Mapping a Large-Scale Arabic Lexical Resource to English WordNet},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3404854},
doi = {10.1145/3404854},
abstract = {Success of Natural Language Processing (NLP) models, just like all advanced machine learning models, rely heavily on large -scale lexical resources. For English, English WordNet (EWN) is a leading example of a large-scale resource that has enabled advances in Natural Language Understanding (NLU) tasks such as word sense disambiguation, question answering, sentiment analysis, and emotion recognition. EWN includes sets of cognitive synonyms called synsets, which are interlinked by means of conceptual-semantic and lexical relations and where each synset expresses a distinct concept. However, other languages are still lagging behind in having large-scale and rich lexical resources similar to EWN. In this article, we focus on enabling the development of such resources for Arabic. While there have been efforts in developing an Arabic WordNet (AWN), the current version of AWN has its limitations in size and in lacking transliteration standards, which are important for compatibility with Arabic NLP tools. Previous efforts for extending AWN resulted in a lexicon, called ArSenL, that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer (SAMA) and EWN synset terms, and that resulted in inaccurate mapping of Arabic lemmas to EWN’s synsets. Furthermore, there has been limited exploration of other expansion methods due to expensive manual validation needed. To address these limitations of simultaneously having large-scale size with high accuracy and standard representations, the mapping problem is formulated as a link prediction problem between a large-scale Arabic lexicon and EWN, where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related. We use a semi-supervised approach to create a training dataset by finding common terms in the large-scale Arabic resource and AWN. This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models. We propose the use of a two-step Boosting method, where the first step aims at linking English translations of SAMA’s terms to EWN’s synsets. The second step uses surface similarity between SAMA’s glosses and EWN’s synsets. The method results in a new large-scale Arabic lexicon that we call ArSenL 2.0 as a sequel to the previously developed sentiment lexicon ArSenL. A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state-of-the-art link prediction methods. Compared to previously developed ArSenL, ArSenL 2.0 included a larger set of sentimentally charged adjectives and verbs. It also showed higher linking accuracy on the ground truth data compared to previous ArSenL. For extrinsic evaluation, ArSenL 2.0 was used for sentiment analysis and showed, here, too, higher accuracy compared to previous ArSenL.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {80},
numpages = {38},
keywords = {arabic wordnet expansion, Link prediction, arabic natural language processing, wordnet, arabic sentiment lexicon, lexical resources}
}

@article{10.1145/3392046,
author = {Schmidt, Dirk},
title = {Grading Tibetan Children’s Literature: A Test Case Using the NLP Readability Tool “Dakje”},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3392046},
doi = {10.1145/3392046},
abstract = {Worldwide, literacy is on the rise. This historically unprecedented surge—especially over the past 200 years—has changed nearly everything about the ancient technology of reading. Who reads is changing: Literacy is no longer just for elite, professional readers, but for anyone and everyone. What and why we read is changing: We do not just read difficult texts for academic, religious, legal, or record-keeping purposes; we also read easy texts to be entertained, to access information, and to communicate with each other on a daily basis. And how we read is changing: Memorization, recitation, and oral performance has given way to a rapid, silent, individual activity.Many of these democratizing changes have been made possible by technology. This has included advances in methods and materials that have made reading and writing easy, cheap, and widely available—like paper, the printing press, and the digital revolution. But perhaps the biggest reason literacy has become so widespread has been its ability to reach people in their own natural languages. More recently, this progress has been enhanced by NLP tools, like readability editors, that have helped authors, journalists, and other writing professionals make simple, clear content suitable for both beginning readers and widespread audiences.To that end, this article introduces a new readability tool, “Dakje,” alongside a specific use case, and demonstrates how it can help benefit literacy in the Tibetan languages. This NLP software works by word-splitting Tibetan text and analyzing those words using level lists that are based on frequency analysis from corpora. Users then have instant access to statistics on the readability of their word choices so they can make edits for easy-to-read text. In our test-case, Dakje helped us reduce sentence complexity by 34%, total word count by 10%, and non-level vocabulary use from 16% to 1% when compared to an original English-to-Tibetan translation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {75},
numpages = {19},
keywords = {readability, speech corpus, graded readers, literacy, corpus linguistics, Tibetan, diglossia, children’s literature}
}

@article{10.1145/3407911,
author = {Xi, Xuefeng and Pi, Zhou and Zhou, Guodong},
title = {Global Encoding for Long Chinese Text Summarization},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3407911},
doi = {10.1145/3407911},
abstract = {Text summarization is one of the significant tasks of natural language processing, which automatically converts text into a summary. Some summarization systems, for short/long English, and short Chinese text, benefit from advances in the neural encoder-decoder model because of the availability of large datasets. However, the long Chinese text summarization research has been limited to datasets of a couple of hundred instances. This article aims to explore the long Chinese text summarization task. To begin with, we construct a first large-scale, long Chinese text summarization corpus, the Long Chinese Summarization of Police Inquiry Record Text (LCSPIRT). Based on this corpus, we propose a sequence-to-sequence (Seq2Seq) model that incorporates a global encoding process with an attention mechanism. Our model achieves a competitive result on the LCSPIRT corpus compared with several benchmark methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {84},
numpages = {17},
keywords = {Text summarization, corpus building, long Chinese text}
}

@article{10.1145/3400396,
author = {Malhas, Rana and Elsayed, Tamer},
title = {<i>AyaTEC</i>: Building a Reusable Verse-Based Test Collection for Arabic Question Answering on the Holy Qur’An},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3400396},
doi = {10.1145/3400396},
abstract = {The absence of publicly available reusable test collections for Arabic question answering on the Holy Qur’an has impeded the possibility of fairly comparing the performance of systems in that domain. In this article, we introduce AyaTEC, a reusable test collection for verse-based question answering on the Holy Qur’an, which serves as a common experimental testbed for this task. AyaTEC includes 207 questions (with their corresponding 1,762 answers) covering 11 topic categories of the Holy Qur’an that target the information needs of both curious and skeptical users. To the best of our effort, the answers to the questions (each represented as a sequence of verses) in AyaTEC were exhaustive—that is, all qur’anic verses that directly answered the questions were exhaustively extracted and annotated. To facilitate the use of AyaTEC in evaluating the systems designed for that task, we propose several evaluation measures to support the different types of questions and the nature of verse-based answers while integrating the concept of partial matching of answers in the evaluation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {78},
numpages = {21},
keywords = {Classical Arabic, evaluation}
}

@article{10.1145/3406209,
author = {M, Poornima Devi. and Sornam, M.},
title = {Classification of Ancient Handwritten Tamil Characters on Palm Leaf Inscription Using Modified Adaptive Backpropagation Neural Network with GLCM Features},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3406209},
doi = {10.1145/3406209},
abstract = {The core aspiration of this proposed work is to classify Tamil characters inscribed in the palm leaf manuscript using an Artificial Neural Network. Tamil palm leaf manuscript characters in the form of images were processed and segmented using contour-based convex hull bounding box segmentation. The segmented characters were transformed into two forms: Binary Coded Value and the Gray-Level Co-occurrence Matrix (GLCM) feature. The features extracted from the segmented characters were trained by the proposed method of the Modified Adaptive Backpropagation Network (MABPN) algorithm with Shannon activation function. Weight initialization plays an important role in the Backpropagation Neural Network, and hence Nguyen-Widrow weight initialization was introduced to initialize the weights instead of random weight initialization in the proposed method. The models evaluated are MABPN with Shannon activation function using Nguyen-Widrow weight initialization in two forms of input: Binary Coded Value and GLCM feature extracted values. The proposed method with GLCM features as input gave a promising result over binary coded transform.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {82},
numpages = {24},
keywords = {ABPN, Nguyen-Widrow, machine learning, Shannon, GLCM}
}

@article{10.1145/3402885,
author = {Wang, Hao and Tao, Qiongxing and Du, Siyuan and Luo, Xiangfeng},
title = {An Extensible Framework of Leveraging Syntactic Skeleton for Semantic Relation Classification},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3402885},
doi = {10.1145/3402885},
abstract = {Relation classification is one of the most fundamental upstream tasks in natural language processing and information extraction. State-of-the-art approaches make use of various deep neural networks (DNNs) to extract higher-level features directly. They can easily access to accurate classification results by taking advantage of both local entity features and global sentential features. Recent works on relation classification devote efforts to modify these neural networks, but less attention has been paid to the feature design concerning syntax. However, from a linguistic perspective, syntactic features are essential for relation classification. In this article, we present a novel linguistically motivated approach that enhances relation classification by imposing additional syntactic constraints. We investigate to leverage syntactic skeletons along with the sentential contexts to identify hidden relation types. The syntactic skeletons are extracted under the guidance of prior syntax knowledge. During extraction, the input sentences are recursively decomposed into syntactically shorter and simpler chunks. Experimental results on the SemEval-2010 Task 8 benchmark show that incorporating syntactic skeletons into current DNN models enhances the task of relation classification. Our systems significantly surpass two strong baseline systems. One of the substantial advantages of our proposal is that this framework is extensible for most current DNN models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {77},
numpages = {21},
keywords = {relation classification, syntactic skeleton, Neural network}
}

@article{10.1145/3414901,
author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Liu, Junxin and Huang, Yongfeng and Xie, Xing},
title = {Detecting Entities of Works for Chinese Chatbot},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3414901},
doi = {10.1145/3414901},
abstract = {Chatbots such as Xiaoice have gained huge popularity in recent years. Users frequently mention their favorite works such as songs and movies in conversations with chatbots. Detecting these entities can help design better chat strategies and improve user experience. Existing named entity recognition methods are mainly designed for formal texts, and their performance on the informal chatbot conversation texts may not be optimal. In addition, these methods rely on massive manually annotated data for model training. In this article, we propose a neural approach to detect entities of works for Chinese chatbot. Our approach is based on a language model (LM) long-short term memory (LSTM) convolutional neural network (CNN) conditional random value (CRF), or LM-LSTM-CNN-CRF, framework, which contains a language model to generate context-aware character embeddings, a Bi-LSTM network to learn contextual character representations from global contexts, a CNN to learn character representations from local contexts, and a CRF layer to jointly decode the character label sequence. In addition, we propose an automatic text annotation method via quote marks to reduce the effort of manual annotation. Besides, we propose an iterative data purification method to improve the quality of the automatically constructed labeled data. Massive experiments on a real-world dataset validate that our approach can achieve good performance on entity detection for Chinese chatbots.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {88},
numpages = {13},
keywords = {neural network, Named entity recognition, chatbot}
}

@article{10.1145/3373266,
author = {Alkhatib, Manar and Monem, Azza Abdel and Shaalan, Khaled},
title = {Deep Learning for Arabic Error Detection and Correction},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373266},
doi = {10.1145/3373266},
abstract = {Research on tools for automating the proofreading of Arabic text has received much attention in recent years. There is an increasing demand for applications that can detect and correct Arabic spelling and grammatical errors to improve the quality of Arabic text content and application input. Our review of previous studies indicates that few Arabic spell-checking research efforts appropriately address the detection and correction of ill-formed words that do not conform to the Arabic morphology system. Even fewer systems address the detection and correction of erroneous well-formed Arabic words that are either contextually or semantically inconsistent within the text. We introduce an approach that investigates employing deep neural network technology for error detection in Arabic text. We have developed a systematic framework for spelling and grammar error detection, as well as correction at the word level, based on a bidirectional long short-term memory mechanism and word embedding, in which a polynomial network classifier is at the top of the system. To get conclusive results, we have developed the most significant gold standard annotated corpus to date, containing 15 million fully inflected Arabic words. The data were collected from diverse text sources and genres, in which every erroneous and ill-formed word has been annotated, validated, and manually revised by Arabic specialists. This valuable asset is available for the Arabic natural language processing research community. The experimental results confirm that our proposed system significantly outperforms the performance of Microsoft Word 2013 and Open Office Ayaspell 3.4, which have been used in the literature for evaluating similar research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {71},
numpages = {13},
keywords = {word embedding, bidirectional long short-term memory, polynomial network classifier, Error detection, error correction}
}

@article{10.1145/3388970,
author = {Hao, Ming and Xu, Bo and Liang, Jing-Yi and Zhang, Bo-Wen and Yin, Xu-Cheng},
title = {Chinese Short Text Classification with Mutual-Attention Convolutional Neural Networks},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3388970},
doi = {10.1145/3388970},
abstract = {The methods based on the combination of word-level and character-level features can effectively boost performance on Chinese short text classification. A lot of works concatenate two-level features with little processing, which leads to losing feature information. In this work, we propose a novel framework called Mutual-Attention Convolutional Neural Networks, which integrates word and character-level features without losing too much feature information. We first generate two matrices with aligned information of two-level features by multiplying word and character features with a trainable matrix. Then, we stack them as a three-dimensional tensor. Finally, we generate the integrated features using a convolutional neural network. Extensive experiments on six public datasets demonstrate improved performance of our new framework over current methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {61},
numpages = {13},
keywords = {convolutional neural networks, mutual-attention, feature integration, Short text classification, word-level and character-level}
}

@article{10.1145/3389791,
author = {Zhou, Long and Zhang, Jiajun and Kang, Xiaomian and Zong, Chengqing},
title = {Deep Neural Network--Based Machine Translation System Combination},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389791},
doi = {10.1145/3389791},
abstract = {Deep neural networks (DNNs) have provably enhanced the state-of-the-art natural language process (NLP) with their capability of feature learning and representation. As one of the more challenging NLP tasks, neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy and word coverage. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this article, we propose a deep neural network--based system combination framework leveraging both minimum Bayes-risk decoding and multi-source NMT, which take as input the N-best outputs of NMT and SMT systems and produce the final translation. In particular, we apply the proposed model to both RNN and self-attention networks with different segmentation granularity. We verify our approach empirically through a series of experiments on resource-rich Chinese⇒English and low-resource English⇒Vietnamese translation tasks. Experimental results demonstrate the effectiveness and universality of our proposed approach, which significantly outperforms the conventional system combination methods and the best individual system output.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {65},
numpages = {19},
keywords = {low-resource translation, system combination, NMT, SMT, DNN, minimal Bayes-risk decoding}
}

@article{10.1145/3394113,
author = {Bai, Ruirui and Wang, Zhongqing and Kong, Fang and Li, Shoushan and Zhou, Guodong},
title = {Neural Co-Training for Sentiment Classification with Product Attributes},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3394113},
doi = {10.1145/3394113},
abstract = {Sentiment classification aims to detect polarity from a piece of text. The polarity is usually positive or negative, and the text genre is usually product review. The challenges of sentiment classification are that it is hard to capture semantic of reviews, and the labeled data is hard to annotate. Therefore, we propose neural co-training to learn the semantic representation of each review using the neural network model, and learn the information from unlabeled data using a co-training framework. In particular, we use the attention-based bi-directional Gated Recurrent Unit (Att-BiGRU) to model the semantic content of each review and regard different categories of the target product as different views. We then use a co-training framework to learn and predict the unlabeled reviews with different views. Experiment results with the Yelp dataset demonstrate the effectiveness of our approach.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {74},
numpages = {17},
keywords = {co-training, Semi-supervised sentiment classification, product attributes}
}

@article{10.1145/3397501,
author = {Zitouni, Imed},
title = {Editorial from the New Editor-in-Chief: The Era of Natural Language Processing Innovations on Asian and Low-Resource Languages},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3397501},
doi = {10.1145/3397501},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {48e},
numpages = {2}
}

@article{10.1145/3390092,
author = {Ameur, Mohamed Seghir Hadj and Belkebir, Riadh and Guessoum, Ahmed},
title = {Robust Arabic Text Categorization by Combining Convolutional and Recurrent Neural Networks},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3390092},
doi = {10.1145/3390092},
abstract = {Text Categorization is an important task in the area of Natural Language Processing (NLP). Its goal is to learn a model that can accurately classify any textual document for a given language into one of a set of predefined categories. In the context of the Arabic language, several approaches have been proposed to tackle this problem, many of which are based on the bag-of-words assumption. Even though these methods usually produce good results for the classification task, they often fail to capture contextual dependencies from textual data. On the other hand, deep learning architectures that are usually based on Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) do not suffer from such a limitation and have recently shown very promising results in various NLP applications. In this work, we use deep learning models that combine RNN and CNN for the task of Arabic text categorization using static, dynamic, and fine-tuned word embeddings. The experimental results reported on the Open Source Arabic Corpora (OSAC) dataset have shown the effectiveness and high performance of our proposed models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {66},
numpages = {16},
keywords = {Arabic language, recurrent neural networks, deep learning, Natural language processing, Arabic text categorization, pretrained word embeddings, Arabic text classification, convolutional neural networks}
}

@article{10.1145/3390298,
author = {Chimalamarri, Santwana and Sitaram, Dinkar and Jain, Ashritha},
title = {Morphological Segmentation to Improve Crosslingual Word Embeddings for Low Resource Languages},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3390298},
doi = {10.1145/3390298},
abstract = {Crosslingual word embeddings developed from multiple parallel corpora help in understanding the relationships between languages and improving the prediction quality of machine translation. However, in low resource languages with complex and agglutinative morphologies, inducing good-quality crosslingual embeddings becomes challenging due to the problem of complex morphological forms and rare words. This is true even for languages that share common linguistic structure. In our work, we have shown that performing a simple morphological segmentation upon the corpora prior to the generation of crosslingual word embeddings for both roots and suffixes greatly improves the prediction quality and captures semantic similarities more effectively. To exhibit this, we have chosen two related languages: Telugu and Kannada of the Dravidian language family. We have also tested our method upon a widely spoken North Indian language, Hindi, belonging to the Indo-European language family, and have observed encouraging results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {69},
numpages = {15},
keywords = {crosslingual embeddings, supervised learning, bilingual embeddings, morphologically rich languages, morphology, linear transformation, word2vec, Word embeddings, machine translation}
}

@article{10.1145/3388971,
author = {Chen, Junjie and Hou, Hongxu and Gao, Jing},
title = {Inside Importance Factors of Graph-Based Keyword Extraction on Chinese Short Text},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3388971},
doi = {10.1145/3388971},
abstract = {Keywords are considered to be important words in the text and can provide a concise representation of the text. With the surge of unlabeled short text on the Internet, automatic keyword extraction task has proven useful in other information processing applications. Graph-based approaches are prevalent unsupervised models for this task. However, most of these methods emphasize the importance of the relation between words without considering other importance factors. Furthermore, when measuring the importance of a word in a text, the damping factor is set to 0.85 following PageRank. To the best of our knowledge, there is no existing work investigating the impact of the damping factor on the keyword extraction task. In addition, there are few publicly available labeled Chinese short text datasets for this task. In this article, we investigate the importance parts of words in a given document and propose an improved graph-based method for keyword extraction from short documents. Moreover, we analyze the impact of importance factors on performance. We also provide annotated long and short Chinese datasets for this task. The model is performed on Chinese and English datasets, and results show that our model obtains improvements in performance over the previous unsupervised models on short documents. Comparative experiments show that the damping factor is related to the text length, which is neglected in traditional methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {63},
numpages = {15},
keywords = {importance rank, keyword extraction, Short text}
}

@article{10.1145/3398070,
author = {Dhar, Ankita and Mukherjee, Himadri and Dash, Niladri Sekhar and Roy, Kaushik},
title = {CESS-A System to Categorize Bangla Web Text Documents},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3398070},
doi = {10.1145/3398070},
abstract = {Technology has evolved remarkably, which has led to an exponential increase in the availability of digital text documents of disparate domains over the Internet. This makes the retrieval of the information a very much time- and resource-consuming task. Thus, a system that can categorize such documents based on their domains can truly help the users in obtaining the required information with relative ease and also reduce the workload of the search engines. This article presents a text categorization system (CESS) that categorizes text document using newly proposed hybrid features that combines term frequency-inverse document frequency-inverse class frequency and modified chi-square methods. Experiments were performed on real-world Bangla documents from eight domains comprises of 24,29,857 tokens, and the highest accuracy of 99.91% has been obtained with multilayer perceptron-based classification. Also, the experiments were tested on Reuters-21578 and 20 Newsgroups datasets and obtained accuracies of 97.29% and 94.67%, respectively, to show the language-independent nature of the system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {73},
numpages = {18},
keywords = {multilayer perceptron, Text categorization, modified chi-square, Bangla texts, tf-idf-icf}
}

@article{10.1145/3394137,
author = {Li, Ying and Huang, Jizhou and Fan, Miao and Lei, Jinyi and Wang, Haifeng and Chen, Enhong},
title = {Personalized Query Auto-Completion for Large-Scale POI Search at Baidu Maps},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3394137},
doi = {10.1145/3394137},
abstract = {Query auto-completion (QAC) is a featured function that has been widely adopted by many sub-domains of search. It can dramatically reduce the number of typed characters and avoid spelling mistakes. These merits of QAC are highlighted to improve user satisfaction, especially when users intend to type in a query on mobile devices. In this article, we will present our industrial solution to the personalized QAC for the point of interest (POI) search at Baidu Maps, a well-known Web mapping service on mobiles in China. The industrial solution makes a good tradeoff between the offline effectiveness of a novel neural learning model that we devised for feature generation and the online efficiency of an off-the-shelf learning to rank (LTR) approach for the real-time suggestion. Besides some practical lessons from how a real-world QAC system is built and deployed in Baidu Maps to facilitate a large number of users in searching tens of millions of POIs, we mainly explore two specific features for the personalized QAC function of the POI search engine: the spatial-temporal characteristics of POIs and the historically queried POIs of individual users.We leverage the large-volume POI search logs in Baidu Maps to conduct offline evaluations of our personalized QAC model measured by multiple metrics, including Mean Reciprocal Rank (MRR), Success Rate (SR), and normalized Discounted Cumulative Gain (nDCG). Extensive experimental results demonstrate that the personalized model enhanced by the proposed features can achieve substantial improvements (i.e., +3.29% MRR, +3.78% SR@1, +5.17% SR@3, +1.96% SR@5, and +3.62% nDCG@5). After deploying this upgraded model into the POI search engine at Baidu Maps for A/B testing online, we observe that some other critical indicators, such as the average number of keystrokes and the average typing speed at keystrokes in a QAC session, which are also related to user satisfaction, decrease as well by 1.37% and 1.69%, respectively. So the conclusion is that the two kinds of features contributed by us are quite helpful in personalized mapping services for industrial practice.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {70},
numpages = {16},
keywords = {neural networks, large-scale point of interest (POI) search, personalized query auto-completion (QAC), learning to rank (LTR), Baidu Maps}
}

@article{10.1145/3397967,
author = {Orhan, Umut and Arslan, En\i{}s},
title = {Learning Word-Vector Quantization: A Case Study in Morphological Disambiguation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3397967},
doi = {10.1145/3397967},
abstract = {We introduced a new classifier named Learning Word-vector Quantization (LWQ) to solve morphological ambiguities in Turkish, which is an agglutinative language. First, a new and morphologically annotated corpus, and then its datasets are prepared with a series of processes. According to datasets, LWQ finds optimal word-vectors positions by moving them in the Euclidean space. LWQ does morphological disambiguation in two steps: First, it defines all solution candidates of an ambiguous word using a morphological analyzer; second, it chooses the best candidate according to its total distances to neighbor words that are not ambiguous. To show LWQ's performance, we have conducted many tests on the corpus by considering the consistency of classification. In the experiments, we achieve 98.4% correct classification ratio to choose correct parse output, which is an excellent level for the literature.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {72},
numpages = {18},
keywords = {Turkish morphological disambiguation, Learning word-vector quantization, classification, Learning vector quantization}
}

@article{10.1145/3382187,
author = {Khalil, Hussein and Osman, Taha and Miltan, Mohammed},
title = {Extracting Arabic Composite Names Using Genitive Principles of Arabic Grammar},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3382187},
doi = {10.1145/3382187},
abstract = {Named Entity Recognition (NER) is a basic prerequisite of using Natural Language Processing (NLP) for information retrieval. Arabic NER is especially challenging as the language is morphologically rich and has short vowels with no capitalisation convention. This article presents a novel rule-based approach that uses linguistic grammar-based techniques to extract Arabic composite names from Arabic text. Our approach uniquely exploits the genitive Arabic grammar rules; in particular, the rules regarding the identification of definite nouns (معرفة) and indefinite nouns (نكرة) to support the process of extracting composite names. Based on domain knowledge and Arabic Genitive Rules (AGR), the developed approach formalises a set of syntactical rules and linguistic patterns that initially use genitive patterns to classify definiteness within phrases and then extracts proper composite names from the unstructured text. The developed novel approach does not place any constraints on the length of the Arabic composite name and our initial experimentation demonstrated high recall and precision results when the NER algorithm was applied to a financial domain corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {57},
numpages = {16},
keywords = {Arabic named entity recognition, domain knowledge, Arabic language grammar, natural language processing}
}

@article{10.1145/3383330,
author = {Garg, Kanika and Lobiyal, D. K.},
title = {Hindi EmotionNet: A Scalable Emotion Lexicon for Sentiment Classification of Hindi Text},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383330},
doi = {10.1145/3383330},
abstract = {In this study, we create an emotion lexicon for the Hindi language called Hindi EmotionNet. It can assign emotional affinity to words in IndoWordNet. This lexicon contains 3,839 emotion words, with 1,246 positive and 2,399 negative words. We also introduce ambiguous (217 words) and neutral (95 words) emotions to Hindi. Positive emotion words covered nine types of positive emotions, negative emotion words covered eleven types of negative emotions, ambiguous emotion words covered seven types of ambiguous emotions, and neutral emotion words covered two neutral emotions. The proposed Hindi EmotionNet was then applied to opinion classification and emotion classification. We introduce a centrality-based approach for emotion classification that uses degree, closeness, betweenness, and page rank as centrality measures. We also created a dataset of Hindi based on screenplays, stories, and blogs in the language. We translated emotion data from SemEval 2017 into Hindi for further comparison. The proposed approach delivered promising results on opinion and emotion classification, with an accuracy of 85.78% for the former and 75.91% for the latter.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {52},
numpages = {35},
keywords = {Hindi, emotion classification, centrality, sentiment analysis}
}

@article{10.1145/3389790,
author = {Marie, Benjamin and Fujita, Atsushi},
title = {Iterative Training of Unsupervised Neural and Statistical Machine Translation Systems},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389790},
doi = {10.1145/3389790},
abstract = {Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that only rely on monolingual corpora. However, previous work also showed that unsupervised statistical machine translation (USMT) performs better than unsupervised NMT (UNMT), especially for distant language pairs. To take advantage of the superiority of USMT over UNMT, and considering that SMT suffers from well-known limitations overcome by NMT, we propose to define UNMT as NMT trained with the supervision of synthetic parallel data generated by USMT. This way we can exploit USMT up to its limits while ultimately relying on full-fledged NMT models to generate translations. We show significant improvements in translation quality over previous work and also that further improvements can be obtained by alternatively and iteratively training USMT and UNMT. Without the need of a dedicated architecture for UNMT, our simple approach can straightforwardly benefit from any recent and future advances in supervised NMT. Our systems achieve a new state-of-the-art for unsupervised machine translation in all of our six translation tasks for five diverse language pairs, surpassing even supervised SMT or NMT in some tasks. Furthermore, our analysis shows how crucial the comparability between the monolingual corpora used for unsupervised training is in improving translation quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {68},
numpages = {21},
keywords = {semantic similarity, phrase table induction, knowledge acquisition, low-resource language pair, Machine translation}
}

@article{10.1145/3389035,
author = {Lou, Yinxia and Zhang, Yue and Li, Fei and Qian, Tao and Ji, Donghong},
title = {Emoji-Based Sentiment Analysis Using Attention Networks},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389035},
doi = {10.1145/3389035},
abstract = {Emojis are frequently used to express moods, emotions, and feelings in social media. There has been much research on emojis and sentiments. However, existing methods mainly face two limitations. First, they treat emojis as binary indicator features and rely on handcrafted features for emoji-based sentiment analysis. Second, they consider the sentiment of emojis and texts separately, not fully exploring the impact of emojis on the sentiment polarity of texts. In this article, we investigate a sentiment analysis model based on bidirectional long short-term memory, and the model has two advantages compared with the existing work. First, it does not need feature engineering. Second, it utilizes the attention approach to model the impact of emojis on text. An evaluation on 10,042 manually labeled Sina Weibo showed that our model achieves much better performance compared with several strong baselines. To facilitate the related research, our corpus will be publicly available at https://github.com/yx100/emoji.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {64},
numpages = {13},
keywords = {social media, deep learning, attention, Sentiment analysis, emoji}
}

@article{10.1145/3383772,
author = {Das, Ayan and Sarkar, Sudeshna},
title = {A Survey of the Model Transfer Approaches to Cross-Lingual Dependency Parsing},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383772},
doi = {10.1145/3383772},
abstract = {Cross-lingual dependency parsing approaches have been employed to develop dependency parsers for the languages for which little or no treebanks are available using the treebanks of other languages. A language for which the cross-lingual parser is developed is usually referred to as the target language and the language whose treebank is used to train the cross-lingual parser model is referred to as the source language. The cross-lingual parsing approaches for dependency parsing may be broadly classified into three categories: model transfer, annotation projection, and treebank translation. This survey provides an overview of the various aspects of the model transfer approach of cross-lingual dependency parsing. In this survey, we present a classification of the model transfer approaches based on the different aspects of the method. We discuss some of the challenges associated with cross-lingual parsing and the techniques used to address these challenges. In order to address the difference in vocabulary between two languages, some approaches use only non-lexical features of the words to train the models while others use shared representations of the words. Some approaches address the morphological differences by chunk-level transfer rather than word-level transfer. The syntactic differences between the source and target languages are sometimes addressed by transforming the source language treebanks or by combining the resources of multiple source languages. Besides cross-lingual transfer parser models may be developed for a specific target language or it may be trained to parse sentences of multiple languages. With respect to the above-mentioned aspects, we look at the different ways in which the methods can be classified. We further classify and discuss the different approaches from the perspective of the corresponding aspects. We also demonstrate the performance of the transferred models under different settings corresponding to the classification aspects on a common dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {67},
numpages = {60},
keywords = {multi-source dependency parsing, cross-lingual model transfer, delexicalization, cross-lingual dependency parsing, treebank transformation, Dependency parsing, direct model transfer}
}

@article{10.1145/3389021,
author = {Xu, Fan and Luo, Jian and Wang, Mingwen and Zhou, Guodong},
title = {Speech-Driven End-to-End Language Discrimination toward Chinese Dialects},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389021},
doi = {10.1145/3389021},
abstract = {Language discrimination among similar languages, varieties, and dialects is a challenging natural language processing task. The traditional text-driven focus leads to poor results. In this article, we explore the effectiveness of speech-driven features toward language discrimination among Chinese dialects. First, we systematically explore the appropriateness of speech-driven MFCC features toward CNN-based language discrimination. Then, we design an end-to-end speech recognition model based on HMM-DNN to predict Chinese dialect words. We adopt attention mechanism to extract the discriminative words related to different Chinese dialects. Finally, through a CNN, we combine the word-level embedding and the MFCC-based features. Evaluation of two benchmark Chinese dialect corpora shows the appropriateness and effectiveness of the proposed speech-driven approach to fine-grained Chinese dialect discrimination compared to the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {62},
numpages = {24},
keywords = {language discrimination, text-driven features, Speech-driven features, Chinese dialect, attention}
}

@article{10.1145/3387634,
author = {Liu, Maofu and Zhang, Yukun and Li, Wenjie and Ji, Donghong},
title = {Joint Model of Entity Recognition and Relation Extraction with Self-Attention Mechanism},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3387634},
doi = {10.1145/3387634},
abstract = {In recent years, the joint model of entity recognition (ER) and relation extraction (RE) has attracted more and more attention in the healthcare and medical domains. However, there are some problems with the prior work. The joint model cannot extract all the relations for a specific entity, and the majority of joint models heavily rely on complex artificial features or professional natural language processing (NLP) tools. In this article, we construct a novel joint model that can simultaneously extract all medical entities and relations from medicine Chinese instructions. Moreover, the self-attention mechanism is introduced to the joint model to learn word intra-sentence dependencies. The proposed model is evaluated using a medicine Chinese instruction dataset that we collect and an open dataset provided in CoNLL-2004. Experimental results show that the model with self-attention achieves the state-of-the-art performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {59},
numpages = {19},
keywords = {self-attention, Joint model, entity recognition, medicine chinese instruction, relation extraction}
}

@article{10.1145/3387633,
author = {Wang, Kexin and Zhou, Yu and Zhang, Jiajun and Wang, Shaonan and Zong, Chengqing},
title = {Structurally Comparative Hinge Loss for Dependency-Based Neural Text Representation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3387633},
doi = {10.1145/3387633},
abstract = {Dependency-based graph convolutional networks (DepGCNs) are proven helpful for text representation to handle many natural language tasks. Almost all previous models are trained with cross-entropy (CE) loss, which maximizes the posterior likelihood directly. However, the contribution of dependency structures is not well considered by CE loss. As a result, the performance improvement gained by using the structure information can be narrow due to the failure in learning to rely on this structure information. To face the challenge, we propose the novel structurally comparative hinge (SCH) loss function for DepGCNs. SCH loss aims at enlarging the margin gained by structural representations over non-structural ones. From the perspective of information theory, this is equivalent to improving the conditional mutual information of model decision and structure information given text. Our experimental results on both English and Chinese datasets show that by substituting SCH loss for CE loss on various tasks, for both induced structures and structures from an external parser, performance is improved without additional learnable parameters. Furthermore, the extent to which certain types of examples rely on the dependency structure can be measured directly by the learned margin, which results in better interpretability. In addition, through detailed analysis, we show that this structure margin has a positive correlation with task performance and structure induction of DepGCNs, and SCH loss can help model focus more on the shortest dependency path between entities. We achieve the new state-of-the-art results on TACRED, IMDB, and Zh. Literature datasets, even compared with ensemble and BERT baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {58},
numpages = {19},
keywords = {graph convolutional networks, loss function, Text representation}
}

@article{10.1145/3387632,
author = {Kumar, H. R. Shiva and Ramakrishnan, A. G.},
title = {Lipi Gnani: A Versatile OCR for Documents in Any Language Printed in Kannada Script},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3387632},
doi = {10.1145/3387632},
abstract = {A Kannada OCR, called Lipi Gnani, has been designed and developed from scratch, with the motivation of it being able to convert printed text or poetry in Kannada script, without any restriction on vocabulary. The training and test sets have been collected from more than 35 books published from 1970 to 2002, and this includes books written in Halegannada and pages containing Sanskrit slokas written in Kannada script. The coverage of the OCR is nearly complete in the sense that it recognizes all punctuation marks, special symbols, and Indo-Arabic and Kannada numerals. Several minor and major original contributions have been done in developing this OCR at different processing stages, such as binarization, character segmentation, recognition, and Unicode mapping. This has created a Kannada OCR that performs as good as, and in some cases better than, Google’s Tesseract OCR, as shown by the results. To the best of our knowledge, this is the maiden report of a complete Kannada OCR, handling all issues involved. Currently, there is no dictionary-based postprocessing, and the obtained results are due solely to the recognition process. Four benchmark test databases containing scanned pages from books in Kannada, Sanskrit, Konkani, and Tulu languages, but all of them printed in Kannada script, have been created. The word-level recognition accuracy of Lipi Gnani is 5.3% higher on the Kannada dataset than that of Google’s Tesseract OCR, 8.5% higher on the Sanskrit dataset, and 23.4% higher on the datasets of Konkani and Tulu.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {60},
numpages = {23},
keywords = {Tulu, Konkani, Sanskrit, Kannada, segmentation, SVM, OCR, Halegannada}
}

@article{10.1145/3383201,
author = {Udomcharoenchaikit, Can and Boonkwan, Prachya and Vateekul, Peerapon},
title = {Adversarial Evaluation of Robust Neural Sequential Tagging Methods for Thai Language},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383201},
doi = {10.1145/3383201},
abstract = {Sequential tagging tasks, such as Part-Of-Speech (POS) tagging and Named-Entity Recognition, are the building blocks of many natural language processing applications. Although prior works have reported promising results in standard settings, they often underperform on non-standard text, such as microblogs and social media. In this article, we introduce an adversarial evaluation scheme for the Thai language by creating adversarial examples based on known spelling errors. Furthermore, we propose novel methods including UNK masking, condition initialization with affixation embeddings, and untied-directional self-attention mechanism to enhance robustness and interpretability of the neural networks. We conducted experiments on two Thai corpora: BEST2010 and ORCHID. Our adversarial evaluation schemes reveal that bidirectional LSTM (BiLSTM) do not perform well on adversarial examples. Our best methods match the performance of the BiLSTM baseline model and outperform it on adversarial examples.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {53},
numpages = {25},
keywords = {Neural networks, part-of-speech tagging, named-entity recognition}
}

@article{10.1145/3384202,
author = {Sugandhi and Kumar, Parteek and Kaur, Sanmeet},
title = {Sign Language Generation System Based on Indian Sign Language Grammar},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3384202},
doi = {10.1145/3384202},
abstract = {Sign Language (SL), also known as gesture-based language, is used by people with hearing loss to convey their messages. SL interpreters are required for people who do not have the knowledge of SL, but interpreters are not readily available. Thus, a machine-based translation system is required to translate the text into SL. In this article, a system is implemented for translating English text into Indian Sign Language (ISL). It acts as a tool for human-computer interaction and eliminates the need for an ISL human interpreter for communicating with people who have hearing loss. The system features a rich corpus of English words and commonly used sentences. It consists of components such as an ISL parser, the Hamburg Notation System, the Signing Gesture Mark-up Language, and 3D avatar animation for generating SL according to ISL grammar. The proposed system has been tested rigorously by SL users. The results proved that the proposed system is highly efficient and achieves an average score of accuracy (i.e., 4.2 for English words and 3.8 for sentences on a scale from 1 to 5). The performance of proposed system has also been evaluated using the BiLingual Evaluation Understudy score, which results in 0.95 accuracy. The proposed system and mobile application together has the potential to bring individuals with hearing loss and their entourage together.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {54},
numpages = {26},
keywords = {Indian Sign Language (ISL), speech to sign, hearing loss people, text to sign, Sign Language (SL), human-computer interaction, communication}
}

@article{10.1145/3383200,
author = {B\"{u}y\"{u}k, Osman},
title = {Context-Dependent Sequence-to-Sequence Turkish Spelling Correction},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383200},
doi = {10.1145/3383200},
abstract = {In this article, we make use of sequence-to-sequence (seq2seq) models for spelling correction in the agglutinative Turkish language. In the baseline system, misspelled and target words are split into their letters and the letter sequences are fed into the seq2seq model. We prefer letters as the unit of the model due to the agglutinative nature of Turkish, which results in an impractical dictionary size when words are used as a dictionary unit. In order to improve the baseline performance, we incorporate right and left context of the misspelled words. All context words are represented with their first three consonants in the context-dependent model. We train the seq2seq models using a large text corpus collected automatically from the Internet. The corpus contains approximately 4 million sentences. We randomly introduce substitution, deletion, and insertion spelling errors to the words in the corpus. We test the performance of the proposed context-dependent seq2seq model using synthetic and realistic test sets. The synthetic test set is constructed similar to the training set. The realistic test set contains human-made misspellings from Twitter messages. In the experiments, we observed that the proposed context-dependent model performs significantly better than the baseline system. Its correction accuracy reaches 94% on the synthetic dataset. Additionally, the proposed method provides 2.1% absolute improvement over a state-of-the-art Turkish spelling correction system on the Twitter test set.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {56},
numpages = {16},
keywords = {agglutinative language, Spelling correction, sequence-to-sequence models}
}

@article{10.1145/3383306,
author = {Ahmad, Muhammad Tayyab and Malik, Muhammad Kamran and Shahzad, Khurram and Aslam, Faisal and Iqbal, Asif and Nawaz, Zubair and Bukhari, Faisal},
title = {Named Entity Recognition and Classification for Punjabi Shahmukhi},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383306},
doi = {10.1145/3383306},
abstract = {Named entity recognition (NER) refers to the identification of proper nouns from natural language text and classifying them into named entity types, such as person, location, and organization. Due to the widespread applications of NER, numerous NER techniques and benchmark datasets have been developed for both Western and Asian languages. Even though Shahmukhi script of the Punjabi language has been used by nearly three fourths of the Punjabi speakers worldwide, Gurmukhi has been the main focus of research activities. Specifically, a benchmark NER corpus for Shahmukhi is non-existent, which has thwarted the commencement of NER research for the Shahmukhi script. To this end, this article presents the development and specifications of the first-ever NER corpus for Shahmukhi. The newly developed corpus is composed of 318,275 tokens and 16,300 named entities, including 11,147 persons, 3,140 locations, and 2,013 organizations. To establish the strength of our corpus, we have compared the specifications of our corpus with its Gurmukhi counterparts. Furthermore, we have demonstrated the usability of our corpus using five supervised learning techniques, including two state-of-the-art deep learning techniques. The results are compared, and valuable insights about the behaviors of the most effective technique are discussed.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {51},
numpages = {13},
keywords = {Low-resource languages, Punjabi, Shahmukhi, Asian languages, named entity recognition}
}

@article{10.1145/3378414,
author = {Zarnoufi, Randa and Jaafar, Hamid and Abik, Mounia},
title = {Machine Normalization: Bringing Social Media Text from Non-Standard to Standard Form},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3378414},
doi = {10.1145/3378414},
abstract = {User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation--like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {49},
numpages = {30},
keywords = {matrix language, Text normalization, social media, code switching normalization, standard languages, dialects, word sense disambiguation, multilingual vertical context, automatic language identification}
}

@article{10.1145/3383202,
author = {Sarwar, Raheem and Rutherford, Attapol T. and Hassan, Saeed-Ul and Rakthanmanon, Thanawin and Nutanong, Sarana},
title = {Native Language Identification of Fluent and Advanced Non-Native Writers},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383202},
doi = {10.1145/3383202},
abstract = {Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top-k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors’ classifier on the identified top-k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French, and German. Our experimental studies show that our solution outperforms competitive methods and reports more than 80% accuracy across languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {55},
numpages = {19},
keywords = {text classification, forensic investigation, native language identification, stylometry, Author profiling}
}

@article{10.1145/3373267,
author = {Che, Wanjin and Yu, Zhengtao and Yu, Zhiqiang and Wen, Yonghua and Guo, Junjun},
title = {Towards Integrated Classification Lexicon for Handling Unknown Words in Chinese-Vietnamese Neural Machine Translation},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373267},
doi = {10.1145/3373267},
abstract = {In Neural Machine Translation (NMT), due to the limitations of the vocabulary, unknown words cannot be translated properly, which brings suboptimal performance of the translation system. For resource-scarce NMT that have small-scale training corpus, the effect is amplified. The traditional approach of amplifying the scale of the corpus is not applicable, because the parallel corpus is difficult to obtain in a resource-scarce setting; however, it is easy to obtain and utilize external knowledge, bilingual lexicon, and other resources. Therefore, we propose classification lexicon approach for processing unknown words in the Chinese-Vietnamese NMT task. Specifically, three types of unknown Chinese-Vietnamese words are classified and their corresponding classification lexicon are constructed by word alignment, Wikipedia extraction, and rule-based methods, respectively. After translation, the unknown words are restored by lexicon for post-processing. Experiment results on Chinese-Vietnamese, English-Vietnamese, and Mongolian-Chinese translations show that our approach significantly improves the accuracy and the performance of NMT especially in a resource-scarce setting.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {42},
numpages = {17},
keywords = {unknown words, resource-scarce, Neural machine translation, classification lexicon}
}

@article{10.1145/3380967,
author = {Bhattu, S. Nagesh and Nunna, Satya Krishna and Somayajulu, D. V. L. N. and Pradhan, Binay},
title = {Improving Code-Mixed POS Tagging Using Code-Mixed Embeddings},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3380967},
doi = {10.1145/3380967},
abstract = {Social media data has become invaluable component of business analytics. A multitude of nuances of social media text make the job of conventional text analytical tools difficult. Code-mixing of text is a phenomenon prevalent among social media users, wherein words used are borrowed from multiple languages, though written in the commonly understood roman script. All the existing supervised learning methods for tasks such as Parts Of Speech (POS) tagging for code-mixed social media (CMSM) text typically depend on a large amount of training data. Preparation of such large training data is resource-intensive, requiring expertise in multiple languages. Though the preparation of small dataset is possible, the out of vocabulary (OOV) words pose major difficulty, while learning models from CMSM text as the number of different ways of writing non-native words in roman script is huge. POS tagging for code-mixed text is non-trivial, as tagging should deal with syntactic rules of multiple languages. The important research question addressed by this article is whether abundantly available unlabeled data can help in resolving the difficulties posed by code-mixed text for POS tagging. We develop an approach for scraping and building word embeddings for code-mixed text illustrating it for Bengali-English, Hindi-English, and Telugu-English code-mixing scenarios. We used a hierarchical deep recurrent neural network with linear-chain CRF layer on top of it to improve the performance of POS tagging in CMSM text by capturing contextual word features and character-sequence–based information. We prepared a labeled resource for POS tagging of CMSM text by correcting 19% of labels from an existing resource. A detailed analysis of the performance of our approach with varying levels of code-mixing is provided. The results indicate that the F1-score of our approach with custom embeddings is better than the CRF-based baseline by 5.81%, 5.69%, and 6.3% in Bengali, Hindi, and Telugu languages, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {50},
numpages = {31},
keywords = {Parts of speech tagging, deep neural networks, code-mixed (multi-lingual) text}
}

@article{10.1145/3377707,
author = {Yang, Jingxuan and Cui, Haotian and Li, Si and Gao, Sheng and Guo, Jun and Lu, Zhengdong},
title = {Outline Extraction with Question-Specific Memory Cells},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377707},
doi = {10.1145/3377707},
abstract = {Outline extraction has been widely applied in online consultation to help experts quickly understand individual cases. Given a specific case described as unstructured plain text, outline extraction aims to make a summary for this case by answering a set of questions, which in fact is a new type of machine reading comprehension task. Inspired by a recently popular memory network, we propose a novel question-specific memory cell network (QSMCN) to extract information related to multiple questions on-the-fly as it reads texts. QSMCN constructs a specific memory cell for each question, which is sequentially expanded in recurrent neural network style. Each cell contains three specific vectors to first identify whether current input is related to corresponding question and then update question-specific case representation. We add a penalization term in the loss function to make extracted knowledge more reasonable and interpretable. To support this study, we construct a new outline extraction corpus, InjuryCase,1 which is composed of 3,995 real Chinese occupational injury cases. Experimental results show that our method makes a significant improvement. We further apply the proposed framework on two multi-aspect extraction tasks and find that the proposed model also remarkably outperforms existing state-of-the-art methods of the aspect extraction task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {48},
numpages = {17},
keywords = {aspect extraction, Outline extraction, memory cell network}
}

@article{10.1145/3359990,
author = {Somsap, Sittichai and Seresangtakul, Pusadee},
title = {Isarn Dharma Word Segmentation Using a Statistical Approach with Named Entity Recognition},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3359990},
doi = {10.1145/3359990},
abstract = {In this study, we developed an Isarn Dharma word segmentation system. We mainly focused on solving the word ambiguity and unknown word problems in unsegmented Isarn Dharma text. Ambiguous Isarn Dharma words occur frequently in word construction due to the writing style without tone markers. Thus, words can be interpreted as having different tones and meanings in the same writing text. To overcome these problems, we developed an Isarn Dharma character cluster–(IDCC) based statistical model and affixation and integrated it with the named entity recognition method (IDCC-C-based statistical model and affixation with named entity recognition (NER)). This method integrates the IDCC-based and character-based statistical models to distinguish the word boundaries. The IDCC-based statistical model utilizes the IDCC feature to disambiguate any ambiguous words. The unknown words are handled using the character-based statistical model, based on the character features. In addition, linguistic knowledge is employed to detect the boundaries of a new word based on the construction morphology and NER. In evaluations, we compared the proposed method with various word segmentation methods. The experimental results showed that the proposed method performed slightly better than the other methods when the corpus size increased. Using the test set, the proposed method obtained the best F-measure of 92.19, an F-measure that was better than the IDCC longest matching grouping at 2.85.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {27},
numpages = {16},
keywords = {statistical model, Isarn Tham, Isarn Dharma, word segmentation approach}
}

@article{10.1145/3365679,
author = {Park, Cheoneum and Song, Heejun and Lee, Changki},
title = {S<sup>3</sup>-NET: SRU-Based Sentence and Self-Matching Networks for Machine Reading Comprehension},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365679},
doi = {10.1145/3365679},
abstract = {Machine reading comprehension question answering (MRC-QA) is the task of understanding the context of a given passage to find a correct answer within it. A passage is composed of several sentences; therefore, the length of the input sentence becomes longer, leading to diminished performance. In this article, we propose S3-NET, which adds sentence-based encoding to solve this problem. S3-NET, which is based on a simple recurrent unit architecture, is a deep learning model that solves the MRC-QA by applying matching network to sentence-level encoding. In addition, S3-NET utilizes self-matching networks to compute attention weight for its own recurrent neural network sequences. We perform MRC-QA for the SQuAD dataset of English and MindsMRC dataset of Korean. The experimental results show that for SQuAD, the S3-NET model proposed in this article produces 71.91% and 74.12% exact match and 81.02% and 82.34% F1 in single and ensemble models, respectively, and for MindsMRC, our model achieves 69.43% and 71.28% exact match and 81.53% and 82.77% F1 in single and ensemble models, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {35},
numpages = {14},
keywords = {hierarchical model, simple recurrent unit, question answering, sentence and self-matching network, Korean MRC-QA, Machine reading comprehension}
}

@article{10.1145/3377708,
author = {Liu, Xiao-Yang and Zhang, Yimeng and Liao, Yukang and Jiang, Ling},
title = {Dynamic Updating of the Knowledge Base for a Large-Scale Question Answering System},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377708},
doi = {10.1145/3377708},
abstract = {Today, the knowledge base question answering (KB-QA) system is promising to achieve a large-scale high-quality reply in the e-commerce industry. However, there exist two major challenges to efficiently support large-scale KB-QA systems. On the one hand, it is difficult to serve tens of thousands of online stores (i.e., constrained by the tuning and deployment time), and it would perform poorly if the systems start without a sufficient number of chat records. On the other hand, current KB-QA systems cannot be updated in an efficient way due to the high cost of knowledge base (KB) updating. In this article, we propose an automatic learning scheme for KB-QA systems, called ALKB-QA, using a vector modeling method to address the preceding two main challenges. The ALKB-QA system provides online stores with basic KB templates that are suitable for many common occasions, and this feature enables the ability to deploy chatbots for a large number of online stores in a short time. Then, the KBs are further updated automatically to adapt to their own businesses (meet different specific needs), leading to increased reply accuracy. Our work has three main contributions. First, the proposed ALKB-QA system has a good business model in the e-commerce industry (serving tens of thousands of online stores with low cost), breaking the scalability limitations of existing KB-QA systems. Second, we assess the reply accuracy of the proposed ALKB-QA system using human evaluations, and the results show that it outperforms human annotation-base approaches. Third, we launched our ALKB-QA system as a real-world business application, and it supports tens of thousands of online stores.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {45},
numpages = {13},
keywords = {Knowledge base question answering, knowledge base, automatic learning}
}

@article{10.1145/3377704,
author = {Du, Qianlong and Zong, Chengqing and Su, Keh-Yih},
title = {Conducting Natural Language Inference with Word-Pair-Dependency and Local Context},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377704},
doi = {10.1145/3377704},
abstract = {This article proposes to conduct natural language inference with novel Enhanced-Relation-Head-Dependent triplets (RHD triplets), which are constructed via enhancing each word in the RHD triplet with its associated local context. Most previous approaches based on deep neural network (DNN) for this task either perform token alignment without considering syntactic dependency among words, or directly use tree- LSTM to generate passage representation with irrelevant information. To improve token alignment and inference judgment with word-pair-dependency, the RHD triplet structure is first proposed. To avoid incorporating irrelevant information, this proposed approach performs comparison directly on each triplet-pair of the given passage-pair (instead of comparing each triplet in a passage with the content merged from the whole opposite passage). Furthermore, to take local context into consideration while conducting token alignment and inference judgment, we also enhance the words of the triplets with their associated local context to improve the performance. Experimental results show that the proposed approach is better than most previous approaches that adopt tree structures, and its performance is comparable to other state-of-the-art approaches (however, our approach is more human comprehensible).},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {47},
numpages = {23},
keywords = {textual entailment, word-pair dependency, syntactic relation, relation triplet, Natural language inference, local context}
}

@article{10.1145/3374212,
author = {Mi, Chenggang and Xie, Lei and Zhang, Yanning},
title = {Loanword Identification in Low-Resource Languages with Minimal Supervision},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3374212},
doi = {10.1145/3374212},
abstract = {Bilingual resources play a very important role in many natural language processing tasks, especially the tasks in cross-lingual scenarios. However, it is expensive and time consuming to build such resources. Lexical borrowing happens in almost every language. This inspires us to detect these loanwords effectively, and to use the “loanword (in receipt language)”-“donor word (in donor language)” to extend the bilingual resource for NLP tasks in low-resource languages. In this article, we propose a novel method to identify loanwords in Uyghur. The most important advantage of this method is that the model only relies on large amount of monolingual corpora and only a small scale of annotated data. Our loanword identification model includes two parts: loanword candidate generation and loanword prediction. In the first part, we use two large-scale monolingual corpora and a small bilingual dictionary to train a cross-lingual embedding model. Since semantic unrelated words often cannot be treated as loanword pairs, a loanword candidate list will be generated according to this model and a word list in Uyghur. In the second part, we predict from the preceding candidates based on a log-linear model that integrates several features such as pronunciation similarity, part-of-speech tags, and hybrid language modeling. To evaluate the effectiveness of our proposed method, we conduct two types of experiments: loanword identification and OOV translation. Experimental results showed that (1) our proposed method achieved significant F1 improvements compared to other models in all four loanword identification tasks in Uyghur, and (2) after extending the existing translation models with loanword identification results, OOV rates in several language pairs reduced significantly and the translation performance improved.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {43},
numpages = {22},
keywords = {Uyghur loanword, cross-lingual embedding, loanword identification, low-resource neural machine translation, out-of-vocabulary, pronunciation similarity}
}

@article{10.1145/3360016,
author = {Beseiso, Majdi and Elmousalami, Haytham},
title = {Subword Attentive Model for Arabic Sentiment Analysis: A Deep Learning Approach},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3360016},
doi = {10.1145/3360016},
abstract = {Social media data is unstructured data where these big data are exponentially increasing day to day in many different disciplines. Analysis and understanding the semantics of these data are a big challenge due to its variety and huge volume. To address this gap, unstructured Arabic texts have been studied in this work owing to their abundant appearance in social media Web sites. This work addresses the difficulty of handling unstructured social media texts, particularly when the data at hand is very limited. This intelligent data augmentation technique that handles the problem of less availability of data are used. This article has proposed a novel architecture for hand Arabic words classification and understands based on convolutional neural networks (CNNs) and recurrent neural networks. Moreover, the CNN technique is the most powerful for the analysis of Arabic tweets and social network analysis. The main technique used in this work is character-level CNN and a recurrent neural network stacked on top of one another as the classification architecture. These two techniques give 95% accuracy in the Arabic texts dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {29},
numpages = {17},
keywords = {gated recurrent unit, data augmentation, sentiment evaluation, Arabic, convolutional neural network, unstructured texts}
}

@article{10.1145/3359991,
author = {Abbas, Muhammad Raihan and Asif, Dr. Khadim Hussain},
title = {Punjabi to ISO 15919 and Roman Transliteration with Phonetic Rectification},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3359991},
doi = {10.1145/3359991},
abstract = {Transliteration removes the script barriers. Unfortunately, Punjabi is written in four different scripts, i.e., Gurmukhi, Shahmukhi, Devnagri, and Latin. The Latin script is understandable for nearly all factions of the Punjabi community. The objective of our work is to transliterate the Punjabi Gurmukhi script into Latin script. There has been considerable progress in Punjabi to Latin transliteration, but the accuracy of present-day systems is less than 50% (Google Translator has approximately 45% accuracy). We do not have the facility of a rich parallel corpus for Punjabi, so we cannot use the corpus-based techniques of machine learning that are in vogue these days. The existing systems of transliteration follow grapheme-based approach. The grapheme-based transliteration is unable to handle many scenarios such as tones, inherent schwa, glottal stops, nasalization, and gemination. In this article, the grapheme-based transliteration has been augmented with phonetic rectification where the Punjabi script is rectified phonetically before applying character-to-character mapping. Handling the inherent short vowel schwa was the major challenge in phonetic rectification. Instead of following the fixed syllabic pattern, we devised a generic finite state transducer to insert schwa. The accuracy of our transliteration system is approximately 96.82%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {28},
numpages = {20},
keywords = {computational linguistics, Punjabi, natural language processing, Transliteration, phonology}
}

@article{10.1145/3377407,
author = {Liu, Shih-Hung and Chen, Kuan-Yu and Chen, Berlin},
title = {Enhanced Language Modeling with Proximity and Sentence Relatedness Information for Extractive Broadcast News Summarization},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377407},
doi = {10.1145/3377407},
abstract = {The primary task of extractive summarization is to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important theme of the original document. Recently, language modeling (LM) has been proven to be a promising modeling framework for performing this task in an unsupervised manner. However, there still remain three fundamental challenges facing the existing LM-based methods, which we set out to tackle in this article. The first one is how to construct a more accurate sentence model in this framework without resorting to external sources of information. The second is how to take into account sentence-level structural relationships, in addition to word-level information within a document, for important sentence selection. The last one is how to exploit the proximity cues inherent in sentences to obtain a more accurate estimation of respective sentence models. Specifically, for the first and second challenges, we explore a novel, principled approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to render sentence-level structural relationships within the document, leading to better summarization effectiveness. For the third challenge, we investigate several formulations of proximity cues for use in sentence modeling involved in the LM-based summarization framework, free of the strict bag-of-words assumption. Furthermore, we also present various ensemble methods that seamlessly integrate proximity and sentence relatedness information into sentence modeling. Extensive experiments conducted on a Mandarin broadcast news summarization task show that such integration of proximity and sentence relatedness information is indeed beneficial for speech summarization. Our proposed summarization methods can significantly boost the performance of an LM-based strong baseline (e.g., with a maximum ROUGE-2 improvement of 26.7% relative) and also outperform several state-of-the-art unsupervised methods compared in the article.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {46},
numpages = {19},
keywords = {Extractive summarization, overlapped clustering, proximity information, language modeling}
}

@article{10.1145/3377851,
author = {Li, Yachao and Li, Junhui and Zhang, Min and Li, Yixin and Zou, Peng},
title = {Improving Neural Machine Translation with Linear Interpolation of a Short-Path Unit},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377851},
doi = {10.1145/3377851},
abstract = {In neural machine translation (NMT), the source and target words are at the two ends of a large deep neural network, normally mediated by a series of non-linear activations. The problem with such consequent non-linear activations is that they significantly decrease the magnitude of the gradient in a deep neural network, and thus gradually loosen the interaction between source words and their translations. As a result, a source word may be incorrectly translated into a target word out of its translational equivalents. In this article, we propose short-path units (SPUs) to strengthen the association of source and target words by allowing information flow over adjacent layers effectively via linear interpolation. In particular, we enrich three critical NMT components with SPUs: (1) an enriched encoding model with SPU, which interpolates source word embeddings linearly into source annotations; (2) an enriched decoding model with SPU, which enables the source context linearly flow to target-side hidden states; and (3) an enriched output model with SPU, which further allows linear interpolation of target-side hidden states into output states. Experimentation on Chinese-to-English, English-to-German, and low-resource Tibetan-to-Chinese translation tasks demonstrates that the linear interpolation of SPUs significantly improves the overall translation quality by 1.88, 1.43, and 3.75 BLEU, respectively. Moreover, detailed analysis shows that our approaches much strengthen the association of source and target words. From the preceding, we can see that our proposed model is effective both in rich- and low-resource scenarios.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {44},
numpages = {16},
keywords = {short-path units, Neural machine translation, low resource, neural networks}
}

@article{10.1145/3365916,
author = {Kim, Hyun and Na, Seung-Hoon},
title = {Uniformly Interpolated Balancing for Robust Prediction in Translation Quality Estimation: A Case Study of English-Korean Translation},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365916},
doi = {10.1145/3365916},
abstract = {There has been growing interest among researchers in quality estimation (QE), which attempts to automatically predict the quality of machine translation (MT) outputs. Most existing works on QE are based on supervised approaches using quality-annotated training data. However, QE training data quality scores readily become imbalanced or skewed: QE data are mostly composed of high translation quality sentence pairs but the data lack low translation quality sentence pairs. The use of imbalanced data with an induced quality estimator tends to produce biased translation quality scores with “high” translation quality scores assigned even to poorly translated sentences. To address the data imbalance, this article proposes a simple, efficient procedure called uniformly interpolated balancing to construct more balanced QE training data by inserting greater uniformness to training data. The proposed uniformly interpolated balancing procedure is based on the preparation of two different types of manually annotated QE data: (1) default skewed data and (2) near-uniform data. First, we obtain default skewed data in a naive manner without considering the imbalance by manually annotating qualities on MT outputs. Second, we obtain near-uniform data in a selective manner by manually annotating a subset only, which is selected from the automatically quality-estimated sentence pairs. Finally, we create uniformly interpolated balanced data by combining these two types of data, where one half originates from the default skewed data and the other half originates from the near-uniform data. We expect that uniformly interpolated balancing reflects the intrinsic skewness of the true quality distribution and manages the imbalance problem. Experimental results on an English-Korean quality estimation task show that the proposed uniformly interpolated balancing leads to robustness on both skewed and uniformly distributed quality test sets when compared to the test sets of other non-balanced datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {37},
numpages = {27},
keywords = {uniformly interpolated balancing, imbalanced data, Predictor-Estimator, Translation quality estimation}
}

@article{10.1145/3344920,
author = {Zhu, Wenhao and Jin, Xin and Liu, Shuang and Lu, Zhiguo and Zhang, Wu and Yan, Ke and Wei, Baogang},
title = {Enhanced Double-Carrier Word Embedding via Phonetics and Writing},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3344920},
doi = {10.1145/3344920},
abstract = {Word embeddings, which map words into a unified vector space, capture rich semantic information. From a linguistic point of view, words have two carriers, speech and writing. Yet the most recent word embedding models focus on only the writing carrier and ignore the role of the speech carrier in semantic expressions. However, in the development of language, speech appears before writing and plays an important role in the development of writing. For phonetic language systems, the written forms are secondary symbols of spoken ones. Based on this idea, we carried out our work and proposed double-carrier word embedding (DCWE). We used DCWE to conduct a simulation of the generation order of speech and writing. We trained written embedding based on phonetic embedding. The final word embedding fuses writing and phonetic embedding. To illustrate that our model can be applied to most languages, we selected Chinese, English, and Spanish as examples and evaluated these models through word similarity and text classification experiments.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {20},
numpages = {18},
keywords = {linguistic, Word embedding, phonetic embedding}
}

@article{10.1145/3345518,
author = {Xu, Ge and Yang, Xiaoyan and Cai, Yuanzheng and Ruan, Zhiqiang and Wang, Tao and Liao, Xiangwen},
title = {Extracting Polarity Shifting Patterns from Any Corpus Based on Natural Annotation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3345518},
doi = {10.1145/3345518},
abstract = {In recent years, online sentiment texts are generated by users in various domains and in different languages. Binary polarity classification (positive or negative) on business sentiment texts can help both companies and customers to evaluate products or services. Sometimes, the polarity of sentiment texts can be modified, making the polarity classification difficult. In sentiment analysis, such modification of polarity is termed as polarity shifting, which shifts the polarity of a sentiment clue (emotion, evaluation, etc.). It is well known that detection of polarity shifting can help improve sentiment analysis in texts. However, to detect polarity shifting in corpora is challenging: (1) polarity shifting is normally sparse in texts, making human annotation difficult; (2) corpora with dense polarity shifting are few; we may need polarity shifting patterns from various corpora.In this article, an approach is presented to extract polarity shifting patterns from any text corpus. For the first time, we proposed to select texts rich in polarity shifting by the idea of natural annotation, which is used to replace human annotation. With a sequence mining algorithm, the selected texts are used to generate polarity shifting pattern candidates, and then we rank them by C-value before human annotation. The approach is tested on different corpora and different languages. The results show that our approach can capture various types of polarity shifting patterns, and some patterns are unique to specific corpora. Therefore, for better performance, it is reasonable to construct polarity shifting patterns directly from the given corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {23},
numpages = {16},
keywords = {prior polarity, polarity shifting, Sentiment analysis, natural annotation, sequence mining}
}

@article{10.1145/3373268,
author = {Ding, Chenchen and Yee, Sann Su Su and Pa, Win Pa and Soe, Khin Mar and Utiyama, Masao and Sumita, Eiichiro},
title = {A Burmese (Myanmar) Treebank: Guideline and Analysis},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373268},
doi = {10.1145/3373268},
abstract = {A 20,000-sentence Burmese (Myanmar) treebank on news articles has been released under a CC BY-NC-SA license. Complete phrase structure annotation was developed for each sentence from the morphologically annotated data prepared in previous work of Ding et&nbsp;al. [1]. As the final result of the Burmese component in the Asian Language Treebank Project, this is the first large-scale, open-access treebank for the Burmese language. The annotation details and features of this treebank are presented.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {40},
numpages = {13},
keywords = {treebank, phrase structure, Burmese (Myanmar)}
}

@article{10.1145/3372246,
author = {Mirzaei, Azadeh and Sedghi, Fatemeh and Safari, Pegah},
title = {Semantic Role Labeling System for Persian Language},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3372246},
doi = {10.1145/3372246},
abstract = {In this article, we present an automatic semantic role labeling system in Persian consisting of two modules: argument identification for specifying argument spans and argument classification for categorizing their semantic roles. Our modules have been trained on Persian Proposition Bank in which predicate-argument information is manually added as a layer on top of Persian Dependency Treebank with about 30,000 sentences. Therefore, our system was trained on 216,871 verbal predicates and 42,386 nonverbal ones consisting of 40,813 nouns and 1,573 adjectives with 33 semantic classes. As a supervised method, we used maximum entropy for building an argument identifier that results in human-level accuracy of 99% and support vector machine for an argument classifier with an F1 of 84. Regarding both verbal and nonverbal predicates with an expanded role set, we achieved reasonable results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {39},
numpages = {12},
keywords = {Semantic role labeling, argument classification, Persian Proposition Bank, argument identification}
}

@article{10.1145/3372244,
author = {Zhou, Xiao and Ling, Zhen-Hua and Dai, Li-Rong},
title = {Learning and Modeling Unit Embeddings Using Deep Neural Networks for Unit-Selection-Based Mandarin Speech Synthesis},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3372244},
doi = {10.1145/3372244},
abstract = {A method of learning and modeling unit embeddings using deep neutral networks (DNNs) is presented in this article for unit-selection-based Mandarin speech synthesis. Here, a unit embedding is defined as a fixed-length embedding vector for a phone-sized unit candidate in a corpus. Modeling phone-sized embedding vectors instead of frame-sized acoustic features can better measure the long-term dependencies among consecutive units in an utterance. First, a DNN with an embedding layer is built to learn the embedding vectors of all unit candidates in the corpus from scratch. In order to enable the extracted embedding vectors to carry both acoustic and linguistic information of unit candidates, a multitarget learning strategy is designed for the DNN. Its optional prediction targets include frame-level acoustic features, unit durations, monophone and tone identifiers, and context classes. Then, another two DNNs are constructed to map linguistic features toward the extracted embedding vectors. One of them employs the unit vectors of preceding phones besides the linguistic features of current phone as its input. At synthesis time, the distances between the unit vectors predicted by these two DNNs and the ones derived from unit candidates are used as a part of the target cost and a part of the concatenation cost, respectively. Our experiments on a Mandarin speech synthesis corpus demonstrate that learning and modeling unit embeddings improve the naturalness of hidden Markov model (HMM)-based unit selection speech synthesis. Furthermore, integrating multiple targets for learning unit embeddings achieves better performance than using only acoustic targets according to our subjective evaluation results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {38},
numpages = {14},
keywords = {multitarget learning, hidden Markov model, unit embedding, Speech synthesis, deep neural network, unit selection}
}

@article{10.1145/3373608,
author = {Song, Hyun-Je and Park, Seong-Bae},
title = {Korean Part-of-Speech Tagging Based on Morpheme Generation},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373608},
doi = {10.1145/3373608},
abstract = {Two major problems of Korean part-of-speech (POS) tagging are that the word-spacing unit is not mapped one-to-one to a POS tag and that morphemes should be recovered during POS tagging. Therefore, this article proposes a novel two-step Korean POS tagger that solves the problems. This tagger first generates a sequence of lemmatized and recovered morphemes that can be mapped one-to-one to a POS tag using an encoder-decoder architecture derived from a POS-tagged corpus. Then, the POS tag of each morpheme in the generated sequence is finally determined by a standard sequence labeling method. Since the knowledge for segmenting and recovering morphemes is extracted automatically from a POS-tagged corpus by an encoder-decoder architecture, the POS tagger is constructed without a dictionary nor handcrafted linguistic rules. The experimental results on a standard dataset show that the proposed method outperforms existing POS taggers with its state-of-the-art performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {41},
numpages = {10},
keywords = {Part-of-speech tagging, morphologically complex languages, morpheme generation}
}

@article{10.1145/3365244,
author = {Yu, Zhiqiang and Yu, Zhengtao and Guo, Junjun and Huang, Yuxin and Wen, Yonghua},
title = {Efficient Low-Resource Neural Machine Translation with Reread and Feedback Mechanism},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365244},
doi = {10.1145/3365244},
abstract = {How to utilize information sufficiently is a key problem in neural machine translation (NMT), which is effectively improved in rich-resource NMT by leveraging large-scale bilingual sentence pairs. However, for low-resource NMT, lack of bilingual sentence pairs results in poor translation performance; therefore, taking full advantage of global information in the encoding-decoding process is effective for low-resource NMT. In this article, we propose a novel reread-feedback NMT architecture (RFNMT) for using global information. Our architecture builds upon the improved sequence-to-sequence neural network and consists of a double-deck attention-based encoder-decoder framework. In our proposed architecture, the information generated by the first-pass encoding and decoding process flows to the second-pass encoding process for more sufficient parameters initialization and information use. Specifically, we first propose a “reread” mechanism to transfer the outputs of the first-pass encoder to the second-pass encoder, and then the output is used for the initialization of the second-pass encoder. Second, we propose a “feedback” mechanism that transfers the first-pass decoder’s outputs to a second-pass encoder via an important weight model and an improved gated recurrent unit (GRU). Experiments on multiple datasets show that our approach achieves significant improvements over state-of-the-art NMT systems, especially in low-resource settings.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {34},
numpages = {13},
keywords = {neural machine translation, feedback, Low-resource, reread}
}

@article{10.1145/3365832,
author = {Sarwar, Raheem and Porthaveepong, Thanasarn and Rutherford, Attapol and Rakthanmanon, Thanawin and Nutanong, Sarana},
title = {<i>StyloThai:</i>: A Scalable Framework for Stylometric Authorship Identification of Thai Documents},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365832},
doi = {10.1145/3365832},
abstract = {Authorship identification helps to identify the true author of a given anonymous document from a set of candidate authors. The applications of this task can be found in several domains, such as law enforcement agencies and information retrieval. These application domains are not limited to a specific language, community, or ethnicity. However, most of the existing solutions are designed for English, and a little attention has been paid to Thai. These existing solutions are not directly applicable to Thai due to the linguistic differences between these two languages. Moreover, the existing solution designed for Thai is unable to (i) handle outliers in the dataset, (ii) scale when the size of the candidate authors set increases, and (iii) perform well when the number of writing samples for each candidate author is low. We identify a stylometric feature space for the Thai authorship identification task. Based on our feature space, we present an authorship identification solution that uses the probabilistic k nearest neighbors classifier by transforming each document into a collection of point sets. Specifically, this document transformation allows us to (i) use set distance measures associated with an outlier handling mechanism, (ii) capture stylistic variations within a document, and (iii) produce multiple predictions for a query document. We create a new Thai authorship identification corpus containing 547 documents from 200 authors, which is significantly larger than the corpus used by the existing study (an increase of 32 folds in terms of the number of candidate authors). The experimental results show that our solution can overcome the limitations of the existing solution and outperforms all competitors with an accuracy level of 91.02%. Moreover, we investigate the effectiveness of each stylometric features category with the help of an ablation study. We found that combining all categories of the stylometric features outperforms the other combinations. Finally, we cross compare the feature spaces and classification methods of all solutions. We found that (i) our solution can scale as the number of candidate authors increases, (ii) our method outperforms all the competitors, and (iii) our feature space provides better performance than the feature space used by the existing study.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {36},
numpages = {15},
keywords = {similarity search, stylometry, Authorship analysis, Thai authorship identification}
}

@article{10.1145/3345517,
author = {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader, Ikhlas},
title = {Wasf-Vec: Topology-Based Word Embedding for Modern Standard Arabic and Iraqi Dialect Ontology},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3345517},
doi = {10.1145/3345517},
abstract = {Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {22},
numpages = {27},
keywords = {phonology, Arabic language, words ontology, dialect, Topology, words classification, Word embedding, orthographic, morphology, words features, 2D visualizing, class-based language modeling}
}

@article{10.1145/3364319,
author = {Masmoudi, Abir and Khmekhem, Mariem Ellouze and Khrouf, Mourad and Belguith, Lamia Hadrich},
title = {Transliteration of Arabizi into Arabic Script for Tunisian Dialect},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3364319},
doi = {10.1145/3364319},
abstract = {The evolution of information and communication technology has markedly influenced communication between correspondents. This evolution has facilitated the transmission of information and has engendered new forms of written communication (email, chat, SMS, comments, etc.). Most of these messages and comments are written in Latin script, also called Arabizi. Moreover, the language used in social media and SMS messaging is characterized by the use of informal and non-standard vocabulary, such as repeated letters for emphasis, typos, non-standard abbreviations, and nonlinguistic content like emoticons. Since the Tunisian dialect suffers from the unavailability of basic tools and linguistic resources compared to Modern Standard Arabic, we resort to the use of these written sources as a starting point to build large corpora automatically. In the context of natural language processing and to benefit from these networks’ data, transliterating from Arabizi to Arabic script is a necessary step because most recently available tools for processing the Tunisian dialect expect Arabic script input. Indeed, the transliteration task can help construct and enrich parallel corpora and dictionaries for the Tunisian dialect and can be useful for developing various natural language processing applications such as sentiment analysis, opinion mining, topic detection, and machine translation. In this article, we focus on converting the Tunisian dialect text that is written in Latin script to Arabic script following the Conventional Orthography for Dialectal Arabic. Then, we propose two models to transliterate Arabizi into Arabic script for the Tunisian dialect, namely a rule-based model and a discriminative model as a sequence classification task based on conditional random fields). In the first model, we use a set of transliteration rules to convert the Tunisian dialect Arabizi texts to Arabic script. In the second model, transliteration is performed both at word and character levels. In the end, our models got a character error rate of 10.47%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {32},
numpages = {21},
keywords = {Arabizi corpus, CRF model, diacritization, rule-based approach, Natural language processing, transliteration, Tunisian dialect}
}

@article{10.1145/3342356,
author = {Harikrishna, D. M. and Rao, K. Sreenivasa},
title = {Children’s Story Classification in Indian Languages Using Linguistic and Keyword-Based Features},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342356},
doi = {10.1145/3342356},
abstract = {The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend. In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end. We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {30},
numpages = {22},
keywords = {linguistic features, story classification, sparse representation, keyword features, text-to-speech, K-nearest neighbour, support vector machines, linear discriminant analysis, latent semantic analysis, vector space model, naive Bayes}
}

@article{10.1145/3357612,
author = {Jung, Hun-Young and Lee, Jong-Hyeok and Min, Eunju and Na, Seung-Hoon},
title = {Word Reordering for Translation into Korean Sign Language Using Syntactically-Guided Classification},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3357612},
doi = {10.1145/3357612},
abstract = {Machine translation aims to break the language barrier that prevents communication with others and increase access to information. Deaf people face huge language barriers in their daily lives, including access to digital and spoken information. There are very few digital resources for sign language processing. In this article, we present a transfer-based machine translation system for translating Korean-to-Korean Sign Language (KSL) glosses, mainly composed of (1) dictionary-based lexical transfer and (2) a hybrid syntactic transfer based on a data-driven model. In particular, we formulate complicated word reordering problems in syntactic transfer as multi-class classification tasks and propose “syntactically guided” data-driven syntactic transfer. The core part of our study is a neural classification model for reordering order-important constituent pairs with a reordering task that is newly designed for Korean-to-KSL translation. The experiment results evaluated on news transcript data show that the proposed system achieves a BLEU score of 0.512 and a RIBES score of 0.425, significantly improving upon the baseline system performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {31},
numpages = {20},
keywords = {neural networks, Sign language, word reordering, machine translation, sentence embedding}
}

@article{10.1145/3364533,
author = {Mukherjee, Subham and Kumar, Pradeep and Roy, Partha Pratim},
title = {Fusion of Spatio-Temporal Information for Indic Word Recognition Combining Online and Offline Text Data},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3364533},
doi = {10.1145/3364533},
abstract = {We present a novel Indic handwritten word recognition scheme by fusion of spatio-temporal information extracted from handwritten images. The main challenge in Indic word recognition lies in its complexity because of modifiers, touching characters, and compound characters. Hidden Markov Models (HMMs) are being used to model such data due to their ability to learn sequential data, however, the recognition performance is not satisfactory. We propose here a Long Short-Term Memory (LSTM)-based architecture for offline Indic word recognition. Offline recognition methods usually involve spatial data, whereas it has been observed that online recognition schemes show better performance than the offline methodologies. Online information usually refers to the temporal information obtained from the strokes of the pen tip while writing, which is missing in offline word images. In this article, an effort has been made to extract the online temporal information from offline images using stroke recovery and later it is combined with spatial information in LSTM architecture. During recognition, the character models are trained using both offline and extracted pseudo-online handwritten data separately. Finally, a novel fusion scheme has been used to combine them together. From the experiment, it is noted that recognition performance of handwritten Indic words improves considerably due to the fusion scheme of spatial and temporal data.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {33},
numpages = {24},
keywords = {Fusion, LSTM, Indic Word Recognition, Offline to Online Conversion}
}

@article{10.1145/3359988,
author = {Gupta, Deepak and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {A Deep Neural Network Framework for English Hindi Question Answering},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3359988},
doi = {10.1145/3359988},
abstract = {In this article, we propose a unified deep neural network framework for multilingual question answering (QA). The proposed network deals with the multilingual questions and answers snippets. The input to the network is a pair of factoid question and snippet in the multilingual environment (English and Hindi), and output is the relevant answer from the snippet. We begin by generating the snippet using a graph-based language-independent algorithm, which exploits the lexico-semantic similarity between the sentences. The soft alignment of the question words from the English and Hindi languages has been used to learn the shared representation of the question. The learned shared representation of question and attention-based snippet representation are passed as an input to the answer extraction layer of the network, which extracts the answer span from the snippet. Evaluation on a standard multilingual QA dataset shows the state-of-the-art performance with 39.44 Exact Match (EM) and 44.97 F1 values. Similarly, we achieve the performance of 50.11 Exact Match (EM) and 53.77 F1 values on Translated SQuAD dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {25},
numpages = {22},
keywords = {neural networks, snippet generation, attention mechanism, character embedding, gated recurrent units, Question answering, low-resourced languages}
}

@article{10.1145/3358414,
author = {Yu, Hongfei and Zhou, Xiaoqing and Duan, Xiangyu and Zhang, Min},
title = {Layer-Wise De-Training and Re-Training for ConvS2S Machine Translation},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3358414},
doi = {10.1145/3358414},
abstract = {The convolutional sequence-to-sequence (ConvS2S) machine translation system is one of the typical neural machine translation (NMT) systems. Training the ConvS2S model tends to get stuck in a local optimum in our pre-studies. To overcome this inferior behavior, we propose to de-train a trained ConvS2S model in a mild way and retrain to find a better solution globally. In particular, the trained parameters of one layer of the NMT network are abandoned by re-initialization while other layers’ parameters are kept at the same time to kick off re-optimization from a new start point and safeguard the new start point not too far from the previous optimum. This procedure is executed layer by layer until all layers of the ConvS2S model are explored. Experiments show that when compared to various measures for escaping from the local optimum, including initialization with random seeds, adding perturbations to the baseline parameters, and continuing training (con-training) with the baseline models, our method consistently improves the ConvS2S translation quality across various language pairs and achieves better performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {26},
numpages = {15},
keywords = {ConvS2S, neural machine translation, local optimum}
}

@article{10.1145/3341726,
author = {Imankulova, Aizhan and Sato, Takayuki and Komachi, Mamoru},
title = {Filtered Pseudo-Parallel Corpus Improves Low-Resource Neural Machine Translation},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3341726},
doi = {10.1145/3341726},
abstract = {Large-scale parallel corpora are essential for training high-quality machine translation systems; however, such corpora are not freely available for many language translation pairs. Previously, training data has been augmented by pseudo-parallel corpora obtained by using machine translation models to translate monolingual corpora into the source language. However, in low-resource language pairs, in which only low-accurate machine translation systems can be used, translation quality degrades when a pseudo-parallel corpus is naively used. To improve machine translation performance with low-resource language pairs, we propose a method to effectively expand the training data via filtering the pseudo-parallel corpus using quality estimation based on sentence-level round-trip translation. For experiments with three language pairs that utilized small, medium, and large size parallel corpora, BLEU scores significantly improved for low-resource language pairs. Additionally, the effects of iterative bootstrapping on translation performance quality is investigated; resultingly, it is confirmed that bootstrapping can further improve the translation performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {24},
numpages = {16},
keywords = {sentence-level similarity metrics, Pseudo-parallel corpus, round-trip translation, filtering, low-resource language pairs, bootstrapping}
}

@article{10.1145/3342351,
author = {Srivastava, Jyoti and Sanyal, Sudip and Srivastava, Ashish Kumar},
title = {An Automatic and a Machine-Assisted Method to Clean Bilingual Corpus},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342351},
doi = {10.1145/3342351},
abstract = {Two different methods of corpus cleaning are presented in this article. One is a machine-assisted technique, which is good to clean small-sized parallel corpus, and the other is an automatic method, which is suitable for cleaning large-sized parallel corpus. A baseline SMT (MOSES) system is used to evaluate these methods. The machine-assisted technique used two features: word alignment and length of the source and target language sentence. These features are used to detect mistranslations in the corpus, which are then handled by a human translator. Experiments of this method are conducted on the English-to-Indian Language Machine Translation (EILMT) corpus (English-Hindi). The Bilingual Evaluation Understudy (BLEU) score is improved by 0.47% for the clean corpus. Automatic method of corpus cleaning uses a combination of two features. One feature is length of source and target language sentence and the second feature is Viterbi alignment score generated by Hidden Markov Model for each sentence pair. Two different threshold values are used for these two features. These values are decided by using a small-sized manually annotated parallel corpus of 206 sentence pairs. Experiments of this method are conducted on the HindEnCorp corpus, released in the workshop of the Association of Computational Linguistics (ACL 2014). The BLEU score is improved by 0.6% on clean corpus. A comparison of the two methods is also presented on EILMT corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {13},
numpages = {19},
keywords = {Bilingual Corpus Cleaning, Statistical Machine Translation}
}

@article{10.1145/3345627,
author = {Dehkharghani, Rahim},
title = {SentiFars: A Persian Polarity Lexicon for Sentiment Analysis},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3345627},
doi = {10.1145/3345627},
abstract = {There is no doubt about the usefulness of public opinion toward different issues in social media and the World Wide Web. Extracting the feelings of people about an issue from text is not straightforward. Polarity lexicons that assign polarity tags or scores to words and phrases play an important role in sentiment analysis systems. As English is the richest language in this area, getting benefits from existing English resources in order to build new ones has attracted the interest of many researchers in recent years. In this article, we propose a new translation-based approach for building polarity resources in resource-lean languages such as Persian. The results of empirical evaluation of the proposed approach prove its effectiveness. The generated resource is the largest publicly available polarity lexicon for Persian.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {21},
numpages = {12},
keywords = {Sentiment analysis, polarity lexicon, classifier combination, translation, polarity extraction}
}

@article{10.1145/3344788,
author = {Zhou, Guangyou and Fang, Yizhen and Peng, Yehong and Lu, Jiaheng},
title = {Neural Conversation Generation with Auxiliary Emotional Supervised Models},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3344788},
doi = {10.1145/3344788},
abstract = {An important aspect of developing dialogue agents involves endowing a conversation system with emotion perception and interaction. Most existing emotion dialogue models lack the adaptability and extensibility of different scenes because of their limitation to require a specified emotion category or their reliance on a fixed emotional dictionary. To overcome these limitations, we propose a neural conversation generation with auxiliary emotional supervised model (nCG-ESM) comprising a sequence-to-sequence (Seq2Seq) generation model and an emotional classifier used as an auxiliary model. The emotional classifier was trained to predict the emotion distributions of the dialogues, which were then used as emotion supervised signals to guide the generation model to generate diverse emotional responses. The proposed nCG-ESM is flexible enough to generate responses with emotional diversity, including specified or unspecified emotions, which can be adapted and extended to different scenarios. We conducted extensive experiments on the popular dataset of Weibo post--response pairs. Experimental results showed that the proposed model was capable of producing more diverse, appropriate, and emotionally rich responses, yielding substantial gains in diversity scores and human evaluations.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {19},
numpages = {17},
keywords = {sequence-to-sequence model, Neural conversation, natural language processing}
}

@article{10.1145/3343258,
author = {Zhu, Qingfu and Zhang, Weinan and Cui, Lei and Liu, Ting},
title = {Order-Sensitive Keywords Based Response Generation in Open-Domain Conversational Systems},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3343258},
doi = {10.1145/3343258},
abstract = {External keywords are crucial for response generation models to address the generic response problems in open-domain conversational systems. The occurrence of keywords in a response depends heavily on the order of the keywords as they are generated sequentially. Meanwhile, the order of keywords also affects the semantics of a response. Previous keywords based methods mainly focus on the composite of keywords, while the order of keywords has not been sufficiently discussed. In this work, we propose an order-sensitive keywords based model to explore the influence of the order of keywords in open-domain response generation. It automatically inferences the most suitable order that is optimized to generate a natural and relevant response, and subsequently generates the response using the ordered keywords as building blocks. We conducted experiments on a public Twitter dataset and the results show that our approach outperforms the state-of-the-art baselines in both automatic and human evaluations.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {18},
numpages = {18},
keywords = {sequence-to-sequence, conversational system, Order sensitive, response generation}
}

@article{10.1145/3342482,
author = {Ji, Yatu and Hou, Hongxu and Chen, Junjie and Wu, Nier},
title = {Adversarial Training for Unknown Word Problems in Neural Machine Translation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342482},
doi = {10.1145/3342482},
abstract = {Nearly all of the work in neural machine translation (NMT) is limited to a quite restricted vocabulary, crudely treating all other words the same as an &lt; unk&gt; symbol. For the translation of language with abundant morphology, unknown (UNK) words also come from the misunderstanding of the translation model to the morphological changes. In this study, we explore two ways to alleviate the UNK problem in NMT: a new generative adversarial network (added value constraints and semantic enhancement) and a preprocessing technique that mixes morphological noise. The training process is like a win-win game in which the players are three adversarial sub models (generator, filter, and discriminator). In this game, the filter is to emphasize the discriminator’s attention to the negative generations that contain noise and improve the training efficiency. Finally, the discriminator cannot easily discriminate the negative samples generated by the generator with filter and human translations. The experimental results show that the proposed method significantly improves over several strong baseline models across various language pairs and the newly emerged Mongolian-Chinese task is state-of-the-art.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {17},
numpages = {12},
keywords = {generative adversarial network, Neural machine translation, value iteration, UNK}
}

@article{10.1145/3329709,
author = {Mehmood, Khawar and Essam, Daryl and Shafi, Kamran and Malik, Muhammad Kamran},
title = {Sentiment Analysis for a Resource Poor Language—Roman Urdu},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3329709},
doi = {10.1145/3329709},
abstract = {Sentiment analysis is an important sub-task of Natural Language Processing that aims to determine the polarity of a review. Most of the work done on sentiment analysis is for the resource-rich languages of the world, but very limited work has been done on resource-poor languages. In this work, we focus on developing a Sentiment Analysis System for Roman Urdu, which is a resource-poor language. To this end, a dataset of 11,000 reviews has been gathered from six different domains. Comprehensive annotation guidelines were defined and the dataset was annotated using the multi-annotator methodology. Using the annotated dataset, state-of-the-art algorithms were used to build a sentiment analysis system. To improve the results of these algorithms, four different studies were carried out based on: word-level features, character level features, and feature union. The best results showed that we could reduce the error rate by 12% from the baseline (80.07%). Also, to see if the improvements are statistically significant, we applied t-test and Confidence Interval on the obtained results and found that the best results of each study are statistically significant from the baseline.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {10},
numpages = {15},
keywords = {Resource poor language, Roman Urdu, Roman Urdu sentiment analysis}
}

@article{10.1145/3342352,
author = {Prakash, Jeena J. and Rajan, Golda Brunet and Murthy, Hema A.},
title = {Importance of Signal Processing Cues in Transcription Correction for Low-Resource Indian Languages},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342352},
doi = {10.1145/3342352},
abstract = {Accurate phonetic transcriptions are crucial for building robust acoustic models for speech recognition as well as speech synthesis applications. Phonetic transcriptions are not usually provided with speech corpora. A lexicon is used to generate phone-level transcriptions of speech corpora with sentence-level transcriptions. When lexical entries are not available, letter-to-sound (LTS) rules are used. Whether it is a lexicon or LTS, the rules for pronunciation are generic and may not match the spoken utterance. This can lead to transcription errors. The objective of this study is to address the issue of mismatch between the transcription and its acoustic realisation. In particular, the issue of vowel deletions is studied. Group-delay-based segmentation is used to determine insertion/deletion of vowels in the speech utterance. The transcriptions are corrected in the training data based on this. The corrected data are used in automatic speech recognition (ASR) and text to speech synthesis (TTS) systems. ASR and TTS systems built with the corrected transcriptions show improvements in the performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {14},
numpages = {26},
keywords = {automatic speech recognition, Transcription mismatch errors, signal processing cue based on group delay, text-to-speech synthesis, hidden Markov model-forced Viterbi alignment}
}

@article{10.1145/3342354,
author = {Chakrabarty, Abhisek and Chaturvedi, Akshay and Garain, Utpal},
title = {NeuMorph: Neural Morphological Tagging for Low-Resource Languages—An Experimental Study for Indic Languages},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342354},
doi = {10.1145/3342354},
abstract = {This article deals with morphological tagging for low-resource languages. For this purpose, five Indic languages are taken as reference. In addition, two severely resource-poor languages, Coptic and Kurmanji, are also considered. The task entails prediction of the morphological tag (case, degree, gender, etc.) of an in-context word. We hypothesize that to predict the tag of a word, considering its longer context such as the entire sentence is not always necessary. In this light, the usefulness of convolution operation is studied resulting in a convolutional neural network (CNN) based morphological tagger. Our proposed model (BLSTM-CNN) achieves insightful results in comparison to the present state-of-the-art. Following the recent trend, the task is carried out under three different settings: single language, across languages, and across keys. Whereas the previous models used only character-level features, we show that the addition of word vectors along with character-level embedding significantly improves the performance of all the models. Since obtaining high-quality word vectors for resource-poor languages remains a challenge, in that scenario, the proposed character-level BLSTM-CNN proves to be most effective.1},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {16},
numpages = {19},
keywords = {Indic languages, Tagging, convolutional neural network, multitask learning, recurrent neural network}
}

@article{10.1145/3329707,
author = {Wang, Wei and Huang, Degen and Cao, Jingxiang},
title = {Chinese Syntax Parsing Based on Sliding Match of Semantic String},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3329707},
doi = {10.1145/3329707},
abstract = {Different from the current syntax parsing based on deep learning, we present a novel Chinese parsing method, which is based on Sliding Match of Semantic String (SMOSS). (1) Training stage: In a treebank, headwords of tree nodes are represented by semantic codes given in the Synonym Dictionary (Tongyici Cilin). N-gram semantic templates are extracted from every layer of a syntax tree by means of sliding window to establish one N-gram semantic template library. (2) Parsing stage: Words of a sentence, including headwords of chunks, are represented by the semantic codes from Tongyici Cilin. With the sliding window method, N-gram semantic code strings are extracted to match with the templates in the N-gram semantic template library; subsequently, the mapping information of the matched templates is employed to guide the chunking of semantic code strings. The Chinese syntax parsing is completed through continuous matching and chunking. On the same training scale, N-gram semantic template can create favorable conditions for flexible matching and improve the syntax parsing performance. With train and test sets from the Tsinghua Chinese Treebank (TCT), the results are F1-score 99.71% (closed test) and F1-score 70.43% (open test), respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {7},
numpages = {14},
keywords = {semantic code template, Chinese parsing, SMOSS, sliding match of semantic string}
}

@article{10.1145/3329713,
author = {Bakhshaei, Somayeh and Safabakhsh, Reza and Khadivi, Shahram},
title = {Matching Graph, a Method for Extracting Parallel Information from Comparable Corpora},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3329713},
doi = {10.1145/3329713},
abstract = {Comparable corpora are valuable alternatives for the expensive parallel corpora. They comprise informative parallel fragments that are useful resources for different natural language processing tasks. In this work, a generative model is proposed for efficient extraction of parallel fragments from a pair of comparable documents. The core of the proposed model is a graph called the Matching Graph. The ability of the Matching Graph to be trained on a small initial seed makes it a proper model for language pairs suffering from the scarce resource problem. Experiments show that the Matching Graph performs significantly better than other recently published models. According to the experiments on English-Persian and Arabic-Persian language pairs, the extracted parallel fragments can be used instead of parallel data for training statistical machine translation systems. Results reveal that the extracted fragments in the best case are able to retrieve about 90% of the information of a statistical machine translation system that is trained on a parallel corpus. Moreover, it is shown that using the extracted fragments as additional information for training statistical machine translation systems leads to an improvement of about 2% for English-Persian and about 1% for Arabic-Persian translation on BLEU score.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {11},
numpages = {29},
keywords = {Information extraction, Persian, statistical machine translation, generative model, and Arabic languages, natural language processing, English, parallel fragments, comparable corpora}
}

@article{10.1145/3342353,
author = {Han, Dong and Li, Junhui and Li, Yachao and Zhang, Min and Zhou, Guodong},
title = {Explicitly Modeling Word Translations in Neural Machine Translation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342353},
doi = {10.1145/3342353},
abstract = {In this article, we show that word translations can be explicitly incorporated into NMT effectively to avoid wrong translations. Specifically, we propose three cross-lingual encoders to explicitly incorporate word translations into NMT: (1) Factored encoder, which encodes a word and its translation in a vertical way; (2) Gated encoder, which uses a gated mechanism to selectively control the amount of word translations moving forward; and (3) Mixed encoder, which stitchingly learns a word and its translation annotations over sequences where words and their translations are alternatively mixed. Besides, we first use a simple word dictionary approach and then a word sense disambiguation (WSD) approach to effectively model the word context for better word translation. Experimentation on Chinese-to-English translation demonstrates that all proposed encoders are able to improve the translation accuracy for both traditional RNN-based NMT and recent self-attention-based NMT (hereafter referred to as Transformer). Specifically, Mixed encoder yields the most significant improvement of 2.0 in BLEU on the RNN-based NMT, while Gated encoder improves 1.2 in BLEU on Transformer. This indicates the usefulness of an WSD approach in modeling word context for better word translation. This also indicates the effectiveness of our proposed cross-lingual encoders in explicitly modeling word translations to avoid wrong translations in NMT. Finally, we discuss in depth how word translations benefit different NMT frameworks from several perspectives.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {15},
numpages = {17},
keywords = {word sense disambiguation, cross-lingual encoder, Neural machine translation, word translation}
}

@article{10.1145/3314935,
author = {Rudra, Koustav and Sharma, Ashish and Bali, Kalika and Choudhury, Monojit and Ganguly, Niloy},
title = {Identifying and Analyzing Different Aspects of English-Hindi Code-Switching in Twitter},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314935},
doi = {10.1145/3314935},
abstract = {Code-switching or the juxtaposition of linguistic units from two or more languages in a single utterance, has, in recent times, become very common in text, thanks to social media and other computer mediated forms of communication. In this exploratory study of English-Hindi code-switching on Twitter, we automatically create a large corpus of code-switched tweets and devise techniques to identify the relationship between successive components in a code-switched tweet. More specifically, we identify pragmatic functions such as narrative-evaluative, negative reinforcement, translation or semantically equivalent statements, and so on characterizing the relation between successive components. We analyze the difference/similarity between switching patterns in code-switched and monolingual multi-component tweets. We observe strong dominance of narrative-evaluative (non-opinion to opinion or vice versa) switching in case of both code-switched and monolingual multi-component tweets in around 40% of cases. Polarity switching appears to be a prevalent switching phenomenon (10%) specifically in code-switched tweets (three to four times higher than monolingual multi-component tweets) where preference of expressing negative sentiment in Hindi is approximately twice compared to English. Positive reinforcement appears to be an important pragmatic function for English multi-component tweets, whereas negative reinforcement plays a key role for Devanagari multi-component tweets. Our results also indicate that the extent and nature of code-switching also strongly depend on the topic (sports, politics, etc.) of discussion.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {29},
numpages = {28},
keywords = {pragmatic function, opinion and sentiment detection, code-switching, multi-component tweets, Twitter}
}

@article{10.1145/3341110,
author = {Liu, Dayiheng and Xue, Yang and He, Feng and Chen, Yuanyuan and Lv, Jiancheng},
title = {μ-Forcing: Training Variational Recurrent Autoencoders for Text Generation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3341110},
doi = {10.1145/3341110},
abstract = {It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {12},
numpages = {17},
keywords = {variational recurrent autoencoders, Variational autoencoders, uninformative latent variables issues}
}

@article{10.1145/3326497,
author = {Liu, Yijia and Che, Wanxiang and Wang, Yuxuan and Zheng, Bo and Qin, Bing and Liu, Ting},
title = {Deep Contextualized Word Embeddings for Universal Dependency Parsing},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3326497},
doi = {10.1145/3326497},
abstract = {Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem—universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and dependency parser, and it leads to consistent performance improvements. Experimental results show the model using ELMo outperforms the state-of-the-art baseline by an average of 0.91 for POS tagging and 1.11 for dependency parsing. Further analysis reveals that the improvements mainly result from the ELMo’s better abstraction ability on the out-of-vocabulary (OOV) words, and the character-level word representation in ELMo contributes a lot to the abstraction. Based on ELMo’s advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {9},
numpages = {17},
keywords = {visualization, deep contextualized word embeddings, universal dependency parsing, POS tagging, Natural language processing, out-of-vocabulary words}
}

@article{10.1145/3297278,
author = {Masmoudi, Abir and Mdhaffar, Salima and Sellami, Rahma and Belguith, Lamia Hadrich},
title = {Automatic Diacritics Restoration for Tunisian Dialect},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3297278},
doi = {10.1145/3297278},
abstract = {Modern Standard Arabic, as well as Arabic dialect languages, are usually written without diacritics. The absence of these marks constitute a real problem in the automatic processing of these data by NLP tools. Indeed, writing Arabic without diacritics introduces several types of ambiguity. First, a word without diacratics could have many possible meanings depending on their diacritization. Second, undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology [12]. In fact, the agglutination property of Arabic might produce a problem that can only be resolved using diacritics. Third, without diacritics a word could have many possible parts of speech (POS) instead of one. This is the case with the words that have the same spelling and POS tag but a different lexical sense, or words that have the same spelling but different POS tags and lexical senses [8]. Finally, there is ambiguity at the grammatical level (syntactic ambiguity). In this article, we propose the first work that investigates the automatic diacritization of Tunisian Dialect texts. We first describe our annotation guidelines and procedure. Then, we propose two major models, namely a statistical machine translation (SMT) and a discriminative model as a sequence classification task based on Conditional Random Fields (CRF). In the second approach, we integrate POS features to influence the generation of diacritics. Diacritics restoration was performed at both the word and the character levels. The results showed high scores of automatic diacritization based on the CRF system (Word Error Rate (WER) 21.44% for CRF and WER 34.6% for SMT).},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {28},
numpages = {18},
keywords = {Natural language processing, CRF model, SMT model, diacritization, POS tagging, Tunisian dialect}
}

@article{10.1145/3329710,
author = {Kanwal, Safia and Malik, Kamran and Shahzad, Khurram and Aslam, Faisal and Nawaz, Zubair},
title = {Urdu Named Entity Recognition: Corpus Generation and Deep Learning Applications},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3329710},
doi = {10.1145/3329710},
abstract = {Named Entity Recognition (NER) plays a pivotal role in various natural language processing tasks, such as machine translation and automatic question-answering systems. Recognizing the importance of NER, a plethora of NER techniques for Western and Asian languages have been developed. However, despite having over 490 million Urdu language speakers worldwide, NER resources for Urdu are either non-existent or inadequate. To fill this gap, this article makes four key contributions. First, we have developed the largest Urdu NER corpus, which contains 926,776 tokens and 99,718 carefully annotated NEs. The developed corpus has at least doubled the number of manually tagged NEs as compared to any of the existing Urdu NER corpora. Second, we have generated six new word embeddings using three different techniques, fastText, Word2vec, and Glove, on two corpora of Urdu text. These are the only publicly available embeddings for the Urdu language, besides the recently released Urdu word embeddings by Facebook. Third, we have pioneered in the application of deep learning techniques, NN and RNN, for Urdu named entity recognition. Finally, we have performed 10-folds of 32 different experiments using the combinations of a traditional supervised learning and deep learning techniques, seven types of word embeddings, and two different Urdu NER datasets. Based on the analysis of the results, several valuable insights are provided about the effectiveness of deep learning techniques, the impact of word embeddings, and variations of datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {8},
numpages = {13},
keywords = {Urdu NER corpus, Word2vec, fastText, word embeddings, Resource poor languages, deep learning}
}

@article{10.1145/3314936,
author = {Trieu, Hai-Long and Tran, Duc-Vu and Ittoo, Ashwin and Nguyen, Le-Minh},
title = {Leveraging Additional Resources for Improving Statistical Machine Translation on Asian Low-Resource Languages},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314936},
doi = {10.1145/3314936},
abstract = {Phrase-based machine translation (MT) systems require large bilingual corpora for training. Nevertheless, such large bilingual corpora are unavailable for most language pairs in the world, causing a bottleneck for the development of MT. For the Asian language pairs—Japanese, Indonesian, Malay paired with Vietnamese—they are also not excluded from the case, in which there are no large bilingual corpora on these low-resource language pairs. Furthermore, although the languages are widely used in the world, there is no prior work on MT, which causes an issue for the development of MT on these languages. In this article, we conducted an empirical study of leveraging additional resources to improve MT for the Asian low-resource language pairs: translation from Japanese, Indonesian, and Malay to Vietnamese. We propose an innovative approach that lies in two strategies of building bilingual corpora from comparable data and phrase pivot translation on existing bilingual corpora of the languages paired with English. Bilingual corpora were built from Wikipedia bilingual titles to enhance bilingual data for the low-resource languages. Additionally, we introduced a combined model of the additional resources to create an effective solution to improve MT on the Asian low-resource languages. Experimental results show the effectiveness of our systems with the improvement of +2 to +7 BLEU points. This work contributes to the development of MT on low-resource languages, especially opening a promising direction for the progress of MT on the Asian language pairs.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {32},
numpages = {22},
keywords = {sentence alignment, low-resource languages, semantic similarity, Statistical machine translation, pivot methods}
}

@article{10.1145/3312573,
author = {Yu, Hui and Xu, Weizhi and Lin, Shouxun and Liu, Qun},
title = {Machine Translation Evaluation Metric Based on Dependency Parsing Model},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3312573},
doi = {10.1145/3312573},
abstract = {Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures cannot represent all the information in the trees because their lengths are limited. To sufficiently use the reference syntax information, a new automatic evaluation metric is proposed based on the dependency parsing model. First, a dependency parsing model is trained using the reference dependency tree for each sentence. Then, the hypothesis is parsed by this dependency parsing model and the corresponding hypothesis dependency tree is generated. The quality of hypothesis can be judged by the quality of the hypothesis dependency tree. Unigram F-score is included in the new metric so that lexicon similarity is obtained. According to experimental results, the proposed metric can perform better than METEOR and BLEU on system level and get comparable results with METEOR on sentence level. To further improve the performance, we also propose a combined metric which gets the best performance on the sentence level and on the system level.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {44},
numpages = {15},
keywords = {machine translation, Automatic evaluation metric, dependency parsing model}
}

@article{10.1145/3325886,
author = {Das, Ayan and Sarkar, Sudeshna},
title = {Transform, Combine, and Transfer: Delexicalized Transfer Parser for Low-Resource Languages},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3325886},
doi = {10.1145/3325886},
abstract = {Transfer parsing has been used for developing dependency parsers for languages with no treebank by using transfer from treebanks of other languages (source languages). In delexicalized transfer, parsed words are replaced by their part-of-speech tags. Transfer parsing may not work well if a language does not follow uniform syntactic structure with respect to its different constituent patterns. Earlier work has used information derived from linguistic databases to transform a source language treebank to reduce the syntactic differences between the source and the target languages.We propose a transformation method where a source language pattern is transformed stochastically to one of the multiple possible patterns followed in the target language. The transformed source language treebank can be used to train a delexicalized parser in the target language. We show that this method significantly improves the average performance of single-source delexicalized transfer parsers.We also show that, in the multi-source settings, parsers trained using a concatenation of transformed source language treebanks work better when a subset of the source language treebanks is used rather than concatenating all of them or only one.However, the problem of selecting the subset of treebanks whose combination gives the best-performing parser from the set of all the available treebanks is hard. We propose a greedy selection heuristic based on the labelled attachment scores of the corresponding single-source parsers trained using the treebanks after transformation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {4},
numpages = {30},
keywords = {delexicalization, Transfer parsing, cross-lingual transfer parsing, syntax, dependency parsing}
}

@article{10.1145/3321129,
author = {Kong, Fang and Zhang, Min and Zhou, Guodong},
title = {Chinese Zero Pronoun Resolution: A Chain-to-Chain Approach},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321129},
doi = {10.1145/3321129},
abstract = {Chinese zero pronoun (ZP) resolution plays a critical role in discourse analysis. Different from traditional mention-to-mention approaches, this article proposes a chain-to-chain approach to improve the performance of ZP resolution in three aspects. First, consecutive ZPs are clustered into coreferential chains, each working as one independent anaphor as a whole. In this way, those ZPs far away from their overt antecedents can be bridged via other consecutive ZPs in the same coreferential chains and thus better resolved. Second, common noun phrases (NPs) are automatically grouped into coreferential chains using traditional approaches, each working as one independent antecedent candidate as a whole. That is, those NPs occurring in the same coreferential chain are viewed as one antecedent candidate as a whole, and ZP resolution is made between ZP coreferential chains and common NP coreferential chains. In this way, the performance can be much improved due to the effective reduction of the search space by pruning singletons and negative instances. Third and finally, additional features from ZP and common NP coreferential chains are employed to better represent anaphors and their antecedent candidates, respectively. Comprehensive experiments on the OntoNotes V5.0 corpus show that our chain-to-chain approach significantly outperforms the state-of-the-art mention-to-mention approaches. To our knowledge, this is the first work to resolve zero pronouns in a chain-to-chain way.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {2},
numpages = {21},
keywords = {Chinese zero pronoun resolution, chain-to-chain approach, chain-level features, zero pronoun coreferential chains}
}

@article{10.1145/3325884,
author = {Yin, Qingyu and Zhang, Weinan and Zhang, Yu and Liu, Ting},
title = {Chinese Zero Pronoun Resolution: A Collaborative Filtering-Based Approach},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3325884},
doi = {10.1145/3325884},
abstract = {Semantic information that has been proven to be necessary to the resolution of common noun phrases is typically ignored by most existing Chinese zero pronoun resolvers. This is because that zero pronouns convey no descriptive information, which makes it almost impossible to calculate semantic similarities between the zero pronoun and its candidate antecedents. Moreover, most of traditional approaches are based on the single-candidate model, which considers the candidate antecedents of a zero pronoun in isolation and thus overlooks their reciprocities. To address these problems, we first propose a neural-network-based zero pronoun resolver (NZR) that is capable of generating vector-space semantics of zero pronouns and candidate antecedents. On the basis of NZR, we develop the collaborative filtering-based framework for Chinese zero pronoun resolution task, exploring the reciprocities between the candidate antecedents of a zero pronoun to more rationally re-estimate their importance. Experimental results on the Chinese portion of the OntoNotes 5.0 corpus are encouraging: Our proposed model substantially surpasses the Chinese zero pronoun resolution baseline systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {3},
numpages = {20},
keywords = {collaborative filtering, deep neural network, Zero pronoun resolution}
}

@article{10.1145/3321128,
author = {Wang, Hongmin and Yang, Jie and Zhang, Yue},
title = {From Genesis to Creole Language: Transfer Learning for Singlish Universal Dependencies Parsing and POS Tagging},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321128},
doi = {10.1145/3321128},
abstract = {Singlish can be interesting to the computational linguistics community both linguistically, as a major low-resource creole based on English, and computationally, for information extraction and sentiment analysis of regional social media. In our conference paper, Wang et al. (2017), we investigated part-of-speech (POS) tagging and dependency parsing for Singlish by constructing a treebank under the Universal Dependencies scheme and successfully used neural stacking models to integrate English syntactic knowledge for boosting Singlish POS tagging and dependency parsing, achieving the state-of-the-art accuracies of 89.50% and 84.47% for Singlish POS tagging and dependency, respectively. In this work, we substantially extend Wang et al. (2017) by enlarging the Singlish treebank to more than triple the size and with much more diversity in topics, as well as further exploring neural multi-task models for integrating English syntactic knowledge. Results show that the enlarged treebank has achieved significant relative error reduction of 45.8% and 15.5% on the base model, 27% and 10% on the neural multi-task model, and 21% and 15% on the neural stacking model for POS tagging and dependency parsing, respectively. Moreover, the state-of-the-art Singlish POS tagging and dependency parsing accuracies have been improved to 91.16% and 85.57%, respectively. We make our treebanks and models available for further research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {1},
numpages = {29},
keywords = {transfer learning, Singlish, Dependency parsing, universal dependencies, part-of-speech tagging, neural stacking, creole language, multi-task network}
}

@article{10.1145/3325885,
author = {Ding, Chenchen and Aye, Hnin Thu Zar and Pa, Win Pa and Nwet, Khin Thandar and Soe, Khin Mar and Utiyama, Masao and Sumita, Eiichiro},
title = {Towards Burmese (Myanmar) Morphological Analysis: Syllable-Based Tokenization and Part-of-Speech Tagging},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3325885},
doi = {10.1145/3325885},
abstract = {This article presents a comprehensive study on two primary tasks in Burmese (Myanmar) morphological analysis: tokenization and part-of-speech (POS) tagging. Twenty thousand Burmese sentences of newswire are annotated with two-layer tokenization and POS-tagging information, as one component of the Asian Language Treebank Project. The annotated corpus has been released under a CC BY-NC-SA license, and it is the largest open-access database of annotated Burmese when this manuscript was prepared in 2017. Detailed descriptions of the preparation, refinement, and features of the annotated corpus are provided in the first half of the article. Facilitated by the annotated corpus, experiment-based investigations are presented in the second half of the article, wherein the standard sequence-labeling approach of conditional random fields and a long short-term memory (LSTM)-based recurrent neural network (RNN) are applied and discussed. We obtained several general conclusions, covering the effect of joint tokenization and POS-tagging and importance of ensemble from the viewpoint of stabilizing the performance of LSTM-based RNN. This study provides a solid basis for further studies on Burmese processing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {5},
numpages = {34},
keywords = {morphological analysis, annotated corpus, tokenization, LSTM-based RNN, CRF, POS-tagging, Burmese (Myanmar)}
}

@article{10.1145/3325887,
author = {Liu, Dayiheng and Yang, Kexin and Qu, Qian and Lv, Jiancheng},
title = {Ancient–Modern Chinese Translation with a New Large Training Dataset},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3325887},
doi = {10.1145/3325887},
abstract = {Ancient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. However, the lack of large-scale parallel corpus limits the study of machine translation in ancient–modern Chinese. In this article, we propose an ancient–modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale ancient–modern Chinese parallel corpus that contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality ancient–modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {6},
numpages = {13},
keywords = {neural machine translation, bilingual text alignment, Ancient–Modern Chinese parallel corpus}
}

@article{10.1145/3314938,
author = {Gao, Shengxiang and Huang, Jihao and Xue, Mingya and Yu, Zhengtao and Wang, Zhuo and Zhang, Yang},
title = {Syntax-Based Chinese-Vietnamese Tree-to-Tree Statistical Machine Translation with Bilingual Features},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314938},
doi = {10.1145/3314938},
abstract = {Because of the scarcity of bilingual corpora, current Chinese--Vietnamese machine translation is far from satisfactory. Considering the differences between Chinese and Vietnamese, we investigate whether linguistic differences can be used to supervise machine translation and propose a method of syntax-based Chinese--Vietnamese tree-to-tree statistical machine translation with bilingual features. Analyzing the syntax differences between Chinese and Vietnamese, we define some linguistic difference-based rules, such as attributive position, time adverbial position, and locative adverbial position, and create rewards for similar rules. These rewards are integrated into the extraction of tree-to-tree translation rules, and we optimize the pruning of the search space during the decoding phase. The experiments on Chinese--Vietnamese bilingual sentence translation show that the proposed method performs better than several compared methods. Further, the results show that syntactic difference features, with search pruning, can improve the accuracy of machine translation without degrading the efficiency.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {36},
numpages = {20},
keywords = {Statistical machine translation, Chinese-Vietnamese, linguistic features, pruning optimization, tree-to-tree}
}

@article{10.1145/3321127,
author = {Kim, Hyun and Lee, Jong-Hyeok and Na, Seung-Hoon},
title = {Multi-Task Stack Propagation for Neural Quality Estimation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321127},
doi = {10.1145/3321127},
abstract = {Quality estimation is an important task in machine translation that has attracted increased interest in recent years. A key problem in translation-quality estimation is the lack of a sufficient amount of the quality annotated training data. To address this shortcoming, the Predictor-Estimator was proposed recently by introducing “word prediction” as an additional pre-subtask that predicts a current target word with consideration of surrounding source and target contexts, resulting in a two-stage neural model composed of a predictor and an estimator. However, the original Predictor-Estimator is not trained on a continuous stacking model but instead in a cascaded manner that separately trains the predictor from the estimator. In addition, the Predictor-Estimator is trained based on single-task learning only, which uses target-specific quality-estimation data without using other training data that are available from other-level quality-estimation tasks. In this article, we thus propose a multi-task stack propagation, which extensively applies stack propagation to fully train the Predictor-Estimator on a continuous stacking architecture and multi-task learning to enhance the training data from related other-level quality-estimation tasks. Experimental results on WMT17 quality-estimation datasets show that the Predictor-Estimator trained with multi-task stack propagation provides statistically significant improvements over the baseline models. In particular, under an ensemble setting, the proposed multi-task stack propagation leads to state-of-the-art performance at all the sentence/word/phrase levels for WMT17 quality estimation tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {48},
numpages = {18},
keywords = {multi-task learning, stack propagation, Translation quality estimation, predictor-estimator}
}

@article{10.1145/3300050,
author = {Awais, Dr. Muhammad and Shoaib, Dr. Muhammad},
title = {Role of Discourse Information in Urdu Sentiment Classification: A Rule-Based Method and Machine-Learning Technique},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3300050},
doi = {10.1145/3300050},
abstract = {In computational linguistics, sentiment analysis refers to the classification of opinions in a positive class or a negative class. There exist a lot of different methods for sentiment analysis of the English language, but the literature lacks the availability of methods and techniques for Urdu, which is the largely spoken language in the South Asian sub-continent and the national language of Pakistan. The currently available techniques, such as adjective count method known as Bag of Words (BoW), is not sufficient for classification of complex sentiment written in the Urdu language. Also, the performance of available machine-learning techniques (with legacy features), for classification of Urdu sentiments, are not comparable with the achieved accuracy of other languages. In the case of the English language, the discourse information (sub-sentence-level information) boosts the performance of both the BoW method and machine-learning techniques, but there are very few works available that have tested the context-level information for the sentiment analysis of the Urdu language. This research aims to extract the discourse information from the Urdu sentiments and utilise the discourse information to improve the performance and reduce the error rate of existing techniques for Urdu Sentiment classification. The proposed solution extracts the discourse information, suggests a new set of features for machine-learning techniques, and introduces a set of rules to extend the capabilities of the BoW model. The results show that the task has been enhanced significantly and the performance metrics such as recall, precision, and accuracy are increased by 31.25%, 8.46%, and 21.6%, respectively. In future, the proposed technique can be extended to sentiments with more than two sub-opinions, such as for blogs, reviews, and TV talk shows.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {34},
numpages = {37},
keywords = {Urdu sentiment classification, Urdu natural language processing, discourse features, sentiment feature extraction}
}

@article{10.1145/3314942,
author = {Onyenwe, Ikechukwu E. and Hepple, Mark and Chinedu, Uchechukwu and Ezeani, Ignatius},
title = {Toward an Effective Igbo Part-of-Speech Tagger},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314942},
doi = {10.1145/3314942},
abstract = {Part-of-speech (POS) tagging is a well-established technology for most Western European languages and a few other world languages, but it has not been evaluated on Igbo, an agglutinative African language. This article presents POS tagging experiments conducted using an Igbo corpus as a test bed for identifying the POS taggers and the Machine Learning (ML) methods that can achieve a good performance with the small dataset available for the language. Experiments have been conducted using different well-known POS taggers developed for English or European languages, and different training data styles and sizes. Igbo has a number of language-specific characteristics that present a challenge for effective POS tagging. One interesting case is the wide use of verbs (and nominalizations thereof) that have an inherent noun complement, which form “linked pairs” in the POS tagging scheme, but which may appear discontinuously. Another issue is Igbo’s highly productive agglutinative morphology, which can produce many variant word forms from a given root. This productivity is a key cause of the out-of-vocabulary (OOV) words observed during Igbo tagging. We report results of experiments on a promising direction for improving tagging performance on such morphologically-inflected OOV words.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {42},
numpages = {26},
keywords = {POS tagger, Igbo, machine learning, tagset, African language, part-of-speech (POS) tagging, Natural language processing (NLP), language technology, corpora, text processing, morphological analysis, corpus annotation}
}

@article{10.1145/3314943,
author = {Liu, Yang and Wang, Shaonan and Zhang, Jiajun and Zong, Chengqing},
title = {Experience-Based Causality Learning for Intelligent Agents},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314943},
doi = {10.1145/3314943},
abstract = {Understanding causality in text is crucial for intelligent agents. In this article, inspired by human causality learning, we propose an experience-based causality learning framework. Comparing to traditional approaches, which attempt to handle the causality problem relying on textual clues and linguistic resources, we are the first to use experience information for causality learning. Specifically, we first construct various scenarios for intelligent agents, thus, the agents can gain experience from interaction in these scenarios. Then, human participants build a number of training instances for agents of causality learning based on these scenarios. Each instance contains two sentences and a label. Each sentence describes an event that an agent experienced in a scenario, and the label indicates whether the sentence (event) pair has a causal relation. Accordingly, we propose a model that can infer the causality in text using experience by accessing the corresponding event information based on the input sentence pair. Experiment results show that our method can achieve impressive performance on the grounded causality corpus and significantly outperform the conventional approaches. Our work suggests that experience is very important for intelligent agents to understand causality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {45},
numpages = {22},
keywords = {grounded language learning, Intelligent agent, experience, causality learning, virtual environment}
}

@article{10.1145/3314945,
author = {Maimaiti, Mieradilijiang and Liu, Yang and Luan, Huanbo and Sun, Maosong},
title = {Multi-Round Transfer Learning for Low-Resource NMT Using Multiple High-Resource Languages},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314945},
doi = {10.1145/3314945},
abstract = {Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {38},
numpages = {26},
keywords = {multi-round, transfer learning, Neural machine translation, transliteration, high-resource language, low-resource language}
}

@article{10.1145/3314941,
author = {Dahou, Abdelghani and Xiong, Shengwu and Zhou, Junwei and Elaziz, Mohamed Abd},
title = {Multi-Channel Embedding Convolutional Neural Network Model for Arabic Sentiment Classification},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314941},
doi = {10.1145/3314941},
abstract = {With the advent of social network services, Arabs’ opinions on the web have attracted many researchers in recent years toward detecting and classifying sentiments in Arabic tweets and reviews. However, the impact of word embeddings vectors (WEVs) initialization and dataset balance on Arabic sentiment classification using deep learning has not been thoroughly studied. In this article, a multi-channel embedding convolutional neural network (MCE-CNN) is proposed to improve Arabic sentiment classification by learning sentiment features from different text domains, word, and character n-grams levels. MCE-CNN encodes a combination of different pre-trained word embeddings into the embedding block at each embedding channel and trains these channels in parallel. Besides, a separate feature extraction module implemented in a CNN block is used to extract more relevant sentiment features. These channels and blocks help to start training on high-quality WEVs and fine-tuning them. The performance of MCE-CNN is evaluated on several standard balanced and imbalanced datasets to reflect real-world use cases. Experimental results show that MCE-CNN provides a high classification accuracy and benefits from the second embedding channel on both standard Arabic and dialectal Arabic text, which outperforms state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {41},
numpages = {23},
keywords = {convolutional neural network, multi-channel, neural language models, deep learning, Arabic language, Arabic sentiment classification, Arabic word embeddings}
}

@article{10.1145/3308754,
author = {Verma, Pradeepika and Pal, Sukomal and Om, Hari},
title = {A Comparative Analysis on Hindi and English Extractive Text Summarization},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3308754},
doi = {10.1145/3308754},
abstract = {Text summarization is the process of transfiguring a large documental information into a clear and concise form. In this article, we present a detailed comparative study of various extractive methods for automatic text summarization on Hindi and English text datasets of news articles. We consider 13 different summarization techniques, namely, TextRank, LexRank, Luhn, LSA, Edmundson, ChunkRank, TGraph, UniRank, NN-ED, NN-SE, FE-SE, SummaRuNNer, and MMR-SE, and we evaluate their performance using various performance metrics, such as precision, recall, F1, cohesion, non-redundancy, readability, and significance. A thorough analysis is done in eight different parts that exhibits the strengths and limitations of these methods, effect of performance over the summary length, impact of language of a document, and other factors as well. A standard summary evaluation tool (ROUGE) and extensive programmatic evaluation using Python 3.5 in Anaconda environment are used to evaluate their outcome.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {30},
numpages = {39},
keywords = {graph-based techniques, Text summarization, meta-heuristic-based techniques, neural networks-based techniques, latent semantic analysis, ROUGE}
}

@article{10.1145/3314939,
author = {Sun, Ruiyong and Zhao, Yijia and Zhang, Qi and Ding, Keyu and Wang, Shijin and Wei, Cui},
title = {A Neural Semantic Parser for Math Problems Incorporating Multi-Sentence Information},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314939},
doi = {10.1145/3314939},
abstract = {In this article, we study the problem of parsing a math problem into logical forms. It is an essential pre-processing step for automatically solving math problems. Most of the existing studies about semantic parsing mainly focused on the single-sentence level. However, for parsing math problems, we need to take the information of multiple sentences into consideration. To achieve the task, we formulate the task as a machine translation problem and extend the sequence-to-sequence model with a novel two-encoder architecture and a word-level selective mechanism. For training and evaluating the proposed method, we construct a large-scale dataset. Experimental results show that the proposed two-encoder architecture and word-level selective mechanism could bring significant improvement. The proposed method can achieve better performance than the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {37},
numpages = {16},
keywords = {multi-sentence, selective mechanism, math problem solving, Semantic parsing}
}

@article{10.1145/3321125,
author = {Yang, Jun and Yang, Runqi and Lu, Hengyang and Wang, Chongjun and Xie, Junyuan},
title = {Multi-Entity Aspect-Based Sentiment Analysis with Context, Entity, Aspect Memory and Dependency Information},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321125},
doi = {10.1145/3321125},
abstract = {Fine-grained sentiment analysis is a useful tool for producers to understand consumers’ needs as well as complaints about products and related aspects from online platforms. In this article, we define a novel task named “Multi-Entity Aspect-Based Sentiment Analysis (ME-ABSA)”. It investigates the sentiment towards entities and their related aspects. It makes the well-studied aspect-based sentiment analysis a special case of this type, where the number of entities is limited to one. We contribute a new dataset for this task, with multi-entity Chinese posts in it. We propose to model context, entity, and aspect memory to address the task and incorporate dependency information for further improvement. Experiments show that our methods perform significantly better than baseline methods on datasets for both ME-ABSA task and ABSA task. The in-depth analysis further validates the effectiveness of our methods and shows that our methods are capable of generalizing to new (entity, aspect) combinations with little loss of accuracy. This observation indicates that data annotation in real applications can be largely simplified.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {47},
numpages = {22},
keywords = {entity, dependency, aspect, Sentiment analysis}
}

@article{10.1145/3314940,
author = {Saeed, Ali and Nawab, Rao Muhammad Adeel and Stevenson, Mark and Rayson, Paul},
title = {A Sense Annotated Corpus for All-Words Urdu Word Sense Disambiguation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314940},
doi = {10.1145/3314940},
abstract = {Word Sense Disambiguation (WSD) aims to automatically predict the correct sense of a word used in a given context. All human languages exhibit word sense ambiguity, and resolving this ambiguity can be difficult. Standard benchmark resources are required to develop, compare, and evaluate WSD techniques. These are available for many languages, but not for Urdu, despite this being a language with more than 300 million speakers and large volumes of text available digitally. To fill this gap, this study proposes a novel benchmark corpus for the Urdu All-Words WSD task. The corpus contains 5,042 words of Urdu running text in which all ambiguous words (856 instances) are manually tagged with senses from the Urdu Lughat dictionary. A range of baseline WSD models based on n-gram are applied to the corpus, and the best performance (accuracy of 57.71%) is achieved using word 4-gram. The corpus is freely available to the research community to encourage further WSD research in Urdu.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {40},
numpages = {14},
keywords = {sense tagged Urdu corpus, all-words task, Word sense disambiguation}
}

@article{10.1145/3314937,
author = {Dehghan, Mohammad Hossein and Faili, Heshaam},
title = {Converting Dependency Structure Into Persian Phrase Structure},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314937},
doi = {10.1145/3314937},
abstract = {Treebank is one of the important and useful resources in natural language processing represented in two different annotated schemas: phrase and dependency structures. There are many works that convert a phrase structure into a dependency structure and vice versa. Most of them are based that exploit the handcrafted head percolation table and argument table in predefined deterministic ways. In this article, we propose a method to convert a dependency structure into a phrase structure by enriching a trainable model of former hybrid strategy approach. By adding a classifier to the algorithm and using postprocessing modification, the quality of conversion is increased. We evaluate our method in two different languages, English and Persian, and then analyze the errors. The results of our experiments show a 46.01% reduction of error rate in English and 76.50% for Persian compared to our baseline. We build a new phrase structure treebank by converting 10,000 sentences of Persian dependency treebank into corresponding phrase structures and correcting them manually.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {33},
numpages = {21},
keywords = {Treebank, Persian, machine learning, dependency structure, parser, phrase structure}
}

@article{10.1145/3309497,
author = {Nongmeikapam, Kishorjit and Wahengbam, Kanan and Meetei, Oinam Nickson and Tuithung, Themrichon},
title = {Handwritten Manipuri Meetei-Mayek Classification Using Convolutional Neural Network},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3309497},
doi = {10.1145/3309497},
abstract = {A new technique for classifying all 56 different characters of the Manipuri Meetei-Mayek (MMM) is proposed herein. The characters are grouped under five categories, which are Eeyek Eepee (original alphabets), Lom Eeyek (additional letters), Cheising Eeyek (digits), Lonsum Eeyek (letters with short endings), and Cheitap Eeyek (vowel signs. Two related works proposed by previous researchers are studied for understanding the benefits claimed by the proposed deep learning approach in handwritten Manipuri Meetei-Mayek. (1) Histogram of Oriented (HOG) with SVM classifier is implemented for thoroughly understanding how HOG features can influence accuracy. (2) The handwritten samples are trained using simple Convolutional Neural Network (CNN) and compared with the proposed CNN-based architecture. Significant progress has been made in the field of Optical Character Recognition (OCR) for well-known Indian languages as well as globally popular languages. Our work is novel in the sense that there is no record of work available to date that is able to classify all 56 classes of the MMM. It will also serve as a pre-cursor for developing end-to-end OCR software for translating old manuscripts, newspaper archives, books, and so on.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {35},
numpages = {23},
keywords = {optical character recognition (OCR), histogram of oriented gradient (HOG), deep learning, Meetei-Mayek, support vector machine (SVM)}
}

@article{10.1145/3295662,
author = {Badaro, Gilbert and Baly, Ramy and Hajj, Hazem and El-Hajj, Wassim and Shaban, Khaled Bashir and Habash, Nizar and Al-Sallab, Ahmad and Hamdi, Ali},
title = {A Survey of Opinion Mining in Arabic: A Comprehensive System Perspective Covering Challenges and Advances in Tools, Resources, Models, Applications, and Visualizations},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3295662},
doi = {10.1145/3295662},
abstract = {Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {27},
numpages = {52},
keywords = {Sentiment analysis, opinion mining, sentiment analysis applications, sentiment lexicons, arabic natural language processing, deep learning}
}

@article{10.1145/3310283,
author = {Ihasz, Peter Lajos and Kovacs, Mate and Piumarta, Ian and Kryssanov, Victor V.},
title = {A Supplementary Feature Set for Sentiment Analysis in Japanese Dialogues},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3310283},
doi = {10.1145/3310283},
abstract = {Recently, real-time affect-awareness has been applied in several commercial systems, such as dialogue systems and computer games. Real-time recognition of affective states, however, requires the application of costly feature extraction methods and/or labor-intensive annotation of large datasets, especially in the case of Asian languages where large annotated datasets are seldom available. To improve recognition accuracy, we propose the use of cognitive context in the form of “emotion-sensitive” intentions. Intentions are often represented through dialogue acts and, as an emotion-sensitive model of dialogue acts, a tagset of interpersonal-relations-directing interpersonal acts (the IA model) is proposed. The model's adequacy is assessed using a sentiment classification task in comparison with two well-known dialogue act models, the SWBD-DAMSL and the DIT++. For the assessment, five Japanese in-game dialogues were annotated with labels of sentiments and the tags of all three dialogue act models which were used to enhance a baseline sentiment classifier system. The adequacy of the IA tagset is demonstrated by a 9% improvement to the baseline sentiment classifier's recognition accuracy, outperforming the other two models by more than 5%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {39},
numpages = {21},
keywords = {gaming data, Affect-awareness, dialogue acts, Japanese language, sentiment recognition}
}

@article{10.1145/3314934,
author = {Wei, Bingzhen and Ren, Xuancheng and Zhang, Yi and Cai, Xiaoyan and Su, Qi and Sun, Xu},
title = {Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314934},
doi = {10.1145/3314934},
abstract = {Abstractive text summarization is a highly difficult problem, and the sequence-to-sequence model has shown success in improving the performance on the task. However, the generated summaries are often inconsistent with the source content in semantics. In such cases, when generating summaries, the model selects semantically unrelated words with respect to the source content as the most probable output. The problem can be attributed to heuristically constructed training data, where summaries can be unrelated to the source content, thus containing semantically unrelated words and spurious word correspondence. In this article, we propose a regularization approach for the sequence-to-sequence model and make use of what the model has learned to regularize the learning objective to alleviate the effect of the problem. In addition, we propose a practical human evaluation method to address the problem that the existing automatic evaluation method does not evaluate the semantic consistency with the source content properly. Experimental results demonstrate the effectiveness of the proposed approach, which outperforms almost all the existing models. Especially, the proposed approach improves the semantic consistency by 4% in terms of human evaluation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {31},
numpages = {15},
keywords = {natural language processing, Abstractive text summarization, semantic consistency, Chinese social media text}
}

@article{10.1145/3321124,
author = {Yin, Yongjing and Su, Jinsong and Wen, Huating and Zeng, Jiali and Liu, Yang and Chen, Yidong},
title = {POS Tag-Enhanced Coarse-to-Fine Attention for Neural Machine Translation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321124},
doi = {10.1145/3321124},
abstract = {Although neural machine translation (NMT) has certain capability to implicitly learn semantic information of sentences, we explore and show that Part-of-Speech (POS) tags can be explicitly incorporated into the attention mechanism of NMT effectively to yield further improvements. In this article, we propose an NMT model with tag-enhanced attention mechanism. In our model, NMT and POS tagging are jointly modeled via multi-task learning. Besides following common practice to enrich encoder annotations by introducing predicted source POS tags, we exploit predicted target POS tags to refine attention model in a coarse-to-fine manner. Specifically, we first implement a coarse attention operation solely on source annotations and target hidden state, where the produced context vector is applied to update target hidden state used for target POS tagging. Then, we perform a fine attention operation that extends the coarse one by further exploiting the predicted target POS tags. Finally, we facilitate word prediction by simultaneously utilizing the context vector from fine attention and the predicted target POS tags. Experimental results and further analyses on Chinese-English and Japanese-English translation tasks demonstrate the superiority of our proposed model over the conventional NMT models. We release our code at https://github.com/middlekisser/PEA-NMT.git.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {46},
numpages = {14},
keywords = {attention model, Neural machine translation, POS tags}
}

@article{10.1145/3312575,
author = {Costa-Juss\`{a}, Marta R. and Casas, No\'{e} and Escolano, Carlos and Fonollosa, Jos\'{e} A. R.},
title = {Chinese-Catalan: A Neural Machine Translation Approach Based on Pivoting and Attention Mechanisms},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3312575},
doi = {10.1145/3312575},
abstract = {This article innovatively addresses machine translation from Chinese to Catalan using neural pivot strategies trained without any direct parallel data. The Catalan language is very similar to Spanish from a linguistic point of view, which motivates the use of Spanish as pivot language. Regarding neural architecture, we are using the latest state-of-the-art, which is the Transformer model, only based on attention mechanisms. Additionally, this work provides new resources to the community, which consists of a human-developed gold standard of 4,000 sentences between Catalan and Chinese and all the others United Nations official languages (Arabic, English, French, Russian, and Spanish). Results show that the standard pseudo-corpus or synthetic pivot approach performs better than cascade.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {43},
numpages = {8},
keywords = {transformer, Neural machine translation, Chinese-Catalan, pivot approaches}
}

@article{10.1145/3282442,
author = {Jung, Sangkeun and Park, Cheon-Eum and Lee, Changki},
title = {Multitask Pointer Network for Korean Dependency Parsing},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3282442},
doi = {10.1145/3282442},
abstract = {Dependency parsing is a fundamental problem in natural language processing. We introduce a novel dependency-parsing framework called head-pointing--based dependency parsing. In this framework, we cast the Korean dependency parsing problem as a statistical head-pointing and arc-labeling problem. To address this problem, a novel neural network called the multitask pointer network is devised for a neural sequential head-pointing and type-labeling architecture. Our approach does not require any handcrafted features or language-specific rules to parse dependency. Furthermore, it achieves state-of-the-art performance for Korean dependency parsing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {24},
numpages = {10},
keywords = {Dependency parsing, deep learning, multitask pointer networks, head pointing}
}

@article{10.1145/3293442,
author = {Kang, Xiaomian and Zong, Chengqing and Xue, Nianwen},
title = {A Survey of Discourse Representations for Chinese Discourse Annotation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3293442},
doi = {10.1145/3293442},
abstract = {A key element in computational discourse analysis is the design of a formal representation for the discourse structure of a text. With machine learning being the dominant method, it is important to identify a discourse representation that can be used to perform large-scale annotation. This survey provides a systematic analysis of existing discourse representation theories to evaluate whether they are suitable for annotation of Chinese text. Specifically, the two properties, expressiveness and practicality, are introduced to compare the representations of theories based on rhetorical relations and the representations of theories based on entity relations. The comparison systematically reveals linguistic and computational characteristics of the theories. After that, we conclude that none of the existing theories are quite suitable for scalable Chinese discourse annotation because they are not both expressive and practical. Therefore, a new discourse representation needs to be proposed, which should balance the expressiveness and practicality, and cover rhetorical relations and entity relations. Inspired by the conclusions, this survey discusses some preliminary proposals on how to represent the discourse structure that are worth pursuing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {26},
numpages = {25},
keywords = {discourse structure, Discourse representation, discourse theory, discourse analysis}
}

@article{10.1145/3292398,
author = {B\"{o}l\"{u}c\"{u}, Necva and Can, Burcu},
title = {Unsupervised Joint PoS Tagging and Stemming for Agglutinative Languages},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3292398},
doi = {10.1145/3292398},
abstract = {The number of possible word forms is theoretically infinite in agglutinative languages. This brings up the out-of-vocabulary (OOV) issue for part-of-speech (PoS) tagging in agglutinative languages. Since inflectional morphology does not change the PoS tag of a word, we propose to learn stems along with PoS tags simultaneously. Therefore, we aim to overcome the sparsity problem by reducing word forms into their stems. We adopt a Bayesian model that is fully unsupervised. We build a Hidden Markov Model for PoS tagging where the stems are emitted through hidden states. Several versions of the model are introduced in order to observe the effects of different dependencies throughout the corpus, such as the dependency between stems and PoS tags or between PoS tags and affixes. Additionally, we use neural word embeddings to estimate the semantic similarity between the word form and stem. We use the semantic similarity as prior information to discover the actual stem of a word since inflection does not change the meaning of a word. We compare our models with other unsupervised stemming and PoS tagging models on Turkish, Hungarian, Finnish, Basque, and English. The results show that a joint model for PoS tagging and stemming improves on an independent PoS tagger and stemmer in agglutinative languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {25},
numpages = {21},
keywords = {joint learning, part-of-speech (PoS) tagging, Unsupervised learning, stemming, hidden Markov models (HMM), neural word embeddings}
}

@article{10.1145/3278623,
author = {Ahmadi, Sina},
title = {A Rule-Based Kurdish Text Transliteration System},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3278623},
doi = {10.1145/3278623},
abstract = {In this article, we present a rule-based approach for transliterating two of the most used orthographies in Sorani Kurdish. Our work consists of detecting a character in a word by removing the possible ambiguities and mapping it into the target orthography. We describe different challenges in Kurdish text mining and propose novel ideas concerning the transliteration task for Sorani Kurdish. Our transliteration system, named Wergor, achieves 82.79% overall precision and more than 99% in detecting the double-usage characters. We also present a manually transliterated corpus for Kurdish.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {18},
numpages = {8},
keywords = {less-resourced language processing, rule-based approach, Transliteration, Kurdish}
}

@article{10.1145/3265752,
author = {Le, Ngoc Tan and Sadat, Fatiha and Menard, Lucie and Dinh, Dien},
title = {Low-Resource Machine Transliteration Using Recurrent Neural Networks},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3265752},
doi = {10.1145/3265752},
abstract = {Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These models are based on initial alignments between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network--based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pretrained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. Evaluation and experiments involving French and Vietnamese showed that with only a small bilingual pronunciation dictionary available for training the transliteration models, promising results were obtained with a large increase in BLEU scores and a reduction in Translation Error Rate (TER) and Phoneme Error Rate (PER). Moreover, we compared our proposed neural network--based transliteration approach with a statistical one.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {13},
numpages = {14},
keywords = {recurrent neural networks, embeddings, French-Vietnamese, grapheme-to-phoneme, alignment, Machine transliteration, low-resource language}
}

@article{10.1145/3278605,
author = {Alnawas, Anwar and Arici, Nursal},
title = {Sentiment Analysis of Iraqi Arabic Dialect on Facebook Based on Distributed Representations of Documents},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3278605},
doi = {10.1145/3278605},
abstract = {Nowadays, social media is used by many people to express their opinions about a variety of topics. Opinion Mining or Sentiment Analysis techniques extract opinions from user generated contents. Over the years, a multitude of Sentiment Analysis studies has been done about the English language with deficiencies of research in all other languages. Unfortunately, Arabic is one of the languages that seems to lack substantial research, despite the rapid growth of its use on social media outlets. Furthermore, specific Arabic dialects should be studied, not just Modern Standard Arabic. In this paper, we experiment sentiments analysis of Iraqi Arabic dialect using word embedding. First, we made a large corpus from previous works to learn word representations. Second, we generated word embedding model by training corpus using Doc2Vec representations based on Paragraph and Distributed Memory Model of Paragraph Vectors (DM-PV) architecture. Lastly, the represented feature used for training four binary classifiers (Logistic Regression, Decision Tree, Support Vector Machine and Naive Bayes) to detect sentiment. We also experimented different values of parameters (window size, dimension and negative samples). In the light of the experiments, it can be concluded that our approach achieves a better performance for Logistic Regression and Support Vector Machine than the other classifiers.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {20},
numpages = {17},
keywords = {Doc2Vec, facebook, word embedding, Iraqi Arabic Dialect, sentiments analysis}
}

@article{10.1145/3282441,
author = {Singh, Sukhdeep and Sharma, Anuj},
title = {Online Handwritten Gurmukhi Words Recognition: An Inclusive Study},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3282441},
doi = {10.1145/3282441},
abstract = {Identification of offline and online handwritten words is a challenging and complex task. In comparison to Latin and Oriental scripts, the research and study of handwriting recognition at word level in Indic scripts is at its initial phases. The two main methods of handwriting recognition are global and analytical. The present work introduces a novel analytical approach for online handwritten Gurmukhi word recognition based on a minimal set of words and recognizes an input Gurmukhi word as a sequence of characters. We employed a sequential step-by-step approach to recognize online handwritten Gurmukhi words. Considering the massive variability in online Gurmukhi handwriting, the present work employs the completely linked non-homogeneous hidden Markov model. In the present study, we considered the dependent, major-dependent, and super-dependent nature of strokes to form Gurmukhi characters in words. On test sets of online handwritten Gurmukhi datasets, the word-level accuracy rates are 85.98%, 84.80%, 82.40%, and 82.20% in four different modes. Besides the online Gurmukhi word recognition, the present work also provides Gurmukhi handwriting analysis study for varying writing styles and proposes novel techniques for zone detection and rearrangement of strokes. Our proposed algorithms have been successfully employed to online handwritten Gurmukhi word recognition in dependent and independent modes of handwriting.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {21},
numpages = {55},
keywords = {word recognition, boxed rearrangement, hidden Markov models, Gurmukhi, zone detection, Online handwriting recognition}
}

@article{10.1145/3282443,
author = {Y\"{u}cesoy, Veysel and Ko\c{c}, Aykut},
title = {Co-Occurrence Weight Selection in Generation of Word Embeddings for Low Resource Languages},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3282443},
doi = {10.1145/3282443},
abstract = {This study aims to increase the performance of word embeddings by proposing a new weighting scheme for co-occurrence counting. The idea behind this new family of weights is to overcome the disadvantage of distant appearing word pairs, which are indeed semantically close, while representing them in the co-occurrence counting. For high-resource languages, this disadvantage might not be effective due to the high frequency of co-occurrence. However, when there are not enough available resources, such pairs suffer from being distant. To favour such pairs, a weighting scheme based on a polynomial fitting procedure is proposed to shift the weights up for distant words while the weights of nearby words are left almost unchanged. The parameter optimization for new weights and the effects of the weighting scheme are analysed for the English, Italian, and Turkish languages. A small portion of English resources and a quarter of Italian resources are utilized for demonstration purposes, as if these languages are low-resource languages. Performance increase is observed in analogy tests when the proposed weighting scheme is applied to relatively small corpora (i.e., mimicking low-resource languages) of both English and Italian. To show the effectiveness of the proposed scheme in small corpora, it is also shown for a large English corpus that the performance of the proposed weighting scheme cannot outperform the original weights. Since Turkish is relatively a low-resource language, it is demonstrated that the proposed weighting scheme can increase the performance of both analogy and similarity tests when all Turkish Wikipedia pages are utilized as a corpus. The positive effect of the proposed scheme has also been demonstrated in a standard sentiment analysis task for the Turkish language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {22},
numpages = {18},
keywords = {computational linguistics, Word embeddings, co-occurrence weighting}
}

@article{10.1145/3277591,
author = {Bounhas, Ibrahim},
title = {On the Usage of a Classical Arabic Corpus as a Language Resource: Related Research and Key Challenges},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3277591},
doi = {10.1145/3277591},
abstract = {This article presents a literature review of computer-science-related research applied on hadith, a kind of Arabic narration which appeared in the 7th century. We study and compare existent works in several fields of Natural Language Processing (NLP), Information Retrieval (IR), and Knowledge Extraction (KE). Thus, we illicit their main drawbacks and identify some perspectives, which may be considered by the research community. We also study the characteristics of these types of documents, by enumerating the advantages/limits of using hadith as a language resource. Moreover, our study shows that previous studies used different collections of hadiths, thus making it hard to compare their results objectively. Besides, many preprocessing steps are recurrent through these applications, thus wasting a lot of time. Consequently, the key issues for building generic language resources from hadiths are discussed, taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations. The ultimate goal is to structure hadith books for multiple usages, thus building common collections which may be exploited in future applications.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {23},
numpages = {45},
keywords = {Hadith processing, hadith mining, hadith knowledge extraction, hadith retrieval}
}

@article{10.1145/3208358,
author = {Bhattacharya, Paheli and Goyal, Pawan and Sarkar, Sudeshna},
title = {Using Communities of Words Derived from Multilingual Word Vectors for Cross-Language Information Retrieval in Indian Languages},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3208358},
doi = {10.1145/3208358},
abstract = {We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close to each other in this space. Multilingual word embeddings are constructed in such a way that similar words across languages have similar vector representations. We explore the effective use of bilingual and multilingual word embeddings learned from comparable corpora of Indic languages to the task of CLIR.We propose a clustering method based on the multilingual word vectors to group similar words across languages. For this we construct a graph with words from multiple languages as nodes and with edges connecting words with similar vectors. We use the Louvain method for community detection to find communities in this graph. We show that choosing target language words as query translations from the clusters or communities containing the query terms helps in improving CLIR. We also find that better-quality query translations are obtained when words from more languages are used to do the clustering even when the additional languages are neither the source nor the target languages. This is probably because having more similar words across multiple languages helps define well-defined dense subclusters that help us obtain precise query translations.In this article, we demonstrate the use of multilingual word embeddings and word clusters for CLIR involving Indic languages. We also make available a tool for obtaining related words and the visualizations of the multilingual word vectors for English, Hindi, Bengali, Marathi, Gujarati, and Tamil.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {1},
numpages = {27},
keywords = {Multilingual word embeddings, multilingual visualization, cross-language information retrieval, clusters}
}

@article{10.1145/3276773,
author = {Ding, Chenchen and Utiyama, Masao and Sumita, Eiichiro},
title = {NOVA: A Feasible and Flexible Annotation System for Joint Tokenization and Part-of-Speech Tagging},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3276773},
doi = {10.1145/3276773},
abstract = {A feasible and flexible annotation system is designed for joint tokenization and part-of-speech (POS) tagging to annotate those languages without natural definitions of words. This design was motivated by the fact that word separators are not used in many highly analytic East and Southeast Asian languages. Although several of the languages are well-studied, e.g., Chinese and Japanese, many are understudied with low resources, e.g., Burmese (Myanmar) and Khmer. In the first part of the article, the proposed annotation system, named nova, is introduced. nova contains only four basic tags (n, v, a, and o); these tags can be further modified and combined to adapt complex linguistic phenomena in tokenization and POS tagging. In the second part of the article, the feasibility and flexibility of nova is illustrated from the annotation practice on Burmese and Khmer. The relation between nova and two universal POS tagsets is discussed in the final part of the article.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {17},
numpages = {18},
keywords = {Annotation, part-of-speech tagging, Asian languages, tokenization}
}

@article{10.1145/3264735,
author = {Bhattacharya, Nilanjana and Roy, Partha Pratim and Pal, Umapada},
title = {Sub-Stroke-Wise Relative Feature for Online Indic Handwriting Recognition},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3264735},
doi = {10.1145/3264735},
abstract = {The main problem of Bangla (Bengali) and Devanagari handwriting recognition is the shape similarity of characters. There are only a few pieces of work on writer-independent cursive online Indian text recognition, and the shape similarity problem needs more attention from the researchers. To handle the shape similarity problem of cursive characters of Bangla and Devanagari scripts, in this article, we propose a new category of features called ‘sub-stroke-wise relative feature’ (SRF) which are based on relative information of the constituent parts of the handwritten strokes. Relative information among some of the parts within a character can be a distinctive feature as it scales up small dissimilarities and enhances discrimination among similar-looking shapes. Also, contextual anticipatory phenomena are automatically modeled by this type of feature, as it takes into account the influence of previous and forthcoming strokes. We have tested popular state-of-the-art feature sets as well as proposed SRF using various (up to 20,000-word) lexicons and noticed that SRF significantly outperforms the state-of-the-art feature sets for online Bangla and Devanagari cursive word recognition.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {11},
numpages = {16},
keywords = {Online handwriting recognition, cursive text recognition, lexicon driven recognition, Indic script}
}

@article{10.1145/3273931,
author = {Akhtar, Md Shad and Sawant, Palaash and Sen, Sukanta and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {Improving Word Embedding Coverage in Less-Resourced Languages Through Multi-Linguality and Cross-Linguality: A Case Study with Aspect-Based Sentiment Analysis},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3273931},
doi = {10.1145/3273931},
abstract = {In the era of deep learning-based systems, efficient input representation is one of the primary requisites in solving various problems related to Natural Language Processing (NLP), data mining, text mining, and the like. Absence of adequate representation for an input introduces the problem of data sparsity, and it poses a great challenge to solve the underlying problem. The problem is more intensified with resource-poor languages due to the absence of a sufficiently large corpus required to train a word embedding model. In this work, we propose an effective method to improve the word embedding coverage in less-resourced languages by leveraging bilingual word embeddings learned from different corpora. We train and evaluate deep Long Short Term Memory (LSTM)-based architecture and show the effectiveness of the proposed approach for two aspect-level sentiment analysis tasks (i.e., aspect term extraction and sentiment classification). The neural network architecture is further assisted by hand-crafted features for prediction. We apply the proposed model in two experimental setups: multi-lingual and cross-lingual. Experimental results show the effectiveness of the proposed approach against the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {15},
numpages = {22},
keywords = {bilingual word embeddings, Long Short Term Memory (LSTM), deep learning, Aspect-Based Sentiment Analysis (ABSA), cross-lingual sentiment analysis, Sentiment analysis, Indian languages, low-resourced languages, data sparsity}
}

@article{10.1145/3242177,
author = {Jarrar, Mustafa and Zaraket, Fadi and Asia, Rami and Amayreh, Hamzeh},
title = {Diacritic-Based Matching of Arabic Words},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3242177},
doi = {10.1145/3242177},
abstract = {Words in Arabic consist of letters and short vowel symbols called diacritics inscribed atop regular letters. Changing diacritics may change the syntax and semantics of a word; turning it into another. This results in difficulties when comparing words based solely on string matching. Typically, Arabic NLP applications resort to morphological analysis to battle ambiguity originating from this and other challenges. In this article, we introduce three alternative algorithms to compare two words with possibly different diacritics. We propose the Subsume knowledge-based algorithm, the Imply rule-based algorithm, and the Alike machine-learning-based algorithm. We evaluated the soundness, completeness, and accuracy of the algorithms against a large dataset of 86,886 word pairs. Our evaluation shows that the accuracy of Subsume (100%), Imply (99.32%), and Alike (99.53%). Although accurate, Subsume was able to judge only 75% of the data. Both Subsume and Imply are sound, while Alike is not. We demonstrate the utility of the algorithms using a real-life use case -- in lemma disambiguation and in linking hundreds of Arabic dictionaries.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {disambiguation, Arabic, diacritics}
}

@article{10.1145/3238797,
author = {Murthy, Rudra and Khapra, Mitesh M. and Bhattacharyya, Pushpak},
title = {Improving NER Tagging Performance in Low-Resource Languages via Multilingual Learning},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3238797},
doi = {10.1145/3238797},
abstract = {Existing supervised solutions for Named Entity Recognition (NER) typically rely on a large annotated corpus. Collecting large amounts of NER annotated corpus is time-consuming and requires considerable human effort. However, collecting small amounts of annotated corpus for any language is feasible, but the performance degrades due to data sparsity. We address the data sparsity by borrowing features from the data of a closely related language. We use hierarchical neural networks to train a supervised NER system. The feature borrowing from a closely related language happens via the shared layers of the network. The neural network is trained on the combined dataset of the low-resource language and a closely related language, also termed Multilingual Learning. Unlike existing systems, we share all layers of the network between the two languages. We apply multilingual learning for NER in Indian languages and empirically show the benefits over a monolingual deep learning system and a traditional machine-learning system with some feature engineering. Using multilingual learning, we show that the low-resource language NER performance increases mainly due to (1) increased named entity vocabulary, (2) cross-lingual subword features, and (3) multilingual learning playing the role of regularization.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {9},
numpages = {20},
keywords = {deep learning, low-resource languages, multilingual learning, Indian languages, Named entity recognition}
}

@article{10.1145/3265751,
author = {Mrinalini, K. and Nagarajan, T. and Vijayalakshmi, P.},
title = {Pause-Based Phrase Extraction and Effective OOV Handling for Low-Resource Machine Translation Systems},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3265751},
doi = {10.1145/3265751},
abstract = {Machine translation is the core problem for several natural language processing research across the globe. However, building a translation system involving low-resource languages remains a challenge with respect to statistical machine translation (SMT). This work proposes and studies the effect of a phrase-induced hybrid machine translation system for translation from English to Tamil, under a low-resource setting. Unlike conventional hybrid MT systems, the free-word ordering feature of the target language Tamil is exploited to form a re-ordered target language model and to extend the parallel text corpus for training the SMT. In the current work, a novel rule-based phrase-extraction method, implemented using parts-of-speech (POS) and place-of-pause in both languages is proposed, which is used to pre-process the training corpus for developing the back-off phrase-induced SMT. Further, out-of-vocabulary (OOV) words are handled using speech-based transliteration and two-level thesaurus intersection techniques based on the POS tag of the OOV word. To ensure that the input with OOV words does not skip phrase-level translation in the hierarchical model, a phrase-level example-based machine translation approach is adopted to find the closest matching phrase and perform translation followed by OOV replacement. The proposed system results in a bilingual evaluation understudy score of 84.78 and a translation edit rate of 19.12. The performance of the system is compared in terms of adequacy and fluency, with existing translation systems for this specific language pair, and it is observed that the proposed system outperforms its counterparts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {12},
numpages = {22},
keywords = {POS, Low-resource machine translation, PL-EBMT, place-of-pause based phrase extraction, thesaurus intersection}
}

@article{10.1145/3241745,
author = {Na, Seung-hoon and Li, Jianri and Shin, Jong-hoon and Kim, Kangil},
title = {Transition-Based Korean Dependency Parsing Using Hybrid Word Representations of Syllables and Morphemes with LSTMs},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3241745},
doi = {10.1145/3241745},
abstract = {Recently, neural approaches for transition-based dependency parsing have become one of the state-of-the art methods for performing dependency parsing tasks in many languages. In neural transition-based parsing, a parser state representation is first computed from the configuration of a stack and a buffer, which is then fed into a feed-forward neural network model that predicts the next transition action. Given that words are basic elements of a stack and buffer, a parser state representation is considerably affected by how a word representation is defined. In particular, word representation issues become more critical in morphologically rich languages such as Korean, as the set of potential words is not bound but introduce the second-order vocabulary complexity, called the phrase vocabulary complexity due to the agglutinative characteristics of the language. In this article, we propose a hybrid word representation that combines two compositional word representations, each of which is derived from representations of syllables and morphemes, respectively. Our underlying assumption for this hybrid word representation is that, because both syllables and morphemes are two common ways of decomposing Korean words, it is expected that their effects in inducing word representation are complementary to one another. Experimental results carried on Sejong and SPMRL 2014 datasets show that our proposed hybrid word representation leads to the state-of-the-art performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {14},
numpages = {20},
keywords = {dependency parsing, syllables, compositional model, morphemes, Stack LSTM, hybrid approach, word representation}
}

@article{10.1145/3277504,
author = {Kamila, Sabyasachi and Hasanuzzaman, Mohammad and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {Tempo-HindiWordNet: A Lexical Knowledge-Base for Temporal Information Processing},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3277504},
doi = {10.1145/3277504},
abstract = {Temporality has significantly contributed to various Natural Language Processing and Information Retrieval applications. In this article, we first create a lexical knowledge-base in Hindi by identifying the temporal orientation of word senses based on their definition and then use this resource to detect underlying temporal orientation of the sentences. To create the resource, we propose a semi-supervised learning framework, where each synset of the Hindi WordNet is classified into one of the five categories, namely, past, present, future, neutral, and atemporal. The algorithm initiates learning with a set of seed synsets and then iterates following different expansion strategies, viz. probabilistic expansion based on classifier’s confidence and semantic distance based measures. We manifest the usefulness of the resource that we build on an external task, viz. sentence-level temporal classification. The underlying idea is that a temporal knowledge-base can help in classifying the sentences according to their inherent temporal properties. Experiments on two different domains, viz. general and Twitter, show interesting results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {19},
numpages = {22},
keywords = {Hindi, semi-supervised machine learning, sentence-level temporality detection, Temporal sense detection}
}

@article{10.1145/3276473,
author = {Nakamura, Tatsuya and Shirakawa, Masumi and Hara, Takahiro and Nishio, Shojiro},
title = {Wikipedia-Based Relatedness Measurements for Multilingual Short Text Clustering},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3276473},
doi = {10.1145/3276473},
abstract = {Throughout the world, people can post information about their local area in their own languages using social networking services. Multilingual short text clustering is an important task to organize such information, and it can be applied to various applications, such as event detection and summarization. However, measuring the relatedness between short texts written in various languages is a challenging problem. In addition to handling multiple languages, the semantic gaps among all languages must be considered. In this article, we propose two Wikipedia-based semantic relatedness measurement methods for multilingual short text clustering. The proposed methods solve the semantic gap problem by incorporating the inter-language links of Wikipedia into Extended Naive Bayes (ENB), a probabilistic method that can be applied to measure semantic relatedness among monolingual short texts. The proposed methods represent a multilingual short text as a vector of the English version of Wikipedia articles (entities). By transferring texts to a unified vector space, the relatedness between texts in different languages with similar meanings can be increased. We also propose an approach that can improve clustering performance and reduce the processing time by eliminating language-specific entities in the unified vector space. Experimental results on multilingual Twitter message clustering revealed that the proposed methods outperformed cross-lingual explicit semantic analysis, a previously proposed method to measure relatedness between texts in different languages. Moreover, the proposed methods were comparable to ENB applied to texts translated into English using a proprietary translation service. The proposed methods enabled relatedness measurements for multilingual short text clustering without requiring machine translation processes.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {16},
numpages = {25},
keywords = {multilingual text analysis, text clustering, Semantic relatedness, short text analysis}
}

@article{10.1145/3234512,
author = {Li, Junjie and Li, Haoran and Kang, Xiaomian and Yang, Haitong and Zong, Chengqing},
title = {Incorporating Multi-Level User Preference into Document-Level Sentiment Classification},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3234512},
doi = {10.1145/3234512},
abstract = {Document-level sentiment classification aims to predict a user’s sentiment polarity in a document about a product. Most existing methods only focus on review contents and ignore users who post reviews. In fact, when reviewing a product, different users have different word-using habits to express opinions (i.e., word-level user preference), care about different attributes of the product (i.e., aspect-level user preference), and have different characteristics to score the review (i.e., polarity-level user preference). These preferences have great influence on interpreting the sentiment of text. To address this issue, we propose a model called Hierarchical User Attention Network (HUAN), which incorporates multi-level user preference into a hierarchical neural network to perform document-level sentiment classification. Specifically, HUAN encodes different kinds of information (word, sentence, aspect, and document) in a hierarchical structure and imports user embedding and user attention mechanism to model these preferences. Empirical results on two real-world datasets show that HUAN achieves state-of-the-art performance. Furthermore, HUAN can also mine important attributes of products for different users.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {7},
numpages = {17},
keywords = {user preference, hierarchical attention network, Sentiment classification, deep learning}
}

@article{10.1145/3264620,
author = {Jain, Amita and Jain, Minni and Jain, Goonjan and Tayal, Devendra K.},
title = {“UTTAM”: An Efficient Spelling Correction System for Hindi Language Based on Supervised Learning},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3264620},
doi = {10.1145/3264620},
abstract = {In this article, we propose a system called “UTTAM,” for correcting spelling errors in Hindi language text using supervised learning. Unlike other languages, Hindi contains a large set of characters, words with inflections and complex characters, phonetically similar sets of characters, and so on. The complexity increases the possibility of confusion and occasionally leads to entering a wrong character in a word. The existence of spelling errors in text significantly decreases the accuracy of the available resources, like search engine, text editor, and so on. The proposed work is the first approach to correct non-word (Out of Vocabulary) errors as well as real-word errors simultaneously in a sentence of Hindi language. The proposed method investigates the human behavior, i.e., the type and frequency of spelling errors done by humans in Hindi text. Based on the type and frequency of spelling errors, the heterogeneous data is collected in matrices. This data in matrices is used to generate the suitable candidate words for an input word. After generating candidate words, the Viterbi algorithm is applied to perform the word correction. The Viterbi algorithm finds the best sequence of candidate words to correct the input sentence. For Hindi, this work is the first attempt for real-word error correction. For non-word errors, the experiments show that “UTTAM” performs better than the existing systems SpellGuru and Saksham.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {8},
numpages = {26},
keywords = {Natural language processing, viterbi algorithm, Hindi language, spelling correction}
}

@article{10.1145/3232537,
author = {Zhang, Shaoning and Mao, Cunli and Yu, Zhengtao and Wang, Hongbin and Li, Zhongwei and Zhang, Jiafu},
title = {Word Segmentation for Burmese Based on Dual-Layer CRFs},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3232537},
doi = {10.1145/3232537},
abstract = {Burmese is an isolated language, in which the syllable is the smallest unit. Syllable segmentation methods based on matching lead to performance subject to the syllable segmentation effect. This article proposes a word segmentation method with fusion conditions of double syllable features. It combines word segmentation and segmentation of syllables into one process, thus reducing the impact of errors on the syllable segmentation of Burmese. In the first layer of the conditional random fields (CRF) model, Burmese characters as atomic features are integrated into the Burma section of the Barkis Speech Paradigm (Backus normal form) features to realize the Burma syllable sequence tags. In the second layer of the CRFs model, with the syllable marked as input, it realizes the sequence markers through building a feature template with syllables as atomic features. The experimental results show that the proposed method has a better effect compared with the method based on the matching of syllables.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {6},
numpages = {11},
keywords = {syllable segmentation, CRFs, word segmentation, Burmese, BNF}
}

@article{10.1145/3230638,
author = {Huang, Guoping and Zhang, Jiajun and Zhou, Yu and Zong, Chengqing},
title = {Input Method for Human Translators: A Novel Approach to Integrate Machine Translation Effectively and Imperceptibly},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3230638},
doi = {10.1145/3230638},
abstract = {Computer-aided translation (CAT) systems are the most popular tool for helping human translators efficiently perform language translation. To further improve the translation efficiency, there is an increasing interest in applying machine translation (MT) technology to upgrade CAT. To thoroughly integrate MT into CAT systems, in this article, we propose a novel approach: a new input method that makes full use of the knowledge adopted by MT systems, such as translation rules, decoding hypotheses, and n-best translation lists. The proposed input method contains two parts: a phrase generation model, allowing human translators to type target sentences quickly, and an n-gram prediction model, helping users choose perfect MT fragments smoothly. In addition, to tune the underlying MT system to generate the input method preferable results, we design a new evaluation metric for the MT system. The proposed input method integrates MT effectively and imperceptibly, and it is particularly suitable for many target languages with complex characters, such as Chinese and Japanese. The extensive experiments demonstrate that our method saves more than 23% in time and over 42% in keystrokes, and it also improves the translation quality by more than 5 absolute BLEU scores compared with the strong baseline, i.e., post-editing using Google Pinyin.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {4},
numpages = {22},
keywords = {evaluation metric, input method, Machine translation, computer-aided translation}
}

@article{10.1145/3229184,
author = {Su, Ming-Hsiang and Wu, Chung-Hsien and Huang, Kun-Yi and Lin, Wu-Hsuan},
title = {Response Selection and Automatic Message-Response Expansion in Retrieval-Based QA Systems Using Semantic Dependency Pair Model},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3229184},
doi = {10.1145/3229184},
abstract = {This article presents an approach to response selection and message-response (MR) database expansion from the unstructured data on the psychological consultation websites for a retrieval-based question answering (QA) system in a constrained domain for emotional support and comforting. First, we manually construct an initial MR database based on the articles collected from the psychological consultation websites. The Chinese Knowledge and Information Processing probabilistic context-free grammar is adopted to obtain the semantic dependency graphs (SDGs) of all the messages and responses in the initial MR database. For each sentence in the MR database, all the semantic dependencies, each composed of two words and their semantic relation, are extracted from the SDG of the sentence to form a semantic dependency set. Finally, a matrix with the element representing the correlation between the semantic dependencies of the messages and their corresponding responses is constructed as a semantic dependency pair model (SDPM) for response selection. Moreover, as the number of MR pairs in the psychological consultation websites is increasing day by day, the MR database in the QA system should be expanded to meet the needs of the users. For MR database expansion, the unstructured data from the message board are automatically collected. For the collected data, the supervised latent Dirichlet allocation is adopted for event detection and then the event-based delta Bayesian Information Criterion is used for message and response article segmentation. Each extracted message segment is then fed to the constructed retrieval-based QA system to find the best matched response segment and the matching score is also estimated to verify if the new MR pair is suitable to be included in the expanded MR database. Fivefold cross validation was employed to evaluate the performance of the proposed retrieval-based QA system over the expanded MR database based on SDPM. Compared to the&nbsp;vector space model-based method, the Okapi BM25 model, and the deep learning-based sequence-to-sequence with attention model, the proposed approach achieved a more favorable performance according to a statistical significance test. The retrieval accuracy based on MR expansion was also evaluated and a satisfactory result was obtained confirming the effectiveness of the expanded MR database. In addition, the user's satisfaction score of the proposed system was evaluated using the Cronbach's alpha value and the satisfaction score of the proposed SDPM was higher than those of the methods for comparison.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {3},
numpages = {24},
keywords = {Retrieval-based QA system, event detection, and semantic dependency, message and response expansion}
}

@article{10.1145/3226045,
author = {Li, Maoxi and Wang, Mingwen},
title = {Optimizing Automatic Evaluation of Machine Translation with the ListMLE Approach},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3226045},
doi = {10.1145/3226045},
abstract = {Automatic evaluation of machine translation is critical for the evaluation and development of machine translation systems. In this study, we propose a new model for automatic evaluation of machine translation. The proposed model combines standard n-gram precision features and sentence semantic mapping features with neural features, including neural language model probabilities and the embedding distances between translation outputs and their reference translations. We optimize the model with a representative list-wise learning to rank approach, ListMLE, in terms of human ranking assessments. The experimental results on WMT’2015 Metrics task indicated that the proposed approach yields significantly better correlations with human assessments than several state-of-the-art baseline approaches. In particular, the results confirmed that the proposed list-wise learning to rank approach is useful and powerful for optimizing automatic evaluation metrics in terms of human ranking assessments. Deep analysis also demonstrated that optimizing automatic metrics with the ListMLE approach is a reasonable method and adding the neural features can gain considerable improvements compared with the traditional features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {2},
numpages = {18},
keywords = {system-level correlation, segment-level consistency, recurrent neural network language model, learning to rank, Automatic evaluation of machine translation, word embedding}
}

@article{10.1145/3236391,
author = {Altakrori, Malik H. and Iqbal, Farkhund and Fung, Benjamin C. M. and Ding, Steven H. H. and Tubaishat, Abdallah},
title = {Arabic Authorship Attribution: An Extensive Study on Twitter Posts},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3236391},
doi = {10.1145/3236391},
abstract = {Law enforcement faces problems in tracing the true identity of offenders in cybercrime investigations. Most offenders mask their true identity, impersonate people of high authority, or use identity deception and obfuscation tactics to avoid detection and traceability. To address the problem of anonymity, authorship analysis is used to identify individuals by their writing styles without knowing their actual identities. Most authorship studies are dedicated to English due to its widespread use over the Internet, but recent cyber-attacks such as the distribution of Stuxnet indicate that Internet crimes are not limited to a certain community, language, culture, ideology, or ethnicity. To effectively investigate cybercrime and to address the problem of anonymity in online communication, there is a pressing need to study authorship analysis of languages such as Arabic, Chinese, Turkish, and so on. Arabic, the focus of this study, is the fourth most widely used language on the Internet. This study investigates authorship of Arabic discourse/text, especially tiny text, Twitter posts. We benchmark the performance of a profile-based approach that uses n-grams as features and compare it with state-of-the-art instance-based classification techniques. Then we adapt an event-visualization tool that is developed for English to accommodate both Arabic and English languages and visualize the result of the attribution evidence. In addition, we investigate the relative effect of the training set, the length of tweets, and the number of authors on authorship classification accuracy. Finally, we show that diacritics have an insignificant effect on the attribution process and part-of-speech tags are less effective than character-level and word-level n-grams.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {5},
numpages = {51},
keywords = {visualization, social media, twitter, Authorship attribution, short text}
}

@article{10.1145/3213544,
author = {Wang, Limin and Li, Shoushan and Yan, Qian and Zhou, Guodong},
title = {Domain-Specific Named Entity Recognition with Document-Level Optimization},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3213544},
doi = {10.1145/3213544},
abstract = {Previous studies normally formulate named entity recognition (NER) as a sequence labeling task and optimize the solution in the sentence level. In this article, we propose a document-level optimization approach to NER and apply it in a domain-specific document-level NER task. As a baseline, we apply a state-of-the-art approach, i.e., long-short-term memory (LSTM), to perform word classification. On this basis, we define a global objective function with the obtained word classification results and achieve global optimization via Integer Linear Programming (ILP). Specifically, in the ILP-based approach, we propose four kinds of constraints, i.e., label transition, entity length, label consistency, and domain-specific regulation constraints, to incorporate various entity recognition knowledge in the document level. Empirical studies demonstrate the effectiveness of the proposed approach to domain-specific document-level NER.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {33},
numpages = {15},
keywords = {Integer linear programming, Named entity recognition, Chinese language processing}
}

@article{10.1145/3203078,
author = {Wang, Rui and Zhao, Hai and Ploux, Sabine and Lu, Bao-Liang and Utiyama, Masao and Sumita, Eiichiro},
title = {Graph-Based Bilingual Word Embedding for Statistical Machine Translation},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3203078},
doi = {10.1145/3203078},
abstract = {Bilingual word embedding has been shown to be helpful for Statistical Machine Translation (SMT). However, most existing methods suffer from two obvious drawbacks. First, they only focus on simple contexts such as an entire document or a fixed-sized sliding window to build word embedding and ignore latent useful information from the selected context. Second, the word sense but not the word should be the minimal semantic unit; however, most existing methods still use word representation.To overcome these drawbacks, this article presents a novel Graph-Based Bilingual Word Embedding (GBWE) method that projects bilingual word senses into a multidimensional semantic space. First, a bilingual word co-occurrence graph is constructed using the co-occurrence and pointwise mutual information between the words. Then, maximum complete subgraphs (cliques), which play the role of a minimal unit for bilingual sense representation, are dynamically extracted according to the contextual information. Consequently, correspondence analysis, principal component analyses, and neural networks are used to summarize the clique-word matrix into lower dimensions to build the embedding model.Without contextual information, the proposed GBWE can be applied to lexical translation. In addition, given contextual information, GBWE is able to give a dynamic solution for bilingual word representations, which can be applied to phrase translation and generation. Empirical results show that GBWE can enhance the performance of lexical translation, as well as Chinese/French-to-English and Chinese-to-Japanese phrase-based SMT tasks (IWSLT, NTCIR, NIST, and WAT).},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {31},
numpages = {23},
keywords = {bilingual word embedding, Statistical machine translation, lexical translation}
}

@article{10.1145/3196278,
author = {He, Ruifang and Wang, Yaru and Song, Dawei and Zhang, Peng and Jia, Yuan and Li, Aijun},
title = {A Dependency Parser for Spontaneous Chinese Spoken Language},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3196278},
doi = {10.1145/3196278},
abstract = {Dependency analysis is vital for spoken language understanding in spoken dialogue systems. However, existing research has mainly focused on western spoken languages, Japanese, and so on. Little research has been done for spoken Chinese in terms of dependency parsing. Therefore, the new spoken corpus, D-ESCSC (Dependency-Expressive Speech Corpus of Standard Chinese) is built by adding new dependency relations special to spoken Chinese based on a written Chinese annotation scheme. Since spoken Chinese contains typical ill-grammatical phenomena, e.g., translocation, repetition, duplication, and omission, the new atom feature related to punctuation and three feature templates are proposed to improve a graph-based dependency parser. Experimental results on spoken Chinese corpus show that the atom feature and three templates really work and the new parser outperforms the baseline parser. To our best knowledge, it is the first work to report dependency parsing results of spoken Chinese.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {28},
numpages = {13},
keywords = {spoken language, Dependency parsing, graph-based model, spontaneous Chinese}
}

@article{10.1145/3195634,
author = {Suryani, Arie Ardiyanti and Widyantoro, Dwi Hendratmo and Purwarianti, Ayu and Sudaryat, Yayat},
title = {The Rule-Based Sundanese Stemmer},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3195634},
doi = {10.1145/3195634},
abstract = {Our research proposed an iterative Sundanese stemmer by removing the derivational affixes prior to the inflexional. This scheme was chosen because, in the Sundanese affixation, a confix (one of derivational affix) is applied in the last phase of a morphological process. Moreover, most of Sundanese affixes are derivational, so removing the derivational affix as the first step is reasonable. To handle ambiguity, the last recognized affix was returned as the result. As the baseline, a Confix-Stripping Approach that applies Porter Stemmer for the Indonesian language was used. This stemmer shares similarities in terms of affix type, but uses a different stemming order. To observe whether the baseline stems the Sundanese affixed word properly, some features that were not covered by the baseline, such as the infix and allomorph removal, were added. The evaluation was done using 4,453 unique affixed words collected from Sundanese online magazines. The experiment shows that, as a whole, our stemmer outperforms the modified baseline in terms of recognized affixed type accuracy and properly stemmed affixed words. Our stemmer recognized 68.87% of the Sundanese affixed types and produced 96.79% of the correctly affixed words; the modified baseline resulted in 21.70% and 71.59%, respectively},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {27},
numpages = {28},
keywords = {Stemming, over-stemming, under-stemming, affixes, mis-stemming, stem-word}
}

@article{10.1145/3214707,
author = {Zhou, Deyu and Zhang, Zhikai and Zhang, Min-Ling and He, Yulan},
title = {Weakly Supervised POS Tagging without Disambiguation},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3214707},
doi = {10.1145/3214707},
abstract = {Weakly supervised part-of-speech (POS) tagging is to learn to predict the POS tag for a given word in context by making use of partial annotated data instead of the fully tagged corpora. Weakly supervised POS tagging would benefit various natural language processing applications in such languages where tagged corpora are mostly unavailable.In this article, we propose a novel framework for weakly supervised POS tagging based on a dictionary of words with their possible POS tags. In the constrained error-correcting output codes (ECOC)-based approach, a unique L-bit vector is assigned to each POS tag. The set of bitvectors is referred to as a coding matrix with value { 1, -1}. Each column of the coding matrix specifies a dichotomy over the tag space to learn a binary classifier. For each binary classifier, its training data is generated in the following way: each pair of words and its possible POS tags are considered as a positive training example only if the whole set of its possible tags falls into the positive dichotomy specified by the column coding and similarly for negative training examples. Given a word in context, its POS tag is predicted by concatenating the predictive outputs of the L binary classifiers and choosing the tag with the closest distance according to some measure. By incorporating the ECOC strategy, the set of all possible tags for each word is treated as an entirety without the need of performing disambiguation. Moreover, instead of manual feature engineering employed in most previous POS tagging approaches, features for training and testing in the proposed framework are automatically generated using neural language modeling. The proposed framework has been evaluated on three corpora for English, Italian, and Malagasy POS tagging, achieving accuracies of 93.21%, 90.9%, and 84.5% individually, which shows a significant improvement compared to the state-of-the-art approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {35},
numpages = {19},
keywords = {disambiguation, POS tagging, weakly supervised, error-correcting output codes}
}

@article{10.1145/3218820,
author = {Komiya, Kanako and Suzuki, Masaya and Iwakura, Tomoya and Sasaki, Minoru and Shinnou, Hiroyuki},
title = {Comparison of Methods to Annotate Named Entity Corpora},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3218820},
doi = {10.1145/3218820},
abstract = {The authors compared two methods for annotating a corpus for the named entity (NE) recognition task using non-expert annotators: (i) revising the results of an existing NE recognizer and (ii) manually annotating the NEs completely. The annotation time, degree of agreement, and performance were evaluated based on the gold standard. Because there were two annotators for one text for each method, two performances were evaluated: the average performance of both annotators and the performance when at least one annotator is correct. The experiments reveal that semi-automatic annotation is faster, achieves better agreement, and performs better on average. However, they also indicate that sometimes, fully manual annotation should be used for some texts whose document types are substantially different from the training data document types. In addition, the machine learning experiments using semi-automatic and fully manually annotated corpora as training data indicate that the F-measures could be better for some texts when manual instead of semi-automatic annotation was used. Finally, experiments using the annotated corpora for training as additional corpora show that (i) the NE recognition performance does not always correspond to the performance of the NE tag annotation and (ii) the system trained with the manually annotated corpus outperforms the system trained with the semi-automatically annotated corpus with respect to newswires, even though the existing NE recognizer was mainly trained with newswires.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {34},
numpages = {16},
keywords = {Annotation, non-expert annotator, named entity extraction}
}

@article{10.1145/3209885,
author = {Hamdi, Ali and Shaban, Khaled and Zainal, Anazida},
title = {CLASENTI: A Class-Specific Sentiment Analysis Framework},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3209885},
doi = {10.1145/3209885},
abstract = {Arabic text sentiment analysis suffers from low accuracy due to Arabic-specific challenges (e.g., limited resources, morphological complexity, and dialects) and general linguistic issues (e.g., fuzziness, implicit sentiment, sarcasm, and spam). The limited resources problem requires efforts to build new and improved Arabic corpora and lexica. We propose a class-specific sentiment analysis (CLASENTI) framework. The framework includes a new annotation approach to build multi-faceted Arabic corpus and lexicon allowing for simultaneous annotation of different facets, including domains, dialects, linguistic issues, and polarity strengths. Each of these facets has multiple classes (e.g., the nine classes representing dialects found in the Arab world). The new corpus and lexicon annotations facilitate the development of new class-specific classification models and polarity strength calculation. For the new sentiment classification models, we propose a hybrid model combining corpus-based and lexicon-based models. The corpus-based model has two interrelated phases to build; (1) full-corpus classification models for all facets; and (2) class-specific models trained on filtered subsets of the corpus according to the performances of the full-corpus models. To calculate polarity strengths, the lexicon-based model filters the annotated lexicon based on the specific classes of the domain and dialect. As a case study, we collect and annotate 15274 reviews from various sources, including surveys, Facebook comments, and Twitter posts, pertaining to governmental services. In addition, we develop a new web-based application to apply the proposed framework on the case study. CLASENTI framework reaches up to 95% accuracy and 93% F1-Score surpassing the best-known sentiment classifiers implemented in Scikit-learn library that achieve 82% accuracy and 81% F1-Score for Arabic when tested on the same dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {32},
numpages = {28},
keywords = {text mining, Arabic language processing, Sentiment analysis, opinion mining}
}

@article{10.1145/3197566,
author = {Bai, Xuefeng and Cao, Hailong and Zhao, Tiejun},
title = {Improving Vector Space Word Representations Via Kernel Canonical Correlation Analysis},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3197566},
doi = {10.1145/3197566},
abstract = {Cross-lingual word embeddings are representations for vocabularies of two or more languages in one common continuous vector space and are widely used in various natural language processing tasks. A state-of-the-art way to generate cross-lingual word embeddings is to learn a linear mapping, with an assumption that the vector representations of similar words in different languages are related by a linear relationship. However, this assumption does not always hold true, especially for substantially different languages. We therefore propose to use kernel canonical correlation analysis to capture a non-linear relationship between word embeddings of two languages. By extensively evaluating the learned word embeddings on three tasks (word similarity, cross-lingual dictionary induction, and cross-lingual document classification) across five language pairs, we demonstrate that our proposed approach achieves essentially better performances than previous linear methods on all of the three tasks, especially for language pairs with substantial typological difference.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {29},
numpages = {16},
keywords = {Cross-lingual word representation, kernel canonical correlation analysis (KCCA), word embedding evaluation}
}

@article{10.1145/3197657,
author = {Park, Taekeun and Kim, Seung-Hoon},
title = {Novel Character Identification Utilizing Semantic Relation with Animate Nouns in Korean},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3197657},
doi = {10.1145/3197657},
abstract = {For identifying speakers of quoted speech or extracting social networks from literature, it is indispensable to extract character names and nominals. However, detecting proper nouns in the novels translated into or written in Korean is harder than in English because Korean does not have a capitalization feature. In addition, it is almost impossible for any proper noun dictionary to include all kinds of character names that have been created or will be created by authors. Fortunately, a previous study shows that utilizing postpositions for animate nouns is a simple and effective tool for character identification in Korean novels without a proper noun dictionary and a training corpus. In this article, we propose a character identification method utilizing the semantic relation with known animate nouns. For 80 novels in Korean, the proposed method increases the micro- and macro-average recall by 13.68% and 11.86%, respectively, while decreasing the micro-average precision by 0.28% and increasing the macro-average precision by 0.07% compared to the previous study. If we focus on characters that are responsible for more than 1% of the character name mentions in each novel, the micro- and macro-average F-measure of the proposed method are 96.98% and 97.32%, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {30},
numpages = {17},
keywords = {animate nouns, semantic relation, Character identification, novels translated into or written in Korean}
}

@article{10.1145/3195633,
author = {Basiri, Mohammad Ehsan and Kabiri, Arman},
title = {Words Are Important: Improving Sentiment Analysis in the Persian Language by Lexicon Refining},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3195633},
doi = {10.1145/3195633},
abstract = {Lexicon-based sentiment analysis (SA) aims to address the problem of extracting people’s opinions from their comments on the Web using a predefined lexicon of opinionated words. In contrast to the machine learning (ML) approach, lexicon-based methods are domain-independent methods that do not need a large annotated training corpus and hence are faster. This makes the lexicon-based approach prevalent in the SA community. However, the story is different for the Persian language. In contrast to English, using the lexicon-based method in Persian is a new discipline. There are rather limited resources available for SA in Persian, making the accuracy of the existing lexicon-based methods lower than other languages. In the current study, first an exhaustive investigation of the lexicon-based method is performed. Then two new resources are introduced to address the problem of resource scarcity for SA in Persian: a carefully labeled lexicon of sentiment words, PerLex, and a new handmade dataset of about 16,000 rated documents, PerView. Moreover, a new hybrid method using both ML and the lexicon-based approach is presented in which PerLex words are used to train the ML algorithm. Experiments are carried out on our new PerView dataset. Results indicate that the accuracy of PerLex is higher than the existing CNRC, Adjectives, SentiStrength, PerSent, and LexiPers lexicons. In addition, the results show that using PerLex significantly decreases the execution time of the proposed system in comparison to the above-mentioned lexicons. Moreover, the results demonstrate the excellence of using opinionated lexicon terms followed by bigrams as the features employed in the ML method.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {26},
numpages = {18},
keywords = {Sentiment analysis, machine learning, Persian language, lexicon-based approach, opinion mining, PerView dataset}
}

@article{10.1145/3185664,
author = {Ehsani, Razieh and Solak, Ercan and Yildiz, Olcay Taner},
title = {Constructing a WordNet for Turkish Using Manual and Automatic Annotation},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3185664},
doi = {10.1145/3185664},
abstract = {In this article, we summarize the methodology and the results of our 2-year-long efforts to construct a comprehensive WordNet for Turkish. In our approach, we mine a dictionary for synonym candidate pairs and manually mark the senses in which the candidates are synonymous. We marked every pair twice by different human annotators. We derive the synsets by finding the connected components of the graph whose edges are synonym senses. We also mined Turkish Wikipedia for hypernym relations among the senses. We analyzed the resulting WordNet to highlight the difficulties brought about by the dictionary construction methods of lexicographers. After splitting the unusually large synsets, we used random walk–based clustering that resulted in a Zipfian distribution of synset sizes. We compared our results to BalkaNet and automatic thesaurus construction methods using variation of information metric. Our Turkish WordNet is available online.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {24},
numpages = {15},
keywords = {synset, Turkish, WordNet}
}

@article{10.1145/3185663,
author = {Huang, Jizhou and Ding, Shiqiang and Wang, Haifeng and Liu, Ting},
title = {Learning to Recommend Related Entities With Serendipity for Web Search Users},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3185663},
doi = {10.1145/3185663},
abstract = {Entity recommendation, providing entity suggestions to assist users in discovering interesting information, has become an indispensable feature of today’s Web search engine. However, the majority of existing entity recommendation methods are not designed to boost the performance in terms of serendipity, which also plays an important role in the appreciation of users for a recommendation system. To keep users engaged, it is important to take into account serendipity when building an entity recommendation system. In this article, we propose a learning to recommend framework that consists of two components: related entity finding and candidate entity ranking. To boost serendipity performance, three different sets of features that correlate with the three aspects of serendipity are employed in the proposed framework. Extensive experiments are conducted on large-scale, real-world datasets collected from a widely used commercial Web search engine. The experiments show that our method significantly outperforms several strong baseline methods. An analysis on the impact of features reveals that the set of interestingness features is the most powerful feature set, and the set of unexpectedness features can significantly contribute to recommendation effectiveness. In addition, online controlled experiments conducted on a commercial Web search engine demonstrate that our method can significantly improve user engagement against multiple baseline methods. This further confirms the effectiveness of the proposed framework.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {25},
numpages = {22},
keywords = {Serendipity, serendipitous recommendations, entity recommendation, Web search, serendipitous entities, recommender system}
}

@article{10.1145/3178459,
author = {Mohamed, Emad},
title = {Morphological Segmentation and Part-of-Speech Tagging for the Arabic Heritage},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178459},
doi = {10.1145/3178459},
abstract = {We annotate 60,000 words of Classical Arabic (CA) with topics in philosophy, religion, literature, and law with fine-grain segment-based morphological descriptions. We use these annotations for building a morphological segmenter and part-of-speech (POS) tagger for CA. With character-level classification and features from the word and its lexical context, the segmenter achieves a word accuracy of 96.8% with the main issue being a high rate of out-of-vocabulary words. A token-based POS tagger achieves an accuracy of 96.22% with 97.72% on known tokens despite the small size of the corpus. An error analysis shows that most of the tagging errors are results of segmentation and that quality improves with more data being added. The morphological segmenter and tagger have a wide range of potential applications in processing CA, a low-resource variety of the language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {22},
numpages = {13},
keywords = {segmentation, heritage, part-of-speech tagging, Arabic, morphological analysis}
}

@article{10.1145/3182622,
author = {Huang, Degen and Pei, Jiahuan and Zhang, Cong and Huang, Kaiyu and Ma, Jianjun},
title = {Incorporating Prior Knowledge into Word Embedding for Chinese Word Similarity Measurement},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3182622},
doi = {10.1145/3182622},
abstract = {Word embedding-based methods have received increasing attention for their flexibility and effectiveness in many natural language-processing (NLP) tasks, including Word Similarity (WS). However, these approaches rely on high-quality corpus and neglect prior knowledge. Lexicon-based methods concentrate on human’s intelligence contained in semantic resources, e.g., Tongyici Cilin, HowNet, and Chinese WordNet, but they have the drawback of being unable to deal with unknown words. This article proposes a three-stage framework for measuring the Chinese word similarity by incorporating prior knowledge obtained from lexicons and statistics into word embedding: in the first stage, we utilize retrieval techniques to crawl the contexts of word pairs from web resources to extend context corpus. In the next stage, we investigate three types of single similarity measurements, including lexicon similarities, statistical similarities, and embedding-based similarities. Finally, we exploit simple combination strategies with math operations and the counter-fitting combination strategy using optimization method. To demonstrate our system’s efficiency, comparable experiments are conducted on the PKU-500 dataset. Our final results are 0.561/0.516 of Spearman/Pearson rank correlation coefficient, which outperform the state-of-the-art performance to the best of our knowledge. Experiment results on Chinese MC-30 and SemEval-2012 datasets show that our system also performs well on other Chinese datasets, which proves its transferability. Besides, our system is not language-specific and can be applied to other languages, e.g., English.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {23},
numpages = {21},
keywords = {word embedding, prior knowledge, Chinese word similarity}
}

@article{10.1145/3178460,
author = {Salami, Shahram and Shamsfard, Mehrnoush},
title = {Integrating Shallow Syntactic Labels in the Phrase-Boundary Translation Model},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178460},
doi = {10.1145/3178460},
abstract = {Using a novel rule labeling method, this article proposes a hierarchical model for statistical machine translation. The proposed model labels translation rules by matching the boundaries of target side phrases with the shallow syntactic labels including POS tags and chunk labels on the target side of the training corpus. The boundary labels are concatenated if there is no label for the whole target span. Labeling with the classes of boundary words on the target side phrases has been previously proposed as a phrase-boundary model which can be considered as the base form of our model. In the extended model, the labeler uses a POS tag if there is no chunk label in one boundary. Using chunks as phrase labels, the proposed model generalizes the rules to decrease the model sparseness. The sparseness is a more important issue in the language pairs with a lot of differences in the word order because they have less number of aligned phrase pairs for extraction of rules. The extended phrase-boundary model is also applicable for low-resource languages having no syntactic parser. Some experiments are performed with the proposed model, the base phrase-boundary model, and variants of Syntax Augmented Machine Translation (SAMT) in translation from Persian and German to English as source and target languages with different word orders. According to the results, the proposed model improves the translation performance in the quality and decoding time aspects. Using BLEU as our metric, the proposed model has achieved a statistically significant improvement of about 0.5 point over the base phrase-boundary model.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {17},
numpages = {12},
keywords = {POS tag, Chunk label, Hierarchical models, Statistical machine translation}
}

@article{10.1145/3178458,
author = {Jung, Sangkeun and Lee, Changki and Hwang, Hyunsun},
title = {End-to-End Korean Part-of-Speech Tagging Using Copying Mechanism},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178458},
doi = {10.1145/3178458},
abstract = {In this article, we introduce a novel neural architecture for the end-to-end Korean Part-of-Speech (POS) tagging problem. To address the problem, we extend the present recurrent neural network-based sequence-to-sequence models to deal with the key challenges in this task: rare word generation and POS tagging. To overcome these issues, Input-Feeding and Copying mechanism are adopted. Although our approach does not require any manual features or preprocessed pattern matching dictionaries, our best single model achieves an F-score of 97.08. This is competitive with the current state-of-the-art model (F-score 98.03), which requires extensive manual feature processing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {19},
numpages = {8},
keywords = {Part-of-speech tagging, deep learning, copying mechanism}
}

@article{10.1145/3162077,
author = {Jia, Shengbin and E, Shijia and Li, Maozhen and Xiang, Yang},
title = {Chinese Open Relation Extraction and Knowledge Base Establishment},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3162077},
doi = {10.1145/3162077},
abstract = {Named entity relation extraction is an important subject in the field of information extraction. Although many English extractors have achieved reasonable performance, an effective system for Chinese relation extraction remains undeveloped due to the lack of Chinese annotation corpora and the specificity of Chinese linguistics. Here, we summarize three kinds of unique but common phenomena in Chinese linguistics. In this article, we investigate unsupervised linguistics-based Chinese open relation extraction (ORE), which can automatically discover arbitrary relations without any manually labeled datasets, and research the establishment of a large-scale corpus. By mapping the entity relations into dependency-trees and considering the unique Chinese linguistic characteristics, we propose a novel unsupervised Chinese ORE model based on Dependency Semantic Normal Forms (DSNFs). This model imposes no restrictions on the relative positions among entities and relationships and achieves a high yield by extracting relations mediated by verbs or nouns and processing the parallel clauses. Empirical results from our model demonstrate the effectiveness of this method, which obtains stable performance on four heterogeneous datasets and achieves better precision and recall in comparison with several Chinese ORE systems. Furthermore, a large-scale knowledge base of entity and relation, called COER, is established and published by applying our method to web text, which conquers the trouble of lack of Chinese corpora.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {15},
numpages = {22},
keywords = {dependency parsing, linguistics, and knowledge base, unsupervised, Chinese entity relation extraction, open}
}

@article{10.1145/3178456,
author = {She, Xiaohan and Jian, Ping and Zhang, Pengcheng and Huang, Heyan},
title = {Leveraging Hierarchical Deep Semantics to Classify Implicit Discourse Relations via a Mutual Learning Method},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178456},
doi = {10.1145/3178456},
abstract = {This article presents a mutual learning method using hierarchical deep semantics for the classification of implicit discourse relations in English. With the absence of explicit discourse markers, traditional discourse techniques mainly concentrate on discrete linguistic features in this task, which always leads to a data sparseness problem. To relieve this problem, we propose a mutual learning neural model that makes use of multilevel semantic information together, including the distribution of implicit discourse relations, the semantics of arguments, and the co-occurrence of phrases and words. During the training process, the predicting targets of the model, which are the probability of the discourse relation type and the distributed representation of semantic components, are learned jointly and optimized mutually. The experimental results show that this method outperforms the previous works, especially in multiclass identification attributed to the hierarchical semantic representations and the mutual learning strategy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {21},
numpages = {12},
keywords = {Mutual learning neural network, Implicit discourse relation classification, Hierarchical deep semantics}
}

@article{10.1145/3168054,
author = {Marie, Benjamin and Fujita, Atsushi},
title = {Phrase Table Induction Using Monolingual Data for Low-Resource Statistical Machine Translation},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3168054},
doi = {10.1145/3168054},
abstract = {We propose a new method for inducing a phrase-based translation model from a pair of unrelated monolingual corpora. Our method is able to deal with phrases of arbitrary length and to find phrase pairs that are useful for statistical machine translation, without requiring large parallel or comparable corpora. First, our method generates phrase pairs through coupling source and target phrases separately collected from respective monolingual data. Then, for each phrase pair, we compute features using the monolingual data and a small quantity of parallel sentences. Finally, incorrect phrase pairs are pruned, and a phrase table is made using the remaining phrase pairs. In our experiments on French--Japanese and Spanish--Japanese translation tasks under low-resource conditions, we observe that incorporating a phrase table induced by our method to the machine translation system leads to large improvements in translation quality. Furthermore, we show that a phrase table induced by our method can also be useful in a wide range of configurations, including configurations where we have already access to large parallel corpora and configurations where only small monolingual corpora are available.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {16},
numpages = {25},
keywords = {phrase table induction, Machine translation, knowledge acquisition, semantic similarity, low-resourced language pairs}
}

@article{10.1145/3170576,
author = {Sherkawi, Lina and Ghneim, Nada and Dakkak, Oumayma Al},
title = {Arabic Speech Act Recognition Techniques},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3170576},
doi = {10.1145/3170576},
abstract = {This article presents rule-based and statistical-based techniques for Arabic speech act recognition. The proposed techniques classify an utterance into Arabic speech act categories based on three criteria: surface features, cue words, and contextual information. A rule-based expert system has been developed in a bootstrapping manner based on the fact that Arabic language syntax is inherently rule-based. Various machine-learning algorithms have been used to detect Arabic speech act categories: Decision Tree, Na\"{\i}ve Bayes, Neural Network, and SVM. We compare the experimental results for both techniques (machine-learning and rule-based expert systems). Using a corpus of 1,500 sentences, the rule-based expert system achieved an accuracy rate of 98.92%, while the Decision Tree, Na\"{\i}ve Bayes, Neural Network, and SVM achieved an accuracy rate of 97.09%, 96.48%, 93.50%, and 93.70%, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {18},
numpages = {12},
keywords = {grammatical classification, corpus annotation, Bootstrapping, speech act, sentence type recognition}
}

@article{10.1145/3178457,
author = {Sen, Shibaprasad and Bhattacharyya, Ankan and Singh, Pawan Kumar and Sarkar, Ram and Roy, Kaushik and Doermann, David},
title = {Application of Structural and Topological Features to Recognize Online Handwritten Bangla Characters},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178457},
doi = {10.1145/3178457},
abstract = {This article presents a set of novel features for robust online Bangla handwritten character recognition. Two feature extraction methods are presented here. The first describes the transition from background to foreground pixels and vice versa. The second uses a combination of topological features and centre-of-gravity- (CG) based circular features where global information, local information, and Circular Quadrant Mass Distribution information have been extracted. The impact of each along with their combination have also been analyzed. A total of 15,000 isolated online Bangla character samples have been collected and used for the evaluation. A Support Vector Machine classifier records the best recognition rate when the transition count feature, CG-based circular features, and topological features are combined.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {20},
numpages = {16},
keywords = {transition count, circular quadrant mass distribution, global information, local information, bangla script, cg-based circle, Online handwriting recognition}
}

@article{10.1145/3156778,
author = {Wang, Shaonan and Zhang, Jiajun and Zong, Chengqing},
title = {Empirical Exploring Word-Character Relationship for Chinese Sentence Representation},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3156778},
doi = {10.1145/3156778},
abstract = {This article addresses the problem of learning compositional Chinese sentence representations, which represent the meaning of a sentence by composing the meanings of its constituent words. In contrast to English, a Chinese word is composed of characters, which contain rich semantic information. However, this information has not been fully exploited by existing methods. In this work, we introduce a novel, mixed character-word architecture to improve the Chinese sentence representations by utilizing rich semantic information of inner-word characters. We propose two novel strategies to reach this purpose. The first one is to use a mask gate on characters, learning the relation among characters in a word. The second one is to use a max-pooling operation on words to adaptively find the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task. To further verify the generalization ability of our model, we employ the learned sentence representations as features in sentence classification task, question classification task, and sentence entailment task. Results have shown that the proposed mixed character-word sentence representation models outperform both the character-based and word-based models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {14},
numpages = {18},
keywords = {mixed character-word representation, inner-word character, max pooling, composition model, Sentence representation, mask gate}
}

@article{10.1145/3160488,
author = {Fujita, Atsushi and Isabelle, Pierre},
title = {Expanding Paraphrase Lexicons by Exploiting Generalities},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3160488},
doi = {10.1145/3160488},
abstract = {Techniques for generating and recognizing paraphrases, i.e., semantically equivalent expressions, play an important role in a wide range of natural language processing tasks. In the last decade, the task of automatic acquisition of subsentential paraphrases, i.e., words and phrases with (approximately) the same meaning, has been drawing much attention in the research community. The core problem is to obtain paraphrases of high quality in large quantity. This article presents a method for tackling this issue by systematically expanding an initial seed lexicon made up of high-quality paraphrases. This involves automatically capturing morpho-semantic and syntactic generalizations within the lexicon and using them to leverage the power of large-scale monolingual data. Given an input set of paraphrases, our method starts by inducing paraphrase patterns that constitute generalizations over corresponding pairs of lexical variants, such as “amending” and “amendment,” in a fully empirical way. It then searches large-scale monolingual data for new paraphrases matching those patterns. The results of our experiments on English, French, and Japanese demonstrate that our method manages to expand seed lexicons by a large multiple. Human evaluation based on paraphrase substitution tests reveals that the automatically acquired paraphrases are also of high quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {13},
numpages = {36},
keywords = {Paraphrase, lexical variants, semantic similarity, knowledge acquisition}
}

@article{10.1145/3152464,
author = {Naili, Marwa and Chaibi, Anja Habacha and Ghezala, Henda Hajjami Ben},
title = {The Contribution of Stemming and Semantics in Arabic Topic Segmentation},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3152464},
doi = {10.1145/3152464},
abstract = {Topic Segmentation is one of the pillars of Natural Language Processing. Yet there is a remarkable research gap in this field, as far as the Arabic language is concerned. The purpose of this article is to improve Arabic Topic Segmentation (ATS) by inquiring into two segmenters: ArabC99 and ArabTextTiling. This study is carried out on two independent levels: the pre-processing level and the segmentation level. These levels represent the basic steps of topic segmentation. On the pre-processing level, we examine the effect of using different Arabic stemming algorithms on ATS. We find out that Light10 is more appropriate for the pre-processing step. Based on this conclusion, we proceed to the second level by proposing two Arabic segmenters called ArabC99-LS-LSA and ArabTextTiling-LS-LSA. These latter use external semantic knowledge related to the Latent Semantic Analysis (LSA). Based on the evaluation results, we notice that LSA provides improvements in this field. Hence, the main outcome of this article emphasizes the multilevel improvement of ATS based on Light10 and LSA.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {12},
numpages = {25},
keywords = {Arabc99, Arabic stemming algorithms, Arabic topic segmentation, ArabTextTiling}
}

@article{10.1145/3146387,
author = {Onyenwe, Ikechukwu E and Hepple, Mark and Chinedu, Uchechukwu and Ezeani, Ignatius},
title = {A Basic Language Resource Kit Implementation for the Igbo<i>NLP</i> Project},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3146387},
doi = {10.1145/3146387},
abstract = {Igbo, an African language with around 32 million speakers worldwide, is one of the many languages having few or none of the language processing resources needed for advanced language technology applications. In this article, we describe the approach taken to creating an initial set of resources for Igbo, including an electronic text corpus, a part-of-speech (POS) tagset, and a POS-tagged subcorpus. We discuss the approach taken in gathering texts, the preprocessing of these texts, and the development of the POS tagged corpus. We also discuss some of the problems encountered during corpus and tagset development and the solutions arrived at for these problems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {10},
numpages = {23},
keywords = {African language, interannotation agreement, language technology, corpus annotation, part-of-speech (POS) tagging, human annotator, Natural language processing (NLP), text processing, segmentation, tokenization, Igbo, morphology, corpora, tagset, normalization}
}

@article{10.1145/3152537,
author = {Jia, Yanyan and Feng, Yansong and Ye, Yuan and Lv, Chao and Shi, Chongde and Zhao, Dongyan},
title = {Improved Discourse Parsing with Two-Step Neural Transition-Based Model},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3152537},
doi = {10.1145/3152537},
abstract = {Discourse parsing aims to identify structures and relationships between different discourse units. Most existing approaches analyze a whole discourse at once, which often fails in distinguishing long-span relations and properly representing discourse units. In this article, we propose a novel parsing model to analyze discourse in a two-step fashion with different feature representations to characterize intra sentence and inter sentence discourse structures, respectively. Our model works in a transition-based framework and benefits from a stack long short-term memory neural network model. Experiments on benchmark tree banks show that our method outperforms traditional 1-step parsing methods in both English and Chinese.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {11},
numpages = {21},
keywords = {transition-based system, dependency parsing, LSTM, Discourse parsing}
}

@article{10.1145/3138815,
author = {Nasution, Arbi Haza and Murakami, Yohei and Ishida, Toru},
title = {A Generalized Constraint Approach to Bilingual Dictionary Induction for Low-Resource Language Families},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3138815},
doi = {10.1145/3138815},
abstract = {The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction a difficult task for low-resource languages. The pivot language and cognate recognition approaches have been proven useful for inducing bilingual lexicons for such languages. We propose constraint-based bilingual lexicon induction for closely related languages by extending constraints from the recent pivot-based induction technique and further enabling multiple symmetry assumption cycle to reach many more cognates in the transgraph. We further identify cognate synonyms to obtain many-to-many translation pairs. This article utilizes four datasets: one Austronesian low-resource language and three Indo-European high-resource languages. We use three constraint-based methods from our previous work, the Inverse Consultation method and translation pairs generated from Cartesian product of input dictionaries as baselines. We evaluate our result using the metrics of precision, recall, and F-score. Our customizable approach allows the user to conduct cross validation to predict the optimal hyperparameters (cognate threshold and cognate synonym threshold) with various combination of heuristics and number of symmetry assumption cycles to gain the highest F-score. Our proposed methods have statistically significant improvement of precision and F-score compared to our previous constraint-based methods. The results show that our method demonstrates the potential to complement other bilingual dictionary creation methods like word alignment models using parallel corpora for high-resource languages while well handling low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {9},
numpages = {29},
keywords = {cognate recognition, low-resource languages, closely-related languages, pivot-based bilingual lexicon induction, Constraint satisfaction problem}
}

@article{10.1145/3145538,
author = {Bhattacharya, Nilanjana and Pal, Umapada and Roy, Partha Pratim},
title = {Cleaning of Online Bangla Free-Form Handwritten Text},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3145538},
doi = {10.1145/3145538},
abstract = {In the normal free-form handwritten text, repetition (repeated writing of the same stroke several times in the same place), over-writing, and crossing out are very common. In this article, we call the presence of these three types of writing as “noise.” Cleaning to extract useful text from such types of noisy text is an important task for robust recognition. To the best of our knowledge, no work has been reported on cleaning of such noise from online text in any scripts and hence, in this article, we propose an automatic text-cleaning approach for online handwriting recognition. Here, at first, crossing out noise with straight strike-through lines is detected using the straightness criteria of online strokes. Next, regions containing repetition, over-writing, and other types of crossing out are located using the positional information of the overlapping strokes. Stroke density, self-intersections of strokes etc. are computed from the strokes of located regions to predict the type of noise and this type of information is used as follows for their cleaning. For cleaning of crossing outs, all strokes of the crossing-out region are removed. For cleaning repetition and over-writing, strokes written earlier are removed, keeping the latest strokes. Finally, delayed strokes are properly arranged and word is passed to online recognizer. Though recognition of free-form handwriting is quite difficult, in this attempt, we obtained up to 70.71% improvement in word-recognition accuracy after noise cleaning.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {8},
numpages = {25},
keywords = {Free-form handwriting, strike-through text detection, Indic script, lexicon driven approach, Bangla script}
}

@article{10.1145/3132684,
author = {Cheng, Xiyao and Chen, Ying and Cheng, Bixiao and Li, Shoushan and Zhou, Guodong},
title = {An Emotion Cause Corpus for Chinese Microblogs with Multiple-User Structures},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3132684},
doi = {10.1145/3132684},
abstract = {A notably challenging problem in emotion analysis is recognizing the cause of an emotion. Although there have been a few studies on emotion cause detection, most of them work on news reports or a few of them focus on microblogs using a single-user structure (i.e., all texts in a microblog are written by the same user). In this article, we focus on emotion cause detection for Chinese microblogs using a multiple-user structure (i.e., texts in a microblog are successively written by several users). First, based on the fact that the causes of an emotion of a focused user may be provided by other users in a microblog with the multiple-user structure, we design an emotion cause annotation scheme which can deal with such a complicated case, and then provide an emotion cause corpus using the annotation scheme. Second, based on the analysis of the emotion cause corpus, we formalize two emotion cause detection tasks for microblogs (current-subtweet-based emotion cause detection and original-subtweet-based emotion cause detection). Furthermore, in order to examine the difficulty of the two emotion cause detection tasks and the contributions of texts written by different users in a microblog with the multiple-user structure, we choose two popular classification methods (SVM and LSTM) to do emotion cause detection. Our experiments show that the current-subtweet-based emotion cause detection is much more difficult than the original-subtweet-based emotion cause detection, and texts written by different users are very helpful for both emotion cause detection tasks. This study presents a pilot study of emotion cause detection which deals with Chinese microblogs using a complicated structure.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {6},
numpages = {19},
keywords = {emotion cause detection, Emotion cause annotation, emotion cause corpus}
}

@article{10.1145/3137055,
author = {Sarma, Himangshu and Saharia, Navanath and Sharma, Utpal},
title = {Development and Analysis of Speech Recognition Systems for Assamese Language Using HTK},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3137055},
doi = {10.1145/3137055},
abstract = {Language analysis is very important for the native speaker to connect with the digital world. Assamese is a relatively unexplored language. In this report, we analyze different aspects of speech-to-text processing, starting from building a speech corpus, defining syllable rules, and finally developing a speech search engine of Assamese. We have collected about 20 hours of speech in three (viz., read, extempore, and conversation) modes and transcribed it. We also discuss some issues and challenges faced during development of the corpus. We have developed an automatic syllabification model with 11 rules for the Assamese language and found an accuracy of more than 95% in our result. We found 12 different syllable patterns where 5 are found most frequent. The maximum length of a syllable found is four letters. With the help of Hidden Markov Model Toolkit (HTK) 3.5, we used deep learning based neural network for our speech recognition model, where we obtained 78.05% accuracy for automatic transcription of Assamese speech.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {7},
numpages = {14},
keywords = {syllabification, speech corpus, automatic transcription, HTK, Assamese, Speech search engine}
}

@article{10.1145/3133323,
author = {Tran, Phuoc and Dinh, Dien and Le, Tan and Nguyen, Long H. B.},
title = {Linguistic-Relationships-Based Approach for Improving Word Alignment},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3133323},
doi = {10.1145/3133323},
abstract = {The unsupervised word alignments (such as GIZA++) are widely used in the phrase-based statistical machine translation. The quality of the model is proportional to the size and the quality of the bilingual corpus. However, for low-resource language pairs such as Chinese and Vietnamese, a result of unsupervised word alignment sometimes is of low quality due to the sparse data. In addition, this model does not take advantage of the linguistic relationships to improve performance of word alignment. Chinese and Vietnamese have the same language type and have close linguistic relationships. In this article, we integrate the characteristics of linguistic relationships into the word alignment model to enhance the quality of Chinese-Vietnamese word alignment. These linguistic relationships are Sino-Vietnamese and content word. The experimental results showed that our method improved the performance of word alignment as well as the quality of machine translation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {5},
numpages = {16},
keywords = {Sino-Vietnamese, linguistic relationships, Word alignment, content word, Chinese-Vietnamese machine translation}
}

@article{10.1145/3132708,
author = {Almeman, Khalid},
title = {Automatically Building VoIP Speech Parallel Corpora for Arabic Dialects},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3132708},
doi = {10.1145/3132708},
abstract = {This article discusses the process of automatically building Arabic multi-dialect speech corpora using Voice over Internet Protocol (VoIP). The Asterisk framework was adopted to act as the main connection between the parties, for which two virtual machines were created: a sender and a receiver. The sender makes a VoIP call to the receiver using the Asterisk framework, while the receiver records the call automatically, a process that is repeated for all the audio files involved in the corpora. In this work, more than 67,000 automatic calls were made between the sender and receiver machines, generating VoIP Arabic corpora for four Arabic dialects. The resulting corpora can be considered the first Arabic VoIP parallel speech corpora and will be made freely available to researchers in Arabic NLP and speech recognition research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {4},
numpages = {12},
keywords = {arabic multi-dialect, asterisk, VoIP corpora, arabic speech recognition}
}

@article{10.1145/3129208,
author = {S, Sreelekha and Bhattacharyya, Pushpak},
title = {Role of Morphology Injection in SMT: A Case Study from Indian Language Perspective},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3129208},
doi = {10.1145/3129208},
abstract = {Phrase-based Statistical Machine Translation (PBSMT) is commonly used for automatic translation. However, PBSMT runs into difficulty when either or both of the source and target languages are morphologically rich. Factored models are found to be useful for such cases, as they consider word as a vector of factors. These factors can contain any information about the surface word and use it while translating. The objective of the current work is to handle morphological inflections in Hindi, Marathi, and Malayalam using Factored translation models when translating from English. Statistical MT approaches face the problem of data sparsity when translating to a morphologically rich language. It is very unlikely for a parallel corpus to contain all morphological forms of words. We propose a solution to generate these unseen morphological forms and inject them into the original training corpus. We propose a simple and effective solution based on enriching the input with various morphological forms of words. We observe that morphology injection improves the quality of translation in terms of both adequacy and fluency. We verify this with experiments on three morphologically rich languages when translating from English. From the detailed evaluations, we observed an order of magnitude improvement in translation quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {1},
numpages = {31},
keywords = {morphology injection, factored statistical machine translation models, Statistical machine translation}
}

@article{10.1145/3129290,
author = {Malik, Muhammad Kamran},
title = {Urdu Named Entity Recognition and Classification System Using Artificial Neural Network},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3129290},
doi = {10.1145/3129290},
abstract = {Named Entity Recognition and Classification (NERC) is a process of identifying words and classifying them into person names, location names, organization names, and so on. In this article, we discuss the development of an Urdu Named Entity (NE) corpus, called the Kamran-PU-NE (KPU-NE) corpus, for three entity types, that is, Person, Organization, and Location, and marking the remaining tokens as Others (O). We use two supervised learning algorithms, Hidden Markov Model (HMM) and Artificial Neural Network (ANN), for the development of the Urdu NERC system. We annotate the 652852-token corpus taken from 15 different genres with a total of 44480 NEs. The inter-annotator agreement between the two annotators in terms of Kappa k statistic is 73.41%. With HMM, the highest recorded precision, recall, and f-measure values are 55.98%, 83.11%, and 66.90%, respectively, and with ANN, they are 81.05%, 87.54%, and 84.17%, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {2},
numpages = {13},
keywords = {NER Data, NER using Deep Learning, Deep Learning, Resource Poor Languages, Urdu POS tagged Data, Urdu word2vec}
}

@article{10.1145/3109480,
author = {Kim, Hyun and Jung, Hun-Young and Kwon, Hongseok and Lee, Jong-Hyeok and Na, Seung-Hoon},
title = {Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3109480},
doi = {10.1145/3109480},
abstract = {Recently, quality estimation has been attracting increasing interest from machine translation researchers, aiming at finding a good estimator for the “quality” of machine translation output. The common approach for quality estimation is to treat the problem as a supervised regression/classification task using a quality-annotated noisy parallel corpus, called quality estimation data, as training data. However, the available size of quality estimation data remains small, due to the too-expensive cost of creating such data. In addition, most conventional quality estimation approaches rely on manually designed features to model nonlinear relationships between feature vectors and corresponding quality labels. To overcome these problems, this article proposes a novel neural network architecture for quality estimation task—called the predictor-estimator—that considers word prediction as an additional pre-task. The major component of the proposed neural architecture is a word prediction model based on a modified neural machine translation model—a probabilistic model for predicting a target word conditioned on all the other source and target contexts. The underlying assumption is that the word prediction model is highly related to quality estimation models and is therefore able to transfer useful knowledge to quality estimation tasks. Our proposed quality estimation method sequentially trains the following two types of neural models: (1) Predictor: a neural word prediction model trained from parallel corpora and (2) Estimator: a neural quality estimation model trained from quality estimation data. To transfer word a prediction task to a quality estimation task, we generate quality estimation feature vectors from the word prediction model and feed them into the quality estimation model. The experimental results on WMT15 and 16 quality estimation datasets show that our proposed method has great potential in the various sub-challenges.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {3},
numpages = {22},
keywords = {Quality estimation, neural networks, feature extraction, bidirectional language model, word prediction, machine translation}
}

@article{10.1145/3099556,
author = {Passban, Peyman and Liu, Qun and Way, Andy},
title = {Translating Low-Resource Languages by Vocabulary Adaptation from Close Counterparts},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3099556},
doi = {10.1145/3099556},
abstract = {Some natural languages belong to the same family or share similar syntactic and/or semantic regularities. This property persuades researchers to share computational models across languages and benefit from high-quality models to boost existing low-performance counterparts. In this article, we follow a similar idea, whereby we develop statistical and neural machine translation (MT) engines that are trained on one language pair but are used to translate another language. First we train a reliable model for a high-resource language, and then we exploit cross-lingual similarities and adapt the model to work for a close language with almost zero resources. We chose Turkish (Tr) and Azeri or Azerbaijani (Az) as the proposed pair in our experiments. Azeri suffers from lack of resources as there is almost no bilingual corpus for this language. Via our techniques, we are able to train an engine for the Az → English (En) direction, which is able to outperform all other existing models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {29},
numpages = {14},
keywords = {neural machine translation, Statistical machine translation, low-resource languages}
}

@article{10.1145/3099473,
author = {Phani, Shanta and Lahiri, Shibamouli and Biswas, Arindam},
title = {A Supervised Learning Approach for Authorship Attribution of Bengali Literary Texts},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3099473},
doi = {10.1145/3099473},
abstract = {Authorship Attribution is a long-standing problem in Natural Language Processing. Several statistical and computational methods have been used to find a solution to this problem. In this article, we have proposed methods to deal with the authorship attribution problem in Bengali. More specifically, we proposed a supervised framework consisting of lexical and shallow features and investigated the possibility of using topic-modeling-inspired features, to classify documents according to their authors. We have created a corpus from nearly all the literary works of three eminent Bengali authors, consisting of 3,000 disjoint samples. Our models showed better performance than the state-of-the-art, with more than 98% test accuracy for the shallow features and 100% test accuracy for the topic-based features. Further experiments with GloVe vectors [Pennington et al. 2014] showed comparable results, but flexible patterns based on content words and high-frequency words [Schwartz et al. 2013] failed to perform as well as expected.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {28},
numpages = {15},
keywords = {Authorship attribution, Naive bayes, lexical features, machine learning, topic model}
}

@article{10.1145/3099472,
author = {Liu, Shih-Hung and Chen, Kuan-Yu and Hsieh, Yu-Lun and Chen, Berlin and Wang, Hsin-Min and Yen, Hsu-Chun and Hsu, Wen-Lian},
title = {A Position-Aware Language Modeling Framework for Extractive Broadcast News Speech Summarization},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3099472},
doi = {10.1145/3099472},
abstract = {Extractive summarization, a process that automatically picks exemplary sentences from a text (or spoken) document with the goal of concisely conveying key information therein, has seen a surge of attention from scholars and practitioners recently. Using a language modeling (LM) approach for sentence selection has been proven effective for performing unsupervised extractive summarization. However, one of the major difficulties facing the LM approach is to model sentences and estimate their parameters more accurately for each text (or spoken) document. We extend this line of research and make the following contributions in this work. First, we propose a position-aware language modeling framework using various granularities of position-specific information to better estimate the sentence models involved in the summarization process. Second, we explore disparate ways to integrate the positional cues into relevance models through a pseudo-relevance feedback procedure. Third, we extensively evaluate various models originated from our proposed framework and several well-established unsupervised methods. Empirical evaluation conducted on a broadcast news summarization task further demonstrates performance merits of the proposed summarization methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {27},
numpages = {13},
keywords = {Extractive summarization, Speech information, Relevance modeling, Positional language modeling}
}

@article{10.1145/3092743,
author = {Punchimudiyanse, Malinda and Meegama, Ravinda Gayan Narendra},
title = {Animation of Fingerspelled Words and Number Signs of the Sinhala Sign Language},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3092743},
doi = {10.1145/3092743},
abstract = {Sign language is the primary communication medium of the aurally handicapped community. Often, a sign gesture is mapped to a word or a phrase in a spoken language and named as a conversational sign. A fingerspelling sign is a special sign derived to show a single character that matches a character in the alphabet of a given language. This enables the deaf community to express words that do not have a conversational sign, such as a name, using a letter-by-letter technique. Sinhala Sign Language (SSL) uses a phonetic pronunciation mechanism to decode such words due to the presence of one or more modifiers after a consonant. Expressing numbers also have a similar notation, and it is broken down into parts before interpretation in sign gestures.This article presents the variations implemented to make the 3D avatar-based interpreter system look similar to an actual fingerspelled SSL by a human interpreter. To accomplish the task, a phonetic English-based 3D avatar animation system is developed with Blender animation software. The conversion of Sinhala Unicode text to phonetic English and numbers written in digits to sign gestures is done with a Visual Basic.NET (VB.NET) application. The presented application has 61 SSL fingerspelling signs and 40 SSL number signs. It is capable of interpreting any word written using the modern Sinhala alphabet without conversational signs and interprets the numbers that go up to the billions. This is a helpful tool in teaching SSL fingerspelling and number signs of SSL to deaf children.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {24},
numpages = {26},
keywords = {3D signing avatar, Sinhala fingerspelling, number gesture animation, Sinhala sign language}
}

@article{10.1145/3099557,
author = {Kong, Fang and Zhou, Guodong},
title = {A CDT-Styled End-to-End Chinese Discourse Parser},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3099557},
doi = {10.1145/3099557},
abstract = {Discourse parsing is a challenging task and plays a critical role in discourse analysis. Since the release of the Rhetorical Structure Theory Discourse Treebank and the Penn Discourse Treebank, the research on English discourse parsing has attracted increasing attention and achieved considerable success in recent years. At the same time, some preliminary research on certain subtasks about discourse parsing for other languages, such as Chinese, has been conducted. In this article, we present an end-to-end Chinese discourse parser with the Connective-Driven Dependency Tree scheme, which consists of multiple components in a pipeline architecture, such as the elementary discourse unit (EDU) detector, discourse relation recognizer, discourse parse tree generator, and attribution labeler. In particular, the attribution labeler determines two attributions (i.e., sense and centering) for every nonterminal node (i.e., discourse relation) in the discourse parse trees. Systematically, our parser detects all EDUs in a free text, generates the discourse parse tree in a bottom-up way, and determines the sense and centering attributions for all nonterminal nodes by traversing the discourse parse tree. Comprehensive evaluation on the Connective-Driven Dependency Treebank corpus from both component-wise and error-cascading perspectives is conducted to illustrate how each component performs in isolation, and how the pipeline performs with error propagation. Finally, it shows that our end-to-end Chinese discourse parser achieves an overall F1 score of 20% with full automation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {26},
numpages = {17},
keywords = {Connective-Driven Dependency Tree, Discourse parsing, elementary discourse unit, end-to-end Chinese discourse parser, discourse parse tree}
}

@article{10.1145/3086575,
author = {Al-Sallab, Ahmad and Baly, Ramy and Hajj, Hazem and Shaban, Khaled Bashir and El-Hajj, Wassim and Badaro, Gilbert},
title = {AROMA: A Recursive Deep Learning Model for Opinion Mining in Arabic as a Low Resource Language},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3086575},
doi = {10.1145/3086575},
abstract = {While research on English opinion mining has already achieved significant progress and success, work on Arabic opinion mining is still lagging. This is mainly due to the relative recency of research efforts in developing natural language processing (NLP) methods for Arabic, handling its morphological complexity, and the lack of large-scale opinion resources for Arabic. To close this gap, we examine the class of models used for English and that do not require extensive use of NLP or opinion resources. In particular, we consider the Recursive Auto Encoder (RAE). However, RAE models are not as successful in Arabic as they are in English, due to their limitations in handling the morphological complexity of Arabic, providing a more complete and comprehensive input features for the auto encoder, and performing semantic composition following the natural way constituents are combined to express the overall meaning. In this article, we propose A Recursive Deep Learning Model for Opinion Mining in Arabic (AROMA) that addresses these limitations. AROMA was evaluated on three Arabic corpora representing different genres and writing styles. Results show that AROMA achieved significant performance improvements compared to the baseline RAE. It also outperformed several well-known approaches in the literature.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {25},
numpages = {20},
keywords = {Deep Learning, Opinion mining in Arabic, Recursive Auto Encoder, Recursive Neural Networks}
}

@article{10.1145/3086576,
author = {Baly, Ramy and Hajj, Hazem and Habash, Nizar and Shaban, Khaled Bashir and El-Hajj, Wassim},
title = {A Sentiment Treebank and Morphologically Enriched Recursive Deep Models for Effective Sentiment Analysis in Arabic},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3086576},
doi = {10.1145/3086576},
abstract = {Accurate sentiment analysis models encode the sentiment of words and their combinations to predict the overall sentiment of a sentence. This task becomes challenging when applied to morphologically rich languages (MRL). In this article, we evaluate the use of deep learning advances, namely the Recursive Neural Tensor Networks (RNTN), for sentiment analysis in Arabic as a case study of MRLs. While Arabic may not be considered the only representative of all MRLs, the challenges faced and proposed solutions in Arabic are common to many other MRLs. We identify, illustrate, and address MRL-related challenges and show how RNTN is affected by the morphological richness and orthographic ambiguity of the Arabic language. To address the challenges with sentiment extraction from text in MRL, we propose to explore different orthographic features as well as different morphological features at multiple levels of abstraction ranging from raw words to roots. A key requirement for RNTN is the availability of a sentiment treebank; a collection of syntactic parse trees annotated for sentiment at all levels of constituency and that currently only exists in English. Therefore, our contribution also includes the creation of the first Arabic Sentiment Treebank (ArSenTB) that is morphologically and orthographically enriched. Experimental results show that, compared to the basic RNTN proposed for English, our solution achieves significant improvements up to 8% absolute at the phrase level and 10.8% absolute at the sentence level, measured by average F1 score. It also outperforms well-known classifiers including Support Vector Machines, Recursive Auto Encoders, and Long Short-Term Memory by 7.6%, 3.2%, and 1.6% absolute respectively, all models being trained with similar morphological considerations.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {23},
numpages = {21},
keywords = {Sentiment analysis, Arabic morphology, deep learning, sentiment treebank}
}

@article{10.1145/3047406,
author = {Wali, Wafa and Gargouri, Bilel and Hamadou, Adelmajid Ben},
title = {Evaluating the Content of LMF Standardized Dictionaries: A Practical Experiment on Arabic Language},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3047406},
doi = {10.1145/3047406},
abstract = {Since the age of paper versions, dictionaries are often published with anomalies in their content resulting from lexicographer’s mistakes or from the lack of efficiency of automatic enrichment systems. Many of these anomalies are expensive to manually detect and difficult to automatically control, notably with lightly structured models of dictionaries. In this article, we take advantage of the fine structure proposed by the Lexical Markup Framework (LMF) norm to investigate the detection of anomalies in the content of LMF normalized dictionaries. First, we give a theoretical study on the plausible anomalies, such as inconsistency, incoherence, redundancy, and incompleteness. Second, we detail the approach that we propose for the automatic detection of such anomalies. Finally, we report on an experiment carried out on an available normalized dictionary of the Arabic language. The experiment has shown that the proposed approach gives reasonable results in terms of precision and recall.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {22},
numpages = {20},
keywords = {anomalies’ detection, Arabic language, LMF standardized dictionaries, LMF-ISO 24613}
}

@article{10.1145/3047405,
author = {Zhao, Hai and Cai, Deng and Xin, Yang and Wang, Yuzhu and Jia, Zhongye},
title = {A Hybrid Model for Chinese Spelling Check},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3047405},
doi = {10.1145/3047405},
abstract = {Spelling check for Chinese has more challenging difficulties than that for other languages. A hybrid model for Chinese spelling check is presented in this article. The hybrid model consists of three components: one graph-based model for generic errors and two independently trained models for specific errors. In the graph model, a directed acyclic graph is generated for each sentence, and the single-source shortest-path algorithm is performed on the graph to detect and correct general spelling errors at the same time. Prior to that, two types of errors over functional words (characters) are first solved by conditional random fields: the confusion of “在” (at) (pinyin is zai in Chinese), “再” (again, more, then) (pinyin: zai) and “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), and “得” (so that, have to) (pinyin: de). Finally, a rule-based model is exploited to distinguish pronoun usage confusion: “她” (she) (pinyin: ta), “他” (he) (pinyin: ta), and some other common collocation errors. The proposed model is evaluated on the standard datasets released by the SIGHAN Bake-off shared tasks, giving state-of-the-art results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {21},
numpages = {22},
keywords = {graph model, Chinese spelling check, conditional random field, rule-based model, hybrid model}
}

@article{10.1145/3038295,
author = {Tholpadi, Goutham and Bhattacharyya, Chiranjib and Shevade, Shirish},
title = {Corpus-Based Translation Induction in Indian Languages Using Auxiliary Language Corpora from Wikipedia},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3038295},
doi = {10.1145/3038295},
abstract = {Identifying translations from comparable corpora is a well-known problem with several applications. Existing methods rely on linguistic tools or high-quality corpora. Absence of such resources, especially in Indian languages, makes this problem hard; for example, state-of-the-art techniques achieve a mean reciprocal rank of 0.66 for English-Italian, and a mere 0.187 for Telugu-Kannada. In this work, we address the problem of comparable corpora-based translation correspondence induction (CC-TCI) when the only resources available are small noisy comparable corpora extracted from Wikipedia. We observe that translations in the source and target languages have many topically related words in common in other “auxiliary” languages. To model this, we define the notion of a translingual theme, a set of topically related words from auxiliary language corpora, and present a probabilistic framework for CC-TCI. Extensive experiments on 35 comparable corpora showed dramatic improvements in performance. We extend these ideas to propose a method for measuring cross-lingual semantic relatedness (CLSR) between words. To stimulate further research in this area, we make publicly available two new high-quality human-annotated datasets for CLSR. Experiments on the CLSR datasets show more than 200% improvement in correlation on the CLSR task. We apply the method to the real-world problem of cross-lingual Wikipedia title suggestion and build the WikiTSu system. A user study on WikiTSu shows a 20% improvement in the quality of titles suggested.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {20},
numpages = {25},
keywords = {cross-lingual semantic relatedness, Comparable corpora, translation correspondence induction, bilingual lexicon, Wikipedia title suggestion, auxiliary language}
}

@article{10.1145/3003726,
author = {Finch, Andrew and Harada, Taisuke and Tanaka-Ishii, Kumiko and Sumita, Eiichiro},
title = {Inducing a Bilingual Lexicon from Short Parallel Multiword Sequences},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3003726},
doi = {10.1145/3003726},
abstract = {This article proposes a technique for mining bilingual lexicons from pairs of parallel short word sequences. The technique builds a generative model from a corpus of training data consisting of such pairs. The model is a hierarchical nonparametric Bayesian model that directly induces a bilingual lexicon while training. The model learns in an unsupervised manner and is designed to exploit characteristics of the language pairs being mined. The proposed model is capable of utilizing commonly used word-pair frequency information and additionally can employ the internal character alignments within the words themselves. It is thereby capable of mining transliterations and can use reliably aligned transliteration pairs to support the mining of other words in their context. The model is also capable of performing word reordering and word deletion during the alignment process, and it is furthermore capable of operating in the absence of full segmentation information. In this work, we study two mining tasks based on English-Japanese and English-Chinese language pairs, and compare the proposed approach to baselines based on a simpler models that use only word-pair frequency information. Our results show that the proposed method is able to mine bilingual word pairs at higher levels of precision and recall than the baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {15},
numpages = {20},
keywords = {mining, alignment, Bilingual lexicon}
}

@article{10.1145/3028772,
author = {Li, Haoran and Zhang, Jiajun and Zong, Chengqing},
title = {Implicit Discourse Relation Recognition for English and Chinese with Multiview Modeling and Effective Representation Learning},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3028772},
doi = {10.1145/3028772},
abstract = {Discourse relations between two text segments play an important role in many Natural Language Processing (NLP) tasks. The connectives strongly indicate the sense of discourse relations, while in fact, there are no connectives in a large proportion of discourse relations, that is, implicit discourse relations. Compared with explicit relations, implicit relations are much harder to detect and have drawn significant attention. Until now, there have been many studies focusing on English implicit discourse relations, and few studies address implicit relation recognition in Chinese even though the implicit discourse relations in Chinese are more common than those in English. In our work, both the English and Chinese languages are our focus. The key to implicit relation prediction is to properly model the semantics of the two discourse arguments, as well as the contextual interaction between them. To achieve this goal, we propose a neural network based framework that consists of two hierarchies. The first one is the model hierarchy, in which we propose a max-margin learning method to explore the implicit discourse relation from multiple views. The second one is the feature hierarchy, in which we learn multilevel distributed representations from words, arguments, and syntactic structures to sentences. We have conducted experiments on the standard benchmarks of English and Chinese, and the results show that compared with several methods our proposed method can achieve the best performance in most cases.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {19},
numpages = {21},
keywords = {neural network, max-margin learning, multilevel features, Implicit discourse relation}
}

@article{10.1145/3015467,
author = {Das, Arjun and Ganguly, Debasis and Garain, Utpal},
title = {Named Entity Recognition with Word Embeddings and Wikipedia Categories for a Low-Resource Language},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3015467},
doi = {10.1145/3015467},
abstract = {In this article, we propose a word embedding--based named entity recognition (NER) approach. NER is commonly approached as a sequence labeling task with the application of methods such as conditional random field (CRF). However, for low-resource languages without the presence of sufficiently large training data, methods such as CRF do not perform well. In our work, we make use of the proximity of the vector embeddings of words to approach the NER problem. The hypothesis is that word vectors belonging to the same name category, such as a person’s name, occur in close vicinity in the abstract vector space of the embedded words. Assuming that this clustering hypothesis is true, we apply a standard classification approach on the vectors of words to learn a decision boundary between the NER classes. Our NER experiments are conducted on a morphologically rich and low-resource language, namely Bengali. Our approach significantly outperforms standard baseline CRF approaches that use cluster labels of word embeddings and gazetteers constructed from Wikipedia. Further, we propose an unsupervised approach (that uses an automatically created named entity (NE) gazetteer from Wikipedia in the absence of training data). For a low-resource language, the word vectors obtained from Wikipedia are not sufficient to train a classifier. As a result, we propose to make use of the distance measure between the vector embeddings of words to expand the set of Wikipedia training examples with additional NEs extracted from a monolingual corpus that yield significant improvement in the unsupervised NER performance. In fact, our expansion method performs better than the traditional CRF-based (supervised) approach (i.e., F-score of 65.4% vs. 64.2%). Finally, we compare our proposed approach to the official submission for the IJCNLP-2008 Bengali NER shared task and achieve an overall improvement of F-score 11.26% with respect to the best official system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {18},
numpages = {19},
keywords = {language-independent NER, Word embedding, unsupervised NER, CRF-based NER, classifier, Wikipedia-based NER}
}

@article{10.1145/3005447,
author = {Bhat, Riyaz Ahmad and Bhat, Irshad Ahmad and Sharma, Dipti Misra},
title = {Improving Transition-Based Dependency Parsing of Hindi and Urdu by Modeling Syntactically Relevant Phenomena},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3005447},
doi = {10.1145/3005447},
abstract = {In recent years, transition-based parsers have shown promise in terms of efficiency and accuracy. Though these parsers have been extensively explored for multiple Indian languages, there is still considerable scope for improvement by properly incorporating syntactically relevant information. In this article, we enhance transition-based parsing of Hindi and Urdu by redefining the features and feature extraction procedures that have been previously proposed in the parsing literature of Indian languages. We propose and empirically show that properly incorporating syntactically relevant information like case marking, complex predication and grammatical agreement in an arc-eager parsing model can significantly improve parsing accuracy. Our experiments show an absolute improvement of ∼2% LAS for parsing of both Hindi and Urdu over a competitive baseline which uses rich features like part-of-speech (POS) tags, chunk tags, cluster ids and lemmas. We also propose some heuristics to identify ezafe constructions in Urdu texts which show promising results in parsing these constructions.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {17},
numpages = {35},
keywords = {normalized pointwise mutual information, averaged perceptron, shift-reduce parsing, Dependency parsing, treebanks}
}

@article{10.1145/3010088,
author = {Wang, Shaonan and Zong, Chengqing},
title = {Comparison Study on Critical Components in Composition Model for Phrase Representation},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3010088},
doi = {10.1145/3010088},
abstract = {Phrase representation, an important step in many NLP tasks, involves representing phrases as continuous-valued vectors. This article presents detailed comparisons concerning the effects of word vectors, training data, and the composition and objective function used in a composition model for phrase representation. Specifically, we first discuss how the augmented word representations affect the performance of the composition model. Then, we investigate whether different types of training data influence the performance of the composition model and, if so, how they influence it. Finally, we evaluate combinations of different composition and objective functions and discuss the factors related to composition model performance. All evaluations were conducted in both English and Chinese. Our main findings are as follows: (1) The Additive model with semantic enhanced word vectors performs comparably to the state-of-the-art model; (2) The Additive model which updates augmented word vectors and the Matrix model with semantic enhanced word vectors systematically outperforms the state-of-the-art model in bigram and multi-word phrase similarity task, respectively; (3) Representing the high frequency phrases by estimating their surrounding contexts is a good training objective for bigram phrase similarity tasks; and (4) The performance gain of composition model with semantic enhanced word vectors is due to the composition function and the greater weight attached to important words. Previous works focus on the composition function; however, our findings indicate that other components in the composition model (especially word representation) make a critical difference in phrase representation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {16},
numpages = {25},
keywords = {max-margin, Phrase representation, mean square error, retrofitting, composition model, word paraphrasing}
}

@article{10.1145/2997643,
author = {V, Arjun Atreya and Kankaria, Ashish and Bhattacharyya, Pushpak and Ramakrishnan, Ganesh},
title = {Query Expansion in Resource-Scarce Languages: A Multilingual Framework Utilizing Document Structure},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2997643},
doi = {10.1145/2997643},
abstract = {Retrievals in response to queries to search engines in resource-scarce languages often produce no results, which annoys the user. In such cases, at least partially relevant documents must be retrieved. We propose a novel multilingual framework, MultiStructPRF, which expands the query with related terms by (i) using a resource-rich assisting language and (ii) giving varied importance to the expansion terms depending on their position of occurrence in the document. Our system uses the help of an assisting language to expand the query in order to improve system recall. We propose a systematic expansion model for weighting the expansion terms coming from different parts of the document. To combine the expansion terms from query language and assisting language, we propose a heuristics-based fusion model. Our experimental results show an improvement over other PRF techniques in both precision and recall for multiple resource-scarce languages like Marathi, Bengali, Odia, Finnish, and the like. We study the effect of different assisting languages on precision and recall for multiple query languages. Our experiments reveal an interesting fact: Precision is positively correlated with the typological closeness of query language and assisting language, whereas recall is positively correlated with the resource richness of the assisting language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {14},
numpages = {17},
keywords = {resource scarce languages, multilingual retrieval, Query expansion}
}

@article{10.1145/2956236,
author = {Krishnamurthi, Karthik and Panuganti, Vijayapal Reddy and Bulusu, Vishnu Vardhan},
title = {Understanding Document Semantics from Summaries: A Case Study on Hindi Texts},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2956236},
doi = {10.1145/2956236},
abstract = {Summary of a document contains words that actually contribute to the semantics of the document. Latent Semantic Analysis (LSA) is a mathematical model that is used to understand document semantics by deriving a semantic structure based on patterns of word correlations in the document. When using LSA to capture semantics from summaries, it is observed that LSA performs quite well despite being completely independent of any external sources of semantics. However, LSA can be remodeled to enhance its capability to analyze correlations within texts. By taking advantage of the model being language independent, this article presents two stages of LSA remodeling to understand document semantics in the Indian context, specifically from Hindi text summaries. One stage of remodeling is done by providing supplementary information, such as document category and domain information. The second stage of remodeling is done by using a supervised term weighting measure in the process. The remodeled LSA’s performance is empirically evaluated in a document classification application by comparing the accuracies of classification to plain LSA. An improvement in the performance of LSA in the range of 4.7% to 6.2% is achieved from the remodel when compared to the plain model. The results suggest that summaries of documents efficiently capture the semantic structure of documents and is an alternative to full-length documents for understanding document semantics.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {7},
numpages = {20},
keywords = {extractive summary, semantic structure, Dimensionality reduction, document classification, singular value decomposition, supervised term weighting, supplemented latent semantic analysis}
}

@article{10.1145/2988237,
author = {Tran, Phuoc and Dinh, Dien and Nguyen, Long H. B.},
title = {Word Re-Segmentation in Chinese-Vietnamese Machine Translation},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2988237},
doi = {10.1145/2988237},
abstract = {In isolated languages, such as Chinese and Vietnamese, words are not separated by spaces, and a word may be formed by one or more syllables. Therefore, word segmentation (WS) is usually the first process that is implemented in the machine translation process. WS in the source and target languages is based on different training corpora, and WS approaches may not be the same. Therefore, the WS that results in these two languages are not often homologous, and thus word alignment results in many 1-n and n-1 alignment pairs in statistical machine translation, which degrades the performance of machine translation. In this article, we will adjust the WS for both Chinese and Vietnamese in particular and for isolated language pairs in general and make the word boundary of the two languages more symmetric in order to strengthen 1-1 alignments and enhance machine translation performance. We have tested this method on the Computational Linguistics Center’s corpus, which consists of 35,623 sentence pairs. The experimental results show that our method has significantly improved the performance of machine translation compared to the baseline translation system, WS translation system, and anchor language-based WS translation systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {12},
numpages = {22},
keywords = {chinese-vietnamese machine translation, isolated language, Word boundary, word re-segmentation, word segmentation, character}
}

@article{10.1145/2988238,
author = {Sadek, Jawad and Meziane, Farid},
title = {A Discourse-Based Approach for Arabic Question Answering},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2988238},
doi = {10.1145/2988238},
abstract = {The treatment of complex questions with explanatory answers involves searching for arguments in texts. Because of the prominent role that discourse relations play in reflecting text producers’ intentions, capturing the underlying structure of text constitutes a good instructor in this issue. From our extensive review, a system for automatic discourse analysis that creates full rhetorical structures in large-scale Arabic texts is currently unavailable. This is due to the high computational complexity involved in processing a large number of hypothesized relations associated with large texts. Therefore, more practical approaches should be investigated. This article presents a new Arabic Text Parser oriented for question-answering systems dealing with لماذا “why” and كيف “how to” questions. The Text Parser presented here considers the sentence as the basic unit of text and incorporates a set of heuristics to avoid computational explosion. With this approach, the developed question-answering system reached a significant improvement over the baseline with a Recall of 68% and MRR of 0.62.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {11},
numpages = {18},
keywords = {discourse analysis, Arabic question answering, information extraction}
}

@article{10.1145/2968410,
author = {Tursun, Eziz and Ganguly, Debasis and Osman, Turghun and Yang, Ya-Ting and Abdukerim, Ghalip and Zhou, Jun-Lin and Liu, Qun},
title = {A Semisupervised Tag-Transition-Based Markovian Model for Uyghur Morphology Analysis},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2968410},
doi = {10.1145/2968410},
abstract = {Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we investigate the morphological analysis of the Uyghur language, which is the native language of the people in the Xinjiang Uyghur autonomous region of western China. Morphological analysis of Uyghur is challenging primarily because of factors such as (1) ambiguities arising due to the likelihood of association of a multiple number of POS tags with a word stem or a multiple number of functional tags with a word suffix, (2) ambiguous morpheme boundaries, and (3) complex morphopholonogy of the language. Further, the unavailability of a manually annotated training set in the Uyghur language for the purpose of word segmentation makes Uyghur morphological analysis more difficult. In our proposed work, we address these challenges by undertaking a semisupervised approach of learning a Markov model with the help of a manually constructed dictionary of “suffix to tag” mappings in order to predict the most likely tag transitions in the Uyghur morpheme sequence. Due to the linguistic characteristics of Uyghur, we incorporate a prior belief in our model for favoring word segmentations with a lower number of morpheme units. Empirical evaluation of our proposed model shows an accuracy of about 82%. We further improve the effectiveness of the tag transition model with an active learning paradigm. In particular, we manually investigated a subset of words for which the model prediction ambiguity was within the top 20%. Manually incorporating rules to handle these erroneous cases resulted in an overall accuracy of 93.81%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {8},
numpages = {23},
keywords = {Uyghur morphological analysis, Markov model}
}

@article{10.1145/2994600,
author = {Li, Peifeng and Zhou, Guodong and Zhu, Qiaoming},
title = {Minimally Supervised Chinese Event Extraction from Multiple Views},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2994600},
doi = {10.1145/2994600},
abstract = {Although several semi-supervised learning models have been proposed for English event extraction, there are few successful stories in Chinese due to its special characteristics. In this article, we propose a novel minimally supervised model for Chinese event extraction from multiple views. Besides the traditional pattern similarity view (PSV), a semantic relationship view (SRV) is introduced to capture the relevant event mentions from relevant documents. Moreover, a morphological structure view (MSV) is incorporated to both infer more positive patterns and help filter negative patterns via morphological structure similarity. An evaluation of the ACE 2005 Chinese corpus shows that our minimally supervised model significantly outperforms several strong baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {13},
numpages = {16},
keywords = {semantic relationship view, minimally supervised model, morphological structure view, Chinese event extraction}
}

@article{10.1145/2963099,
author = {Li, Junhui and Zhu, Muhua and Lu, Wei and Zhou, Guodong},
title = {Improving Semantic Parsing with Enriched Synchronous Context-Free Grammars in Statistical Machine Translation},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2963099},
doi = {10.1145/2963099},
abstract = {Semantic parsing maps a sentence in natural language into a structured meaning representation. Previous studies show that semantic parsing with synchronous context-free grammars (SCFGs) achieves favorable performance over most other alternatives. Motivated by the observation that the performance of semantic parsing with SCFGs is closely tied to the translation rules, this article explores to extend translation rules with high quality and increased coverage in three ways. First, we examine the difference between word alignments for semantic parsing and statistical machine translation (SMT) to better adapt word alignment in SMT to semantic parsing. Second, we introduce both structure and syntax informed nonterminals, better guiding the parsing in favor of well-formed structure, instead of using a uninformed nonterminal in SCFGs. Third, we address the unknown word translation issue via synthetic translation rules. Last but not least, we use a filtering approach to improve performance via predicting answer type. Evaluation on the standard GeoQuery benchmark dataset shows that our approach greatly outperforms the state of the art across various languages, including English, Chinese, Thai, German, and Greek.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {6},
numpages = {24},
keywords = {word alignment, enriched synchronous context-free grammars, Semantic parsing, statistical machine translation}
}

@article{10.1145/2990191,
author = {Nguyen, Long H. B. and Dinh, Dien and Tran, Phuoc},
title = {An Approach to Construct a Named Entity Annotated English-Vietnamese Bilingual Corpus},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2990191},
doi = {10.1145/2990191},
abstract = {Manually constructing an annotated Named Entity (NE) in a bilingual corpus is a time-consuming, labor--intensive, and expensive process, but this is necessary for natural language processing (NLP) tasks such as cross-lingual information retrieval, cross-lingual information extraction, machine translation, etc. In this article, we present an automatic approach to construct an annotated NE in English-Vietnamese bilingual corpus from a bilingual parallel corpus by proposing an aligned NE method. Basing this corpus on a bilingual corpus in which the initial NEs are extracted from its own language separately, the approach tries to correct unrecognized NEs or incorrectly recognized NEs before aligning the NEs by using a variety of bilingual constraints. The generated corpus not only improves the NE recognition results but also creates alignments between English NEs and Vietnamese NEs, which are necessary for training NE translation models. The experimental results show that the approach outperforms the baseline methods effectively. In the English-Vietnamese NE alignment task, the F-measure increases from 68.58% to 79.77%. Thanks to the improvement of the NE recognition quality, the proposed method also increases significantly: the F-measure goes from 84.85% to 88.66% for the English side and from 75.71% to 85.55% for the Vietnamese side. By providing the additional semantic information for the machine translation systems, the BLEU score increases from 33.04% to 45.11%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {9},
numpages = {17},
keywords = {English-Vietnamese, word alignment constraint, Named entity translation, annotated bilingual corpus}
}

@article{10.1145/2963100,
author = {Chou, Chien-Lung and Chang, Chia-Hui and Huang, Ya-Yun},
title = {Boosted Web Named Entity Recognition via Tri-Training},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2963100},
doi = {10.1145/2963100},
abstract = {Named entity extraction is a fundamental task for many natural language processing applications on the web. Existing studies rely on annotated training data, which is quite expensive to obtain large datasets, limiting the effectiveness of recognition. In this research, we propose a semisupervised learning approach for web named entity recognition (NER) model construction via automatic labeling and tri-training. The former utilizes structured resources containing known named entities for automatic labeling, while the latter makes use of unlabeled examples to improve the extraction performance. Since this automatically labeled training data may contain noise, a self-testing procedure is used as a follow-up to remove low-confidence annotation and prepare higher-quality training data. Furthermore, we modify tri-training for sequence labeling and derive a proper initialization for large dataset training to improve entity recognition. Finally, we apply this semisupervised learning framework for person name recognition, business organization name recognition, and location name extraction. In the task of Chinese NER, an F-measure of 0.911, 0.849, and 0.845 can be achieved, for person, business organization, and location NER, respectively. The same framework is also applied for English and Japanese business organization name recognition and obtains models with performance of a 0.832 and 0.803 F-measure.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {10},
numpages = {23},
keywords = {tri-training initialization, Named entity recognition, and semisupervised learning, tri-training for sequence labeling}
}

@article{10.1145/2934676,
author = {Passban, Peyman and Liu, Qun and Way, Andy},
title = {Boosting Neural POS Tagger for Farsi Using Morphological Information},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2934676},
doi = {10.1145/2934676},
abstract = {Farsi (Persian) is a low-resource language that suffers from the data sparsity problem and a lack of efficient processing tools. Due to their broad application in natural language processing tasks, part-of-speech (POS) taggers are one of those important tools that should be considered in this respect. Despite recent work on Farsi tagging, there is still room for improvement. The best reported accuracy so far is 96%, which in special cases can rise to 96.9%. The main problem with existing taggers is their inefficiency in coping with out-of-vocabulary (OOV) words. Addressing both problems of accuracy and OOV words, we developed a neural network-based POS tagger (NPT) that performs efficiently on Farsi. Despite using less data, NPT provides better results in comparison to state-of-the-art systems. Our proposed tagger performs with an accuracy of 97.4%, with performance highly influenced by morphological features. We carry out a shallow morphological analysis and show considerable improvement over the baseline configuration.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {4},
numpages = {15},
keywords = {Farsi, POS tagging, morphological analysis}
}

@article{10.1145/2933396,
author = {Liu, Liangliang and Cao, Cungen},
title = {A Seed-Based Method for Generating Chinese Confusion Sets},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2933396},
doi = {10.1145/2933396},
abstract = {In natural language, people often misuse a word (called a “confused word”) in place of other words (called “confusing words”). In misspelling corrections, many approaches to finding and correcting misspelling errors are based on a simple notion called a “confusion set.” The confusion set of a confused word consists of confusing words. In this article, we propose a new method of building Chinese character confusion sets.Our method is composed of two major phases. In the first phase, we build a list of seed confusion sets for each Chinese character, which is based on measuring similarity in character pinyin or similarity in character shape. In this phase, all confusion sets are constructed manually, and the confusion sets are organized into a graph, called a “seed confusion graph” (SCG), in which vertices denote characters and edges are pairs of characters in the form (confused character, confusing character).In the second phase, we extend the SCG by acquiring more pairs of (confused character, confusing character) from a large Chinese corpus. For this, we use several word patterns (or patterns) to generate new confusion pairs and then verify the pairs before adding them into a SCG. Comprehensive experiments show that our method of extending confusion sets is effective. Also, we shall use the confusion sets in Chinese misspelling corrections to show the utility of our method.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {5},
numpages = {16},
keywords = {shape similarity, pattern matching, pinyin similarity, Confusion set, context probability}
}

@article{10.1145/2896318,
author = {Singh, Sukhdeep and Sharma, Anuj and Chhabra, Indu},
title = {Online Handwritten Gurmukhi Strokes Dataset Based on Minimal Set of Words},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2896318},
doi = {10.1145/2896318},
abstract = {The online handwriting data are an integral part of data analysis and classification research, as collected handwritten data offers many challenges to group handwritten stroke classes. The present work has been done for grouping handwritten strokes from the Indic script Gurmukhi. Gurmukhi is the script of the popular and widely spoken language Punjabi. The present work includes development of the dataset of Gurmukhi words in the context of online handwriting recognition for real-life use applications, such as maps navigation. We have collected the data of 100 writers from the largest cities in the Punjab region. The writers’ variations, such as writing skill level (beginner, moderate, and expert), gender, right or left handedness, and their adaptability to digital handwriting, have been considered in dataset development. We have introduced a novel technique to form handwritten stroke classes based on a limited set of words. The presence of all alphabets including vowels of Gurmukhi script has been considered before selection of a word. The developed dataset includes 39,411 strokes from handwritten words and forms 72 classes of strokes after using a k-means clustering technique and manual verification through expert and moderate writers. We have achieved recognition results using the Hidden Markov Model as 87.10%, 85.43%, and 84.33% for middle zone strokes when using training data as 66%, 50%, and 80% of the developed dataset. The present work is a step in a direction to find groups for unknown handwriting strokes with reasonably higher levels of accuracy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {1},
numpages = {20},
keywords = {data collection, digital handwriting, Online handwriting recognition, HMM, classification, k-means, clustering}
}

@article{10.1145/2898997,
author = {El-Fiqi, Heba and Petraki, Eleni and Abbass, Hussein A.},
title = {Pairwise Comparative Classification for Translator Stylometric Analysis},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2898997},
doi = {10.1145/2898997},
abstract = {In this article, we present a new type of classification problem, which we call Comparative Classification Problem (CCP), where we use the term data record to refer to a block of instances. Given a single data record with n instances for n classes, the CCP problem is to map each instance to a unique class. This problem occurs in a wide range of applications where the independent and identically distributed assumption is broken down. The primary difference between CCP and classical classification is that in the latter, the assignment of a translator to one record is independent of the assignment of a translator to a different record. In CCP, however, the assignment of a translator to one record within a block excludes this translator from further assignments to any other record in that block. The interdependency in the data poses challenges for techniques relying on the independent and identically distributed (iid) assumption.In the Pairwise CCP (PWCCP), a pair of records is grouped together. The key difference between PWCCP and classical binary classification problems is that hidden patterns can only be unmasked by comparing the instances as pairs. In this article, we introduce a new algorithm, PWC4.5, which is based on C4.5, to manage PWCCP. We first show that a simple transformation—that we call Gradient-Based Transformation (GBT)—can fix the problem of iid in C4.5. We then evaluate PWC4.5 using two real-world corpora to distinguish between translators on Arabic-English and French-English translations. While the traditional C4.5 failed to distinguish between different translators, GBT demonstrated better performance. Meanwhile, PWC4.5 consistently provided the best results over C4.5 and GBT.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {2},
numpages = {26},
keywords = {translator stylometry, Arabic translation, classification}
}

@article{10.1145/2903720,
author = {Qiao, Xiuming and Cao, Hailong and Zhao, Tiejun},
title = {Improving Unsupervised Dependency Parsing with Knowledge from Query Logs},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2903720},
doi = {10.1145/2903720},
abstract = {Unsupervised dependency parsing becomes more and more popular in recent years because it does not need expensive annotations, such as treebanks, which are required for supervised and semi-supervised dependency parsing. However, its accuracy is still far below that of supervised dependency parsers, partly due to the fact that their parsing model is insufficient to capture linguistic phenomena underlying texts. The performance for unsupervised dependency parsing can be improved by mining knowledge from the texts and by incorporating it into the model. In this article, syntactic knowledge is acquired from query logs to help estimate better probabilities in dependency models with valence. The proposed method is language independent and obtains an improvement of 4.1% unlabeled accuracy on the Penn Chinese Treebank by utilizing additional dependency relations from the Sogou query logs and Baidu query logs. Morever, experiments show that the proposed model achieves improvements of 8.07% on CoNLL 2007 English using the AOL query logs. We believe query logs are useful sources of syntactic knowledge for many natural language processing (NLP) tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {3},
numpages = {12},
keywords = {natural annotations, query logs, Dependency parsing, additional knowledge}
}

@article{10.1145/2846095,
author = {Ding, Chenchen and Thu, Ye Kyaw and Utiyama, Masao and Sumita, Eiichiro},
title = {Word Segmentation for Burmese (Myanmar)},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2846095},
doi = {10.1145/2846095},
abstract = {Experiments on various word segmentation approaches for the Burmese language are conducted and discussed in this note. Specifically, dictionary-based, statistical, and machine learning approaches are tested. Experimental results demonstrate that statistical and machine learning approaches perform significantly better than dictionary-based approaches. We believe that this note, based on an annotated corpus of relatively considerable size (containing approximately a half million words), is the first systematic comparison of word segmentation approaches for Burmese. This work aims to discover the properties and proper approaches to Burmese textual processing and to promote further researches on this understudied language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {22},
numpages = {10},
keywords = {word segmentation, Myanmar, syllable, algorithm, Burmese}
}

@article{10.1145/2857053,
author = {Choudhary, Prakash and Nain, Neeta},
title = {A Four-Tier Annotated Urdu Handwritten Text Image Dataset for Multidisciplinary Research on Urdu Script},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2857053},
doi = {10.1145/2857053},
abstract = {This article introduces a large handwritten text document image corpus dataset for Urdu script named CALAM (Cursive And Language Adaptive Methodologies). The database contains unconstrained handwritten sentences along with their structural annotations for the offline handwritten text images with their XML representation. Urdu is the fourth most frequently used language in the world, but due to its complex cursive writing script and low resources, it is still a thrust area for document image analysis. Here, a unified approach is applied in the development of an Urdu corpus by collecting printed texts, handwritten texts, and demographic information of writers on a single form. CALAM contains 1,200 handwritten text images, 3,043 lines, 46,664 words, and 101,181 ligatures. For capturing maximum variance among the words and handwritten styles, data collection is distributed among six categories and 14 subcategories. Handwritten forms were filled out by 725 different writers belonging to different geographical regions, ages, and genders with diverse educational backgrounds. A structure has been designed to annotate handwritten Urdu script images at line, word, and ligature levels with an XML standard to provide a ground truth of each image at different levels of annotation. This corpus would be very useful for linguistic research in benchmarking and providing a testbed for evaluation of handwritten text recognition techniques for Urdu script, signature verification, writer identification, digital forensics, classification of printed and handwritten text, categorization of texts as per use, and so on. The experimental results of some recently developed handwritten text line segmentation techniques experimented on the proposed dataset are also presented in the article for asserting its viability and usability.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {26},
numpages = {23},
keywords = {OCR algorithms benchmarking, Urdu handwritten text, corpus, annotation}
}

@article{10.1145/2857052,
author = {Zhang, Tongtao and Chowdhury, Aritra and Dhulekar, Nimit and Xia, Jinjing and Knight, Kevin and Ji, Heng and Yener, B\"{u}lent and Zhao, Liming},
title = {From Image to Translation: Processing the Endangered Nyushu Script},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2857052},
doi = {10.1145/2857052},
abstract = {The lack of computational support has significantly slowed down automatic understanding of endangered languages. In this paper, we take Nyushu (simplified Chinese: 女书; literally: “women’s writing”) as a case study to present the first computational approach that combines Computer Vision and Natural Language Processing techniques to deeply understand an endangered language. We developed an end-to-end system to read a scanned hand-written Nyushu article, segment it into characters, link them to standard characters, and then translate the article into Mandarin Chinese. We propose several novel methods to address the new challenges introduced by noisy input and low resources, including Nyushu-specific feature selection for character segmentation and linking, and character linking lattice based Machine Translation. The end-to-end system performance indicates that the system is a promising approach and can serve as a standard benchmark.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {23},
numpages = {16},
keywords = {nyushu, Endangered languages, translation, recognition}
}

@article{10.1145/2846093,
author = {Hakro, Dil Nawaz and Talib, Abdullah Zawawi},
title = {Printed Text Image Database for Sindhi OCR},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2846093},
doi = {10.1145/2846093},
abstract = {Document Image Understanding (DIU) and Electronic Document Management are active fields of research involving image understanding, interpretation, efficient handling, and routing of documents as well as their retrieval. Research on most of the noncursive scripts (Latin) has matured, whereas research on the cursive (connected) scripts is still moving toward perfection. Many researchers are currently working on the cursive scripts (Arabic and other scripts adopting it) around the world so that the difficulties and challenges in document understanding and handling of these scripts can be overcome. Sindhi script has the largest extension of the original Arabic alphabet among languages adopting the Arabic script; it contains 52 characters, compared to 28 characters in the original Arabic alphabet, in order to accommodate more sounds for the language. There are 24 differentiating characters with some possessing four dots. For Sindhi OCR research and development, a database is needed for training and testing of Sindhi text images. We have developed a large database containing over 4 billion words and 15 billion characters in 150 various fonts in four font weights and four styles. The database contents were collected from various sources including websites, books, and theses. A custom-built application was also developed to create a text image from a text document that supports various fonts and sizes. The database considers words, characters, characters with spaces, and lines. The database is freely available as a partial or full database by sending an email to one of the authors.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {21},
numpages = {18},
keywords = {Text image database, Sindhi optical character recognition}
}

@article{10.1145/2891105,
author = {Sarigil, Erdem and Yilmaz, Oguz and Altingovde, Ismail Sengor and Ozcan, Rifat and Ulusoy, \"{O}zg\"{U}r},
title = {A “Suggested” Picture of Web Search in Turkish},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2891105},
doi = {10.1145/2891105},
abstract = {Although query log analysis provides crucial insights about Web users’ search interests, conducting such analyses is almost impossible for some languages, as large-scale and public query logs are quite scarce. In this study, we first survey the existing query collections in Turkish and discuss their limitations. Next, we adopt a novel strategy to obtain a set of Turkish queries using the query autocompletion services from the four major search engines and provide the first large-scale analysis of Web queries and their results in Turkish.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {24},
numpages = {11},
keywords = {Turkish query characteristics, query spelling correction}
}

@article{10.1145/2873068,
author = {Norimatsu, Jun-Ya and Yasuhara, Makoto and Tanaka, Toru and Yamamoto, Mikio},
title = {A Fast and Compact Language Model Implementation Using Double-Array Structures},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2873068},
doi = {10.1145/2873068},
abstract = {The language model is a widely used component in fields such as natural language processing, automatic speech recognition, and optical character recognition. In particular, statistical machine translation uses language models, and the translation speed and the amount of memory required are greatly affected by the performance of the language model implementation.We propose a fast and compact implementation of n-gram language models that increases query speed and reduces memory usage by using a double-array structure, which is known to be a fast and compact trie data structure. We propose two types of implementation: one for backward suffix trees and the other for reverse tries. The data structure is optimized for space efficiency by embedding model parameters into otherwise unused spaces in the double-array structure.We show that the reverse trie version of our method is among the smallest state-of-the-art implementations in terms of model size with almost the same speed as the implementation that performs fastest on perplexity calculation tasks. Similarly, we achieve faster decoding while keeping compact model sizes, and we confirm that our method can utilize the efficiency of the double-array structure to achieve a balance between speed and size on translation tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {27},
numpages = {27},
keywords = {n-gram model, trie, machine translation, Language model, double array}
}

@article{10.1145/2856105,
author = {Goswami, Mukesh M. and Mitra, Suman K.},
title = {Classification of Printed Gujarati Characters Using Low-Level Stroke Features},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2856105},
doi = {10.1145/2856105},
abstract = {This article presents an elegant technique for extracting the low-level stroke features, such as endpoints, junction points, line elements, and curve elements, from offline printed text using a template matching approach. The proposed features are used to classify a subset of characters from Gujarati script. The database consists of approximately 16,782 samples of 42 middle-zone symbols from the Gujarati character set collected from three different sources: machine printed books, newspapers, and laser printed documents. The purpose of this division is to add variety in terms of size, font type, style, ink variation, and boundary deformation. The experiments are performed on the database using a k-nearest neighbor (kNN) classifier and results are compared with other widely used structural features, namely Chain Codes (CC), Directional Element Features (DEF), and Histogram of Oriented Gradients (HoG). The results show that the features are quite robust against the variations and give comparable performance with other existing works.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {25},
numpages = {26},
keywords = {Characters classification, stroke features, Gujarati characters}
}

@article{10.1145/2890497,
author = {Bhowmik, Tapan Kumar and Parui, Swapan Kumar and Roy, Utpal and Schomaker, Lambert},
title = {Bangla Handwritten Character Segmentation Using Structural Features: A Supervised and Bootstrapping Approach},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2890497},
doi = {10.1145/2890497},
abstract = {In this article, we propose a new framework for segmentation of Bangla handwritten word images into meaningful individual symbols or pseudo-characters. Existing segmentation algorithms are not usually treated as a classification problem. However, in the present study, the segmentation algorithm is looked upon as a two-class supervised classification problem. The method employs an SVM classifier to select the segmentation points on the word image on the basis of various structural features. For training of the SVM classifier, an unannotated training set is prepared first using candidate segmenting points. The training set is then clustered, and each cluster is labeled manually with minimal manual intervention. A semi-automatic bootstrapping technique is also employed to enlarge the training set from new samples. The overall architecture describes a basic step toward building an annotation system for the segmentation problem, which has not so far been investigated. The experimental results show that our segmentation method is quite efficient in segmenting not only word images but also handwritten texts. As a part of this work, a database of Bangla handwritten word images has also been developed. Considering our data collection method and a statistical analysis of our lexicon set, we claim that the relevant characteristics of an ideal lexicon set are present in our handwritten word image database.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {29},
numpages = {26},
keywords = {Supervised classification based segmentation, Bangla handwriting database, handwriting segmentation, annotation, structural features, bootstrapping, SVM classifier}
}

@article{10.1145/2890496,
author = {Yang, Haitong and Zong, Chengqing},
title = {Learning Generalized Features for Semantic Role Labeling},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2890496},
doi = {10.1145/2890496},
abstract = {This article makes an effort to improve Semantic Role Labeling (SRL) through learning generalized features. The SRL task is usually treated as a supervised problem. Therefore, a huge set of features are crucial to the performance of SRL systems. But these features often lack generalization powers when predicting an unseen argument. This article proposes a simple approach to relieve the issue. A strong intuition is that arguments occurring in similar syntactic positions are likely to bear the same semantic role, and, analogously, arguments that are lexically similar are likely to represent the same semantic role. Therefore, it will be informative to SRL if syntactic or lexical similar arguments can activate the same feature. Inspired by this, we embed the information of lexicalization and syntax into a feature vector for each argument and then use K-means to make clustering for all feature vectors of training set. For an unseen argument to be predicted, it will belong to the same cluster as its similar arguments of training set. Therefore, the clusters can be thought of as a kind of generalized feature. We evaluate our method on several benchmarks. The experimental results show that our approach can significantly improve the SRL performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {28},
numpages = {16},
keywords = {similar arguments, Semantic role labeling, generalized features, K-means}
}

@article{10.1145/2800786,
author = {Sadek, Jawad and Meziane, Farid},
title = {Extracting Arabic Causal Relations Using Linguistic Patterns},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2800786},
doi = {10.1145/2800786},
abstract = {Identifying semantic relations is a crucial step in discourse analysis and is useful for many applications in both language and speech technology. Automatic detection of Causal relations therefore has gained popularity in the literature within different frameworks. The aim of this article is the automatic detection and extraction of Causal relations that are explicitly expressed in Arabic texts. To fulfill this goal, a Pattern Recognizer model was developed to signal the presence of cause--effect information within sentences from nonspecific domain texts. This model incorporates approximately 700 linguistic patterns so that parts of the sentence representing the cause and those representing the effect can be distinguished. The patterns were constructed based on different sets of syntactic features by analyzing a large untagged Arabic corpus. In addition, the model was boosted with three independent algorithms to deal with certain types of grammatical particles that indicate causation. With this approach, the proposed model achieved an overall recall of 81% and a precision of 78%. Evaluation results revealed that the justification particles play a key role in detecting Causal relations. To the best of our knowledge, no previous studies have been dedicated to dealing with this type of relation in the Arabic language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {14},
numpages = {20},
keywords = {information extraction, causal relations, Patterns matching, Arabic discourse relations}
}

@article{10.1145/2835494,
author = {Chakrabarty, Abhisek and Garain, Utpal},
title = {BenLem (A Bengali Lemmatizer) and Its Role in WSD},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2835494},
doi = {10.1145/2835494},
abstract = {A lemmatization algorithm for Bengali has been developed and evaluated. Its effectiveness for word sense disambiguation (WSD) is also investigated. One of the key challenges for computer processing of highly inflected languages is to deal with the frequent morphological variations of the root words appearing in the text. Therefore, a lemmatizer is essential for developing natural language processing (NLP) tools for such languages. In this experiment, Bengali, which is the national language of Bangladesh and the second most popular language in the Indian subcontinent, has been taken as a reference. In order to design the Bengali lemmatizer (named as BenLem), possible transformations through which surface words are formed from lemmas are studied so that appropriate reverse transformations can be applied on a surface word to get the corresponding lemma back. BenLem is found to be capable of handling both inflectional and derivational morphology in Bengali. It is evaluated on a set of 18 news articles taken from the FIRE Bengali News Corpus consisting of 3,342 surface words (excluding proper nouns) and found to be 81.95% accurate. The role of the lemmatizer is then investigated for Bengali WSD. Ten highly polysemous Bengali words are considered for sense disambiguation. The FIRE corpus and a collection of Tagore’s short stories are considered for creating the WSD dataset. Different WSD systems are considered for this experiment, and it is noticed that BenLem improves the performance of all the WSD systems and the improvements are statistically significant.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {12},
numpages = {18},
keywords = {evaluation, lemmatizer, word sense disambiguation (WSD), Bengali, Indic languages}
}

@article{10.1145/2820902,
author = {Zhou, Hao and Huang, Shujian and Zhou, Junsheng and Zhang, Yue and Chen, Huadong and Dai, Xinyu and Cheng, Chuan and Chen, Jiajun},
title = {Enhancing Shift-Reduce Constituent Parsing with Action N-Gram Model},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2820902},
doi = {10.1145/2820902},
abstract = {Current shift-reduce parsers “understand” the context by embodying a large number of binary indicator features with a discriminative model. In this article, we propose the action n-gram model, which utilizes the action sequence to help parsing disambiguation. The action n-gram model is trained on action sequences produced by parsers with the n-gram estimation method, which gives a smoothed maximum likelihood estimation of the action probability given a specific action history. We show that incorporating action n-gram models into a state-of-the-art parsing framework could achieve parsing accuracy improvements on three datasets across two languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {13},
numpages = {17},
keywords = {Shift-reduce constituent parsing, action n-gram model, action history}
}

@article{10.1145/2789210,
author = {Elayeb, Bilel and Bounhas, Ibrahim},
title = {Arabic Cross-Language Information Retrieval: A Review},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2789210},
doi = {10.1145/2789210},
abstract = {Cross-language information retrieval (CLIR) deals with retrieving relevant documents in one language using queries expressed in another language. As CLIR tools rely on translation techniques, they are challenged by the properties of highly derivational and flexional languages like Arabic. Much work has been done on CLIR for different languages including Arabic. In this article, we introduce the reader to the motivations for solving some problems related to Arabic CLIR approaches. The evaluation of these approaches is discussed starting from the 2001 and 2002 TREC Arabic CLIR tracks, which aim to objectively evaluate CLIR systems. We also study many other research works to highlight the unresolved problems or those that require further investigation. These works are discussed in the light of a deep study of the specificities and the tasks of Arabic information retrieval (IR). Particular attention is given to translation techniques and CLIR resources, which are key issues challenging Arabic CLIR. To push research in this field, we discuss how a new standard collection can improve Arabic IR and CLIR tracks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {18},
numpages = {44},
keywords = {Arabic translation knowledge, Arabic cross-language information retrieval, stopwords removal, tokenization, stemming, transliteration, complex morphology}
}

@article{10.1145/2815619,
author = {Li, Maoxi and Wang, Mingwen and Li, Hanxi and Xu, Fan},
title = {Modeling Monolingual Character Alignment for Automatic Evaluation of Chinese Translation},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2815619},
doi = {10.1145/2815619},
abstract = {Automatic evaluation of machine translations is an important task. Most existing evaluation metrics rely on matching the same word or letter n-grams. This strategy leads to poor results on Chinese translations because one has to rely merely on matching identical characters. In this article, we propose a new evaluation metric that allows different characters with the same or similar meaning to match. An Indirect Hidden Markov Model (IHMM) is proposed to align the Chinese translation with human references at the character level. In the model, the emission probabilities are estimated by character similarity, including character semantic similarity and character surface similarity, and transition probabilities are estimated by a heuristic distance-based distortion model. When evaluating the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks, the experimental results indicate that the proposed metric has a significantly better correlation with human evaluation than the state-of-the-art machine translation metrics (i.e., BLEU, Meteor Universal, and TESLA-CELAB). This study shows that it is important to allow different characters to match in the evaluation of Chinese translations and that the IHMM is a reasonable approach for the alignment of Chinese characters.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {16},
numpages = {18},
keywords = {Automatic evaluation, IHMM, system-level correlation, segment-level consistency, Chinese character, monolingual character alignment, Chinese translation, word order, synonym matching}
}

@article{10.1145/2812809,
author = {Abuaiadah, Diab},
title = {Using Bisect K-Means Clustering Technique in the Analysis of Arabic Documents},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2812809},
doi = {10.1145/2812809},
abstract = {In this article, I have investigated the performance of the bisect K-means clustering algorithm compared to the standard K-means algorithm in the analysis of Arabic documents. The experiments included five commonly used similarity and distance functions (Pearson correlation coefficient, cosine, Jaccard coefficient, Euclidean distance, and averaged Kullback-Leibler divergence) and three leading stemmers. Using the purity measure, the bisect K-means clearly outperformed the standard K-means in all settings with varying margins. For the bisect K-means, the best purity reached 0.927 when using the Pearson correlation coefficient function, while for the standard K-means, the best purity reached 0.884 when using the Jaccard coefficient function. Removing stop words significantly improved the results of the bisect K-means but produced minor improvements in the results of the standard K-means. Stemming provided additional minor improvement in all settings except the combination of the averaged Kullback-Leibler divergence function and the root-based stemmer, where the purity was deteriorated by more than 10%. These experiments were conducted using a dataset with nine categories, each of which contains 300 documents.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {17},
numpages = {13},
keywords = {Arabic stemmers, bisect K-means, Information retrieval, similarity measures, K-means}
}

@article{10.1145/2816816,
author = {Zhao, Yinggong and Huang, Shujian and Dai, Xin-Yu and Chen, Jiajun},
title = {Adaptation of Language Models for SMT Using Neural Networks with Topic Information},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2816816},
doi = {10.1145/2816816},
abstract = {Neural network language models (LMs) are shown to be effective in improving the performance of statistical machine translation (SMT) systems. However, state-of-the-art neural network LMs usually use words before the current position as context and neglect global topic information, which can help machine translation (MT) systems to select better translation candidates from a higher perspective. In this work, we propose improvement of the state-of-the-art feedforward neural language model with topic information. Two main issues need to be tackled when adding topics into neural network LMs for SMT: one is how to incorporate topics to the neural network; the other is how to get target-side topic distribution before translation. We incorporate topics by appending topic distribution to the input layer of a feedforward LM. We adopt a multinomial logistic-regression (MLR) model to predict the target-side topic distribution based on source side information. Moreover, we propose a feedforward neural network model to learn joint representations on the source side for topic prediction. LM experiments demonstrate that the perplexity on validation set can be greatly reduced by the topic-enhanced feedforward LM, and the prediction of target-side topics can be improved dramatically with the MLR model equipped with the joint source representations. A final MT experiment, conducted on a large-scale Chinese--English dataset, shows that our feedforward LM with predicted topics improves the translation performance against a strong baseline.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {19},
numpages = {15},
keywords = {multinomial logistic regression, topic model, joint representation, feedforward neural network language model, Statistical machine translation}
}

@article{10.1145/2818381,
author = {Ding, Chenchen and Sakanushi, Keisuke and Touji, Hirona and Yamamoto, Mikio},
title = {Inter-, Intra-, and Extra-Chunk Pre-Ordering for Statistical Japanese-to-English Machine Translation},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2818381},
doi = {10.1145/2818381},
abstract = {A rule-based pre-ordering approach is proposed for statistical Japanese-to-English machine translation using the dependency structure of source-side sentences. A Japanese sentence is pre-ordered to an English-like order at the morpheme level for a statistical machine translation system during the training and decoding phase to resolve the reordering problem. In this article, extra-chunk pre-ordering of morphemes is proposed, which allows Japanese functional morphemes to move across chunk boundaries. This contrasts with the intra-chunk reordering used in previous approaches, which restricts the reordering of morphemes within a chunk. Linguistically oriented discussions show that correct pre-ordering cannot be realized without extra-chunk movement of morphemes. The proposed approach is compared with five rule-based pre-ordering approaches designed for Japanese-to-English translation and with a language independent statistical pre-ordering approach on a standard patent dataset and on a news dataset obtained by crawling Internet news sites. Two state-of-the-art statistical machine translation systems, one phrase-based and the other hierarchical phrase-based, are used in experiments. Experimental results show that the proposed approach outperforms the compared approaches on automatic reordering measures (Kendall’s τ, Spearman’s ρ, fuzzy reordering score, and test set RIBES) and on the automatic translation precision measure of test set BLEU score.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {20},
numpages = {28},
keywords = {rule-based, pre-ordering, dependency structure, English, morpheme, chunk, Japanese}
}

@article{10.1145/2835493,
author = {Yang, Haitong and Zhou, Yu and Zong, Chengqing},
title = {Bilingual Semantic Role Labeling Inference via Dual Decomposition},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2835493},
doi = {10.1145/2835493},
abstract = {This article focuses on bilingual Semantic Role Labeling (SRL); its goal is to annotate semantic roles on both sides of the parallel bilingual texts (bi-texts). Since rich bilingual information is encoded, bilingual SRL has been applied in many natural-language processing (NLP) tasks such as machine translation (MT), cross-lingual information retrieval (IR), and the like. A feasible way of performing bilingual SRL is using monolingual SRL systems to perform SRL on each side of bi-texts separately. However, it is difficult to obtain consistent SRL results on both sides of bi-texts in this way. Some works have tried to jointly infer bilingual SRL because there are many complementary language cues on both sides of bi-texts and they reported better performance than monolingual systems. However, there are two limits in the existing methods. First, the existing methods often require high inference costs due to the complex objective function. Second, the existing methods fully adopt the candidates generated by monolingual SRL systems, but many candidates are discarded in the argument pruning or identification stage of monolingual systems. In this article, we propose two strategies to overcome these limits. We utilize a simple but efficient technique: Dual Decomposition to search for consistent results for both sides of bi-texts. On the other hand, we propose a method called Bi-Directional Projection (BDP) to recover arguments discarded in monolingual SRL systems.We evaluate our method on a standard parallel benchmark: the OntoNotes dataset. The experimental results show that our method yields significant improvements over the state-of-the-art monolingual systems. In addition, our approach is also better and faster than existing methods due to BDP and Dual Decomposition.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {15},
numpages = {21},
keywords = {Lagrange dual decomposition, Semantic role labeling, bi-directional projection, bi-texts}
}

@article{10.1145/2843942,
author = {Wang, Rui and Utiyama, Masao and Goto, Isao and Sumita, Eiichiro and Zhao, Hai and Lu, Bao-Liang},
title = {Converting Continuous-Space Language Models into <i>N</i>-Gram Language Models with Efficient Bilingual Pruning for Statistical Machine Translation},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2843942},
doi = {10.1145/2843942},
abstract = {The Language Model (LM) is an essential component of Statistical Machine Translation (SMT). In this article, we focus on developing efficient methods for LM construction. Our main contribution is that we propose a Natural N-grams based Converting (NNGC) method for transforming a Continuous-Space Language Model (CSLM) to a Back-off N-gram Language Model (BNLM). Furthermore, a Bilingual LM Pruning (BLMP) approach is developed for enhancing LMs in SMT decoding and speeding up CSLM converting. The proposed pruning and converting methods can convert a large LM efficiently by working jointly. That is, a LM can be effectively pruned before it is converted from CSLM without sacrificing performance, and further improved if an additional corpus contains out-of-domain information. For different SMT tasks, our experimental results indicate that the proposed NNGC and BLMP methods outperform the existing counterpart approaches significantly in BLEU and computational cost.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {11},
numpages = {26},
keywords = {neural network language model, language model pruning, Machine translation, continuous-space language model}
}

@article{10.1145/2786978,
author = {Yeh, Jui-Feng},
title = {Speech Act Identification Using Semantic Dependency Graphs with Probabilistic Context-Free Grammars},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2786978},
doi = {10.1145/2786978},
abstract = {We propose an approach for identifying the speech acts of speakers’ utterances in conversational spoken dialogue that involves using semantic dependency graphs with probabilistic context-free grammars (PCFGs). The semantic dependency graph based on the HowNet knowledge base is adopted to model the relationships between words in an utterance parsed by PCFG. Dependency relationships between words within the utterance are extracted by decomposing the semantic dependency graph according to predefined events. The corresponding values of semantic slots are subsequently extracted from the speaker's utterances according to the corresponding identified speech act. The experimental results obtained when using the proposed approach indicated that the accuracy rates of speech act detection and task completion were 95.6% and 77.4% for human-generated transcription (REF) and speech-to-text recognition output (STT), respectively, and the average numbers of turns of each dialogue were 8.3 and 11.8 for REF and STT, respectively. Compared with Bayes classifier, partial pattern tree, and Bayesian-network-based approaches, we obtained 14.1%, 9.2%, and 3% improvements in the accuracy of speech act identification, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {5},
numpages = {28},
keywords = {semantic dependency graph, Spoken language processing, probabilistic context-free grammars, conversational dialogue systems, speech act identification}
}

@article{10.1145/2790079,
author = {Jain, Amita and Lobiyal, D. K.},
title = {Fuzzy Hindi WordNet and Word Sense Disambiguation Using Fuzzy Graph Connectivity Measures},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2790079},
doi = {10.1145/2790079},
abstract = {In this article, we propose Fuzzy Hindi WordNet, which is an extended version of Hindi WordNet. The proposed idea of fuzzy relations and their role in modeling Fuzzy Hindi WordNet is explained. We mathematically define fuzzy relations and the composition of these fuzzy relations for this extended version. We show that the concept of composition of fuzzy relations can be used to infer a relation between two words that otherwise are not directly related in Hindi WordNet. Then we propose fuzzy graph connectivity measures that include both local and global measures. These measures are used in determining the significance of a concept (which is represented as a vertex in the fuzzy graph) in a specific context. Finally, we show how these extended measures solve the problem of word sense disambiguation (WSD) effectively, which is useful in many natural language processing applications to improve their performance. Experiments on standard sense tagged corpus for WSD show better results when Fuzzy Hindi WordNet is used in place of Hindi WordNet.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {8},
numpages = {31},
keywords = {lexicon, Hindi WordNet, Centrality, fuzzy logic, word sense disambiguation, fuzzy graph}
}

@article{10.1145/2794399,
author = {Tsai, Richard Tzong-Han},
title = {Collective Web-Based Parenthetical Translation Extraction Using Markov Logic Networks},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2794399},
doi = {10.1145/2794399},
abstract = {Parenthetical translations are translations of terms in otherwise monolingual text that appear inside parentheses. Parenthetical translations extraction (PTE) is the task of extracting parenthetical translations from natural language documents. One of the main difficulties in PTE is to detect the left boundary of the translated term in preparenthetical text. In this article, we propose a collective approach that employs Markov logic to model multiple constraints used in the PTE task. We show how various constraints can be formulated and combined in a Markov logic network (MLN). Our experimental results show that the proposed collective PTE approach significantly outperforms a current state-of-the-art method, improving the average F-measure up to 27.11% compared to the previous word alignment approach. It also outperforms an individual MLN-based system by 8.2% and a system based on conditional random fields by 5.9%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {7},
numpages = {14},
keywords = {entity translation, named entity translation, markov logic network, Parenthetical translation extraction}
}

@article{10.1145/2833088,
author = {Kertkeidkachorn, Natthawut and Punyabukkana, Proadpran and Suchato, Atiwong},
title = {Acoustic Features for Hidden Conditional Random Fields--Based Thai Tone Classification},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2833088},
doi = {10.1145/2833088},
abstract = {In the Thai language, tone information is necessary for Thai speech recognition systems. Previous studies show that many acoustic cues are attributed to shapes of tones. Nevertheless, most Thai tone classification studies mainly adopted F0 values and their derivatives without considering other acoustic features. In this article, other acoustic features for Thai tone classification are investigated. In the experiment, energy values and spectral information represented by three spectral-based features including the LPC-based feature, PLP-based feature, and MFCC-based feature are applied to the HCRF-based Thai tone classification, which was reported as the best approach for Thai tone classification. The energy values provide an error rate reduction of 22.40% in the isolated word scenario, while there are slight improvements in the continuous speech scenario. On the contrary, spectral-based features greatly contribute to Thai tone classification in the continuous-speech scenario, whereas spectral-based features slightly degrade performances in the isolated-word scenario. The best achievement in the continuous-speech scenario is obtained from the PLP-based feature, which yields an error rate reduction of 13.90%. Therefore, findings in this article are that energy values and spectral-based features, especially the PLP-based feature, are the main contributors to the improvement of the performances of Thai tone classification in the isolated-word scenario and the continuous-speech scenario, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {9},
numpages = {26},
keywords = {energy, tone features, Thai tone classification, hidden conditional random fields, acoustic features, spectral information}
}

@article{10.1145/2833089,
author = {Chu, Chenhui and Nakazawa, Toshiaki and Kurohashi, Sadao},
title = {Integrated Parallel Sentence and Fragment Extraction from Comparable Corpora: A Case Study on Chinese--Japanese Wikipedia},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2833089},
doi = {10.1145/2833089},
abstract = {Parallel corpora are crucial for statistical machine translation (SMT); however, they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract either parallel sentences or fragments from them for SMT. In this article, we propose an integrated system to extract both parallel sentences and fragments from comparable corpora. We first apply parallel sentence extraction to identify parallel sentences from comparable sentences. We then extract parallel fragments from the comparable sentences. Parallel sentence extraction is based on a parallel sentence candidate filter and classifier for parallel sentence identification. We improve it by proposing a novel filtering strategy and three novel feature sets for classification. Previous studies have found it difficult to accurately extract parallel fragments from comparable sentences. We propose an accurate parallel fragment extraction method that uses an alignment model to locate the parallel fragment candidates and an accurate lexicon-based filter to identify the truly parallel fragments. A case study on the Chinese--Japanese Wikipedia indicates that our proposed methods outperform previously proposed methods, and the parallel data extracted by our system significantly improves SMT performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {10},
numpages = {22},
keywords = {parallel sentence, parallel fragment, comparable corpora, Integrated system}
}

@article{10.1145/2742547,
author = {Wang, Ting-Xuan and Lu, Wen-Hsiang},
title = {Constructing Complex Search Tasks with Coherent Subtask Search Goals},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2742547},
doi = {10.1145/2742547},
abstract = {Nowadays, due to the explosive growth of web content and usage, users deal with their complex search tasks by web search engines. However, conventional search engines consider a search query corresponding only to a simple search task. In order to accomplish a complex search task, which consists of multiple subtask search goals, users usually have to issue a series of queries. For example, the complex search task “travel to Dubai” may involve several subtask search goals, including reserving hotel room, surveying Dubai landmarks, booking flights, and so forth. Therefore, a user can efficiently accomplish his or her complex search task if search engines can predict the complex search task with a variety of subtask search goals. In this work, we propose a complex search task model (CSTM) to deal with this problem. The CSTM first groups queries into complex search task clusters, and then generates subtask search goals from each complex search task cluster. To raise the performance of CSTM, we exploit four web resources including community question answering, query logs, search engine result pages, and clicked pages. Experimental results show that our CSTM is effective in identifying the comprehensive subtask search goals of a complex search task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {6},
numpages = {29},
keywords = {subtask search goal, query clustering, Complex search task}
}

@article{10.1145/2710018,
author = {Khanduja, Deepti and Nain, Neeta and Panwar, Subhash},
title = {A Hybrid Feature Extraction Algorithm for Devanagari Script},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2710018},
doi = {10.1145/2710018},
abstract = {The efficiency of any character recognition technique is directly dependent on the accuracy of the generated feature set that could uniquely represent a character and hence correctly recognize it. This article proposes a hybrid approach combining the structural features of the character and a mathematical model of curve fitting to simulate the best features of a character. As a preprocessing step, skeletonization of the character is performed using an iterative thinning algorithm based on Raster scan of the character image. Then, a combination of structural features of the character like number of endpoints, loops, and intersection points is calculated. Further, the thinned character image is statistically zoned into partitions, and a quadratic curve-fitting model is applied on each partition forming a feature vector of the coefficients of the optimally fitted curve. This vector is combined with the spatial distribution of the foreground pixels for each zone and hence script-independent feature representation. The approach has been evaluated experimentally on Devanagari scripts. The algorithm achieves an average recognition accuracy of 93.4%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {2},
numpages = {10},
keywords = {Thinning, raster scan, curve fitting, zoning}
}

@article{10.1145/2723144,
author = {Wushouer, Mairidan and Lin, Donghui and Ishida, Toru and Hirayama, Katsutoshi},
title = {A Constraint Approach to Pivot-Based Bilingual Dictionary Induction},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2723144},
doi = {10.1145/2723144},
abstract = {High-quality bilingual dictionaries are very useful, but such resources are rarely available for lower-density language pairs, especially for those that are closely related. Using a third language to link two other languages is a well-known solution and usually requires only two input bilingual dictionaries A-B and B-C to automatically induce the new one, A-C. This approach, however, has never been demonstrated to utilize the complete structures of the input bilingual dictionaries, and this is a key failing because the dropped meanings negatively influence the result. This article proposes a constraint approach to pivot-based dictionary induction where language A and C are closely related. We create constraints from language similarity and model the structures of the input dictionaries as a Boolean optimization problem, which is then formulated within the Weighted Partial Max-SAT framework, an extension of Boolean Satisfiability (SAT). All of the encoded CNF (Conjunctive Normal Form), the predominant input language of modern SAT/MAX-SAT solvers, formulas are evaluated by a solver to produce the target (output) bilingual dictionary. Moreover, we discuss alternative formalizations as a comparison study. We designed a tool that uses the Sat4j library as the default solver to implement our method and conducted an experiment in which the output bilingual dictionary achieved better quality than the baseline method.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {4},
numpages = {26},
keywords = {Bilingual dictionary induction, low-resource languages, Weighted Partial Max-SAT, pivot language, constraint satisfaction problem}
}

@article{10.1145/2764456,
author = {Shatnawi, Maad and Abdallah, Sherief},
title = {Improving Handwritten Arabic Character Recognition by Modeling Human Handwriting Distortions},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2764456},
doi = {10.1145/2764456},
abstract = {Handwritten Arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and the unavailability of large public databases of handwritten characters and words. The use of synthetic data for training and testing handwritten character recognition systems is one of the possible solutions to provide several variations for these characters and to overcome the lack of large databases. While this can be using arbitrary distortions, such as image noise and randomized affine transformations, such distortions are not realistic. In this work, we model real distortions in handwriting using real handwritten Arabic character examples and then use these distortion models to synthesize handwritten examples that are more realistic. We show that the use of our proposed approach leads to significant improvements across different machine-learning classification algorithms.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {3},
numpages = {12},
keywords = {Arabic character recognition, synthetic data, human handwriting distortion, affine transformations, congealing}
}

@article{10.1145/2738045,
author = {Costa-Juss\`{a}, Marta R. and Centelles, Jordi},
title = {Description of the Chinese-to-Spanish Rule-Based Machine Translation System Developed Using a Hybrid Combination of Human Annotation and Statistical Techniques},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2738045},
doi = {10.1145/2738045},
abstract = {Two of the most popular Machine Translation (MT) paradigms are rule based (RBMT) and corpus based, which include the statistical systems (SMT). When scarce parallel corpus is available, RBMT becomes particularly attractive. This is the case of the Chinese--Spanish language pair.This article presents the first RBMT system for Chinese to Spanish. We describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules.The final system is freely available online and open source. Although performance lags behind standard SMT systems for an in-domain test set, the results show that the RBMT’s coverage is competitive and it outperforms the SMT system in an out-of-domain test set. This RBMT system is available to the general public, it can be further enhanced, and it opens up the possibility of creating future hybrid MT systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {1},
numpages = {13},
keywords = {statistical techniques, Rule-based Machine Translation, Chinese-to-Spanish}
}

@article{10.1145/2822264,
author = {Liu, Xiaodong and Cheng, Fei and Duh, Kevin and Matsumoto, Yuji},
title = {A Hybrid Ranking Approach to Chinese Spelling Check},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2822264},
doi = {10.1145/2822264},
abstract = {We propose a novel framework for Chinese Spelling Check (CSC), which is an automatic algorithm to detect and correct Chinese spelling errors. Our framework contains two key components: candidate generation and candidate ranking. Our framework differs from previous research, such as Statistical Machine Translation (SMT) based model or Language Model (LM) based model, in that we use both SMT and LM models as components of our framework for generating the correction candidates, in order to obtain maximum recall; to improve the precision, we further employ a Support Vector Machines (SVM) classifier to rank the candidates generated by the SMT and the LM. Experiments show that our framework outperforms other systems, which adopted the same or similar resources as ours in the SIGHAN 7 shared task; even comparing with the state-of-the-art systems, which used more resources, such as a considerable large dictionary, an idiom dictionary and other semantic information, our framework still obtains competitive results. Furthermore, to address the resource scarceness problem for training the SMT model, we generate around 2 million artificial training sentences using the Chinese character confusion sets, which include a set of Chinese characters with similar shapes and similar pronunciations, provided by the SIGHAN 7 shared task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {16},
numpages = {17},
keywords = {candidate ranking, Chinese spelling check, candidate generation}
}

@article{10.1145/2823512,
title = {TALLIP Perspectives: Editorial Commentary: The State of the Journal},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2823512},
doi = {10.1145/2823512},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {19},
numpages = {3}
}

@article{10.1145/2791389,
author = {Hsieh, Yu-Ming and Bai, Ming-Hong and Huang, Shu-Ling and Chen, Keh-Jiann},
title = {Correcting Chinese Spelling Errors with Word Lattice Decoding},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2791389},
doi = {10.1145/2791389},
abstract = {Chinese spell checkers are more difficult to develop because of two language features: 1) there are no word boundaries, and a character may function as a word or a word morpheme; and 2) the Chinese character set contains more than ten thousand characters. The former makes it difficult for a spell checker to detect spelling errors, and the latter makes it difficult for a spell checker to construct error models. We develop a word lattice decoding model for a Chinese spell checker that addresses these difficulties. The model performs word segmentation and error correction simultaneously, thereby solving the word boundary problem. The model corrects nonword errors as well as real-word errors. In order to better estimate the error distribution of large character sets for error models, we also propose a methodology to extract spelling error samples automatically from the Google web 1T corpus. Due to the large quantity of data in the Google web 1T corpus, many spelling error samples can be extracted, better reflecting spelling error distributions in the real world. Finally, in order to improve the spell checker for real applications, we produce n-best suggestions for spelling error corrections. We test our proposed approach with the Bakeoff 2013 CSC Datasets; the results show that the proposed methods with the error model significantly outperform the performance of Chinese spell checkers that do not use error models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {18},
numpages = {23},
keywords = {word lattice, noisy channel model, Chinese spelling error checking, computer-assisted language learning, word segmentation, unknown word detection}
}

@article{10.1145/2818354,
author = {Lee, Lung-Hao and Levow, Gina-Anne and Wu, Shih-Hung and Liu, Chao-Lin},
title = {Introduction to the Special Issue on Chinese Spell Checking},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2818354},
doi = {10.1145/2818354},
abstract = {This special issue contains four articles based on and expanded from systems presented at the SIGHAN-7 Chinese Spelling Check Bakeoff. We provide an overview of the approaches and designs for Chinese spelling checkers presented in these articles. We conclude this introductory article with a summary of possible future directions.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {14},
numpages = {4},
keywords = {detection models, correction models, confusion sets, Spelling errors}
}

@article{10.1145/2826235,
author = {Yeh, Jui-Feng and Chen, Wen-Yi and Su, Mao-Chuan},
title = {Chinese Spelling Checker Based on an Inverted Index List with a Rescoring Mechanism},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2826235},
doi = {10.1145/2826235},
abstract = {An approach is proposed for Chinese spelling error detection and correction, in which an inverted index list with a rescoring mechanism is used. The inverted index list is a structure for mapping from word to desired sentence, and for representing nodes in lattices constructed through character expansion (according to predefined phonologically and visually similar character sets). Pruning based on a contextual dependency confidence measure was used to markedly reduce the search space and computational complexity. Relevant mapping relations between the original input and desired input were obtained using a scoring mechanism composed of class-based language and maximum entropy correction models containing character, word, and contextual features. The proposed method was evaluated using data sets provided by SigHan 7 bakeoff. The experimental results show that the proposed method achieved acceptable performance in terms of recall rate or precision rate in error sentence detection and error location detection, and it outperformed other approaches in error location detection and correction.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {17},
numpages = {28},
keywords = {language model, Spelling checker, inverted index list, correction model, maximum entropy, contextual information}
}

@article{10.1145/2826234,
author = {Chen, Kuan-Yu and Wang, Hsin-Min and Chen, Hsin-Hsi},
title = {A Probabilistic Framework for Chinese Spelling Check},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/2826234},
doi = {10.1145/2826234},
abstract = {Chinese spelling check (CSC) is still an unsolved problem today since there are many homonymous or homomorphous characters. Recently, more and more CSC systems have been proposed. To the best of our knowledge, language modeling is one of the major components among these systems because of its simplicity and moderately good predictive power. After deeply analyzing the school of research, we are aware that most of the systems only employ the conventional n-gram language models. The contributions of this article are threefold. First, we propose a novel probabilistic framework for CSC, which naturally combines several important components, such as the substitution model and the language model, to inherit their individual merits as well as to overcome their limitations. Second, we incorporate the topic language models into the CSC system in an unsupervised fashion. The topic language models can capture the long-span semantic information from a word (character) string while the conventional n-gram language models can only preserve the local regularity information. Third, we further integrate Web resources with the proposed framework to enhance the overall performance. Our rigorously empirical experiments demonstrate the consistent and utility performance of the proposed framework in the CSC task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {15},
numpages = {17},
keywords = {probabilistic, Language model, spelling check, topic modeling, Chinese}
}

@article{10.1145/2699939,
author = {Liu, Xiaodong and Duh, Kevin and Matsumoto, Yuji},
title = {Multilingual Topic Models for Bilingual Dictionary Extraction},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2699939},
doi = {10.1145/2699939},
abstract = {A machine-readable bilingual dictionary plays a crucial role in many natural language processing tasks, such as statistical machine translation and cross-language information retrieval. In this article, we propose a framework for extracting a bilingual dictionary from comparable corpora by exploiting a novel combination of topic modeling and word aligners such as the IBM models. Using a multilingual topic model, we first convert a comparable document-aligned corpus into a parallel topic-aligned corpus. This novel topic-aligned corpus is similar in structure to the sentence-aligned corpus frequently employed in statistical machine translation and allows us to extract a bilingual dictionary using a word alignment model.The main advantages of our framework is that (1) no seed dictionary is necessary for bootstrapping the process, and (2) multilingual comparable corpora in more than two languages can also be exploited. In our experiments on a large-scale Wikipedia dataset, we demonstrate that our approach can extract higher precision dictionaries compared to previous approaches and that our method improves further as we add more languages to the dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {11},
numpages = {22},
keywords = {comparable corpus, Bilingual dictionary, multilingual topic model}
}

@article{10.1145/2699925,
author = {Goto, Isao and Utiyama, Masao and Sumita, Eiichiro and Kurohashi, Sadao},
title = {Preordering Using a Target-Language Parser via Cross-Language Syntactic Projection for Statistical Machine Translation},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2699925},
doi = {10.1145/2699925},
abstract = {When translating between languages with widely different word orders, word reordering can present a major challenge. Although some word reordering methods do not employ source-language syntactic structures, such structures are inherently useful for word reordering. However, high-quality syntactic parsers are not available for many languages. We propose a preordering method using a target-language syntactic parser to process source-language syntactic structures without a source-language syntactic parser. To train our preordering model based on ITG, we produced syntactic constituent structures for source-language training sentences by (1) parsing target-language training sentences, (2) projecting constituent structures of the target-language sentences to the corresponding source-language sentences, (3) selecting parallel sentences with highly synchronized parallel structures, (4) producing probabilistic models for parsing using the projected partial structures and the Pitman-Yor process, and (5) parsing to produce full binary syntactic structures maximally synchronized with the corresponding target-language syntactic structures, using the constraints of the projected partial structures and the probabilistic models. Our ITG-based preordering model is trained using the produced binary syntactic structures and word alignments. The proposed method facilitates the learning of ITG by producing highly synchronized parallel syntactic structures based on cross-language syntactic projection and sentence selection. The preordering model jointly parses input sentences and identifies their reordered structures. Experiments with Japanese--English and Chinese--English patent translation indicate that our method outperforms existing methods, including string-to-tree syntax-based SMT, a preordering method that does not require a parser, and a preordering method that uses a source-language dependency parser.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {13},
numpages = {23},
keywords = {syntactic projection, Preordering, inversion transduction grammar, constituent structure}
}

@article{10.1145/2700051,
author = {Na, Seung-Hoon},
title = {Conditional Random Fields for Korean Morpheme Segmentation and POS Tagging},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2700051},
doi = {10.1145/2700051},
abstract = {There has been recent interest in statistical approaches to Korean morphological analysis. However, previous studies have been based mostly on generative models, including a hidden Markov model (HMM), without utilizing discriminative models such as a conditional random field (CRF). We present a two-stage discriminative approach based on CRFs for Korean morphological analysis. Similar to methods used for Chinese, we perform two disambiguation procedures based on CRFs: (1) morpheme segmentation and (2) POS tagging. In morpheme segmentation, an input sentence is segmented into sequences of morphemes, where a morpheme unit is either atomic or compound. In the POS tagging procedure, each morpheme (atomic or compound) is assigned a POS tag. Once POS tagging is complete, we carry out a post-processing of the compound morphemes, where each compound morpheme is further decomposed into atomic morphemes, which is based on pre-analyzed patterns and generalized HMMs obtained from the given tagged corpus. Experimental results show the promise of our proposed method.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {10},
numpages = {16},
keywords = {Korean morphological analysis, morpheme segmentation, POS tagging, Conditional random fields}
}

@article{10.1145/2699940,
author = {Li, Xiaoqing and Zong, Chengqing and Su, Keh-yih},
title = {A Unified Model for Solving the OOV Problem of Chinese Word Segmentation},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/2699940},
doi = {10.1145/2699940},
abstract = {This article proposes a unified, character-based, generative model to incorporate additional resources for solving the out-of-vocabulary (OOV) problem of Chinese word segmentation, within which different types of additional information can be utilized independently in corresponding submodels. This article mainly addresses the following three types of OOV: unseen dictionary words, named entities, and suffix-derived words, none of which are handled well by current approaches. The results show that our approach can effectively improve the performance of the first two types with positive interaction in F-score. Additionally, we also analyze reason that suffix information is not helpful. After integrating the proposed generative model with the corresponding discriminative approach, our evaluation on various corpora---including SIGHAN-2005, CIPS-SIGHAN-2010, and the Chinese Treebank (CTB)---shows that our integrated approach achieves the best performance reported in the literature on all testing sets when additional information and resources are allowed.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {12},
numpages = {29},
keywords = {out-of-vocabulary words, model integration, Chinese word segmentation, domain adaptation}
}

@article{10.1145/2661637,
author = {Shen, Han-ping and Wu, Chung-hsien and Tsai, Pei-shan},
title = {Model Generation of Accented Speech Using Model Transformation and Verification for Bilingual Speech Recognition},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2661637},
doi = {10.1145/2661637},
abstract = {Nowadays, bilingual or multilingual speech recognition is confronted with the accent-related problem caused by non-native speech in a variety of real-world applications. Accent modeling of non-native speech is definitely challenging, because the acoustic properties in highly-accented speech pronounced by non-native speakers are quite divergent. The aim of this study is to generate highly Mandarin-accented English models for speakers whose mother tongue is Mandarin. First, a two-stage, state-based verification method is proposed to extract the state-level, highly-accented speech segments automatically. Acoustic features and articulatory features are successively used for robust verification of the extracted speech segments. Second, Gaussian components of the highly-accented speech models are generated from the corresponding Gaussian components of the native speech models using a linear transformation function. A decision tree is constructed to categorize the transformation functions and used for transformation function retrieval to deal with the data sparseness problem. Third, a discrimination function is further applied to verify the generated accented acoustic models. Finally, the successfully verified accented English models are integrated into the native bilingual phone model set for Mandarin-English bilingual speech recognition. Experimental results show that the proposed approach can effectively alleviate recognition performance degradation due to accents and can obtain absolute improvements of 4.1%, 1.8%, and 2.7% in word accuracy for bilingual speech recognition compared to that using traditional ASR approaches, MAP-adapted, and MLLR-adapted ASR methods, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {6},
numpages = {24},
keywords = {bilingual speech recognition, Accented speech, articulatory feature}
}

@article{10.1145/2671014,
author = {Sundaram, Suresh and Ramakrishnan, A. G.},
title = {Bigram Language Models and Reevaluation Strategy for Improved Recognition of Online Handwritten Tamil Words},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2671014},
doi = {10.1145/2671014},
abstract = {This article describes a postprocessing strategy for online, handwritten, isolated Tamil words. Contributions have been made with regard to two issues hardly addressed in the online Indic word recognition literature, namely, use of (1) language models exploiting the idiosyncrasies of Indic scripts and (2) expert classifiers for the disambiguation of confused symbols.The input word is first segmented into its individual symbols, which are recognized using a primary support vector machine (SVM) classifier. Thereafter, we enhance the recognition accuracy by utilizing (i) a bigram language model at the symbol or character level and (ii) expert classifiers for reevaluating and disambiguating the different sets of confused symbols. The symbol-level bigram model is used in a traditional Viterbi framework. The concept of a character comprising multiple symbols is unique to Dravidian languages such as Tamil. This multi-symbol feature of Tamil characters has been exploited in proposing a novel, prefix-tree-based character-level bigram model that does not use Viterbi search; rather it reduces the search space for each input symbol based on its left context.For disambiguating confused symbols, a dynamic time-warping approach is proposed to automatically identify the parts of the online trace that discriminates between the confused classes. Fine classification of these regions by dedicated expert SVMs reduces the extent of confusions between such symbols. The integration of segmentation, prefix-tree-based language model and disambiguation of confused symbols is presented on a set of 15,000 handwritten isolated online Tamil words. Our results show recognition accuracies of 93.0% and 81.6% at the symbol and word level, respectively, as compared to the baseline classifier performance of 88.4% and 65.1%, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {8},
numpages = {28},
keywords = {Online Tamil words, expert classifiers, language models, support vector machines (SVM), reevaluation}
}

@article{10.1145/2665077,
author = {Awajan, Arafat},
title = {Keyword Extraction from Arabic Documents Using Term Equivalence Classes},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2665077},
doi = {10.1145/2665077},
abstract = {The rapid growth of the Internet and other computing facilities in recent years has resulted in the creation of a large amount of text in electronic form, which has increased the interest in and importance of different automatic text processing applications, including keyword extraction and term indexing. Although keywords are very useful for many applications, most documents available online are not provided with keywords. We describe a method for extracting keywords from Arabic documents. This method identifies the keywords by combining linguistics and statistical analysis of the text without using prior knowledge from its domain or information from any related corpus. The text is preprocessed to extract the main linguistic information, such as the roots and morphological patterns of derivative words. A cleaning phase is then applied to eliminate the meaningless words from the text. The most frequent terms are clustered into equivalence classes in which the derivative words generated from the same root and the non-derivative words generated from the same stem are placed together, and their count is accumulated. A vector space model is then used to capture the most frequent N-gram in the text. Experiments carried out using a real-world dataset show that the proposed method achieves good results with an average precision of 31% and average recall of 53% when tested against manually assigned keywords.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {7},
numpages = {18},
keywords = {Keyword extraction, term equivalence classes, Arabic natural language processing, text analysis}
}

@article{10.1145/2699927,
author = {Zhang, Jiajun and Liu, Shujie and Li, Mu and Zhou, Ming and Zong, Chengqing},
title = {Towards Machine Translation in Semantic Vector Space},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2699927},
doi = {10.1145/2699927},
abstract = {Measuring the quality of the translation rules and their composition is an essential issue in the conventional statistical machine translation (SMT) framework. To express the translation quality, the previous lexical and phrasal probabilities are calculated only according to the co-occurrence statistics in the bilingual corpus and may be not reliable due to the data sparseness problem. To address this issue, we propose measuring the quality of the translation rules and their composition in the semantic vector embedding space (VES). We present a recursive neural network (RNN)-based translation framework, which includes two submodels. One is the bilingually-constrained recursive auto-encoder, which is proposed to convert the lexical translation rules into compact real-valued vectors in the semantic VES. The other is a type-dependent recursive neural network, which is proposed to perform the decoding process by minimizing the semantic gap (meaning distance) between the source language string and its translation candidates at each state in a bottom-up structure. The RNN-based translation model is trained using a max-margin objective function that maximizes the margin between the reference translation and the n-best translations in forced decoding. In the experiments, we first show that the proposed vector representations for the translation rules are very reliable for application in translation modeling. We further show that the proposed type-dependent, RNN-based model can significantly improve the translation quality in the large-scale, end-to-end Chinese-to-English translation evaluation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {9},
numpages = {26},
keywords = {vector embedding space, max-margin training, statistical machine translation, semantic meaning distance, recursive neural network}
}

@article{10.1145/2658997,
author = {Uematsu, Sumire and Matsuzaki, Takuya and Hanaoka, Hiroki and Miyao, Yusuke and Mima, Hideki},
title = {Integrating Multiple Dependency Corpora for Inducing Wide-Coverage Japanese CCG Resources},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2658997},
doi = {10.1145/2658997},
abstract = {A novel method to induce wide-coverage Combinatory Categorial Grammar (CCG) resources for Japanese is proposed in this article. For some languages including English, the availability of large annotated corpora and the development of data-based induction of lexicalized grammar have enabled deep parsing, i.e., parsing based on lexicalized grammars. However, deep parsing for Japanese has not been widely studied. This is mainly because most Japanese syntactic resources are represented in chunk-based dependency structures, while previous methods for inducing grammars are dependent on tree corpora. To translate syntactic information presented in chunk-based dependencies to phrase structures as accurately as possible, integration of annotation from multiple dependency-based corpora is proposed. Our method first integrates dependency structures and predicate-argument information and converts them into phrase structure trees. The trees are then transformed into CCG derivations in a similar way to previously proposed methods. The quality of the conversion is empirically evaluated in terms of the coverage of the obtained CCG lexicon and the accuracy of the parsing with the grammar. While the transforming process used in this study is specialized for Japanese, the framework of our method would be applicable to other languages for which dependency-based analysis has been regarded as more appropriate than phrase structure-based analysis due to morphosyntactic features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {1},
numpages = {24},
keywords = {Combinatory Categorial Grammar, grammar development, dependency annotation, Japanese parsing}
}

@article{10.1145/2629574,
author = {Ramrakhiyani, Nitin and Majumder, Prasenjit},
title = {Approaches to Temporal Expression Recognition in Hindi},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2629574},
doi = {10.1145/2629574},
abstract = { Temporal annotation of plain text is considered a useful component of modern information retrieval tasks. In this work, different approaches for identification and classification of temporal expressions in Hindi are developed and analyzed. First, a rule-based approach is developed, which takes plain text as input and based on a set of hand-crafted rules, produces a tagged output with identified temporal expressions. This approach performs with a strict F1-measure of 0.83. In another approach, a CRF-based classifier is trained with human tagged data and is then tested on a test dataset. The trained classifier identifies the time expressions from plain text and further classifies them to various classes. This approach performs with a strict F1-measure of 0.78. Next, the CRF is replaced by an SVM-based classifier and the same experiment is performed with the same features. This approach is shown to be comparable to the CRF and performs with a strict F1-measure of 0.77. Using the rule base information as an additional feature enhances the performances to 0.86 and 0.84 for the CRF and SVM respectively. With three different comparable systems performing the extraction task, merging them to take advantage of their positives is the next step. As the first merge experiment, rule-based tagged data is fed to the CRF and SVM classifiers as additional training data. Evaluation results report an increase in F1-measure of the CRF from 0.78 to 0.8. Second, a voting-based approach is implemented, which chooses the best class for each token from the outputs of the three approaches. This approach results in the best performance for this task with a strict F1-measure of 0.88. In this process a reusable gold standard dataset for temporal tagging in Hindi is also developed. Named the ILTIMEX2012 corpus, it consists of 300 manually tagged Hindi news documents.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {2},
numpages = {22},
keywords = {temporal annotation, Time tagging, Indian language time tagging}
}

@article{10.1145/2641567,
author = {Ketui, Nongnuch and Theeramunkong, Thanaruk and Onsuwan, Chutamanee},
title = {An EDU-Based Approach for Thai Multi-Document Summarization and Its Application},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2641567},
doi = {10.1145/2641567},
abstract = {Due to lack of a word/phrase/sentence boundary, summarization of Thai multiple documents has several challenges in unit segmentation, unit selection, duplication elimination, and evaluation dataset construction. In this article, we introduce Thai Elementary Discourse Units (TEDUs) and their derivatives, called Combined TEDUs (CTEDUs), and then present our three-stage method of Thai multi-document summarization, that is, unit segmentation, unit-graph formulation, and unit selection and summary generation. To examine performance of our proposed method, a number of experiments are conducted using 50 sets of Thai news articles with their manually constructed reference summaries. Based on measures of ROUGE-1, ROUGE-2, and ROUGE-SU4, the experimental results show that: (1) the TEDU-based summarization outperforms paragraph-based summarization; (2) our proposed graph-based TEDU weighting with importance-based selection achieves the best performance; and (3) unit duplication consideration and weight recalculation help improve summary quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {4},
numpages = {26},
keywords = {unit selection, EDU-based approach, Thai text summarization, Multi-document summarization}
}

@article{10.1145/2693190.2693191,
author = {Kumari, B. Venkata Seshu and Rao, Ramisetty Rajeshwara},
title = {Improving Telugu Dependency Parsing Using Combinatory Categorial Grammar Supertags},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2693190.2693191},
doi = {10.1145/2693190.2693191},
abstract = {We show that Combinatory Categorial Grammar (CCG) supertags can improve Telugu dependency parsing. In this process, we first extract a CCG lexicon from the dependency treebank. Using both the CCG lexicon and the dependency treebank, we create a CCG treebank using a chart parser. Exploring different morphological features of Telugu, we develop a supertagger using maximum entropy models. We provide CCG supertags as features to the Telugu dependency parser (MST parser). We get an improvement of 1.8% in the unlabelled attachment score and 2.2% in the labelled attachment score. Our results show that CCG supertags improve the MST parser, especially on verbal arguments for which it has weak rates of recovery.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {3},
numpages = {10},
keywords = {MST parser, Dependency parsing, combinatory categorial grammar, Telugu, Indian languages}
}

@article{10.1145/2710043,
author = {Sproat, Richard},
title = {TALLIP Perspectives: Editorial Commentary: The Broadened Focus of the Journal},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/2710043},
doi = {10.1145/2710043},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {5},
numpages = {1}
}

@article{10.1145/2629545,
author = {Bang, Jeesoo and Lee, Jonghoon and Lee, Gary Geunbae and Chung, Minhwa},
title = {Pronunciation Variants Prediction Method to Detect Mispronunciations by Korean Learners of English},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2629545},
doi = {10.1145/2629545},
abstract = {This article presents an approach to nonnative pronunciation variants modeling and prediction. The pronunciation variants prediction method was developed by generalized transformation-based error-driven learning (GTBL). The modified goodness of pronunciation (GOP) score was applied to effective mispronunciation detection using logistic regression machine learning under the pronunciation variants prediction. English-read speech data uttered by Korean-speaking learners of English were collected, then pronunciation variation knowledge was extracted from the differences between the canonical phonemes and the actual phonemes of the speech data. With this knowledge, an error-driven learning approach was designed that automatically learns phoneme variation rules from phoneme-level transcriptions. The learned rules generate an extended recognition network to detect mispronunciations. Three different mispronunciation detection methods were tested including our logistic regression machine learning method with modified GOP scores and mispronunciation preference features; all three methods yielded significant improvement in predictions of pronunciation variants, and our logistic regression method showed the best performance.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {16},
numpages = {21},
keywords = {mispronunciation detection, Pronunciation variants prediction}
}

@article{10.1145/2637478,
author = {Liu, Lemao and Zhao, Tiejun and Watanabe, Taro and Cao, Hailong and Zhu, Conghui},
title = {Discriminative Training for Log-Linear Based SMT: Global or Local Methods},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2637478},
doi = {10.1145/2637478},
abstract = {In statistical machine translation, the standard methods such as MERT tune a single weight with regard to a given development data. However, these methods suffer from two problems due to the diversity and uneven distribution of source sentences. First, their performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. Second, the sentence level translation quality is not assured since tuning is performed on the document level rather than on sentence level. In contrast with the standard global training in which a single weight is learned, we propose novel local training methods to address these two problems. We perform training and testing in one step by locally learning the sentence-wise weight for each input sentence. Since the time of each tuning step is unnegligible and learning sentence-wise weights for the entire test set means many passes of tuning, it is a great challenge for the efficiency of local training. We propose an efficient two-phase method to put the local training into practice by employing the ultraconservative update. On NIST Chinese-to-English translation tasks with both medium and large scales of training data, our local training methods significantly outperform standard methods with the maximal improvements up to 2.0 BLEU points, meanwhile their efficiency is comparable to that of the standard methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {17},
numpages = {25},
keywords = {local training, log-linear model, ultraconservative update, global training}
}

@article{10.1145/2629575,
author = {Zhuang, Yi and Li, Qing and Chiu, Dickson K. W. and Wu, Zhiang and Hu, Haiyang},
title = {Efficient Personalized Probabilistic Retrieval of Chinese Calligraphic Manuscript Images in Mobile Cloud Environment},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2629575},
doi = {10.1145/2629575},
abstract = {Ancient language manuscripts constitute a key part of the cultural heritage of mankind. As one of the most important languages, Chinese historical calligraphy work has contributed to not only the Chinese cultural heritage but also the world civilization at large, especially for Asia. To support deeper and more convenient appreciation of Chinese calligraphy works, based on our previous work on the probabilistic retrieval of historical Chinese calligraphic character manuscripts repositories, we propose a system framework of the multi-feature-based Chinese calligraphic character images probabilistic retrieval in the mobile cloud network environment, which is called the DPRC. To ensure retrieval efficiency, we further propose four enabling techniques: (1) DRL-based probability propagation, (2) optimal data placement scheme, (3) adaptive data robust transmission algorithm, and (4) index support filtering scheme. Comprehensive experiments are conducted to testify the effectiveness and efficiency of our proposed DPRC method.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {18},
numpages = {38},
keywords = {probabilistic retrieval, Chinese calligraphic character, sentiment}
}

@article{10.1145/2656620,
author = {Sproat, Richard},
title = {The State of the Journal},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2656620},
doi = {10.1145/2656620},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {15},
numpages = {2}
}

@article{10.1145/2611521,
author = {Paik, Jiaul H. and Pal, Dipasree and Parui, Swapan K.},
title = {Incremental Blind Feedback: An Effective Approach to Automatic Query Expansion},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2611521},
doi = {10.1145/2611521},
abstract = {Automatic query expansion (AQE) is a useful technique for enhancing the effectiveness of information retrieval systems. In this article, we propose a novel AQE algorithm which first adopts a systematic incremental approach to choose feedback documents from the top retrieved set and then selects the expansion terms aggregating the scores from each feedback set. We also devise a term selection measure and a number of weighting schemes based on easily computable features. A set of experiments with a large number of standard test collections reveals that the proposed incremental blind feedback algorithm outperforms a number of state-of-the-art query expansion methods with remarkable significance and consistency.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {13},
numpages = {22},
keywords = {search, Query expansion, document ranking, pseudo-relevance feedback, query refinement}
}

@article{10.1145/2629670,
author = {Saharia, Navanath and Sharma, Utpal and Kalita, Jugal},
title = {Stemming Resource-Poor Indian Languages},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2629670},
doi = {10.1145/2629670},
abstract = {Stemming is a basic method for morphological normalization of natural language texts. In this study, we focus on the problem of stemming several resource-poor languages from Eastern India, viz., Assamese, Bengali, Bishnupriya Manipuri and Bodo. While Assamese, Bengali and Bishnupriya Manipuri are Indo-Aryan, Bodo is a Tibeto-Burman language. We design a rule-based approach to remove suffixes from words. To reduce over-stemming and under-stemming errors, we introduce a dictionary of frequent words. We observe that, for these languages a dominant amount of suffixes are single letters creating problems during suffix stripping. As a result, we introduce an HMM-based hybrid approach to classify the mis-matched last character. For each word, the stem is extracted by calculating the most probable path in four HMM states. At each step we measure the stemming accuracy for each language. We obtain 94% accuracy for Assamese and Bengali and 87%, and 82% for Bishnupriya Manipuri and Bodo, respectively, using the hybrid approach. We compare our work with Morfessor [Creutz and Lagus 2005]. As of now, there is no reported work on stemming for Bishnupriya Manipuri and Bodo. Our results on Assamese and Bengali show significant improvement over prior published work [Sarkar and Bandyopadhyay 2008; Sharma et al. 2002, 2003].},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {14},
numpages = {26},
keywords = {Bodo, suffix stripping, Bishnupriya Manipuri, Markov model, Stemming, Bengali, resource-poor languages, Assamese}
}

@article{10.1145/2629622,
author = {A., Bharath and Madhvanath, Sriganesh},
title = {Allograph Modeling for Online Handwritten Characters in Devanagari Using Constrained Stroke Clustering},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2629622},
doi = {10.1145/2629622},
abstract = {Writer-specific character writing variations such as those of stroke order and stroke number are an important source of variability in the input when handwriting is captured “online” via a stylus and a challenge for robust online recognition of handwritten characters and words. It has been shown by several studies that explicit modeling of character allographs is important for achieving high recognition accuracies in a writer-independent recognition system. While previous approaches have relied on unsupervised clustering at the character or stroke level to find the allographs of a character, in this article we propose the use of constrained clustering using automatically derived domain constraints to find a minimal set of stroke clusters. The allographs identified have been applied to Devanagari character recognition using Hidden Markov Models and Nearest Neighbor classifiers, and the results indicate substantial improvement in recognition accuracy and/or reduction in memory and computation time when compared to alternate modeling techniques.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {12},
numpages = {21},
keywords = {online handwriting recognition, constrained stroke clustering, allograph modeling, Devanagari character recognition}
}

@article{10.1145/2644810,
author = {Na, Hwidong and Lee, Jong-Hyeok},
title = {Linguistic Analysis of Non-ITG Word Reordering between Language Pairs with Different Word Order Typologies},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2644810},
doi = {10.1145/2644810},
abstract = {The Inversion Transduction Grammar (ITG) constraints have been widely used for word reordering in machine translation studies. They are, however, so restricted that some types of word reordering cannot be handled properly. We analyze three corpora between SVO and SOV languages: Chinese-Korean, English-Japanese, and English-Korean. In our analysis, sentences that require non-ITG word reordering are manually categorized. We also report the results for two quantitative measures that reveal the significance of non-ITG word reordering. In conclusion, we suggest that ITG constraints are insufficient to deal with word reordering in real situations.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {11},
numpages = {12},
keywords = {corpus analysis, inversion transduction grammar, Machine translation}
}

@article{10.1145/2556948,
author = {Esmaili, Kyumars Sheykh and Salavati, Shahin and Datta, Anwitaman},
title = {Towards Kurdish Information Retrieval},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2556948},
doi = {10.1145/2556948},
abstract = {The Kurdish language is an Indo-European language spoken in Kurdistan, a large geographical region in the Middle East. Despite having a large number of speakers, Kurdish is among the less-resourced languages and has not seen much attention from the IR and NLP research communities. This article reports on the outcomes of a project aimed at providing essential resources for processing Kurdish texts.A principal output of this project is Pewan, the first standard Test Collection to evaluate Kurdish Information Retrieval systems. The other language resources that we have built include a lightweight stemmer and a list of stopwords.Our second principal contribution is using these newly-built resources to conduct a thorough experimental study on Kurdish documents. Our experimental results show that normalization, and to a lesser extent, stemming, can greatly improve the performance of Kurdish IR systems.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {7},
numpages = {18},
keywords = {Sorani Kurdish, cross-lingual information retrieval, test collection, stemming, Kurmanji Kurdish, Kurdish language}
}

@article{10.1145/2605292,
author = {Rubin, Victoria L.},
title = {TALIP Perspectives, Guest Editorial Commentary: Pragmatic and Cultural Considerations for Deception Detection in Asian Languages},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2605292},
doi = {10.1145/2605292},
abstract = {In hopes of sparking a discussion, I argue for much needed research on automated deception detection in Asian languages. The task of discerning truthful texts from deceptive ones is challenging, but a logical sequel to opinion mining. I suggest that applied computational linguists pursue broader interdisciplinary research on cultural differences and pragmatic use of language in Asian cultures, before turning to detection methods based on a primarily Western (English-centric) worldview. Deception is fundamentally human, but how do various cultures interpret and judge deceptive behavior?},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {10},
numpages = {8}
}

@article{10.1145/2601401,
author = {Keskes, Iskandar and Zitoune, Farah Benamara and Belguith, Lamia Hadrich},
title = {Splitting Arabic Texts into Elementary Discourse Units},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2601401},
doi = {10.1145/2601401},
abstract = {In this article, we propose the first work that investigates the feasibility of Arabic discourse segmentation into elementary discourse units within the segmented discourse representation theory framework. We first describe our annotation scheme that defines a set of principles to guide the segmentation process. Two corpora have been annotated according to this scheme: elementary school textbooks and newspaper documents extracted from the syntactically annotated Arabic Treebank. Then, we propose a multiclass supervised learning approach that predicts nested units. Our approach uses a combination of punctuation, morphological, lexical, and shallow syntactic features. We investigate how each feature contributes to the learning process. We show that an extensive morphological analysis is crucial to achieve good results in both corpora. In addition, we show that adding chunks does not boost the performance of our system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {9},
numpages = {23},
keywords = {Arabic language, elementary discourse units, Discourse segmentation}
}

@article{10.1145/2540988,
author = {Sulaiman, Suliana and Omar, Khairuddin and Omar, Nazlia and Murah, Mohd Zamri and Rahman, Hamdan Abdul},
title = {The Effectiveness of a Jawi Stemmer for Retrieving Relevant Malay Documents in Jawi Characters},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2540988},
doi = {10.1145/2540988},
abstract = {The Malay language has two types of writing script, known as Rumi and Jawi. Most previous stemmer results have reported on Malay Rumi characters and only a few have tested Jawi characters. In this article, a new Jawi stemmer has been proposed and tested for document retrieval. A total of 36 queries and datasets from the transliterated Jawi Quran were used. The experiment shows that the mean average precision for a “stemmed Jawi” document is 8.43%. At the same time, the mean average precision for a “nonstemmed Jawi” document is 5.14%. The result from a paired sample t-test showed that the use of a “stemmed Jawi” document increased the precision in document retrieval. Further experiments were performed to examine the precision of the relevant documents that were retrieved at various cutoff points for all 36 queries. The results for the “stemmed Jawi” document showed a significantly different start, at a cutoff of 40, compared with the “nonstemmed Jawi” documents. This result shows the usefulness of a Jawi stemmer for retrieving relevant documents in the Jawi script.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {6},
numpages = {21},
keywords = {stemming, Jawi document retrieval, Malay stemmer, Jawi stemmer}
}

@article{10.1145/2617590,
author = {Sharma, Manoj Kumar and Samanta, Debasis},
title = {Word Prediction System for Text Entry in Hindi},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2617590},
doi = {10.1145/2617590},
abstract = {Word prediction is treated as an efficient technique to enhance text entry rate. Existing word prediction systems predict a word when a user correctly enters the initial few characters of the word. In fact, a word prediction system fails if the user makes errors in the initial input. Therefore, there is a need to develop a word prediction system that predicts desired words while coping with errors in initial entries. This requirement is more relevant in the case of text entry in Indian languages, which are involved with a large set of alphabets, words with complex characters and inflections, phonetically similar sets of characters, etc. In fact, text composition in Indian languages involves frequent spelling errors, which presents a challenge to develop an efficient word prediction system. In this article, we address this problem and propose a novel word prediction system. Our proposed approach has been tried with Hindi, the national language of India. Experiments with users substantiate 43.77% keystroke savings, 92.49% hit rate, and 95.82% of prediction utilization with the proposed word prediction system. Our system also reduces the spelling error by 89.75%.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {8},
numpages = {29},
keywords = {text entry system, Word prediction, Hindi text entry, virtual keyboard, text entry rate enhancement}
}

@article{10.1145/2537128,
author = {Goto, Isao and Utiyama, Masao and Sumita, Eiichiro and Tamura, Akihiro and Kurohashi, Sadao},
title = {Distortion Model Based on Word Sequence Labeling for Statistical Machine Translation},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2537128},
doi = {10.1145/2537128},
abstract = {This article proposes a new distortion model for phrase-based statistical machine translation. In decoding, a distortion model estimates the source word position to be translated next (subsequent position; SP) given the last translated source word position (current position; CP). We propose a distortion model that can simultaneously consider the word at the CP, the word at an SP candidate, the context of the CP and an SP candidate, relative word order among the SP candidates, and the words between the CP and an SP candidate. These considered elements are called rich context. Our model considers rich context by discriminating label sequences that specify spans from the CP to each SP candidate. It enables our model to learn the effect of relative word order among SP candidates as well as to learn the effect of distances from the training data. In contrast to the learning strategy of existing methods, our learning strategy is that the model learns preference relations among SP candidates in each sentence of the training data. This leaning strategy enables consideration of all of the rich context simultaneously. In our experiments, our model had higher BLUE and RIBES scores for Japanese-English, Chinese-English, and German-English translation compared to the lexical reordering models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {2},
numpages = {21},
keywords = {machine translation, Distortion model, reordering}
}

@article{10.1145/2559789,
author = {Church, Kenneth},
title = {TALIP Perspectives, Guest Editorial Commentary: What Counts (and What Ought to Count)?},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2559789},
doi = {10.1145/2559789},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {5},
numpages = {5}
}

@article{10.1145/2540989,
author = {Str\"{o}tgen, Jannik and Armiti, Ayser and Van Canh, Tran and Zell, Julian and Gertz, Michael},
title = {Time for More Languages: Temporal Tagging of Arabic, Italian, Spanish, and Vietnamese},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2540989},
doi = {10.1145/2540989},
abstract = {Most of the research on temporal tagging so far is done for processing English text documents. There are hardly any multilingual temporal taggers supporting more than two languages. Recently, the temporal tagger HeidelTime has been made publicly available, supporting the integration of new languages by developing language-dependent resources without modifying the source code.In this article, we describe our work on developing such resources for two Asian and two Romance languages: Arabic, Vietnamese, Spanish, and Italian. While temporal tagging of the two Romance languages has been addressed before, there has been almost no research on Arabic and Vietnamese temporal tagging so far. Furthermore, we analyze language-dependent challenges for temporal tagging and explain the strategies we followed to address them. Our evaluation results on publicly available and newly annotated corpora demonstrate the high quality of our new resources for the four languages, which we make publicly available to the research community.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {1},
numpages = {21},
keywords = {Arabic NLP, TIMEX3, HeidelTime, Temporal tagging, Vietnamese NLP}
}

@article{10.1145/2529994,
author = {Kim, Seokhwan and Jeong, Minwoo and Lee, Jonghoon and Lee, Gary Geunbae},
title = {Cross-Lingual Annotation Projection for Weakly-Supervised Relation Extraction},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2529994},
doi = {10.1145/2529994},
abstract = {Although researchers have conducted extensive studies on relation extraction in the last decade, statistical systems based on supervised learning are still limited, because they require large amounts of training data to achieve high performance level. In this article, we propose cross-lingual annotation projection methods that leverage parallel corpora to build a relation extraction system for a resource-poor language without significant annotation efforts. To make our method more reliable, we introduce two types of projection approaches with noise reduction strategies. We demonstrate the merit of our method using a Korean relation extraction system trained on projected examples from an English-Korean parallel corpus. Experiments show the feasibility of our approaches through comparison to other systems based on monolingual resources.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {3},
numpages = {26},
keywords = {cross-lingual annotation projection, weakly-supervised learning, Relation extraction}
}

@article{10.1145/2537129,
author = {Yahya, Adnan and Salhi, Ali},
title = {Arabic Text Categorization Based on Arabic Wikipedia},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2537129},
doi = {10.1145/2537129},
abstract = {This article describes an algorithm for categorizing Arabic text, relying on highly categorized corpus-based datasets obtained from the Arabic Wikipedia by using manual and automated processes to build and customize categories. The categorization algorithm was built by adopting a simple categorization idea then moving forward to more complex ones. We applied tests and filtration criteria to reach the best and most efficient results that our algorithm can achieve. The categorization depends on the statistical relations between the input (test) text and the reference (training) data supported by well-defined Wikipedia-based categories. Our algorithm supports two levels for categorizing Arabic text; categories are grouped into a hierarchy of main categories and subcategories. This introduces a challenge due to the correlation between certain subcategories and overlap between main categories. We argue that our algorithm achieved good performance compared to other methods reported in the literature.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {4},
numpages = {20},
keywords = {Arabic natural language processing, light stemming, text categorization, categorized corpora, Arabic Wikipedia, text analysis}
}

@article{10.1145/2523057.2523058,
author = {Sproat, Richard},
title = {TALIP Perspectives},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2523057.2523058},
doi = {10.1145/2523057.2523058},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {13},
numpages = {2}
}

@article{10.1145/2523057.2523059,
author = {Chu, Chenhui and Nakazawa, Toshiaki and Kawahara, Daisuke and Kurohashi, Sadao},
title = {Chinese-Japanese Machine Translation Exploiting Chinese Characters},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2523057.2523059},
doi = {10.1145/2523057.2523059},
abstract = {The Chinese and Japanese languages share Chinese characters. Since the Chinese characters in Japanese originated from ancient China, many common Chinese characters exist between these two languages. Since Chinese characters contain significant semantic information and common Chinese characters share the same meaning in the two languages, they can be quite useful in Chinese-Japanese machine translation (MT). We therefore propose a method for creating a Chinese character mapping table for Japanese, traditional Chinese, and simplified Chinese, with the aim of constructing a complete resource of common Chinese characters. Furthermore, we point out two main problems in Chinese word segmentation for Chinese-Japanese MT, namely, unknown words and word segmentation granularity, and propose an approach exploiting common Chinese characters to solve these problems. We also propose a statistical method for detecting other semantically equivalent Chinese characters other than the common ones and a method for exploiting shared Chinese characters in phrase alignment. Results of the experiments carried out on a state-of-the-art phrase-based statistical MT system and an example-based MT system show that our proposed approaches can improve MT performance significantly, thereby verifying the effectiveness of shared Chinese characters for Chinese-Japanese MT.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {16},
numpages = {25},
keywords = {phrase alignment, segmentation, Chinese characters, Chinese-Japanese, machine translation}
}

@article{10.1145/2505126,
author = {Paul, Michael and Finch, Andrew and Sumita, Eiichrio},
title = {How to Choose the Best Pivot Language for Automatic Translation of Low-Resource Languages},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2505126},
doi = {10.1145/2505126},
abstract = {Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome language resource limitations for certain language pairs. Due to the richness of available language resources, English is, in general, the pivot language of choice. However, factors like language relatedness can also effect the choice of the pivot language for a given language pair, especially for Asian languages, where language resources are currently quite limited. In this article, we provide new insights into what factors make a pivot language effective and investigate the impact of these factors on the overall pivot translation performance for translation between 22 Indo-European and Asian languages. Experimental results using state-of-the-art statistical machine translation techniques revealed that the translation quality of 54.8% of the language pairs improved when a non-English pivot language was chosen. Moreover, 81.0% of system performance variations can be explained by a combination of factors such as language family, vocabulary, sentence length, language perplexity, translation model entropy, reordering, monotonicity, and engine performance.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {14},
numpages = {17},
keywords = {pivot language selection, translation quality indicators, Asian languages, Machine translation}
}

@article{10.1145/2518100,
author = {Goto, Isao and Utiyama, Masao and Sumita, Eiichiro},
title = {Post-Ordering by Parsing with ITG for Japanese-English Statistical Machine Translation},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2518100},
doi = {10.1145/2518100},
abstract = {Word reordering is a difficult task for translation between languages with widely different word orders, such as Japanese and English. A previously proposed post-ordering method for Japanese-to-English translation first translates a Japanese sentence into a sequence of English words in a word order similar to that of Japanese, then reorders the sequence into an English word order. We employed this post-ordering framework and improved upon its reordering method. The existing post-ordering method reorders the sequence of English words via SMT, whereas our method reorders the sequence by (1) parsing the sequence using ITG to obtain syntactic structures which are similar to Japanese syntactic structures, and (2) transferring the obtained syntactic structures into English syntactic structures according to the ITG. The experiments using Japanese-to-English patent translation demonstrated the effectiveness of our method and showed that both the RIBES and BLEU scores were improved over compared methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {17},
numpages = {22},
keywords = {parsing, inversion transduction grammar, Machine translation, post-ordering}
}

@article{10.1145/2505984,
author = {Huang, Chung-Chi and Chen, Mei-Hua and Yang, Ping-Che and Chang, Jason S.},
title = {A Computer-Assisted Translation and Writing System},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2505984},
doi = {10.1145/2505984},
abstract = {We introduce a method for learning to predict text and grammatical construction in a computer-assisted translation and writing framework. In our approach, predictions are offered on the fly to help the user make appropriate lexical and grammar choices during the translation of a source text, thus improving translation quality and productivity. The method involves automatically generating general-to-specific word usage summaries (i.e., writing suggestion module), and automatically learning high-confidence word- or phrase-level translation equivalents (i.e., translation suggestion module). At runtime, the source text and its translation prefix entered by the user are broken down into n-grams to generate grammar and translation predictions, which are further combined and ranked via translation and language models. These ranked prediction candidates are iteratively and interactively displayed to the user in a pop-up menu as translation or writing hints. We present a prototype writing assistant, TransAhead, that applies the method to a human-computer collaborative environment. Automatic and human evaluations show that novice translators or language learners substantially benefit from our system in terms of translation performance (i.e., translation accuracy and productivity) and language learning (i.e., collocation usage and grammar). In general, our methodology of inline grammar and text predictions or suggestions has great potential in the field of computer-assisted translation, writing, or even language learning.},
journal = {ACM Transactions on Asian Language Information Processing},
month = oct,
articleno = {15},
numpages = {20},
keywords = {translation model and language model, grammar and text prediction, grammar pattern, Computer-assisted translation, word usage, computer-assisted language learning}
}

@article{10.1145/2499955.2499958,
author = {Iwakura, Tomoya and Takamura, Hiroya and Okumura, Manabu},
title = {A Named Entity Recognition Method Based on Decomposition and Concatenation of Word Chunks},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2499955.2499958},
doi = {10.1145/2499955.2499958},
abstract = {We propose a named entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. Our method identifies word chunks with a base chunker, such as a noun phrase chunker, and then recognizes NEs from the recognized word chunk sequences. By using word chunks, we can obtain features that cannot be obtained in word-sequence-based recognition methods, such as the first word of a word chunk, the last word of a word chunk, and so on. However, each word chunk may include a part of an NE or multiple NEs. To solve this problem, we use the following operators: SHIFT for separating the first word from a word chunk, POP for separating the last word from a word chunk, JOIN for concatenating two word chunks, and REDUCE for assigning an NE label to a word chunk. We evaluate our method on a Japanese NE recognition dataset that includes about 200,000 annotations of 191 types of NEs from over 8,500 news articles. The experimental results show that the training and processing speeds of our method are faster than those of a linear-chain structured perceptron and a semi-Markov perceptron, while maintaining high accuracy.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {10},
numpages = {18},
keywords = {Extended named entity recognition}
}

@article{10.1145/2499955.2499957,
author = {Fukunishi, Takaaki and Finch, Andrew and Yamamoto, Seiichi and Sumita, Eiichiro},
title = {A Bayesian Alignment Approach to Transliteration Mining},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2499955.2499957},
doi = {10.1145/2499955.2499957},
abstract = {In this article we present a technique for mining transliteration pairs using a set of simple features derived from a many-to-many bilingual forced-alignment at the grapheme level to classify candidate transliteration word pairs as correct transliterations or not. We use a nonparametric Bayesian method for the alignment process, as this process rewards the reuse of parameters, resulting in compact models that align in a consistent manner and tend not to over-fit. Our approach uses the generative model resulting from aligning the training data to force-align the test data. We rely on the simple assumption that correct transliteration pairs would be well modeled and generated easily, whereas incorrect pairs---being more random in character---would be more costly to model and generate. Our generative model generates by concatenating bilingual grapheme sequence pairs. The many-to-many generation process is essential for handling many languages with non-Roman scripts, and it is hard to train well using a maximum likelihood techniques, as these tend to over-fit the data. Our approach works on the principle that generation using only grapheme sequence pairs that are in the model results in a high probability derivation, whereas if the model is forced to introduce a new parameter in order to explain part of the candidate pair, the derivation probability is substantially reduced and severely reduced if the new parameter corresponds to a sequence pair composed of a large number of graphemes. The features we extract from the alignment of the test data are not only based on the scores from the generative model, but also on the relative proportions of each sequence that are hard to generate. The features are used in conjunction with a support vector machine classifier trained on known positive examples together with synthetic negative examples to determine whether a candidate word pair is a correct transliteration pair. In our experiments, we used all data tracks from the 2010 Named-Entity Workshop (NEWS’10) and use the performance of the best system for each language pair as a reference point. Our results show that the new features we propose are powerfully predictive, enabling our approach to achieve levels of performance on this task that are comparable to the state of the art.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {9},
numpages = {22},
keywords = {transliteration mining, Dirichlet process model, Gibbs sampling, katakana, Transliteration, Bayesian alignment}
}

@article{10.1145/2499955.2499960,
author = {Sudoh, Katsuhito and Wu, Xianchao and Duh, Kevin and Tsukada, Hajime and Nagata, Masaaki},
title = {Syntax-Based Post-Ordering for Efficient Japanese-to-English Translation},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2499955.2499960},
doi = {10.1145/2499955.2499960},
abstract = {This article proposes a novel reordering method for efficient two-step Japanese-to-English statistical machine translation (SMT) that isolates reordering from SMT and solves it after lexical translation. This reordering problem, called post-ordering, is solved as an SMT problem from Head-Final English (HFE) to English. HFE is syntax-based reordered English that is very successfully used for reordering with English-to-Japanese SMT. The proposed method incorporates its advantage into the reverse direction, Japanese-to-English, and solves the post-ordering problem by accurate syntax-based SMT with target language syntax. Two-step SMT with the proposed post-ordering empirically reduces the decoding time of the accurate but slow syntax-based SMT by its good approximation using intermediate HFE. The proposed method improves the decoding speed of syntax-based SMT decoding by about six times with comparable translation accuracy in Japanese-to-English patent translation experiments.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {12},
numpages = {15},
keywords = {statistical machine translation, post-ordering, long-distance reordering, Japanese-to-English translation}
}

@article{10.1145/2499955.2499956,
author = {Sproat, Richard},
title = {EDITORIAL Greetings from the New Editor-in-Chief},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2499955.2499956},
doi = {10.1145/2499955.2499956},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {8},
numpages = {1}
}

@article{10.1145/2499955.2499959,
author = {Izumi, Tomoko and Imamura, Kenji and Asami, Taichi and Saito, Kuniko and Kikui, Genichiro and Sato, Satoshi},
title = {Normalizing Complex Functional Expressions in Japanese Predicates: Linguistically-Directed Rule-Based Paraphrasing and Its Application},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2499955.2499959},
doi = {10.1145/2499955.2499959},
abstract = {The growing need for text mining systems, such as opinion mining, requires a deep semantic understanding of the target language. In order to accomplish this, extracting the semantic information of functional expressions plays a crucial role, because functional expressions such as would like to and can’t are key expressions to detecting customers’ needs and wants. However, in Japanese, functional expressions appear in the form of suffixes, and two different types of functional expressions are merged into one predicate: one influences the factual meaning of the predicate while the other is merely used for discourse purposes. This triggers an increase in surface forms, which hinders information extraction systems. In this article, we present a novel normalization technique that paraphrases complex functional expressions into simplified forms that retain only the crucial meaning of the predicate. We construct paraphrasing rules based on linguistic theories in syntax and semantics. The results of experiments indicate that our system achieves a high accuracy of 79.7%, while it reduces the differences in functional expressions by up to 66.7%. The results also show an improvement in the performance of predicate extraction, providing encouraging evidence of the usability of paraphrasing as a means of normalizing different language expressions.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {11},
numpages = {20},
keywords = {sentiment analysis, Paraphrasing, text mining, functional expressions, factuality analysis, linguistic theories, opinion mining}
}

@article{10.1145/2461316.2461319,
author = {Fujita, Sanae and Fujino, Akinori},
title = {Word Sense Disambiguation by Combining Labeled Data Expansion and Semi-Supervised Learning Method},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2461316.2461319},
doi = {10.1145/2461316.2461319},
abstract = {Lack of labeled data is one of the severest problems facing word sense disambiguation (WSD). We overcome the problem by proposing a method that combines automatic labeled data expansion (Step 1) and semi-supervised learning (Step 2). The Step 1 and 2 methods are both effective, but their combination yields a synergistic effect.In this article, in Step 1, we automatically extract reliable labeled data from raw corpora using dictionary example sentences, even the infrequent and unseen senses (which are not likely to appear in labeled data). Next, in Step 2, we apply a semi-supervised classifier and achieve an improvement using easy-to-get unlabeled data. In this step, we also show that we can guess even unseen senses.We target a SemEval-2010 Japanese WSD task, which is a lexical sample task. Both Step 1 and Step 2 methods performed better than the best published result (76.4 %). Furthermore, the combined method achieved much higher accuracy (84.2 %). In this experiment, up to 50 % of unseen senses are classified correctly. However, the number of unseen senses are small, therefore, we delete one senses per word and apply our proposed method; the results show that the method is effective and robust even for unseen senses.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {7},
numpages = {26},
keywords = {Unseen senses, example sentences, hybrid generative/discriminative approaches, lexicon}
}

@article{10.1145/2461316.2461318,
author = {Hinkle, Lauren and Brouillette, Albert and Jayakar, Sujay and Gathings, Leigh and Lezcano, Miguel and Kalita, Jugal},
title = {Design and Evaluation of Soft Keyboards for Brahmic Scripts},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2461316.2461318},
doi = {10.1145/2461316.2461318},
abstract = {Despite being spoken by a large percentage of the world, Indic languages in general lack user-friendly and efficient methods for text input. These languages have poor or no support for typing. Soft keyboards, because of their ease of installation and lack of reliance on specific hardware, are a promising solution as an input device for many languages. Developing an acceptable soft keyboard requires the frequency analysis of characters in order to design a layout that minimizes text-input time. This article proposes the use of various development techniques, layout variations, and evaluation methods for the creation of soft keyboards for Brahmic scripts. We propose that using optimization techniques such as genetic algorithms and multi-objective Pareto optimization to develop multi-layer keyboards will increase the speed at which text can be entered.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {6},
numpages = {37},
keywords = {Assamese, mobile devices, Pareto optimization, Indic languages, soft keyboards, Genetic algorithms}
}

@article{10.1145/2461316.2461317,
author = {Sun, Xu and Okazaki, Naoaki and Tsujii, Jun’ichi and Wang, Houfeng},
title = {Learning Abbreviations from Chinese and English Terms by Modeling Non-Local Information},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2461316.2461317},
doi = {10.1145/2461316.2461317},
abstract = {The present article describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model and the label encoding with global information. Although the two approaches compete with one another, we find they are also highly complementary. We propose a combination of the two approaches, and we will show the proposed method outperforms all of the existing methods on abbreviation generation datasets. In order to reduce computational complexity of learning non-local information, we further present an online training method, which can arrive the objective optimum with accelerated training speed. We used a Chinese newswire dataset and a English biomedical dataset for experiments. Experiments revealed that the proposed abbreviation generator with non-local information achieved the best results for both the Chinese and English languages.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {5},
numpages = {17},
keywords = {Abbreviation processing, stochastic learning, non-local information, machine learning}
}

@article{10.1145/2425327.2425329,
author = {Jiang, Mike Tian-Jian and Lee, Tsung-Hsien and Hsu, Wen-Lian},
title = {The Left and Right Context of a Word: Overlapping Chinese Syllable Word Segmentation with Minimal Context},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2425327.2425329},
doi = {10.1145/2425327.2425329},
abstract = {Since a Chinese syllable can correspond to many characters (homophones), the syllable-to-character conversion task is quite challenging for Chinese phonetic input methods (CPIM). There are usually two stages in a CPIM: 1. segment the syllable sequence into syllable words, and 2. select the most likely character words for each syllable word. A CPIM usually assumes that the input is a complete sentence, and evaluates the performance based on a well-formed corpus. However, in practice, most Pinyin users prefer progressive text entry in several short chunks, mainly in one or two words each (most Chinese words consist of two or more characters). Short chunks do not provide enough contexts to perform the best possible syllable-to-character conversion, especially when a chunk consists of overlapping syllable words. In such cases, a conversion system often selects the boundary of a word with the highest frequency. Short chunk input is even more popular on platforms with limited computing power, such as mobile phones. Based on the observation that the relative strength of a word can be quite different when calculated leftwards or rightwards, we propose a simple division of the word context into the left context and the right context. Furthermore, we design a double ranking strategy for each word to reduce the number of errors in Step 1. Our strategy is modeled as the minimum feedback arc set problem on bipartite tournament with approximate solutions derived from genetic algorithm. Experiments show that, compared to the frequency-based method (FBM) (low memory and fast) and the conditional random fields (CRF) model (larger memory and slower), our double ranking strategy has the benefits of less memory and low power requirement with competitive performance. We believe a similar strategy could also be adopted to disambiguate conflicting linguistic patterns effectively.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {2},
numpages = {23},
keywords = {syllable-to-word conversion, Chinese phonetic input methods}
}

@article{10.1145/2425327.2425331,
author = {Sundaram, Suresh and Ramakrishnan, A. G.},
title = {Attention-Feedback Based Robust Segmentation of Online Handwritten Isolated Tamil Words},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2425327.2425331},
doi = {10.1145/2425327.2425331},
abstract = {In this article, we propose a lexicon-free, script-dependent approach to segment online handwritten isolated Tamil words into its constituent symbols. Our proposed segmentation strategy comprises two modules, namely the (1) Dominant Overlap Criterion Segmentation (DOCS) module and (2) Attention Feedback Segmentation (AFS) module. Based on a bounding box overlap criterion in the DOCS module, the input word is first segmented into stroke groups. A stroke group may at times correspond to a part of a valid symbol (over-segmentation) or a merger of valid symbols (under-segmentation). Attention on specific features in the AFS module serve in detecting possibly over-segmented or under-segmented stroke groups. Thereafter, feedbacks from the SVM classifier likelihoods and stroke-group based features are considered in modifying the suspected stroke groups to form valid symbols.The proposed scheme is tested on a set of 10000 isolated handwritten words (containing 53,246 Tamil symbols). The results show that the DOCS module achieves a symbol-level segmentation accuracy of 98.1%, which improves to as high as 99.7% after the AFS strategy. This in turn entails a symbol recognition rate of 83.9% (at the DOCS module) and 88.4% (after the AFS module). The resulting word recognition rates at the DOCS and AFS modules are found to be, 50.9% and 64.9% respectively, without any postprocessing.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {4},
numpages = {25},
keywords = {online Tamil words, Tamil, Handwriting recognition, Dominant Overlap Criterion Segmentation (DOCS) module, Support Vector Machines (SVM), stroke group, Attention Feedback Segmentation (AFS) module}
}

@article{10.1145/2425327.2425328,
author = {Hao, Tianyong and Zhu, Chunshen},
title = {Toward a Professional Platform for Chinese Character Conversion},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2425327.2425328},
doi = {10.1145/2425327.2425328},
abstract = {Increasing communication among Chinese-speaking regions using respectively traditional and simplified Chinese character systems has highlighted the subtle-yet-extensive differences between the two systems, which can lead to unexpected hindrance in converting characters from one to the other. This article proposes a new priority-based multi-data resources management model, with a new algorithm called Fused Conversion algorithm from Multi-Data resources (FCMD), to ensure more context-sensitive, human controllable, and thus more reliable conversions, by drawing on reverse maximum matching, n-gram-based statistical model and pattern-based learning and matching. After parameter training on the Tagged Chinese Gigaword corpus, its conversion precision reaches 91.5% in context-sensitive cases, the most difficult part in the conversion, with an overall precision rate at 99.8%, a significant improvement over the state-of-the-art models. The conversion platform based on the model has extra features such as data resource selection and n-grams self-learning ability, providing a more sophisticated tool good especially for high-end professional uses.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {1},
numpages = {22},
keywords = {multi-data resources, FCMD algorithm, Chinese character conversion, reverse maximum matching, pattern learning, n-gram}
}

@article{10.1145/2425327.2425330,
author = {Bach, Ngo Xuan and Minh, Nguyen Le and Oanh, Tran Thi and Shimazu, Akira},
title = {A Two-Phase Framework for Learning Logical Structures of Paragraphs in Legal Articles},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2425327.2425330},
doi = {10.1145/2425327.2425330},
abstract = {Analyzing logical structures of texts is important to understanding natural language, especially in the legal domain, where legal texts have their own specific characteristics. Recognizing logical structures in legal texts does not only help people in understanding legal documents, but also in supporting other tasks in legal text processing.In this article, we present a new task, learning logical structures of paragraphs in legal articles, which is studied in research on Legal Engineering. The goals of this task are recognizing logical parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the first phase, we model the problem of recognizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recognize them. In the second phase, we propose a graph-based method to group logical parts into logical structures. We consider the problem of finding a subset of complete subgraphs in a weighted-edge complete graph, where each node corresponds to a logical part, and a complete subgraph corresponds to a logical structure. We also present an integer linear programming formulation for this optimization problem. Our models achieve 74.37% in recognizing logical parts, 80.08% in recognizing logical structures, and 58.36% in the whole task on the Japanese National Pension Law corpus. Our work provides promising results for further research on this interesting task.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {32},
keywords = {maximum entropy model, conditional random fields, Legal text processing, Japanese National Pension Law, integer linear programming, support vector machines, logical structures, graph-based methods, logical parts, sequence learning}
}

@article{10.1145/2382593.2382594,
author = {Mitamura, Teruko and Kando, Noriko and Takeda, Koichi},
title = {Introduction to the Special Issue on RITE},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382594},
doi = {10.1145/2382593.2382594},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {12},
numpages = {2}
}

@article{10.1145/2382593.2382595,
author = {Miyao, Yusuke and Shima, Hideki and Kanayama, Hiroshi and Mitamura, Teruko},
title = {Evaluating Textual Entailment Recognition for University Entrance Examinations},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382595},
doi = {10.1145/2382593.2382595},
abstract = {The present article addresses an attempt to apply questions in university entrance examinations to the evaluation of textual entailment recognition. Questions in several fields, such as history and politics, primarily test the examinee’s knowledge in the form of choosing true statements from multiple choices. Answering such questions can be regarded as equivalent to finding evidential texts from a textbase such as textbooks and Wikipedia. Therefore, this task can be recast as recognizing textual entailment between a description in a textbase and a statement given in a question. We focused on the National Center Test for University Admission in Japan and converted questions into the evaluation data for textual entailment recognition by using Wikipedia as a textbase. Consequently, it is revealed that nearly half of the questions can be mapped into textual entailment recognition; 941 text pairs were created from 404 questions from six subjects. This data set is provided for a subtask of NTCIR RITE (Recognizing Inference in Text), and 16 systems from six teams used the data set for evaluation. The evaluation results revealed that the best system achieved a correct answer ratio of 56%, which is significantly better than a random choice baseline.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {13},
numpages = {16},
keywords = {university entrance examination, Textual entailment recognition}
}

@article{10.1145/2382593.2382596,
author = {Pham, Minh Quang Nhat and Nguyen, Minh Le and Shimazu, Akira},
title = {Learning to Recognize Textual Entailment in Japanese Texts with the Utilization of Machine Translation},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382596},
doi = {10.1145/2382593.2382596},
abstract = {Recognizing Textual Entailment (RTE) is a fundamental task in Natural Language Understanding. The task is to decide whether the meaning of a text can be inferred from the meaning of another one. In this article, we conduct an empirical study of recognizing textual entailment in Japanese texts, in which we adopt a machine learning-based approach to the task. We quantitatively analyze the effects of various entailment features, machine learning algorithms, and the impact of RTE resources on the performance of an RTE system. This article also investigates the use of machine translation for the RTE task and determines whether machine translation can be used to improve the performance of our RTE system. Experimental results achieved on benchmark data sets show that our machine learning-based RTE system outperforms the baseline methods based on lexical matching and syntactic matching. The results also suggest that the machine translation component can be utilized to improve the performance of the RTE system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {14},
numpages = {23},
keywords = {machine learning, Japanese texts, machine translation, Textual entailment}
}

@article{10.1145/2382593.2382598,
author = {Shibata, Tomohide and Kurohashi, Sadao},
title = {Predicate-Argument Structure-Based Textual Entailment Recognition System Exploiting Wide-Coverage Lexical Knowledge},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382598},
doi = {10.1145/2382593.2382598},
abstract = {This article proposes a predicate-argument structure based Textual Entailment Recognition system exploiting wide-coverage lexical knowledge. Different from conventional machine learning approaches where several features obtained from linguistic analysis and resources are utilized, our proposed method regards a predicate-argument structure as a basic unit, and performs the matching/alignment between a text and hypothesis. In matching between predicate-arguments, wide-coverage relations between words/phrases such as synonym and is-a are utilized, which are automatically acquired from a dictionary, Web corpus, and Wikipedia.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {16},
numpages = {23},
keywords = {lexical knowledge, predicate-argument structure, RTE}
}

@article{10.1145/2382593.2382599,
author = {Shih, Chengwei and Lee, Chengwei and Tsai, Richard Tzonghan and Hsu, Wenlian},
title = {Validating Contradiction in Texts Using Online Co-Mention Pattern Checking},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382599},
doi = {10.1145/2382593.2382599},
abstract = {Detecting contradictive statements is a foundational and challenging task for text understanding applications such as textual entailment. In this article, we aim to address the problem of the shortage of specific background knowledge in contradiction detection. A novel contradiction detecting approach based on the distribution of the query composed of critical mismatch combinations on the Internet is proposed to tackle the problem. By measuring the availability of mismatch conjunction phrases (MCPs), the background knowledge about two target statements can be implicitly obtained for identifying contradictions. Experiments on three different configurations show that the MCP-based approach achieves remarkable improvement on contradiction detection and can significantly improve the performance of textual entailment recognition.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {17},
numpages = {21},
keywords = {contradiction detection, Chinese, Web mining, Textual entailment}
}

@article{10.1145/2382593.2382597,
author = {Qiu, Xipeng and Cao, Ling and Liu, Zhao and Huang, Xuanjing},
title = {Recognizing Inference in Texts with Markov Logic Networks},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382597},
doi = {10.1145/2382593.2382597},
abstract = {Recognizing inference in texts (RITE) attracts growing attention of natural language processing (NLP) researchers in recent years. In this article, we propose a novel approach to recognize inference with probabilistic logical reasoning. Our approach is built on Markov logic networks (MLNs) framework, which is a probabilistic extension of first-order logic. We design specific semantic rules based on the surface, syntactic, and semantic representations of texts, and map these rules to logical representations. We also extract information from some knowledge bases as common sense logic rules. Then we utilize MLNs framework to make predictions with combining statistical and logical reasoning. Experiment results shows that our system can achieve better performance than state-of-the-art RITE systems.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {15},
numpages = {23},
keywords = {Markov logic networks, logical reasoning, Recognizing inference in text}
}

@article{10.1145/2382593.2382600,
author = {Watanabe, Yotaro and Mizuno, Junta and Nichols, Eric and Narisawa, Katsuma and Nabeshima, Keita and Okazaki, Naoaki and Inui, Kentaro},
title = {Leveraging Diverse Lexical Resources for Textual Entailment Recognition},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2382593.2382600},
doi = {10.1145/2382593.2382600},
abstract = {Since the problem of textual entailment recognition requires capturing semantic relations between diverse expressions of language, linguistic and world knowledge play an important role. In this article, we explore the effectiveness of different types of currently available resources including synonyms, antonyms, hypernym-hyponym relations, and lexical entailment relations for the task of textual entailment recognition. In order to do so, we develop an entailment relation recognition system which utilizes diverse linguistic analyses and resources to align the linguistic units in a pair of texts and identifies entailment relations based on these alignments. We use the Japanese subset of the NTCIR-9 RITE-1 dataset for evaluation and error analysis, conducting ablation testing and evaluation on hand-crafted alignment gold standard data to evaluate the contribution of individual resources. Error analysis shows that existing knowledge sources are effective for RTE, but that their coverage is limited, especially for domain-specific and other low-frequency expressions. To increase alignment coverage on such expressions, we propose a method of alignment inference that uses syntactic and semantic dependency information to identify likely alignments without relying on external resources. Evaluation adding alignment inference to a system using all available knowledge sources shows improvements in both precision and recall of entailment relation recognition.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {18},
numpages = {22},
keywords = {alignment, Textual entailment, lexical resources}
}

@article{10.1145/2334801.2334802,
author = {Isozaki, Hideki and Sudoh, Katsuhito and Tsukada, Hajime and Duh, Kevin},
title = {HPSG-Based Preprocessing for English-to-Japanese Translation},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2334801.2334802},
doi = {10.1145/2334801.2334802},
abstract = {Japanese sentences have completely different word orders from corresponding English sentences. Typical phrase-based statistical machine translation (SMT) systems such as Moses search for the best word permutation within a given distance limit (distortion limit). For English-to-Japanese translation, we need a large distance limit to obtain acceptable translations, and the number of translation candidates is extremely large. Therefore, SMT systems often fail to find acceptable translations within a limited time. To solve this problem, some researchers use rule-based preprocessing approaches, which reorder English words just like Japanese by using dozens of rules. Our idea is based on the following two observations: (1) Japanese is a typical head-final language, and (2) we can detect heads of English sentences by a head-driven phrase structure grammar (HPSG) parser. The main contributions of this article are twofold: First, we demonstrate how off-the-shelf, state-of-the-art HPSG parser enables us to write the reordering rules in an abstract level and can easily improve the quality of English-to-Japanese translation. Second, we also show that syntactic heads achieve better results than semantic heads. The proposed method outperforms the best system of NTCIR-7 PATMT EJ task.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {8},
numpages = {16},
keywords = {English, SOV, SVO, Japanese, Machine translation, HPSG}
}

@article{10.1145/2334801.2334805,
author = {Duc, Nguyen Tuan and Bollegala, Danushka and Ishizuka, Mitsuru},
title = {Cross-Language Latent Relational Search between Japanese and English Languages Using a Web Corpus},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2334801.2334805},
doi = {10.1145/2334801.2334805},
abstract = {Latent relational search is a novel entity retrieval paradigm based on the proportional analogy between two entity pairs. Given a latent relational search query {(Japan, Tokyo), (France, ?)}, a latent relational search engine is expected to retrieve and rank the entity “Paris” as the first answer in the result list. A latent relational search engine extracts entities and relations between those entities from a corpus, such as the Web. Moreover, from some supporting sentences in the corpus, (e.g., “Tokyo is the capital of Japan” and “Paris is the capital and biggest city of France”), the search engine must recognize the relational similarity between the two entity pairs. In cross-language latent relational search, the entity pairs as well as the supporting sentences of the first entity pair and of the second entity pair are in different languages. Therefore, the search engine must recognize similar semantic relations across languages. In this article, we study the problem of cross-language latent relational search between Japanese and English using Web data. To perform cross-language latent relational search in high speed, we propose a multi-lingual indexing method for storing entities and lexical patterns that represent the semantic relations extracted from Web corpora. We then propose a hybrid lexical pattern clustering algorithm to capture the semantic similarity between lexical patterns across languages. Using this algorithm, we can precisely measure the relational similarity between entity pairs across languages, thereby achieving high precision in the task of cross-language latent relational search. Experiments show that the proposed method achieves an MRR of 0.605 on Japanese-English cross-language latent relational search query sets and it also achieves a reasonable performance on the INEX Entity Ranking task.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {11},
numpages = {33},
keywords = {analogical search, cross-language relational search, latent relational analysis, Latent relational search}
}

@article{10.1145/2334801.2334803,
author = {Zhang, Lidan and Chan, Kwop-Ping},
title = {Adaptive Bayesian HMM for Fully Unsupervised Chinese Part-of-Speech Induction},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2334801.2334803},
doi = {10.1145/2334801.2334803},
abstract = {We propose an adaptive Bayesian hidden Markov model for fully unsupervised part-of-speech (POS) induction. The proposed model with its inference algorithm has two extensions to the first-order Bayesian HMM with Dirichlet priors. First our algorithm infers the optimal number of hidden states from the training corpus rather than fixes the dimensionality of state space beforehand. The second extension studies the Chinese unknown word processing module which measures similarities from both morphological properties and context distribution. Experimental results showed that both of these two extensions can help to find the optimal categories for Chinese in terms of both unsupervised clustering metrics and grammar induction accuracies on the Chinese Treebank.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {9},
numpages = {22},
keywords = {Dirichlet distribution, Bayesian HMM, Part-of-speech induction, variational inference, Chinese language model}
}

@article{10.1145/2334801.2334804,
author = {Lee, Jinsik and Lee, Sungjin and Lee, Jonghoon and Kim, Byeongchang and Lee, Gary Geunbae},
title = {Stacking Model-Based Korean Prosodic Phrasing Using Speaker Variability Reduction and Linguistic Feature Engineering},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2334801.2334804},
doi = {10.1145/2334801.2334804},
abstract = {This article presents a prosodic phrasing model for a general purpose Korean speech synthesis system. To reflect the factors affecting prosodic phrasing in the model, linguistically motivated machine-learning features were investigated. These features were effectively incorporated using a stacking model. The phrasing performance was also improved through feature engineering. The corpus used in the experiment is a 4,392-sentence corpus (55,015 words with an average of 13 words per sentence). Because the corpus contains speaker-dependent variability and such variability is not appropriately reflected in a general purpose speech synthesis system, a method to reduce such variability is proposed. In addition, the entire set of data used in the experiment is provided to the public for future use in comparative research.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {10},
numpages = {22},
keywords = {Korean, linguistic feature, speech synthesis, prosody, Prosodic phrasing, stacking model, phrase break prediction}
}

@article{10.1145/2184436.2184439,
author = {Andrade, Daniel and Matsuzaki, Takuya and Tsujii, Jun’ichi},
title = {Statistical Extraction and Comparison of Pivot Words for Bilingual Lexicon Extension},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2184436.2184439},
doi = {10.1145/2184436.2184439},
abstract = {Bilingual dictionaries can be automatically extended by new translations using comparable corpora. The general idea is based on the assumption that similar words have similar contexts across languages. However, previous studies have mainly focused on Indo-European languages, or use only a bag-of-words model to describe the context. Furthermore, we argue that it is helpful to extract only the statistically significant context, instead of using all context. The present approach addresses these issues in the following manner. First, based on the context of a word with an unknown translation (query word), we extract salient pivot words. Pivot words are words for which a translation is already available in a bilingual dictionary. For the extraction of salient pivot words, we use a Bayesian estimation of the point-wise mutual information to measure statistical significance. In the second step, we match these pivot words across languages to identify translation candidates for the query word. We therefore calculate a similarity score between the query word and a translation candidate using the probability that the same pivots will be extracted for both the query word and the translation candidate. The proposed method uses several context positions, namely, a bag-of-words of one sentence, and the successors, predecessors, and siblings with respect to the dependency parse tree of the sentence. In order to make these context positions comparable across Japanese and English, which are unrelated languages, we use several heuristics to adjust the dependency trees appropriately. We demonstrate that the proposed method significantly increases the accuracy of word translations, as compared to previous methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {6},
numpages = {31},
keywords = {bilingual dictionary creation, dependency parse tree information, comparable corpora, Bayesian statistical methods}
}

@article{10.1145/2184436.2184438,
author = {Wang, Hongling and Zhou, Guodong},
title = {Toward a Unified Framework for Standard and Update Multi-Document Summarization},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2184436.2184438},
doi = {10.1145/2184436.2184438},
abstract = {This article presents a unified framework for extracting standard and update summaries from a set of documents. In particular, a topic modeling approach is employed for salience determination and a dynamic modeling approach is proposed for redundancy control. In the topic modeling approach for salience determination, we represent various kinds of text units, such as word, sentence, document, documents, and summary, using a single vector space model via their corresponding probability distributions over the inherent topics of given documents or a related corpus. Therefore, we are able to calculate the similarity between any two text units via their topic probability distributions. In the dynamic modeling approach for redundancy control, we consider the similarity between the summary and the given documents, and the similarity between the sentence and the summary, besides the similarity between the sentence and the given documents, for standard summarization while for update summarization, we also consider the similarity between the sentence and the history documents or summary. Evaluation on TAC 2008 and 2009 in English language shows encouraging results, especially the dynamic modeling approach in removing the redundancy in the given documents. Finally, we extend the framework to Chinese multi-document summarization and experiments show the effectiveness of our framework.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {5},
numpages = {18},
keywords = {dynamic modeling, Multi-document summarization, topic modeling, latent Dirichlet allocation}
}

@article{10.1145/2184436.2184440,
author = {Wang, Kun and Zong, Chengqing and Su, Keh-Yih},
title = {Integrating Generative and Discriminative Character-Based Models for Chinese Word Segmentation},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2184436.2184440},
doi = {10.1145/2184436.2184440},
abstract = {Among statistical approaches to Chinese word segmentation, the word-based n-gram (generative) model and the character-based tagging (discriminative) model are two dominant approaches in the literature. The former gives excellent performance for the in-vocabulary (IV) words; however, it handles out-of-vocabulary (OOV) words poorly. On the other hand, though the latter is more robust for OOV words, it fails to deliver satisfactory performance for IV words. These two approaches behave differently due to the unit they use (word vs. character) and the model form they adopt (generative vs. discriminative). In general, character-based approaches are more robust than word-based ones, as the vocabulary of characters is a closed set; and discriminative models are more robust than generative ones, since they can flexibly include all kinds of available information, such as future context.This article first proposes a character-based n-gram model to enhance the robustness of the generative approach. Then the proposed generative model is further integrated with the character-based discriminative model to take advantage of both approaches. Our experiments show that this integrated approach outperforms all the existing approaches reported in the literature. Afterwards, a complete and detailed error analysis is conducted. Since a significant portion of the critical errors is related to numerical/foreign strings, character-type information is then incorporated into the model to further improve its performance. Last, the proposed integrated approach is tested on cross-domain corpora, and a semi-supervised domain adaptation algorithm is proposed and shown to be effective in our experiments.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {7},
numpages = {41},
keywords = {generative model, model integration, domain adaptation, Chinese word segmentation, character-based approach, discriminative model}
}

@article{10.1145/2184436.2184437,
author = {He, Yulan},
title = {Incorporating Sentiment Prior Knowledge for Weakly Supervised Sentiment Analysis},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/2184436.2184437},
doi = {10.1145/2184436.2184437},
abstract = {This article presents two novel approaches for incorporating sentiment prior knowledge into the topic model for weakly supervised sentiment analysis where sentiment labels are considered as topics. One is by modifying the Dirichlet prior for topic-word distribution (LDA-DP), the other is by augmenting the model objective function through adding terms that express preferences on expectations of sentiment labels of the lexicon words using generalized expectation criteria (LDA-GE). We conducted extensive experiments on English movie review data and multi-domain sentiment dataset as well as Chinese product reviews about mobile phones, digital cameras, MP3 players, and monitors. The results show that while both LDA-DP and LDA-GE perform comparably to existing weakly supervised sentiment classification algorithms, they are much simpler and computationally efficient, rendering them more suitable for online and real-time sentiment classification on the Web. We observed that LDA-GE is more effective than LDA-DP, suggesting that it should be preferred when considering employing the topic model for sentiment analysis. Moreover, both models are able to extract highly domain-salient polarity words from text.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {4},
numpages = {19},
keywords = {Sentiment analysis, generalized expectation, latent Dirichlet allocation, weakly supervised sentiment classification}
}

@article{10.1145/2090176.2090179,
author = {Chang, Ru-Yng and Wu, Chung-Hsien and Prasetyo, Philips Kokoh},
title = {Error Diagnosis of Chinese Sentences Using Inductive Learning Algorithm and Decomposition-Based Testing Mechanism},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2090176.2090179},
doi = {10.1145/2090176.2090179},
abstract = {This study presents a novel approach to error diagnosis of Chinese sentences for Chinese as second language (CSL) learners. A penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm is presented for error diagnosis of Chinese sentences. The pFOIL algorithm integrates inductive logic programming (ILP), First-Order Inductive Learning (FOIL), and a penalized log-likelihood function for error diagnosis. This algorithm considers the uncertain, imperfect, and conflicting characteristics of Chinese sentences to infer error types and produce human-interpretable rules for further error correction. In a pFOIL algorithm, relation pattern background knowledge and quantized t-score background knowledge are proposed to characterize a sentence and then used for likelihood estimation. The relation pattern background knowledge captures the morphological, syntactic and semantic relations among the words in a sentence. One or two kinds of the extracted relations are then integrated into a pattern to characterize a sentence. The quantized t-score values are used to characterize various relations of a sentence for quantized t-score background knowledge representation. Afterwards, a decomposition-based testing mechanism which decomposes a sentence into background knowledge set needed for each error type is proposed to infer all potential error types and causes of the sentence. With the pFOIL method, not only the error types but also the error causes and positions can be provided for CSL learners. Experimental results reveal that the pFOIL method outperforms the C4.5, maximum entropy, and Naive Bayes classifiers in error classification.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {24},
keywords = {inductive learning algorithm and decomposition-based testing mechanism, Chinese as second language (CSL) learner error sentence diagnosis, inductive logic programming}
}

@article{10.1145/2090176.2090178,
author = {Zaghouani, Wajdi},
title = {RENAR: A Rule-Based Arabic Named Entity Recognition System},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2090176.2090178},
doi = {10.1145/2090176.2090178},
abstract = {Named entity recognition has served many natural language processing tasks such as information retrieval, machine translation, and question answering systems. Many researchers have addressed the name identification issue in a variety of languages and recently some research efforts have started to focus on named entity recognition for the Arabic language. We present a working Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date, and number, as well as quotations (direct reported speech) by and about people. The named entity recognition (NER) system was not developed for Arabic, but instead a multilingual NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This article thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {2},
numpages = {13},
keywords = {information extraction, rule-based systems, Arabic natural language processing, Named entity recognition}
}

@article{10.1145/2090176.2090177,
author = {Pal, Umapada and Jayadevan, Ramachandran and Sharma, Nabin},
title = {Handwriting Recognition in Indian Regional Scripts: A Survey of Offline Techniques},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/2090176.2090177},
doi = {10.1145/2090176.2090177},
abstract = {Offline handwriting recognition in Indian regional scripts is an interesting area of research as almost 460 million people in India use regional scripts. The nine major Indian regional scripts are Bangla (for Bengali and Assamese languages), Gujarati, Kannada, Malayalam, Oriya, Gurumukhi (for Punjabi language), Tamil, Telugu, and Nastaliq (for Urdu language). A state-of-the-art survey about the techniques available in the area of offline handwriting recognition (OHR) in Indian regional scripts will be of a great aid to the researchers in the subcontinent and hence a sincere attempt is made in this article to discuss the advancements reported in this regard during the last few decades. The survey is organized into different sections. A brief introduction is given initially about automatic recognition of handwriting and official regional scripts in India. The nine regional scripts are then categorized into four subgroups based on their similarity and evolution information. The first group contains Bangla, Oriya, Gujarati and Gurumukhi scripts. The second group contains Kannada and Telugu scripts and the third group contains Tamil and Malayalam scripts. The fourth group contains only Nastaliq script (Perso-Arabic script for Urdu), which is not an Indo-Aryan script. Various feature extraction and classification techniques associated with the offline handwriting recognition of the regional scripts are discussed in this survey. As it is important to identify the script before the recognition step, a section is dedicated to handwritten script identification techniques. A benchmarking database is very important for any pattern recognition related research. The details of the datasets available in different Indian regional scripts are also mentioned in the article. A separate section is dedicated to the observations made, future scope, and existing difficulties related to handwriting recognition in Indian regional scripts. We hope that this survey will serve as a compendium not only for researchers in India, but also for policymakers and practitioners in India. It will also help to accomplish a target of bringing the researchers working on different Indian scripts together. Looking at the recent developments in OHR of Indian regional scripts, this article will provide a better platform for future research activities.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {1},
numpages = {35},
keywords = {OCR, Indic script recognition, offline handwriting recognition, handwritten documents, Indian regional languages, survey of offline techniques}
}

@article{10.1145/2025384.2025385,
author = {Du, Jinhua and Way, Andy},
title = {Improved Chinese--English SMT with Chinese “DE” Construction Classification and Reordering},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2025384.2025385},
doi = {10.1145/2025384.2025385},
abstract = {Syntactic reordering on the source side has been demonstrated to be helpful and effective for handling different word orders between source and target languages in SMT. In this article, we focus on the Chinese (DE) construction which is flexible and ubiquitous in Chinese and has many different ways to be translated into English so that it is a major source of word order differences in terms of translation quality. This article carries out the Chinese “DE” construction study for Chinese--English SMT in which we propose a new classifier model---discriminative latent variable model (DPLVM)---with new features to improve the classification accuracy and indirectly improve the translation quality compared to a log-linear classifier. The DE classifier is used to recognize DE structures in both training and test sentences of Chinese, and then perform word reordering to make the Chinese sentences better match the word order of English. In order to investigate the impact of the DE classification and reordering in the source side on different types of SMT systems (namely PB-SMT, hierarchical PB-SMT (HPB-SMT) as well as the syntax-based SMT (SAMT)), we conduct a series of experiments on NIST 2005 and 2008 test sets to verify the effectiveness of our proposed model. The experimental results show that the MT systems using the data reordered by our proposed model outperform the baseline systems by 3.01% and 4.03% relative points on the NIST 2005 test set, 4.64% and 4.62% relative points on the NIST 2008 test set in terms of BLEU score for PB-SMT and HPB-SMT respectively. However, the DE classification method does not perform significantly well for SAMT. Additionally, we also conducted some experiments to evaluate our DE classification and reordering approach on the word alignment and phrase table in terms of these three types of SMT systems.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {17},
numpages = {22},
keywords = {Chinese DE construction, log-linear model, source-side reordering, dynamic probabilistic latent variable model}
}

@article{10.1145/2025384.2025389,
author = {Wang, Baoxun and Liu, Bingquan and Wang, Xiaolong and Sun, Chengjie and Zhang, Deyuan},
title = {Deep Learning Approaches to Semantic Relevance Modeling for Chinese Question-Answer Pairs},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2025384.2025389},
doi = {10.1145/2025384.2025389},
abstract = {The human-generated question-answer pairs in the Web social communities are of great value for the research of automatic question-answering technique. Due to the large amount of noise information involved in such corpora, it is still a problem to detect the answers even though the questions are exactly located. Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. Since both the questions and their answers usually contain a small number of sentences, the relevance modeling methods have to overcome the problem of word feature sparsity. In this article, the deep learning principle is introduced to address the semantic relevance modeling task. Two deep belief networks with different architectures are proposed by us to model the semantic relevance for the question-answer pairs. According to the investigation of the textual similarity between the community-driven question-answering (cQA) dataset and the forum dataset, a learning strategy is adopted to promote our models’ performance on the social community corpora without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {21},
numpages = {16},
keywords = {question-answer pairs, semantic relevance, Deep belief network}
}

@article{10.1145/2025384.2025387,
author = {Li, Lishuang and Wang, Peng and Huang, Degen and Zhao, Lian},
title = {Mining English-Chinese Named Entity Pairs from Comparable Corpora},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2025384.2025387},
doi = {10.1145/2025384.2025387},
abstract = {Bilingual Named Entity (NE) pairs are valuable resources for many NLP applications. Since comparable corpora are more accessible, abundant and up-to-date, recent researches have concentrated on mining bilingual lexicons using comparable corpora. Leveraging comparable corpora, this research presents a novel approach to mining English-Chinese NE translations by combining multi-dimension features from various information sources for every possible NE pair, which include the transliteration model, English-Chinese matching, Chinese-English matching, translation model, length, and context vector. These features are integrated into one model with linear combination and minimum sample risk (MSR) algorithm. As for the high type-dependence of NE translation, we integrate different features according to different NE types. We experiment with the above individual feature or integrated features to mine person NE (PN) pairs, location NE (LN) pairs and organization NE (ON) pairs. When using transliteration and length to mine PN pairs, we achieve the best performance of 84.9% (F-score). The LN pairs can be mined with the features of transliteration model, length, translation model, English-Chinese matching and Chinese-English matching. And the best performance is 83.4% (F-score). The ON pairs can be mined with the features of English-Chinese matching and Chinese-English matching. It reaches the best performance with 84.1% (F-score).},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {19},
numpages = {19},
keywords = {MSR, mining, Transliteration model, named entity, pairs, English-Chinese matching, comparable corpora, Chinese-English matching, translation model}
}

@article{10.1145/2025384.2025386,
author = {Xiao, Tong and Zhu, Jingbo and Zhu, Muhua},
title = {Language Modeling for Syntax-Based Machine Translation Using Tree Substitution Grammars: A Case Study on Chinese-English Translation},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2025384.2025386},
doi = {10.1145/2025384.2025386},
abstract = {The poor grammatical output of Machine Translation (MT) systems appeals syntax-based approaches within language modeling. However, previous studies showed that syntax-based language modeling using (Context-Free) Treebank Grammars was not very helpful in improving BLEU scores for Chinese-English machine translation. In this article we further study this issue in the context of Chinese-English syntax-based Statistical Machine Translation (SMT) where Synchronous Tree Substitution Grammars (STSGs) are utilized to model the translation process. In particular, we develop a Tree Substitution Grammar-based language model for syntax-based MT, and present three methods to efficiently integrate the proposed language model into MT decoding. In addition, we design a simple and effective method to adapt syntax-based language models for MT tasks. We demonstrate that the proposed methods are able to benefit a state-of-the-art syntax-based MT system. On the NIST Chinese-English MT evaluation corpora, we finally achieve an improvement of 0.6 BLEU points over the baseline.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {18},
numpages = {29},
keywords = {Machine translation, syntax-based language model, tree substitution grammar}
}

@article{10.1145/2025384.2025388,
author = {Liu, Zhiyuan and Zheng, Yabin and Xie, Lixing and Sun, Maosong and Ru, Liyun and Zhang, Yang},
title = {User Behaviors in Related Word Retrieval and New Word Detection: A Collaborative Perspective},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/2025384.2025388},
doi = {10.1145/2025384.2025388},
abstract = {Nowadays, user behavior analysis and collaborative filtering have drawn a large body of research in the machine learning community. The goal is either to enhance the user experience or discover useful information hidden in the data. In this article, we conduct extensive experiments on a Chinese input method data set, which keeps the word lists that users have used. Then, from the collaborative perspective, we aim to solve two tasks in natural language processing, that is, related word retrieval and new word detection. Motivated by the observation that two words are usually highly related to each other if they co-occur frequently in users’ records, we propose a novel semantic relatedness measure between words that takes both user behaviors and collaborative filtering into consideration. We utilize this measure to perform related word retrieval and new word detection tasks. Experimental results on both tasks indicate the applicability and effectiveness of our method.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {20},
numpages = {26},
keywords = {Related words retrieval, user behaviors, collaborative filtering, natural language processing, new word detection}
}

@article{10.1145/2002980.2002986,
author = {Huang, Chung-Chi and Yen, Ho-Ching and Yang, Ping-Che and Huang, Shih-Ting and Chang, Jason S.},
title = {Using Sublexical Translations to Handle the OOV Problem in Machine Translation},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002986},
doi = {10.1145/2002980.2002986},
abstract = {We introduce a method for learning to translate out-of-vocabulary (OOV) words. The method focuses on combining sublexical/constituent translations of an OOV to generate its translation candidates. In our approach, wildcard searches are formulated based on our OOV analysis, aimed at maximizing the probability of retrieving OOVs’ sublexical translations from existing resources of Machine Translation (MT) systems. At run-time, translation candidates of the unknown words are generated from their suitable sublexical translations and ranked based on monolingual and bilingual information. We have incorporated the OOV model into a state-of-the-art machine translation system and experimental results show that our model indeed helps to ease the impact of OOVs on translation quality, especially for sentences containing more OOVs (significant improvement).},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {16},
numpages = {20},
keywords = {sublexical translation, Out-of-vocabulary words, translation model, language model, machine translation, wildcard search query, phrase table}
}

@article{10.1145/2002980.2002985,
author = {Qian, Longhua and Zhou, Guodong and Zhu, Qiaoming},
title = {Employing Constituent Dependency Information for Tree Kernel-Based Semantic Relation Extraction between Named Entities},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002985},
doi = {10.1145/2002980.2002985},
abstract = {This article proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction between named entities. The basic idea is to employ constituent dependency information in keeping the necessary nodes and their head children along the path connecting the two entities in the syntactic parse tree, while removing the noisy information from the tree, eventually leading to a dynamic syntactic parse tree. This article also explores various entity features and their possible combinations via a unified syntactic and semantic tree framework, which integrates both structural syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 English and 2005 Chinese benchmark corpora shows that our dynamic syntactic parse tree much outperforms all previous tree spans, indicating its effectiveness in well representing the structural nature of relation instances while removing redundant information. Moreover, the unified parse and semantic tree significantly outperforms the single syntactic parse tree, largely due to the remarkable contributions from entity-related semantic features such as its type, subtype, mention-level as well as their bi-gram combinations. Finally, the best performance so far in semantic relation extraction is achieved via a composite kernel, which combines this tree kernel with a linear, state-of-the-art, feature-based kernel.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {15},
numpages = {24},
keywords = {convolution tree kernel, Semantic relation extraction, unified syntactic and semantic tree, constituent dependency}
}

@article{10.1145/2002980.2002981,
author = {Chen, Keh-Jiann and Liu, Qun and Xue, Nianwen and Sun, Le},
title = {Introduction to the Special Issue on Chinese Language Processing},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002981},
doi = {10.1145/2002980.2002981},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {11},
numpages = {3}
}

@article{10.1145/2002980.2002984,
author = {Zhang, Peng and Li, Wenjie and Hou, Yuexian and Song, Dawei},
title = {Developing Position Structure-Based Framework for Chinese Entity Relation Extraction},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002984},
doi = {10.1145/2002980.2002984},
abstract = {Relation extraction is the task of finding semantic relations between two entities in text, and is often cast as a classification problem. In contrast to the significant achievements on English language, research progress in Chinese relation extraction is relatively limited. In this article, we present a novel Chinese relation extraction framework, which is mainly based on a 9-position structure. The design of this proposed structure is motivated by the fact that there are some obvious connections between relation types/subtypes and position structures of two entities. The 9-position structure can be captured with less effort than applying deep natural language processing, and is effective to relieve the class imbalance problem which often hurts the classification performance. In our framework, all involved features do not require Chinese word segmentation, which has long been limiting the performance of Chinese language processing. We also utilize some correction and inference mechanisms to further improve the classified results. Experiments on the ACE 2005 Chinese data set show that the 9-position structure feature can provide strong support for Chinese relation extraction. As well as this, other strategies are also effective to further improve the performance.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {14},
numpages = {22},
keywords = {Entity relation extraction, position structure, Chinese language, imbalance class classification}
}

@article{10.1145/2002980.2002983,
author = {Li, Junhui and Zhou, Guodong},
title = {Unified Semantic Role Labeling for Verbal and Nominal Predicates in the Chinese Language},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002983},
doi = {10.1145/2002980.2002983},
abstract = {This article explores unified semantic role labeling (SRL) for both verbal and nominal predicates in the Chinese language. This is done by considering SRL for both verbal and nominal predicates in a unified framework. First, we systematically examine various kinds of features for verbal SRL and nominal SRL, respectively, besides those widely used ones. Then we further improve the performance of nominal SRL with various kinds of verbal evidence, that is, merging the training instances from verbal predicates and integrating various kinds of features derived from SRL for verbal predicates. Finally, we address the issue of automatic predicate recognition, which is essential for nominal SRL. Evaluation on Chinese PropBank and Chinese NomBank shows that our unified approach significantly improves the performance, in particular that of nominal SRL. To the best of our knowledge, this is the first reported work of unified verbal and nominal SRL on Chinese PropBank and NomBank.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {13},
numpages = {21},
keywords = {nominal predicates, unified approach, Semantic role labeling, automatic predicate recognition, verbal predicates}
}

@article{10.1145/2002980.2002982,
author = {Zhu, Muhua and Zhu, Jingbo and Xiao, Tong},
title = {Automatic Treebank Conversion via Informed Decoding - A Case Study on Chinese Treebanks},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/2002980.2002982},
doi = {10.1145/2002980.2002982},
abstract = {Treebanks are valuable resources for syntactic parsing. For some languages such as Chinese, we can obtain multiple constituency treebanks which are developed by different organizations. However, due to discrepancies of underlying annotation standards, such treebanks in general cannot be used together through direct data combination. To enlarge training data for syntactic parsing, we focus in this article on the challenge of unifying standards of disparate treebanks by automatically converting one treebank (source treebank) to fit a different standard which is exhibited by another treebank (target treebank).We propose to convert a treebank in two sequential steps which correspond to the part-of-speech level and syntactic structure level (including tree structures and grammar labels), respectively. Approaches used in both levels can be unified as an informed decoding procedure, where information derived from original annotation in a source treebank is used to guide the conversion conducted by a POS tagger (or a parser in the syntactic structure level) trained on a target treebank. We take two Chinese treebanks as a case study, and experiments on these two treebanks show significant improvements in conversion accuracy over baseline systems, especially in situations where a target treebank is small in size.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {12},
numpages = {24},
keywords = {Chinese POS tagging, Chinese syntactic parsing, informed decoding, treebank conversion}
}

@article{10.1145/1967293.1967296,
author = {Ekbal, Asif and Saha, Sriparna},
title = {Weighted Vote-Based Classifier Ensemble for Named Entity Recognition: A Genetic Algorithm-Based Approach},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1967293.1967296},
doi = {10.1145/1967293.1967296},
abstract = {In this article, we report the search capability of Genetic Algorithm (GA) to construct a weighted vote-based classifier ensemble for Named Entity Recognition (NER). Our underlying assumption is that the reliability of predictions of each classifier differs among the various named entity (NE) classes. Thus, it is necessary to quantify the amount of voting of a particular classifier for a particular output class. Here, an attempt is made to determine the appropriate weights of voting for each class in each classifier using GA. The proposed technique is evaluated for four leading Indian languages, namely Bengali, Hindi, Telugu, and Oriya, which are all resource-poor in nature. Evaluation results yield the recall, precision and F-measure values of 92.08%, 92.22%, and 92.15%, respectively for Bengali; 96.07%, 88.63%, and 92.20%, respectively for Hindi; 78.82%, 91.26%, and 84.59%, respectively for Telugu; and 88.56%, 89.98%, and 89.26%, respectively for Oriya. Finally, we evaluate our proposed approach with the benchmark dataset of CoNLL-2003 shared task that yields the overall recall, precision, and F-measure values of 88.72%, 88.64%, and 88.68%, respectively. Results also show that the vote based classifier ensemble identified by the GA-based approach outperforms all the individual classifiers, three conventional baseline ensembles, and some other existing ensemble techniques. In a part of the article, we formulate the problem of feature selection in any classifier under the single objective optimization framework and show that our proposed classifier ensemble attains superior performance to it.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {9},
numpages = {37},
keywords = {Language independent named entity recognition, feature selection, support vector machine, classifier ensemble, genetic algorithm, conditional random field, maximum entropy}
}

@article{10.1145/1967293.1967294,
author = {Wu, Chung-Hsien and Su, Hung-Yu and Shen, Han-Ping},
title = {Articulation-Disordered Speech Recognition Using Speaker-Adaptive Acoustic Models and Personalized Articulation Patterns},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1967293.1967294},
doi = {10.1145/1967293.1967294},
abstract = {This article presents a novel approach to speaker-adaptive recognition of speech from articulation-disordered speakers without a large amount of adaptation data. An unsupervised, incremental adaptation method is adopted for personalized model adaptation based on the recognized syllables with high recognition confidence from an automatic speech recognition (ASR) system. For articulation pattern discovery, the manually transcribed syllables and the corresponding recognized syllables are associated with each other using articulatory features. The Apriori algorithm is applied to discover the articulation patterns in the corpus, which are then used to construct a personalized pronunciation dictionary to improve the recognition accuracy of the ASR. The experimental results indicate that the proposed adaptation method achieves a syllable error rate reduction of 6.1%, outperforming the conventional adaptation methods that have a syllable error rate reduction of 3.8%. In addition, an average syllable error rate reduction of 5.04% is obtained for the ASR using the expanded pronunciation dictionary.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {7},
numpages = {19},
keywords = {Articulation disorder, ASR, pronunciation variation, acoustical adaptation, Apriori algorithm}
}

@article{10.1145/1967293.1967295,
author = {Paik, Jiaul H. and Parui, Swapan K.},
title = {A Fast Corpus-Based Stemmer},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1967293.1967295},
doi = {10.1145/1967293.1967295},
abstract = {Stemming is a mechanism of word form normalization that transforms the variant word forms to their common root. In an Information Retrieval system, it is used to increase the system’s performance, specifically the recall and desirably the precision. Although its usefulness is shown to be mixed in languages such as English, because morphologically complex languages stemming produces a significant performance improvement. A number of linguistic rule-based stemmers are available for most European languages which employ a set of rules to get back the root word from its variants. But for Indian languages which are highly inflectional in nature, devising a linguistic rule-based stemmer needs some additional resources which are not available. We present an approach which is purely corpus based and finds the equivalence classes of variant words in an unsupervised manner. A set of experiments on four languages using FIRE, CLEF, and TREC test collections shows that our approach provides comparable results with linguistic rule-based stemmers for some languages and gives significant performance improvement for resource constrained languages such as Bengali and Marathi.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {8},
numpages = {16},
keywords = {suffix, corpus, stemming, Marathi, Hungarian, Indian languages, Bengali}
}

@article{10.1145/1967293.1967297,
author = {Liu, C.-L. and Lai, M.-H. and Tien, K.-W. and Chuang, Y.-H. and Wu, S.-H. and Lee, C.-Y.},
title = {Visually and Phonologically Similar Characters in Incorrect Chinese Words: Analyses, Identification, and Applications},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1967293.1967297},
doi = {10.1145/1967293.1967297},
abstract = {Information about students’ mistakes opens a window to an understanding of their learning processes, and helps us design effective course work to help students avoid replication of the same errors. Learning from mistakes is important not just in human learning activities; it is also a crucial ingredient in techniques for the developments of student models. In this article, we report findings of our study on 4,100 erroneous Chinese words. Seventy-six percent of these errors were related to the phonological similarity between the correct and the incorrect characters, 46% were due to visual similarity, and 29% involved both factors. We propose a computing algorithm that aims at replication of incorrect Chinese words. The algorithm extends the principles of decomposing Chinese characters with the Cangjie codes to judge the visual similarity between Chinese characters. The algorithm also employs empirical rules to determine the degree of similarity between Chinese phonemes. To show its effectiveness, we ran the algorithm to select and rank a list of about 100 candidate characters, from more than 5,100 characters, for the incorrectly written character in each of the 4,100 errors. We inspected whether the incorrect character was indeed included in the candidate list and analyzed whether the incorrect character was ranked at the top of the candidate list. Experimental results show that our algorithm captured 97% of incorrect characters for the 4,100 errors, when the average length of the candidate lists was 104. Further analyses showed that the incorrect characters ranked among the top 10 candidates in 89% of the phonologically similar errors and in 80% of the visually similar errors.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {10},
numpages = {39},
keywords = {student modeling, Error analysis of written Chinese text, psycholinguistics, simplified Chinese, computer-assisted language learning, traditional Chinese}
}

@article{10.1145/1929908.1929913,
author = {Abdul-Mageed, Muhammad},
title = {Automatic Detection of Arabic Non-Anaphoric Pronouns for Improving Anaphora Resolution},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929913},
doi = {10.1145/1929908.1929913},
abstract = {Anaphora resolution is one of the most difficult tasks in NLP. The ability to identify non-referential pronouns before attempting an anaphora resolution task would be significant, since the system would not have to attempt resolving such pronouns and hence end up with fewer errors. In addition, the number of non-referential pronouns has been found to be non-trivial in many domains. The task of detecting non-referential pronouns could also be incorporated into a part-of-speech tagger or a parser, or treated as an initial step in semantic interpretation. In this article, I describe a machine learning method for identifying non-referential pronouns in an annotated subsegment of the Penn Arabic Treebank using three different feature settings. I achieve an accuracy of 97.22% with 52 different features extracted from a small window size of -5/+5 tokens surrounding each potentially non-referential pronoun.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {5},
numpages = {11},
keywords = {Arabic pleonastic pronouns, Anaphora resolution, memory-based learning, Arabic expletive pronouns, Arabic non-referential pronouns}
}

@article{10.1145/1929908.1929910,
author = {Condon, S. and Parvaz, D. and Aberdeen, J. and Doran, C. and Freeman, A. and Awad, M.},
title = {Machine Translation Errors: English and Iraqi Arabic},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929910},
doi = {10.1145/1929908.1929910},
abstract = {Errors in machine translations of English-Iraqi Arabic dialogues were analyzed using the methods developed for the Human Translation Error Rate measure (HTER). Human annotations were used to refine the Translation Error Rate (TER) annotations. The analyses were performed on approximately 100 translations into each language from four translation systems. Results include high frequencies of pronoun errors and errors involving the copula in translations to English. High frequencies of errors in subject/person inflection and closed-word classes characterized translations to Iraqi Arabic. There were similar frequencies of word order errors in both translation directions and low frequencies of polarity errors. The problems associated with many errors can be predicted from structural differences between the two languages. Also problematic is the need to insert lexemes not present in the source or vice versa. Some problems associated with deictic elements like pronouns will require knowledge of the discourse context to resolve.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {2},
numpages = {19},
keywords = {evaluation, statistical machine translation, English, error analysis, Arabic}
}

@article{10.1145/1929908.1929909,
author = {Katz, Graham and Diab, Mona},
title = {Introduction to the Special Issue on Arabic Computational Linguistics},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929909},
doi = {10.1145/1929908.1929909},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {1},
numpages = {4}
}

@article{10.1145/1929908.1929911,
author = {Rytting, C. Anton and Zajic, David M. and Rodrigues, Paul and Wayland, Sarah C. and Hettick, Christian and Buckwalter, Tim and Blake, Charles C.},
title = {Spelling Correction for Dialectal Arabic Dictionary Lookup},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929911},
doi = {10.1145/1929908.1929911},
abstract = {The “Did You Mean...?” system, described in this article, is a spelling corrector for Arabic that is designed specifically for L2 learners of dialectal Arabic in the context of dictionary lookup. The authors use an orthographic density metric to motivate the need for a finer-grained ranking method for candidate words than unweighted Levenshtein edit distance. The Did You Mean...? architecture is described, and the authors show that mean reciprocal rank can be improved by tuning operation weights according to sound confusions, and by anticipating likely spelling variants.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {15},
keywords = {Spelling correction, Arabic dialects, weighted finite-state transducers, error correction for non-native language learners, Iraqi Arabic, dictionary lookup}
}

@article{10.1145/1929908.1929914,
author = {Wu, Chung-Hsien and Liang, Wei-Bin and Yeh, Jui-Feng},
title = {Interruption Point Detection of Spontaneous Speech Using Inter-Syllable Boundary-Based Prosodic Features},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929914},
doi = {10.1145/1929908.1929914},
abstract = {This article presents a probabilistic scheme for detecting the interruption point (IP) in spontaneous speech based on inter-syllable boundary-based prosodic features. Because of the high error rate in spontaneous speech recognition, a combined acoustic model considering both syllable and subsyllable recognition units, is firstly used to determine the inter-syllable boundaries and output the recognition confidence of the input speech. Based on the finding that IPs always occur at inter-syllable boundaries, a probability distribution of the prosodic features at the current potential IP is estimated. The Conditional Random Field (CRF) model, which employs the clustered prosodic features of the current potential IP and its preceding and succeeding inter-syllable boundaries, is employed to output the IP likelihood measure. Finally, the confidence of the recognized speech, the probability distribution of the prosodic features and the CRF-based IP likelihood measure are integrated to determine the optimal IP sequence of the input spontaneous speech. In addition, pitch reset and lengthening are also applied to improve the IP detection performance. The Mandarin Conversional Dialogue Corpus is adopted for evaluation. Experimental results show that the proposed IP detection approach obtains 10.56% and 6.5% more effective results than the hidden Markov model and the Maximum Entropy model respectively under the same experimental conditions. Besides, the IP detection error rate can be further reduced by 9.15% using pitch reset and lengthening information. The experimental results confirm that the proposed model based on inter-syllable boundary-based prosodic features can effectively detect the interruption point in spontaneous Mandarin speech.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {6},
numpages = {21},
keywords = {disfluency, conditional random field, feature clustering, Interruption point detection, prosodic feature}
}

@article{10.1145/1929908.1929912,
author = {Kulick, Seth},
title = {Exploiting Separation of Closed-Class Categories for Arabic Tokenization and Part-of-Speech Tagging},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1929908.1929912},
doi = {10.1145/1929908.1929912},
abstract = {Research on the problem of morphological disambiguation of Arabic has noted that techniques developed for lexical disambiguation in English do not easily transfer over, since the affixation present in Arabic creates a very different tag set than for English, encoding both inflectional morphology and more complex tokenization sequences. This work takes a new approach to this problem based on a distinction between the open-class and closed-class categories of tokens, which differ both in their frequencies and in their possible morphological affixations. This separation simplifies the morphological analysis problem considerably, making it possible to use a Conditional Random Field model for joint tokenization and “core” part-of-speech tagging of the open-class items, while the closed-class items are handled by regular expressions. This work is therefore situated between data-driven approaches and those that use a morphological analyzer. For the tasks of tokenization and core part-of-speech tagging, the resulting system outperforms, on the given test set, a system that incorporates a morphological analyzer. We also evaluate the effects of the differences on parser performance when the tagger output is used for parser input.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {4},
numpages = {18},
keywords = {morphological analysis, Arabic}
}

@article{10.1145/1838751.1838754,
author = {Mukund, Smruthi and Srihari, Rohini and Peterson, Erik},
title = {An Information-Extraction System for Urdu---A Resource-Poor Language},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838751.1838754},
doi = {10.1145/1838751.1838754},
abstract = {There has been an increase in the amount of multilingual text on the Internet due to the proliferation of news sources and blogs. The Urdu language, in particular, has experienced explosive growth on the Web. Text mining for information discovery, which includes tasks such as identifying topics, relationships and events, and sentiment analysis, requires sophisticated natural language processing (NLP). NLP systems begin with modules such as word segmentation, part-of-speech tagging, and morphological analysis and progress to modules such as shallow parsing and named entity tagging. While there have been considerable advances in developing such comprehensive NLP systems for English, the work for Urdu is still in its infancy. The tasks of interest in Urdu NLP includes analyzing data sources such as blogs and comments to news articles to provide insight into social and human behavior. All of this requires a robust NLP system. The objective of this work is to develop an NLP infrastructure for Urdu that is customizable and capable of providing basic analysis on which more advanced information extraction tools can be built. This system assimilates resources from various online sources to facilitate improved named entity tagging and Urdu-to-English transliteration. The annotated data required to train the learning models used here is acquired by standardizing the currently limited resources available for Urdu. Techniques such as bootstrap learning and resource sharing from a syntactically similar language, Hindi, are explored to augment the available annotated Urdu data. Each of the new Urdu text processing modules has been integrated into a general text-mining platform. The evaluations performed demonstrate that the accuracies have either met or exceeded the state of the art.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {15},
numpages = {43},
keywords = {shallow parsing, Urdu natural language processing, part of speech tagging, transliterations, named entity tagging, text mining, bootstrap learning}
}

@article{10.1145/1838751.1838753,
author = {Chinnakotla, Manoj K. and Damani, Om P. and Satoskar, Avijit},
title = {Transliteration for Resource-Scarce Languages},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838751.1838753},
doi = {10.1145/1838751.1838753},
abstract = {Today, parallel corpus-based systems dominate the transliteration landscape. But the resource-scarce languages do not enjoy the luxury of large parallel transliteration corpus. For these languages, rule-based transliteration is the only viable option. In this article, we show that by properly harnessing the monolingual resources in conjunction with manually created rule base, one can achieve reasonable transliteration performance. We achieve this performance by exploiting the power of Character Sequence Modeling (CSM), which requires only monolingual resources. We present the results of our rule-based system for Hindi to English, English to Hindi, and Persian to English transliteration tasks. We also perform extrinsic evaluation of transliteration systems in the context of Cross Lingual Information Retrieval. Another important contribution of our work is to explain the widely varying accuracy numbers reported in transliteration literature, in terms of the entropy of the language pairs and the datasets involved.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {14},
numpages = {30},
keywords = {resource-scarce languages, prefix-based partial match (PPM), Transliteration, cross entropy, character sequence modeling}
}

@article{10.1145/1838751.1838752,
author = {Kumaran, A. and Khapra, Mitesh M. and Bhattacharyya, Pushpak},
title = {Compositional Machine Transliteration},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838751.1838752},
doi = {10.1145/1838751.1838752},
abstract = {Machine transliteration is an important problem in an increasingly multilingual world, as it plays a critical role in many downstream applications, such as machine translation or crosslingual information retrieval systems. In this article, we propose compositional machine transliteration systems, where multiple transliteration components may be composed either to improve existing transliteration quality, or to enable transliteration functionality between languages even when no direct parallel names corpora exist between them. Specifically, we propose two distinct forms of composition: serial and parallel. Serial compositional system chains individual transliteration components, say, X → Y and Y → Z systems, to provide transliteration functionality, X → Z. In parallel composition evidence from multiple transliteration paths between X → Z are aggregated for improving the quality of a direct system. We demonstrate the functionality and performance benefits of the compositional methodology using a state-of-the-art machine transliteration framework in English and a set of Indian languages, namely, Hindi, Marathi, and Kannada. Finally, we underscore the utility and practicality of our compositional approach by showing that a CLIR system integrated with compositional transliteration systems performs consistently on par with, and sometimes better than, that integrated with a direct transliteration system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {13},
numpages = {29},
keywords = {multiple evidence, transliterability, Machine transliteration, crosslingual information retrieval, resource reusage, compositional machine transliteration}
}

@article{10.1145/1838745.1838750,
author = {Ng, Hwee Tou},
title = {The State of the Journal},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838745.1838750},
doi = {10.1145/1838745.1838750},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {8},
numpages = {1}
}

@article{10.1145/1838745.1838747,
author = {Majumder, Prasenjit and Mitra, Mandar and Pal, Dipasree and Bandyopadhyay, Ayan and Maiti, Samaresh and Pal, Sukomal and Modak, Deboshree and Sanyal, Sucharita},
title = {The FIRE 2008 Evaluation Exercise},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838745.1838747},
doi = {10.1145/1838745.1838747},
abstract = {The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create an evaluation framework in the spirit of TREC (Text REtrieval Conference), CLEF (Cross-Language Evaluation Forum), and NTCIR (NII Test Collection for IR Systems), for Indian language Information Retrieval. The first evaluation exercise conducted by FIRE was completed in 2008. This article describes the test collections used at FIRE 2008, summarizes the approaches adopted by various participants, discusses the limitations of the datasets, and outlines the tasks planned for the next iteration of FIRE.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {10},
numpages = {24},
keywords = {information retrieval, Indian languages, evaluation}
}

@article{10.1145/1838745.1838746,
author = {Harman, Donna and Kando, Noriko and Majumder, Prasenjit and Mitra, Mandar and Peters, Carol},
title = {Introduction to the Special Issue on Indian Language Information Retrieval Part I},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838745.1838746},
doi = {10.1145/1838745.1838746},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {9},
numpages = {3}
}

@article{10.1145/1838745.1838748,
author = {Dolamic, Ljiljana and Savoy, Jacques},
title = {Comparative Study of Indexing and Search Strategies for the Hindi, Marathi, and Bengali Languages},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838745.1838748},
doi = {10.1145/1838745.1838748},
abstract = {The main goal of this article is to describe and evaluate various indexing and search strategies for the Hindi, Bengali, and Marathi languages. These three languages are ranked among the world’s 20 most spoken languages and they share similar syntax, morphology, and writing systems. In this article we examine these languages from an Information Retrieval (IR) perspective through describing the key elements of their inflectional and derivational morphologies, and suggest a light and more aggressive stemming approach based on them.In our evaluation of these stemming strategies we make use of the FIRE 2008 test collections, and then to broaden our comparisons we implement and evaluate two language independent indexing methods: the n-gram and trunc-n (truncation of the first n letters). We evaluate these solutions by applying our various IR models, including the Okapi, Divergence from Randomness (DFR) and statistical language models (LM) together with two classical vector-space approaches: tf idf and Lnu-ltc.Experiments performed with all three languages demonstrate that the I(ne)C2 model derived from the Divergence from Randomness paradigm tends to provide the best mean average precision (MAP). Our own tests suggest that improved retrieval effectiveness would be obtained by applying more aggressive stemmers, especially those accounting for certain derivational suffixes, compared to those involving a light stemmer or ignoring this type of word normalization procedure. Comparisons between no stemming and stemming indexing schemes shows that performance differences are almost always statistically significant. When, for example, an aggressive stemmer is applied, the relative improvements obtained are ~28% for the Hindi language, ~42% for Marathi, and ~18% for Bengali, as compared to a no-stemming approach. Based on a comparison of word-based and language-independent approaches we find that the trunc-4 indexing scheme tends to result in performance levels statistically similar to those of an aggressive stemmer, yet better than the 4-gram indexing scheme. A query-by-query analysis reveals the reasons for this, and also demonstrates the advantage of applying a stemming or a trunc-4 indexing scheme.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {11},
numpages = {24},
keywords = {search engines for Asian languages, Indic languages, natural language processing with Indo-European languages, Hindi language, stemmer, Marathi language, Bengali language}
}

@article{10.1145/1838745.1838749,
author = {Leveling, Johannes and Jones, Gareth J. F.},
title = {Sub-Word Indexing and Blind Relevance Feedback for English, Bengali, Hindi, and Marathi IR},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1838745.1838749},
doi = {10.1145/1838745.1838749},
abstract = {The Forum for Information Retrieval Evaluation (FIRE) provides document collections, topics, and relevance assessments for information retrieval (IR) experiments on Indian languages. Several research questions are explored in this article: 1) How to create create a simple, language-independent corpus-based stemmer, 2) How to identify sub-words and which types of sub-words are suitable as indexing units, and 3) How to apply blind relevance feedback on sub-words and how feedback term selection is affected by the type of the indexing unit. More than 140 IR experiments are conducted using the BM25 retrieval model on the topic titles and descriptions (TD) for the FIRE 2008 English, Bengali, Hindi, and Marathi document collections.The major findings are: The corpus-based stemming approach is effective as a knowledge-light term conflation step and useful in the case of few language-specific resources. For English, the corpus-based stemmer performs nearly as well as the Porter stemmer and significantly better than the baseline of indexing words when combined with query expansion. In combination with blind relevance feedback, it also performs significantly better than the baseline for Bengali and Marathi IR.Sub-words such as consonant-vowel sequences and word prefixes can yield similar or better performance in comparison to word indexing. There is no best performing method for all languages. For English, indexing using the Porter stemmer performs best, for Bengali and Marathi, overlapping 3-grams obtain the best result, and for Hindi, 4-prefixes yield the highest MAP. However, in combination with blind relevance feedback using 10 documents and 20 terms, 6-prefixes for English and 4-prefixes for Bengali, Hindi, and Marathi IR yield the highest MAP.Sub-word identification is a general case of decompounding. It results in one or more index terms for a single word form and increases the number of index terms but decreases their average length. The corresponding retrieval experiments show that relevance feedback on sub-words benefits from selecting a larger number of index terms in comparison with retrieval on word forms. Similarly, selecting the number of relevance feedback terms depending on the ratio of word vocabulary size to sub-word vocabulary size almost always slightly increases information retrieval effectiveness compared to using a fixed number of terms for different languages.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
articleno = {12},
numpages = {30},
keywords = {blind relevance feedback, FIRE, evaluation, stemming, Information retrieval, sub-word indexing}
}

@article{10.1145/1781134.1781137,
author = {Naptali, Welly and Tsuchiya, Masatoshi and Nakagawa, Seiichi},
title = {Topic-Dependent Language Model with Voting on Noun History},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1781134.1781137},
doi = {10.1145/1781134.1781137},
abstract = {Language models (LMs) are an important field of study in automatic speech recognition (ASR) systems. LM helps acoustic models find the corresponding word sequence of a given speech signal. Without it, ASR systems would not understand the language and it would be hard to find the correct word sequence. During the past few years, researchers have tried to incorporate long-range dependencies into statistical word-based n-gram LMs. One of these long-range dependencies is topic. Unlike words, topic is unobservable. Thus, it is required to find the meanings behind the words to get into the topic. This research is based on the belief that nouns contain topic information. We propose a new approach for a topic-dependent LM, where the topic is decided in an unsupervised manner. Latent Semantic Analysis (LSA) is employed to reveal hidden (latent) relations among nouns in the context words. To decide the topic of an event, a fixed size word history sequence (window) is observed, and voting is then carried out based on noun class occurrences weighted by a confidence measure. Experiments were conducted on an English corpus and a Japanese corpus: The Wall Street Journal corpus and Mainichi Shimbun (Japanese newspaper) corpus. The results show that our proposed method gives better perplexity than the comparative baselines, including a word-based/class-based n-gram LM, their interpolated LM, a cache-based LM, a topic-dependent LM based on n-gram, and a topic-dependent LM based on Latent Dirichlet Allocation (LDA). The n-best list rescoring was conducted to validate its application in ASR systems.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {7},
numpages = {31},
keywords = {latent semantic analysis, speech recognition, Language model, perplexity, topic dependent}
}

@article{10.1145/1781134.1781136,
author = {Guo, Yuqing and Wang, Haifeng and van Genabith, Josef},
title = {A Linguistically Inspired Statistical Model for Chinese Punctuation Generation},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1781134.1781136},
doi = {10.1145/1781134.1781136},
abstract = {This article investigates a relatively underdeveloped subject in natural language processing---the generation of punctuation marks. From a theoretical perspective, we study 16 Chinese punctuation marks as defined in the Chinese national standard of punctuation usage, and categorize these punctuation marks into three different types according to their syntactic properties. We implement a three-tier maximum entropy model incorporating linguistically-motivated features for generating the commonly used Chinese punctuation marks in unpunctuated sentences output by a surface realizer. Furthermore, we present a method to automatically extract cue words indicating sentence-final punctuation marks as a specialized feature to construct a more precise model. Evaluating on the Penn Chinese Treebank data, the MaxEnt model achieves an f-score of 79.83% for punctuation insertion and 74.61% for punctuation restoration using gold data input, 79.50% for insertion and 73.32% for restoration using parser-based imperfect input. The experiments show that the MaxEnt model significantly outperforms a baseline 5-gram language model that scores 54.99% for punctuation insertion and 52.01% for restoration. We show that our results are not far from human performance on the same task with human insertion f-scores in the range of 81-87% and human restoration in the range of 71-82%. Finally, a manual error analysis of the generation output shows that close to 40% of the mismatched punctuation marks do in fact result in acceptable choices, a fact obscured in the automatic string-matching based evaluation scores.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {6},
numpages = {27},
keywords = {Chinese punctuation marks, sentence realization, maximum entropy model}
}

@article{10.1145/1781134.1781135,
author = {Zhao, Hai and Huang, Chang-Ning and Li, Mu and Lu, Bao-Liang},
title = {A Unified Character-Based Tagging Framework for Chinese Word Segmentation},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1781134.1781135},
doi = {10.1145/1781134.1781135},
abstract = {Chinese word segmentation is an active area in Chinese language processing though it is suffering from the argument about what precisely is a word in Chinese. Based on corpus-based segmentation standard, we launched this study. In detail, we regard Chinese word segmentation as a character-based tagging problem. We show that there has been a potent trend of using a character-based tagging approach in this field. In particular, learning from segmented corpus with or without additional linguistic resources is treated in a unified way in which the only difference depends on how the feature template set is selected. It differs from existing work in that both feature template selection and tag set selection are considered in our approach, instead of the previous feature template focus only technique. We show that there is a significant performance difference as different tag sets are selected. This is especially applied to a six-tag set, which is good enough for most current segmented corpora. The linguistic meaning of a tag set is also discussed. Our results show that a simple learning system with six n-gram feature templates and a six-tag set can obtain competitive performance in the cases of learning only from a training corpus. In cases when additional linguistic resources are available, an ensemble learning technique, assistant segmenter, is proposed and its effectiveness is verified. Assistant segmenter is also proven to be an effective method as segmentation standard adaptation that outperforms existing ones. Based on the proposed approach, our system provides state-of-the-art performance in all 12 corpora of three international Chinese word segmentation bakeoffs.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {5},
numpages = {32},
keywords = {assistant segmenter, Chinese word segmentation, tag set selection, character-based tagging method, conditional random field}
}

@article{10.1145/1731035.1731039,
author = {Baldwin, Timothy and Kim, Sunam and Bond, Francis and Fujita, Sanae and Martinez, David and Tanaka, Takaaki},
title = {A Reexamination of MRD-Based Word Sense Disambiguation},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1731035.1731039},
doi = {10.1145/1731035.1731039},
abstract = {This article reconsiders the task of MRD-based word sense disambiguation, in extending the basic Lesk algorithm to investigate the impact on WSD performance of different tokenization schemes and methods of definition extension. In experimentation over the Hinoki Sensebank and the Japanese Senseval-2 dictionary task, we demonstrate that sense-sensitive definition extension over hyponyms, hypernyms, and synonyms, combined with definition extension and word tokenization leads to WSD accuracy above both unsupervised and supervised baselines. In doing so, we demonstrate the utility of ontology induction and establish new opportunities for the development of baseline unsupervised WSD methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {4},
numpages = {21},
keywords = {word sense disambiguation, Japanese, machine-readable dictionary}
}

@article{10.1145/1731035.1731038,
author = {Tepper, Michael and Xia, Fei},
title = {Inducing Morphemes Using Light Knowledge},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1731035.1731038},
doi = {10.1145/1731035.1731038},
abstract = {Allomorphic variation, or form variation among morphs with the same meaning, is a stumbling block to morphological induction (MI). To address this problem, we present a hybrid approach that uses a small amount of linguistic knowledge in the form of orthographic rewrite rules to help refine an existing MI-produced segmentation. Using rules, we derive underlying analyses of morphs---generalized with respect to contextual spelling differences---from an existing surface morph segmentation, and from these we learn a morpheme-level segmentation. To learn morphemes, we have extended the Morfessor segmentation algorithm [Creutz and Lagus 2004; 2005; 2006] by using rules to infer possible underlying analyses from surface segmentations. A segmentation produced by Morfessor Categories-MAP Software v. 0.9.2 is used as input to our procedure and as a baseline that we evaluate against. To suggest analyses for our procedure, a set of language-specific orthographic rules is needed. Our procedure has yielded promising improvements for English and Turkish over the baseline approach when tested on the Morpho Challenge 2005 and 2007 style evaluations. On the Morpho Challenge 2007 test evaluation, we report gains over the current best unsupervised contestant for Turkish, where our technique shows a 2.5% absolute F-score improvement.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {38},
keywords = {Morphological induction, allomorphy, computational linguistics, machine learning}
}

@article{10.1145/1731035.1731037,
author = {Liu, Feifan and Liu, Yang},
title = {Identification of Soundbite and Its Speaker Name Using Transcripts of Broadcast News Speech},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1731035.1731037},
doi = {10.1145/1731035.1731037},
abstract = {This article presents a pipeline framework for identifying soundbite and its speaker name from Mandarin broadcast news transcripts. Both of the two modules, soundbite segment detection and soundbite speaker name recognition, are based on a supervised classification approach using multiple linguistic features. We systematically evaluated performance for each module as well as the entire system, and investigated the effect of using speech recognition (ASR) output and automatic sentence segmentation. We found that both of the two components impact the pipeline system, with more degradation in the entire system performance due to automatic speaker name recognition errors than soundbite segment detection. In addition, our experimental results show that using ASR output degrades the system performance significantly, and that using automatic sentence segmentation greatly impacts soundbite detection, but has much less effect on speaker name recognition.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {2},
numpages = {19},
keywords = {speaker name recognition, Soundbite detection, sentence segmentation, automatic speech recognition}
}

@article{10.1145/1731035.1731036,
author = {Hsu, Chung-Chian and Chen, Chien-Hsing},
title = {Mining Synonymous Transliterations from the World Wide Web},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1731035.1731036},
doi = {10.1145/1731035.1731036},
abstract = {The World Wide Web has been considered one of the important sources for information. Using search engines to retrieve Web pages can gather lots of information, including foreign information. However, to be better understood by local readers, proper names in a foreign language, such as English, are often transliterated to a local language such as Chinese. Due to different translators and the lack of translation standard, translating foreign proper nouns may result in different transliterations and pose a notorious headache. In particular, it may cause incomplete search results. Using one transliteration as a query keyword will fail to retrieve the Web pages which use a different word as the transliteration. Consequently, important information may be missed. We present a framework for mining synonymous transliterations as many as possible from the Web for a given transliteration. The results can be used to construct a database of synonymous transliterations which can be utilized for query expansion so as to alleviate the incomplete search problem. Experimental results show that the proposed framework can effectively retrieve the set of snippets which may contain synonymous transliterations and then extract the target terms. Most of the extracted synonymous transliterations have higher rank of similarity to the input transliteration compared to other noise terms.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {1},
numpages = {28},
keywords = {synonymous transliteration, cross-lingual information retrieval, text mining, Chinese transliteration, Web mining}
}

@article{10.1145/1644879.1644886,
author = {Moisl, Hermann},
title = {Sura Length and Lexical Probability Estimation in Cluster Analysis of the Qur’An},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644886},
doi = {10.1145/1644879.1644886},
abstract = {Thabet [2005] applied cluster analysis to the Qur’an in the hope of generating a classification of the (suras) that is useful for understanding of its thematic structure. The result was positive, but variation in (sura) length was a problem because clustering of the shorter was found to be unreliable. The present discussion addresses this problem in four parts. The first part summarizes Thabet’s work. The second part argues that unreliable clustering of the shorter is a consequence of poor estimation of lexical population probabilities in those. The third part proposes a solution to the problem based on calculation of a minimum length threshold using concepts from statistical sampling theory followed by selection of and lexical variables based on that threshold. The fourth part applies the proposed solution to a reanalysis of the Qur’an.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {19},
numpages = {19},
keywords = {lexical probability estimation, sampling, document length normalization, Arabic natural language processing, Qur’an, cluster analysis}
}

@article{10.1145/1644879.1644882,
author = {Espa\~{n}a-Bonet, Cristina and Gim\'{e}nez, Jes\'{u}s and M\`{a}rquez, Llu\'{\i}s},
title = {Discriminative Phrase-Based Models for Arabic Machine Translation},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644882},
doi = {10.1145/1644879.1644882},
abstract = {A design for an Arabic-to-English translation system is presented. The core of the system implements a standard phrase-based statistical machine translation architecture, but it is extended by incorporating a local discriminative phrase selection model to address the semantic ambiguity of Arabic. Local classifiers are trained using linguistic information and context to translate a phrase, and this significantly increases the accuracy in phrase selection with respect to the most frequent translation traditionally considered. These classifiers are integrated into the translation system so that the global task gets benefits from the discriminative learning. As a result, we obtain significant improvements in the full translation task at the lexical, syntactic, and semantic levels as measured by an heterogeneous set of automatic evaluation metrics.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {15},
numpages = {20},
keywords = {discriminative learning, Arabic, statistical machine translation, English}
}

@article{10.1145/1644879.1644884,
author = {Zitouni, Imed and Florian, Radu},
title = {Cross-Language Information Propagation for Arabic Mention Detection},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644884},
doi = {10.1145/1644879.1644884},
abstract = {In the last two decades, significant effort has been put into annotating linguistic resources in several languages. Despite this valiant effort, there are still many languages left that have only small amounts of such resources. The goal of this article is to present and investigate a method of propagating information (specifically mention detection) from a resource-rich language into a relatively resource-poor language such as Arabic. Part of the investigation is to quantify the contribution of propagating information in different conditions based on the availability of resources in the target language. Experiments on the language pair Arabic-English show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into Arabic alone (no resources or models in the source language Arabic). Furthermore, results show that propagated features from English do help improve the Arabic system performance even when used in conjunction with all feature types built from the source language. Experiments also show that using propagated features in conjunction with lexically derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available. In addition to Arabic-English language pair, we investigate the effectiveness of our approach on other language pairs such as Chinese-English and Spanish-English.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {17},
numpages = {21},
keywords = {Arabic mention detection, Arabic information extraction}
}

@article{10.1145/1644879.1644883,
author = {Benajiba, Yassine and Zitouni, Imed},
title = {Morphology-Based Segmentation Combination for Arabic Mention Detection},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644883},
doi = {10.1145/1644879.1644883},
abstract = {The Arabic language has a very rich/complex morphology. Each Arabic word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparse compared to other languages such as English, and it is necessary to conduct word segmentation before any natural language processing task. Therefore, the word-segmentation step is worth a deeper study since it is a preprocessing step which shall have a significant impact on all the steps coming afterward. In this article, we present an Arabic mention detection system that has very competitive results in the recent Automatic Content Extraction (ACE) evaluation campaign. We investigate the impact of different segmentation schemes on Arabic mention detection systems and we show how these systems may benefit from more than one segmentation scheme. We report the performance of several mention detection models using different kinds of possible and known segmentation schemes for Arabic text: punctuation separation, Arabic Treebank, and morphological and character-level segmentations. We show that the combination of competitive segmentation styles leads to a better performance. Results indicate a statistically significant improvement when Arabic Treebank and morphological segmentations are combined.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {16},
numpages = {18},
keywords = {Arabic information extraction, Arabic mention detection, Arabic segmentation}
}

@article{10.1145/1644879.1644880,
author = {Shaalan, K. and Farghaly, A.},
title = {Introduction to the Special Issue on Arabic Natural Language Processing},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644880},
doi = {10.1145/1644879.1644880},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {13},
numpages = {3}
}

@article{10.1145/1644879.1644885,
author = {Lamel, Lori and Messaoudi, Abdelkhalek and Gauvain, Jean-Luc},
title = {Automatic Speech-to-Text Transcription in Arabic},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644885},
doi = {10.1145/1644879.1644885},
abstract = {The Arabic language presents a number of challenges for speech recognition, arising in part from the significant differences in the spoken and written forms, in particular the conventional form of texts being non-vowelized. Being a highly inflected language, the Arabic language has a very large lexical variety and typically with several possible (generally semantically linked) vowelizations for each written form. This article summarizes research carried out over the last few years on speech-to-text transcription of broadcast data in Arabic. The initial research was oriented toward processing of broadcast news data in Modern Standard Arabic, and has since been extended to address a larger variety of broadcast data, which as a consequence results in the need to also be able to handle dialectal speech. While standard techniques in speech recognition have been shown to apply well to the Arabic language, taking into account language specificities help to significantly improve system performance.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {18},
numpages = {18},
keywords = {speech-to-text transcription, speech processing, Arabic language processing, automatic speech recognition, mophological decomposition}
}

@article{10.1145/1644879.1644881,
author = {Farghaly, Ali and Shaalan, Khaled},
title = {Arabic Natural Language Processing: Challenges and Solutions},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644881},
doi = {10.1145/1644879.1644881},
abstract = {The Arabic language presents researchers and developers of natural language processing (NLP) applications for Arabic text and speech with serious challenges. The purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of Arabic natural language processing (ANLP). We begin with general features of the Arabic language in Sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. In Section 1 of this article we highlight the significance of the Arabic language today and describe its general properties. Section 2 presents the feature of Arabic Diglossia showing how the sociolinguistic aspects of the Arabic language differ from other languages. The stability of Arabic Diglossia and its implications for ANLP applications are discussed and ways to deal with this problematic property are proposed. Section 3 deals with the properties of the Arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary Arabic texts. We present in Section 4 specific features of the Arabic language such as the nonconcatenative property of Arabic morphology, Arabic as an agglutinative language, Arabic as a pro-drop language, and the challenge these properties pose to ANLP. We also present solutions that have already been adopted by some pioneering researchers in the field. In Section 5 we point out to the lack of formal and explicit grammars of Modern Standard Arabic which impedes the progress of more advanced ANLP systems. In Section 6 we draw our conclusion.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {14},
numpages = {22},
keywords = {Arabic dialects, Arabic script, Modern Standard Arabic}
}

@article{10.1145/1568292.1568293,
author = {Chen, Wenliang and Kawahara, Daisuke and Uchimoto, Kiyotaka and Zhang, Yujie and Isahara, Hitoshi},
title = {Using Short Dependency Relations from Auto-Parsed Data for Chinese Dependency Parsing},
year = {2009},
issue_date = {August 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1568292.1568293},
doi = {10.1145/1568292.1568293},
abstract = {Dependency parsing has become increasingly popular for a surge of interest lately for applications such as machine translation and question answering. Currently, several supervised learning methods can be used for training high-performance dependency parsers if sufficient labeled data are available.However, currently used statistical dependency parsers provide poor results for words separated by long distances. In order to solve this problem, this article presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. The unlabeled data is automatically parsed by using a deterministic dependency parser, which exhibits a relatively high performance for short dependencies between words. We then train another parser that uses the information on short dependency relations extracted from the output of the first parser. The proposed approach achieves an unlabeled attachment score of 86.52%, an absolute 1.24% improvement over the baseline system on the Chinese Treebank data set. The results indicate that the proposed approach improves the parsing performance for longer distance words.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {10},
numpages = {20},
keywords = {Chinese dependency parsing, semi-supervised learning, unlabeled data}
}

@article{10.1145/1568292.1568295,
author = {Nguyen, Cam-Tu and Phan, Xuan-Hieu and Horiguchi, Susumu and Nguyen, Thu-Trang and Ha, Quang-Thuy},
title = {Web Search Clustering and Labeling with Hidden Topics},
year = {2009},
issue_date = {August 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1568292.1568295},
doi = {10.1145/1568292.1568295},
abstract = {Web search clustering is a solution to reorganize search results (also called “snippets”) in a more convenient way for browsing. There are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole Web page.This article introduces a novel framework for clustering Web search results in Vietnamese which targets the three above issues. The main motivation is that by enriching short snippets with hidden topics from huge resources of documents on the Internet, it is able to cluster and label such snippets effectively in a topic-oriented manner without concerning whole Web pages. Our approach is based on recent successful topic analysis models, such as Probabilistic-Latent Semantic Analysis, or Latent Dirichlet Allocation. The underlying idea of the framework is that we collect a very large external data collection called “universal dataset,” and then build a clustering system on both the original snippets and a rich set of hidden topics discovered from the universal data collection. This can be seen as a richer representation of snippets to be clustered. We carry out careful evaluation of our method and show that our method can yield impressive clustering quality.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {12},
numpages = {40},
keywords = {Vietnamese, Latent Dirichlet allocation, Hierarchical Agglomerative Clustering, hidden topics analysis, collocation, cluster labeling, Web search clustering}
}

@article{10.1145/1568292.1568294,
author = {Chanda, Sukalpa and Pal, Umapada and Terrades, Oriol Ramos},
title = {Word-Wise Thai and Roman Script Identification},
year = {2009},
issue_date = {August 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1568292.1568294},
doi = {10.1145/1568292.1568294},
abstract = {In some Thai documents, a single text line of a printed document page may contain words of both Thai and Roman scripts. For the Optical Character Recognition (OCR) of such a document page it is better to identify, at first, Thai and Roman script portions and then to use individual OCR systems of the respective scripts on these identified portions. In this article, an SVM-based method is proposed for identification of word-wise printed Roman and Thai scripts from a single line of a document page. Here, at first, the document is segmented into lines and then lines are segmented into character groups (words). In the proposed scheme, we identify the script of a character group combining different character features obtained from structural shape, profile behavior, component overlapping information, topological properties, and water reservoir concept, etc. Based on the experiment on 10,000 data (words) we obtained 99.62% script identification accuracy from the proposed scheme.},
journal = {ACM Transactions on Asian Language Information Processing},
month = aug,
articleno = {11},
numpages = {21},
keywords = {script identification, Multi-script OCR, SVM, Thai Script}
}

@article{10.1145/1526252.1526254,
author = {He, Xiaodong and Yang, Mei and Gao, Jianfeng and Nguyen, Patrick and Moore, Robert},
title = {Improved Monolingual Hypothesis Alignment for Machine Translation System Combination},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1526252.1526254},
doi = {10.1145/1526252.1526254},
abstract = {This article presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art, TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = may,
articleno = {6},
numpages = {19},
keywords = {Statistical machine translation, word alignment, hidden Markov model, system combination}
}

@article{10.1145/1526252.1526256,
author = {Venkatapathy, Sriram and Bangalore, Srinivas},
title = {Discriminative Machine Translation Using Global Lexical Selection},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1526252.1526256},
doi = {10.1145/1526252.1526256},
abstract = {Statistical phrase-based machine translation models crucially rely on word alignments. The search for word-alignments assumes a model of word locality between source and target languages that is violated in starkly different word-order languages such as English-Hindi. In this article, we present models that decouple the steps of lexical selection and lexical reordering with the aim of minimizing the role of word-alignment in machine translation. Indian languages are morphologically rich and have relatively free-word order where the grammatical role of content words is largely determined by their case markers and not just by their positions in the sentence. Hence, lexical selection plays a far greater role than lexical reordering. For lexical selection, we investigate models that take the entire source sentence into account and evaluate their performance for English-Hindi translation in a tourism domain.},
journal = {ACM Transactions on Asian Language Information Processing},
month = may,
articleno = {8},
numpages = {23},
keywords = {Global lexical selection, machine translation}
}

@article{10.1145/1526252.1526257,
author = {Tsunakawa, Takashi and Okazaki, Naoaki and Liu, Xiao and Tsujii, Jun’ichi},
title = {A Chinese-Japanese Lexical Machine Translation through a Pivot Language},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1526252.1526257},
doi = {10.1145/1526252.1526257},
abstract = {The bilingual lexicon is an expensive but critical resource for multilingual applications in natural language processing. This article proposes an integrated framework for building a bilingual lexicon between the Chinese and Japanese languages. Since the language pair Chinese-Japanese does not include English, which is a central language of the world, few large-scale bilingual resources between Chinese and Japanese have been constructed. One solution to alleviate this problem is to build a Chinese-Japanese bilingual lexicon through English as the pivot language. In addition to the pivotal approach, we can make use of the characteristics of Chinese and Japanese languages that use Han characters. We incorporate a translation model obtained from a small Chinese-Japanese lexicon and use the similarity of the hanzi and kanji characters by using the log-linear model. Our experimental results show that the use of the pivotal approach can improve the translation performance over the translation model built from a small Chinese-Japanese lexicon. The results also demonstrate that the similarity between the hanzi and kanji characters provides a positive effect for translating technical terms.},
journal = {ACM Transactions on Asian Language Information Processing},
month = may,
articleno = {9},
numpages = {21},
keywords = {kanji, Bilingual lexicon, pivot language, hanzi, Han characters, statistical machine translation}
}

@article{10.1145/1526252.1526255,
author = {Ma, Yanjun and Way, Andy},
title = {Bilingually Motivated Word Segmentation for Statistical Machine Translation},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1526252.1526255},
doi = {10.1145/1526252.1526255},
abstract = {We introduce a bilingually motivated word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT). Our approach is motivated from the insight that PB-SMT systems can be improved by optimizing the input representation to reduce the predictive power of translation models. We firstly present an approach to optimize the existing segmentation of both source and target languages for PB-SMT and demonstrate the effectiveness of this approach using a Chinese--English MT task, that is, to measure the influence of the segmentation on the performance of PB-SMT systems. We report a 5.44% relative increase in Bleu score and a consistent increase according to other metrics. We then generalize this method for Chinese word segmentation without relying on any segmenters and show that using our segmentation PB-SMT can achieve more consistent state-of-the-art performance across two domains. There are two main advantages of our approach. First of all, it is adapted to the specific translation task at hand by taking the corresponding source (target) language into account. Second, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains.},
journal = {ACM Transactions on Asian Language Information Processing},
month = may,
articleno = {7},
numpages = {24},
keywords = {Alignment, word segmentation, bilingually motivated, phrase-based statistical machine translation}
}

@article{10.1145/1526252.1526253,
author = {Chiang, David and Koehn, Philipp},
title = {Introduction to the Special Issue on Machine Translation of Asian Languages},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1526252.1526253},
doi = {10.1145/1526252.1526253},
journal = {ACM Transactions on Asian Language Information Processing},
month = may,
articleno = {5},
numpages = {2}
}

@article{10.1145/1482343.1482347,
author = {Chen, Boxing and Zhang, Min and Aw, Ai Ti},
title = {Two-Stage Hypotheses Generation for Spoken Language Translation},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1482343.1482347},
doi = {10.1145/1482343.1482347},
abstract = {Spoken Language Translation (SLT) is the research area that focuses on the translation of speech or text between two spoken languages. Phrase-based and syntax-based methods represent the state-of-the-art for statistical machine translation (SMT). The phrase-based method specializes in modeling local reorderings and translations of multiword expressions. The syntax-based method is enhanced by using syntactic knowledge, which can better model long word reorderings, discontinuous phrases, and syntactic structure. In this article, we leverage on the strength of these two methods and propose a strategy based on multiple hypotheses generation in a two-stage framework for spoken language translation. The hypotheses are generated in two stages, namely, decoding and regeneration. In the decoding stage, we apply state-of-the-art, phrase-based, and syntax-based methods to generate basic translation hypotheses. Then in the regeneration stage, much more hypotheses that cannot be captured by the decoding algorithms are produced from the basic hypotheses. We study three regeneration methods: redecoding, n-gram expansion, and confusion network in the second stage. Finally, an additional reranking pass is introduced to select the translation outputs by a linear combination of rescoring models. Experimental results on the Chinese-to-English IWSLT-2006 challenge task of translating the transcription of spontaneous speech show that the proposed mechanism achieves significant improvements over the baseline of about 2.80 BLEU-score.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {4},
numpages = {22},
keywords = {Spoken language translation, hypotheses generation, statistical machine translation}
}

@article{10.1145/1482343.1482344,
author = {Wu, Chung-Hsien and Li, Haizhou},
title = {Introduction to the Special Issue on Recent Advances in Asian Language Spoken Document Retrieval},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1482343.1482344},
doi = {10.1145/1482343.1482344},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {1},
numpages = {3}
}

@article{10.1145/1482343.1482346,
author = {Lin, Shih-Hsiang and Chen, Berlin and Wang, Hsin-Min},
title = {A Comparative Study of Probabilistic Ranking Models for Chinese Spoken Document Summarization},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1482343.1482346},
doi = {10.1145/1482343.1482346},
abstract = {Extractive document summarization automatically selects a number of indicative sentences, passages, or paragraphs from an original document according to a target summarization ratio, and sequences them to form a concise summary. In this article, we present a comparative study of various probabilistic ranking models for spoken document summarization, including supervised classification-based summarizers and unsupervised probabilistic generative summarizers. We also investigate the use of unsupervised summarizers to improve the performance of supervised summarizers when manual labels are not available for training the latter. A novel training data selection approach that leverages the relevance information of spoken sentences to select reliable document-summary pairs derived by the probabilistic generative summarizers is explored for training the classification-based summarizers. Encouraging initial results on Mandarin Chinese broadcast news data are demonstrated.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {23},
keywords = {relevance information, Spoken document summarization, probabilistic ranking models, extractive summarization}
}

@article{10.1145/1482343.1482345,
author = {Chen, Berlin},
title = {Word Topic Models for Spoken Document Retrieval and Transcription},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1482343.1482345},
doi = {10.1145/1482343.1482345},
abstract = {Statistical language modeling (LM), which aims to capture the regularities in human natural language and quantify the acceptability of a given word sequence, has long been an interesting yet challenging research topic in the speech and language processing community. It also has been introduced to information retrieval (IR) problems, and provided an effective and theoretically attractive probabilistic framework for building IR systems. In this article, we propose a word topic model (WTM) to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language modeling in spoken document retrieval and transcription. The document or the search history as a whole is modeled as a composite WTM model for generating a newly observed word. The underlying characteristics and different kinds of model structures are extensively investigated, while the performance of WTM is thoroughly analyzed and verified by comparison with the well-known probabilistic latent semantic analysis (PLSA) model as well as the other models. The IR experiments are performed on the TDT Chinese collections (TDT-2 and TDT-3), while the large vocabulary continuous speech recognition (LVCSR) experiments are conducted on the Mandarin broadcast news collected in Taiwan. Experimental results seem to indicate that WTM is a promising alternative to the existing models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {2},
numpages = {27},
keywords = {word topic model, speech recognition, information retrieval, adaptation, Language model}
}

@article{10.1145/1450295.1450298,
author = {Che, Wanxiang and Zhang, Min and Aw, AiTi and Tan, ChewLim and Liu, Ting and Li, Sheng},
title = {Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1450295.1450298},
doi = {10.1145/1450295.1450298},
abstract = {As a kind of Shallow Semantic Parsing, Semantic Role Labeling (SRL) is gaining more attention as it benefits a wide range of natural language processing applications. Given a sentence, the task of SRL is to recognize semantic arguments (roles) for each predicate (target verb or noun). Feature-based methods have achieved much success in SRL and are regarded as the state-of-the-art methods for SRL. However, these methods are less effective in modeling structured features. As an extension of feature-based methods, kernel-based methods are able to capture structured features more efficiently in a much higher dimension. Application of kernel methods to SRL has been achieved by selecting the tree portion of a predicate and one of its arguments as feature space, which is named as predicate-argument feature (PAF) kernel. The PAF kernel captures the syntactic tree structure features using convolution tree kernel, however, it does not distinguish between the path structure and the constituent structure. In this article, a hybrid convolution tree kernel is proposed to model different linguistic objects. The hybrid convolution tree kernel consists of two individual convolution tree kernels. They are a Path kernel, which captures predicate-argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments. Evaluations on the data sets of the CoNLL-2005 SRL shared task and the Chinese PropBank (CPB) show that our proposed hybrid convolution tree kernel statistically significantly outperforms the previous tree kernels. Moreover, in order to maximize the system performance, we present a composite kernel through combining our hybrid convolution tree kernel method with a feature-based method extended by the polynomial kernel. The experimental results show that the composite kernel achieves better performance than each of the individual methods and outperforms the best reported system on the CoNLL-2005 corpus when only one syntactic parser is used and on the CPB corpus when automated syntactic parse results and correct syntactic parse results are used respectively.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
articleno = {13},
numpages = {23},
keywords = {Semantic role labeling, hybrid convolution tree kernel}
}

@article{10.1145/1450295.1450297,
author = {Lee, Cheng-Wei and Day, Min-Yuh and Sung, Cheng-Lung and Lee, Yi-Hsun and Jiang, Tian-Jian and Wu, Chia-Wei and Shih, Cheng-Wei and Chen, Yu-Ren and Hsu, Wen-Lian},
title = {Boosting Chinese Question Answering with Two Lightweight Methods: ABSPs and SCO-QAT},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1450295.1450297},
doi = {10.1145/1450295.1450297},
abstract = {Question Answering (QA) research has been conducted in many languages. Nearly all the top performing systems use heavy methods that require sophisticated techniques, such as parsers or logic provers. However, such techniques are usually unavailable or unaffordable for under-resourced languages or in resource-limited situations. In this article, we describe how a top-performing Chinese QA system can be designed by using lightweight methods effectively. We propose two lightweight methods, namely the Sum of Co-occurrences of Question and Answer Terms (SCO-QAT) and Alignment-based Surface Patterns (ABSPs). SCO-QAT is a co-occurrence-based answer-ranking method that does not need extra knowledge, word-ignoring heuristic rules, or tools. It calculates co-occurrence scores based on the passage retrieval results. ABSPs are syntactic patterns trained from question-answer pairs with a multiple alignment algorithm. They are used to capture the relations between terms and then use the relations to filter answers. We attribute the success of the ABSPs and SCO-QAT methods to the effective use of local syntactic information and global co-occurrence information.By using SCO-QAT and ABSPs, we improved the RU-Accuracy of our testbed QA system, ASQA, from 0.445 to 0.535 on the NTCIR-5 dataset. It also achieved the top 0.5 RU-Accuracy on the NTCIR-6 dataset. The result shows that lightweight methods are not only cheaper to implement, but also have the potential to achieve state-of-the-art performances.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
articleno = {12},
numpages = {29},
keywords = {lightweight method, co-occurrence, answer ranking, answer filtering, Chinese question answering, surface pattern}
}

@article{10.1145/1450295.1450296,
author = {Lin, Jeng-Wei and Ho, Jan-Ming and Tseng, Li-Ming and Lai, Feipei},
title = {Variant Chinese Domain Name Resolution},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1450295.1450296},
doi = {10.1145/1450295.1450296},
abstract = {Many efforts in past years have been made to lower the linguistic barriers for non-native English speakers to access the Internet. Internet standard RFC 3490, referred to as IDNA (Internationalizing Domain Names in Applications), focuses on access to IDNs (Internationalized Domain Names) in a range of scripts that is broader in scope than the original ASCII. However, the use of character variants that have similar appearances and/or interpretations could create confusion. A variant IDL (Internationalized Domain Label), derived from an IDL by replacing some characters with their variants, should match the original IDL; and thus a variant IDN does. In RFC 3743, referred to as JET (Joint Engineering Team) Guidelines, it is suggested that zone administrators model this concept of equivalence as an atomic IDL package. When an IDL is registered, an IDL package is created that contains its variant IDLs generated according to the zone-specific Language Variant Tables (LVTs). In addition to the registered IDL, the name holder can request the domain registry to activate some of the variant IDLs, free or by an extra fee. The activated variant IDLs are stored in the zone files, and thus become resolvable. However, an issue of scalability arises when there is a large number of variant IDLs to be activated.In this article, the authors present a resolution protocol that resolves the variant IDLs into the registered IDL, specifically for Han character variants. Two Han characters are said to be variants of each other if they have the same meaning and are pronounced the same. Furthermore, Han character variants usually have similar appearances. It is not uncommon that a Chinese IDL has a large number of variant IDLs. The proposed protocol introduces a new RR (resource record) type, denoted as VarIdx RR, to associate a variant expression of the variant IDLs with the registered IDL. The label of the VarIdx RR, denoted as the variant index, is assigned by an indexing function that is designed to give the same value to all of the variant IDLs enumerated by the variant expression. When one of the variant IDLs is accessed, Internet applications can compute the variant index, look up the VarIdx RRs, and resolve the variant IDL into the registered IDL.The authors examine two sets of Chinese IDLs registered in TWNIC and CNNIC, respectively. The results show that for a registered Chinese IDL, a very small number of VarIdx RRs, usually one or two, are sufficient to activate all of its variant IDLs. The authors also represent a Web redirection service that employs the proposed resolution protocol to redirect a URL addressed by a variant IDN to the URL addressed by the registered IDN. The experiment results show that the proposed protocol successfully resolves the variant IDNs into the registered IDNs.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
articleno = {11},
numpages = {29},
keywords = {localization, internationalized domain name, IDN spoof, conversion between traditional Chinese and simplified Chinese, Han character folding, Han character variant}
}

@article{10.1145/1386869.1386871,
author = {Sharma, Utpal and Kalita, Jugal K. and Das, Rajib K.},
title = {Acquisition of Morphology of an Indic Language from Text Corpus},
year = {2008},
issue_date = {August 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1386869.1386871},
doi = {10.1145/1386869.1386871},
abstract = {This article describes an approach to unsupervised learning of
morphology from an unannotated corpus for a highly inflectional
Indo-European language called Assamese spoken by about 30 million
people. Although Assamese is one of Indias national languages, it
utterly lacks computational linguistic resources. There exists no
prior computational work on this language spoken widely in
northeast India. The work presented is pioneering in this respect.
In this article, we discuss salient issues in Assamese morphology
where the presence of a large number of suffixal determiners,
sandhi, samas, and the propensity to use suffix sequences make
approximately 50% of the words used in written and spoken text
inflected. We implement methods proposed by Gaussier and Goldsmith
on acquisition of morphological knowledge, and obtain F-measure
performance below 60%. This motivates us to present a method more
suitable for handling suffix sequences, enabling us to increase the
F-measure performance of morphology acquisition to almost 70%. We
describe how we build a morphological dictionary for Assamese from
the text corpus. Using the morphological knowledge acquired and the
morphological dictionary, we are able to process small chunks of
data at a time as well as a large corpus. We achieve approximately
85% precision and recall during the analysis of small chunks of
coherent text.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {9},
numpages = {33},
keywords = {Indo-European languages, Assamese, machine learning, Morphology}
}

@article{10.1145/1386869.1386872,
author = {Chen, Jiang-Chun and Jang, Jyh-Shing Roger},
title = {TRUES: Tone Recognition Using Extended Segments},
year = {2008},
issue_date = {August 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1386869.1386872},
doi = {10.1145/1386869.1386872},
abstract = {Tone recognition has been a basic but important task for speech
recognition and assessment of tonal languages, such as Mandarin
Chinese. Most previously proposed approaches adopt a two-step
approach where syllables within an utterance are identified via
forced alignment first, and tone recognition using a variety of
classifiers---such as neural networks, Gaussian mixture models
(GMM), hidden Markov models (HMM), support vector machines
(SVM)---is then performed on each segmented syllable to predict its
tone. However, forced alignment does not always generate accurate
syllable boundaries, leading to unstable voiced-unvoiced detection
and deteriorating performance in tone recognition. Aiming to
alleviate this problem, we propose a robust approach called Tone
Recognition Using Extended Segments (TRUES) for HMM-based
continuous tone recognition. The proposed approach extracts an
unbroken pitch contour from a given utterance based on dynamic
programming over time-domain acoustic features of average magnitude
difference function (AMDF). The pitch contour of each syllable is
then extended for tri-tone HMM modeling, such that the influence
from inaccurate syllable boundaries is lessened. Our experimental
results demonstrate that the proposed TRUES achieves 49.13%
relative error rate reduction over that of the recently proposed
supratone modeling, which is deemed the state of the art of tone
recognition that outperforms several previously proposed
approaches. The encouraging improvement demonstrates the
effectiveness and robustness of the proposed TRUES, as well as the
corresponding pitch determination algorithm which produces unbroken
pitch contours.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {10},
numpages = {23},
keywords = {supratone modeling, context-dependent tone modeling, Continuous tone recognition, extended segment for tone recognition, HMM, Mandarin Chinese}
}

@article{10.1145/1386869.1386870,
author = {Fukumoto, Fumiyo and Suzuki, Yoshimi},
title = {Integrating Cross-Language Hierarchies and Its Application to Retrieving Relevant Documents},
year = {2008},
issue_date = {August 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1386869.1386870},
doi = {10.1145/1386869.1386870},
abstract = {Internet directories such as Yahoo! are an approach to improve
the efficacy and efficiency of Information Retrieval (IR) on the
Web, as pages (documents) are organized into hierarchical
categories, and similar pages are grouped together. Most of the
search engines on the Web service find documents that are assigned
to a single classification hierarchy. Categories in the hierarchy
are carefully defined by human experts and documents are well
organized. However, a single hierarchy in one language is often
insufficient to find all relevant material, as each hierarchy tends
to have some bias in both defining hierarchical structure and
classifying documents. Moreover, documents written in a language
other than the users native language often include large amounts of
information related to the users request. In this article, we
propose a method of integrating cross-language (CL) category
hierarchies, that is, Reuters 96 hierarchy and UDC code hierarchy
of Japanese by estimating category similarities. The method does
not simply merge two different hierarchies into one large hierarchy
but instead extracts sets of similar categories, where each element
of the sets is relevant with each other. It consists of three
steps. First, we classify documents from one hierarchy into
categories with another hierarchy using a cross-language text
classification (CLTC) technique, and extract category pairs of two
hierarchies. Next, we apply \c{C}2 statistics
to these pairs to obtain similar category pairs, and finally we
apply the generating function of the Apriori algorithm
(Apriori-Gen) to the category pairs, and find sets of similar
categories. Moreover, we examined whether integrating hierarchies
helps to support retrieval of documents with similar contents. The
retrieval results showed a 42.7% improvement over the baseline
nonhierarchy model, and a 21.6% improvement over a single
hierarchy.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {8},
numpages = {22},
keywords = {Information integration, text classification, cross-language hierarchies, retrieval of relevant documents}
}

@article{10.1145/1362782.1362783,
author = {Kando, Noriko and Mitamura, Teruko and Sakai, Tetsuya},
title = {Introduction to the NTCIR-6 Special Issue},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1362782.1362783},
doi = {10.1145/1362782.1362783},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
articleno = {4},
numpages = {3}
}

@article{10.1145/1362782.1362784,
author = {Zhou, Dong and Truran, Mark and Brailsford, Tim and Ashman, Helen},
title = {A Hybrid Technique for English-Chinese Cross Language Information Retrieval},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1362782.1362784},
doi = {10.1145/1362782.1362784},
abstract = {In this article we describe a hybrid technique for dictionary-based query translation suitable for English-Chinese cross language information retrieval. This technique marries a graph-based model for the resolution of candidate term ambiguity with a pattern-based method for the translation of out-of-vocabulary (OOV) terms. We evaluate the performance of this hybrid technique in an experiment using several NTCIR test collections. Experimental results indicate a substantial increase in retrieval effectiveness over various baseline systems incorporating machine- and dictionary-based translation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
articleno = {5},
numpages = {35},
keywords = {unknown term translation, disambiguation, graph-based analysis, cross language information retrieval, patterns}
}

@article{10.1145/1362782.1362785,
author = {Higashinaka, Ryuichiro and Isozaki, Hideki},
title = {Automatically Acquiring Causal Expression Patterns from Relation-Annotated Corpora to Improve Question Answering for Why-Questions},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1362782.1362785},
doi = {10.1145/1362782.1362785},
abstract = {This article describes our approach for answering why-questions that we initially introduced at NTCIR-6 QAC-4. The approach automatically acquires causal expression patterns from relation-annotated corpora by abstracting text spans annotated with a causal relation and by mining syntactic patterns that are useful for distinguishing sentences annotated with a causal relation from those annotated with other relations. We use these automatically acquired causal expression patterns to create features to represent answer candidates, and use these features together with other possible features related to causality to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms baselines with a Mean Reciprocal Rank (top-5) of 0.223 when sentences are used as answers and with a MRR (top-5) of 0.326 when paragraphs are used as answers, making it presumably the best-performing fully implemented why-QA system. Experimental results also verified the usefulness of the automatically acquired causal expression patterns.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
articleno = {6},
numpages = {29},
keywords = {pattern mining, question answering, relation-annotated corpus, causal expression}
}

@article{10.1145/1362782.1362786,
author = {Li, Yaoyong and Bontcheva, Kalina},
title = {Adapting Support Vector Machines for F-Term-Based Classification of Patents},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1362782.1362786},
doi = {10.1145/1362782.1362786},
abstract = {Support Vector Machines (SVM) have obtained state-of-the-art results on many applications including document classification. However, previous works on applying SVMs to the F-term patent classification task did not obtain as good results as other learning algorithms such as kNN. This is due to the fact that F-term patent classification is different from conventional document classification in several aspects, mainly because it is a multiclass, multilabel classification problem with semi-structured documents and multi-faceted hierarchical categories.This article describes our SVM-based system and several techniques we developed successfully to adapt SVM for the specific features of the F-term patent classification task. We evaluate the techniques using the NTCIR-6 F-term classification terms assigned to Japanese patents. Moreover, our system participated in the NTCIR-6 patent classification evaluation and obtained the best results according to two of the three metrics used for task performance evaluation. Following the NTCIR-6 participation, we developed two new techniques, which achieved even better scores using all three NTCIR-6 metrics, effectively outperforming all participating systems. This article presents this new work and the experimental results that demonstrate the benefits of the latest approach.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
articleno = {7},
numpages = {19},
keywords = {patent processing, F-term classification, support vector machines}
}

@article{10.1145/1330291.1330293,
author = {Jeong, Minwoo and Lee, Gary Geunbae},
title = {Improving Speech Recognition and Understanding Using Error-Corrective Reranking},
year = {2008},
issue_date = {February 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1330291.1330293},
doi = {10.1145/1330291.1330293},
abstract = {The main issues of practical spoken-language applications for
human-computer interface are how to overcome speech recognition
errors and guarantee the reasonable end-performance of
spoken-language applications. Therefore, handling the erroneously
recognized outputs is a key in developing robust spoken-language
systems. To address this problem, we present a method to improve
the accuracy of speech recognition and performance of
spoken-language applications. The proposed error corrective
reranking approach exploits recognition environment characteristics
and domain-specific semantic information to provide robustness and
adaptability for a spoken-language system. We demonstrate some
experiments of spoken dialogue tasks and empirical results that
show an improvement in accuracy for both speech recognition and
spoken-language understanding. In our experiment, we show an error
reduction of up to 9.7% and 16.8%; of word error rate, and 5.5% and
7.9% of understanding error for the air travel and telebanking
service domains.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {2},
numpages = {26},
keywords = {Error-corrective reranking, automatic speech recognition, improving spoken dialogue system, spoken-language understanding}
}

@article{10.1145/1330291.1330294,
author = {Kuo, June-Jei and Chen, Hsin-Hsi},
title = {Multidocument Summary Generation: Using Informative and Event Words},
year = {2008},
issue_date = {February 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1330291.1330294},
doi = {10.1145/1330291.1330294},
abstract = {Summary generation for multiple documents poses a number of issues including sentence selection, sentence ordering, and sentence reduction over single-document summarization. In addition, the temporal resolution among extracted sentences is also important. This article considers informative words and event words to deal with multidocument summarization. These words indicate the important concepts and relationships in a document or among a set of documents, and can be used to select salient sentences. We present a temporal resolution algorithm, using focusing time and coreference chains, to convert Chinese temporal expressions in a document into calendrical forms. Moreover, we consider the last calendrical form of a sentence as a sentence time stamp to address sentence ordering. Informative words, event words, and temporal words are introduced to a sentence reduction algorithm, which deals with both length constraints and information coverage. Experiments on Chinese-news data sets show significant improvements of both information coverage and readability.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {3},
numpages = {23},
keywords = {Temporal processing, latent semantic analysis, multidocument summary generation, sentence ordering, sentence reduction, sentence selection}
}

@article{10.1145/1330291.1330292,
author = {Chen, Yufeng and Zong, Chengqing},
title = {A Structure-Based Model for Chinese Organization Name Translation},
year = {2008},
issue_date = {February 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1330291.1330292},
doi = {10.1145/1330291.1330292},
abstract = {Named entity (NE) translation is a fundamental task in multilingual natural language processing. The performance of a machine translation system depends heavily on precise translation of the inclusive NEs. Furthermore, organization name (ON) is the most complex NE for translation among all the NEs. In this article, the structure formulation of ONs is investigated and a hierarchical structure-based ON translation model for Chinese-to-English translation system is presented.First, the model performs ON chunking; then both the translation of words within chunks and the process of chunk-reordering are achieved by synchronous context-free grammar (CFG). The CFG rules are extracted from bilingual ON pairs in a training program.The main contributions of this article are: (1) defining appropriate chunk-units for analyzing the internal structure of Chinese ONs; (2) making the chunk-based ON translation feasible and flexible via a hierarchical CFG derivation; and (3) proposing a training architecture to automatically learn the synchronous CFG for constructing ONs with chunk-units from aligned bilingual ON pairs. The experiments show that the proposed approach translates the Chinese ONs into English with an accuracy of 93.75% and significantly improves the performance of a baseline statistical machine translation (SMT) system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = feb,
articleno = {1},
numpages = {30},
keywords = {chunk, rules extraction, organization name, alignment, synchronous context-free grammar, named entity, Machine translation, hierarchical derivation, structural analysis}
}

@article{10.1145/1316457.1316458,
author = {Iida, Ryu and Inui, Kentaro and Matsumoto, Yuji},
title = {Zero-Anaphora Resolution by Learning Rich Syntactic Pattern Features},
year = {2008},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1316457.1316458},
doi = {10.1145/1316457.1316458},
abstract = {We approach the zero-anaphora resolution problem by decomposing it into intrasentential and intersentential zero-anaphora resolution tasks. For the former task, syntactic patterns of zeropronouns and their antecedents are useful clues. Taking Japanese as a target language, we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intrasentential zero-anaphora, which consequently improves the overall performance of zero-anaphora resolution.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {1},
numpages = {22}
}

@article{10.1145/1316457.1316459,
author = {Adriani, Mirna and Asian, Jelita and Nazief, Bobby and Tahaghoghi, S. M.M. and Williams, Hugh E.},
title = {Stemming Indonesian: A Confix-Stripping Approach},
year = {2008},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1316457.1316459},
doi = {10.1145/1316457.1316459},
abstract = {Stemming words to (usually) remove suffixes has applications in text search, machine translation, document summarization, and text classification. For example, English stemming reduces the words "computer," "computing," "computation," and "computability" to their common morphological root, "comput-." In text search, this permits a search for "computers" to find documents containing all words with the stem "comput-." In the Indonesian language, stemming is of crucial importance: words have prefixes, suffixes, infixes, and confixes that make matching related words difficult.This work surveys existing techniques for stemming Indonesian words to their morphological roots, presents our novel and highly accurate CS algorithm, and explores the effectiveness of stemming in the context of general-purpose text information retrieval through ad hoc queries.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {1–33},
numpages = {33},
keywords = {stemming, information retrieval, Indonesian}
}

@article{10.1145/1316457.1316460,
author = {Thao, Pham Thi Xuan and Tri, Tran Quoc and Dien, Dinh and Collier, Nigel},
title = {Named Entity Recognition in Vietnamese Using Classifier Voting},
year = {2008},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1316457.1316460},
doi = {10.1145/1316457.1316460},
abstract = {Named entity recognition (NER) is one of the fundamental tasks in natural-language processing (NLP). Though the combination of different classifiers has been widely applied in several well-studied languages, this is the first time this method has been applied to Vietnamese. In this article, we describe how voting techniques can improve the performance of Vietnamese NER. By combining several state-of-the-art machine-learning algorithms using voting strategies, our final result outperforms individual algorithms and gained an F-measure of 89.12. A detailed discussion about the challenges of NER in Vietnamese is also presented.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
articleno = {3},
numpages = {18},
keywords = {voting, Transformation Based Learning, Named entity recognition, Vietnamese, Conditional Random Fields, Support Vector Machines, Na\"{\i}ve Bayes, C4.5}
}

@article{10.1145/1290002.1290004,
author = {Hussain, Sarmad and Gul, Sana and Waseem, Afifah},
title = {Developing Lexicographic Sorting: An Example for Urdu},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1290002.1290004},
doi = {10.1145/1290002.1290004},
abstract = {Collation or lexicographic sorting is essential to develop multilingual computing. This paper presents the challenges faced in developing collation sequence for a language. The paper discusses both theoretical linguistic and practical standardization and encoding related considerations that need to be addressed for languages for which relevant standards and/or solutions have not been defined. The paper also defines the process, by giving the details of the procedure followed for Urdu language, which is the national language of Pakistan and is spoken by more than 100 million people across the world. The paper is oriented towards organizations involved in developing and using collation standards and the localization industry, and not focused on theoretical issues.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
pages = {10–es},
numpages = {17},
keywords = {Text processing, Urdu}
}

@article{10.1145/1290002.1290003,
author = {Saraswathi, S. and Geetha, T. V.},
title = {Comparison of Performance of Enhanced Morpheme-Based Language Model with Different Word-Based Language Models for Improving the Performance of Tamil Speech Recognition System},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1290002.1290003},
doi = {10.1145/1290002.1290003},
abstract = {This paper describes a new technique of language modeling for a highly inflectional Dravidian language, Tamil. It aims to alleviate the main problems encountered in processing of Tamil language, like enormous vocabulary growth caused by the large number of different forms derived from one word. The size of the vocabulary was reduced by, decomposing the words into stems and endings and storing these sub word units (morphemes) in the vocabulary separately. A enhanced morpheme-based language model was designed for the inflectional language Tamil. The enhanced morpheme-based language model was trained on the decomposed corpus. The perplexity and Word Error Rate (WER) were obtained to check the efficiency of the model for Tamil speech recognition system. The results were compared with word-based bigram and trigram language models, distance based language model, dependency based language model and class based language model. From the results it was analyzed that the enhanced morpheme-based trigram model with Katz back-off smoothing effect improved the performance of the Tamil speech recognition system when compared to the word-based language models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
pages = {9–es},
numpages = {19},
keywords = {word error rate and speech recognition, perplexity, morphemes, Language model}
}

@article{10.1145/1290002.1290005,
author = {Fukumoto, Fumiyo and Suzuki, Yoshimi},
title = {Topic Tracking Based on Bilingual Comparable Corpora and Semisupervised Clustering},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1290002.1290005},
doi = {10.1145/1290002.1290005},
abstract = {In this paper, we address the problem of skewed data in topic tracking: the small number of stories labeled positive as compared to negative stories and propose a method for estimating effective training stories for the topic-tracking task. For a small number of labeled positive stories, we use bilingual comparable, i.e., English, and Japanese corpora, together with the EDR bilingual dictionary, and extract story pairs consisting of positive and associated stories. To overcome the problem of a large number of labeled negative stories, we classified them into clusters. This is done using a semisupervised clustering algorithm, combining k means with EM. The method was tested on the TDT English corpus and the results showed that the system works well when the topic under tracking is talking about an event originating in the source language country, even for a small number of initial positive training stories.},
journal = {ACM Transactions on Asian Language Information Processing},
month = nov,
pages = {11–es},
numpages = {22},
keywords = {topic detection and tracking, EM algorithm, Bilingual comparable corpora, N-gram model, clustering}
}

@article{10.1145/1282080.1282083,
author = {Zhuang, Yl and Zhuang, Yueting and Li, Qing and Chen, Lei},
title = {Interactive High-Dimensional Index for Large Chinese Calligraphic Character Databases},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1282080.1282083},
doi = {10.1145/1282080.1282083},
abstract = {The large numbers of Chinese calligraphic scripts in existence are valuable part of the Chinese cultural heritage. However, due to the shape complexity of these characters, it is hard to employ existing techniques to effectively retrieve and efficiently index them. In this article, using a novel shape-similarity- based retrieval method in which shapes of calligraphic characters are represented by their contour points extracted from the character images, we propose an interactive partial-distance-map(PDM)- based high-dimensional indexing scheme which is designed specifically to speed up the retrieval performance of the large Chinese calligraphic character databases effectively. Specifically, we use the approximate minimal bounding sphere of a query character and utilize users' relevance feedback to refine the query gradually. Comprehensive experiments are conducted to testify the efficiency and effectiveness of this method. In addition, a new k-NN search called Pseudo k-NN (Pk-NN) search is presented to better facilitate the PDM-based character retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {8–es},
numpages = {31},
keywords = {Hyper-centre relocation, Chinese calligraphic character, Pseudo k-NN}
}

@article{10.1145/1282080.1282082,
author = {Xiao, Jinghui and Wang, Xiaolong and Liu, Bingquan},
title = {The Study of a Nonstationary Maximum Entropy Markov Model and Its Application on the Pos-Tagging Task},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1282080.1282082},
doi = {10.1145/1282080.1282082},
abstract = {Sequence labeling is a core task in natural language processing. The maximum entropy Markov model (MEMM) is a powerful tool in performing this task. This article enhances the traditional MEMM by exploiting the positional information of language elements. The stationary hypothesis is relaxed in MEMM, and the nonstationary MEMM (NS-MEMM) is proposed. Several related issues are discussed in detail, including the representation of positional information, NS-MEMM implementation, smoothing techniques, and the space complexity issue. Furthermore, the asymmetric NS-MEMM presents a more flexible way to exploit positional information. In the experiments, NS-MEMM is evaluated on both the Chinese and the English pos-tagging tasks. According to the experimental results, NS-MEMM yields effective improvements over MEMM by exploiting positional information. The smoothing techniques in this article effectively solve the NS-MEMM data-sparseness problem; the asymmetric NS-MEMM is also an improvement by exploiting positional information in a more flexible way.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {7–es},
numpages = {29},
keywords = {Markov property, MEMM, stationary hypothesis, data sparseness problem, Pos-tagging}
}

@article{10.1145/1282080.1282081,
author = {Kuo, Jin-Shea and Li, Haizhou and Yang, Ying-Kuei},
title = {A Phonetic Similarity Model for Automatic Extraction of Transliteration Pairs},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1282080.1282081},
doi = {10.1145/1282080.1282081},
abstract = {This article proposes an approach for the automatic extraction of transliteration pairs from Chinese Web corpora. In this approach, we formulate the machine transliteration process using a syllable-based phonetic similarity model which consists of phonetic confusion matrices and a Chinese character n-gram language model. With the phonetic similarity model, the extraction of transliteration pairs becomes a two-step process of recognition followed by validation: First, in the recognition process, we identify the most probable transliteration in the k-neighborhood of a recognized English word. Then, in the validation process, we qualify the transliteration pair candidates with a hypothesis test. We carry out an analytical study on the statistics of several key factors in English-Chinese transliteration to help formulate phonetic similarity modeling. We then conduct both supervised and unsupervised learning of a phonetic similarity model on a development database. The experimental results validate the effectiveness of the phonetic similarity model by achieving an F-measure of 0.739 in supervised learning. The unsupervised learning approach works almost as well as the supervised one, thus allowing us to deploy automatic extraction of transliteration pairs in the Web space.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {6–es},
numpages = {24},
keywords = {Machine translation, extraction of transliteration pairs, machine transliteration, phonetic confusion probability, phonetic similarity modeling}
}

@article{10.1145/1227850.1227853,
author = {Sakai, Tetsuya},
title = {On the Reliability of Factoid Question Answering Evaluation},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1227850.1227853},
doi = {10.1145/1227850.1227853},
abstract = {This paper compares some existing evaluation metrics for factoid question answering (QA) from the viewpoint of stability and sensitivity, using the NTCIR-4 QAC2 Japanese factoid QA tasks and the Buckley/Voorhees stability method and Voorhees/Buckley swap method. Our main findings are: (1) For QA evaluation with ranked lists containing up to five answers, the fraction of questions with a correct answer within top 5 (NQcorrect5) and that with a correct answer at rank 1 (NQcorrect1) are not as stable and sensitive as reciprocal rank. (2) Q-measure, which can handle multiple correct answers and answer correctness levels, is at least as stable and sensitive as reciprocal rank, provided that a mild gain value assignment is used. Emphasizing answer correctness levels tends to hurt stability and sensitivity, while handling multiple correct answers improves them. As our experimental methods are language-independent, we believe that these findings apply to QA in languages other than Japanese as well.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
pages = {3–es},
numpages = {23},
keywords = {evaluation metrics, Question answering}
}

@article{10.1145/1227850.1227852,
author = {Chen, Yong and Chan, Kwok-Ping},
title = {Using Data Mining Techniques and Rough Set Theory for Language Modeling},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1227850.1227852},
doi = {10.1145/1227850.1227852},
abstract = {In this article, we propose a new postprocessing strategy, word suggestion, based on a multiple word trigger-pair language model for Chinese character recognizers. With the word suggestion strategy, Chinese character recognizers may even achieve a recognition rate greater than the top-n candidate recognition rate. To construct the multiple word trigger-pair model, data mining techniques are used to alleviate the intensive computation problem. Furthermore, rough set theory is first used in the study to discover negatively correlated relationships between words in order to prevent introducing wrong words in the process of word suggestion.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
pages = {2–es},
numpages = {19},
keywords = {Chinese character recognizer, postprocessing}
}

@article{10.1145/1227850.1227854,
author = {Wiseman, Yair and Gefner, Irit},
title = {Conjugation-Based Compression for Hebrew Texts},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1227850.1227854},
doi = {10.1145/1227850.1227854},
abstract = {Traditional compression techniques do not look deeply into the morphology of languages. This can be less critical in languages like English where most of the sequences are illegal according to the grammatical rules of the language, for example, zx, bv or qe; hence the morphology can add a little information that can be beneficial for the compression algorithm. However, this negligence can be a significant flaw in languages like Hebrew where the grammatical rules allow much more freedom in the sequences of letters and, except tet after gimel, any pair is legal; hence compressing without taking the morphological rules into account can yield a poorer compression ratio. This article suggests a tool that optimizes the Burrows-Wheeler algorithm which is an unaware morphological rules compression method. It first preprocesses a Hebrew text file according to the Hebrew conjugation rules, and, after that, it provides the Burrows-Wheeler algorithm with this preprocessed file so that can be compressed better. Experimental results show a significant improvement.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
pages = {4–es},
numpages = {10},
keywords = {Burrows-Wheeler algorithm, semitic languages, Text compression, Root conjugations, Hebrew text analysis}
}

@article{10.1145/1227850.1227851,
author = {Wu, Chung-Hsien and Su, Hung-Yu and Chiu, Yu-Hsien and Lin, Chia-Hung},
title = {Transfer-Based Statistical Translation of Taiwanese Sign Language Using PCFG},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1227850.1227851},
doi = {10.1145/1227850.1227851},
abstract = {This article presents a transfer-based statistical model for Chinese to Taiwanese sign-language (TSL) translation. Two sets of probabilistic context-free grammars (PCFGs) are derived from a Chinese Treebank and a bilingual parallel corpus. In this approach, a three-stage translation model is proposed. First, the input Chinese sentence is parsed into possible phrase structure trees (PSTs) based on the Chinese PCFGs. Second, the Chinese PSTs are then transferred into TSL PSTs according to the transfer probabilities between the context-free grammar (CFG) rules of Chinese and TSL derived from the bilingual parallel corpus. Finally, the TSL PSTs are used to generate the possible translation results. The Viterbi algorithm is adopted to obtain the best translation result via the three-stage translation. For evaluation, three objective evaluation metrics including AER, Top-N, and BLUE and one subjective evaluation metric using MOS were used. Experimental results show that the proposed approach outperforms the IBM Model 3 in the task of Chinese to sign-language translation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
pages = {1–es},
numpages = {18},
keywords = {Taiwanese sign language}
}

@article{10.1145/1227850.1227855,
author = {Hsu, Chung-Chian and Chen, Chien-Hsing and Shih, Tien-Teng and Chen, Chun-Kai},
title = {Measuring Similarity between Transliterations against Noise Data},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1227850.1227855},
doi = {10.1145/1227850.1227855},
abstract = {When editors of newspapers and magazines translate proper nouns from foreign languages into Chinese, the Chinese translation (termed transliterations) they choose will typically be phonetically similar to the original word. With many different translators working without a common standard, there may be many different Chinese transliterations for the same proper noun, such as using the same sounds but different Chinese characters or even using different sounds and characters. This causes confusion for the reader and, more importantly, leads to incomplete Chinese Web search results. This article investigates the similarity comparison of transliterations as a first step toward solving the incomplete search problem. We devise a method based on comparing digitalized Chinese character (or Hanzi) sounds. Along with four other methods based on comparing grapheme or phoneme similarity, we compare their performance of identifying synonymous transliterations against noise words taken from Web pages. Experimental results indicate that our method surpasses the other methods due to its advantage of containing more discriminative information in sound vectors. The method performing the second best is based on a scheme which assigns similarity between phonemes by carefully considering articulatory features of phonemes, including using multivalued features and placing different weights on the features. Among six pinyin schemes used to romanize Chinese transliterations, the Tongyong scheme outperforms the others.},
journal = {ACM Transactions on Asian Language Information Processing},
month = apr,
pages = {5–es},
numpages = {20},
keywords = {phoneme, similarity comparison, grapheme, Chinese transliteration, pinyin, speech signal processing, cross-lingual information retrieval, Romanization}
}

@article{10.1145/1236181.1236184,
author = {Gao, Jianfeng and Nie, Jian-Yun and Zhou, Ming},
title = {Statistical Query Translation Models for Cross-Language Information Retrieval},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236184},
doi = {10.1145/1236181.1236184},
abstract = {Query translation is an important task in cross-language information retrieval (CLIR), which aims to determine the best translation words and weights for a query. This article presents three statistical query translation models that focus on the resolution of query translation ambiguities. All the models assume that the selection of the translation of a query term depends on the translations of other terms in the query. They differ in the way linguistic structures are detected and exploited. The co-occurrence model treats a query as a bag of words and uses all the other terms in the query as the context for translation disambiguation. The other two models exploit linguistic dependencies among terms. The noun phrase (NP) translation model detects NPs in a query, and translates each NP as a unit by assuming that the translation of a term only depends on other terms within the same NP. Similarly, the dependency translation model detects and translates dependency triples, such as verb-object, as units. The evaluations show that linguistic structures always lead to more precise translations. The experiments of CLIR on TREC Chinese collections show that all three models have a positive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best results are obtained by combining the three models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {323–359},
numpages = {37},
keywords = {Query translation, linguistic structures, CLIR, statistical models}
}

@article{10.1145/1236181.1236187,
author = {Phan, Xuan-Hieu and Nguyen, Le-Minh and Inoguchi, Yasushi and Ho, Tu-Bao and Horiguchi, Susumu},
title = {Improving Discriminative Sequential Learning by Discovering Important Association of Statistics},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236187},
doi = {10.1145/1236181.1236187},
abstract = {Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various nonindependent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from a high imbalance among classes, irregular phenomena, and potential ambiguity in the training data. This article presents a data-driven approach that can deal with such difficult data instances by discovering and emphasizing important conjunctions or associations of statistics hidden in the training data. Discovered associations are then incorporated into these models to deal with difficult data instances. Experimental results of phrase-chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large datasets.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {413–438},
numpages = {26},
keywords = {Discriminative sequential learning, feature selection, text segmentation, association rule mining, information extraction}
}

@article{10.1145/1236181.1236182,
author = {Song, Dawei and Nie, Jian-Yun},
title = {Introduction to Special Issue on Reasoning in Natural Language Information Processing},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236182},
doi = {10.1145/1236181.1236182},
abstract = {For any applications related to Natural Language Processing (NLP), reasoning has been recognized as a necessary underlying aspect. Many of the existing work in NLP deals with specific NLP problems in a highly heuristic manner, yet not from an explicit reasoning perspective. Recently, there have been developments on models that allow reasoning in NLP such as language models, logical models, and so on. The goal of this special issue is to present high-quality contributions that integrate reasoning involved in different areas of natural language processing both at theoretical and/or practical levels. In this article, we give a brief overview on some major aspects of explicating reasoning in NLP and summarize the articles included in this special issue.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {291–295},
numpages = {5}
}

@article{10.1145/1236181.1236185,
author = {Liu, Yi and Jin, Rong and Chai, Joyce Y.},
title = {A Statistical Framework for Query Translation Disambiguation},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236185},
doi = {10.1145/1236181.1236185},
abstract = {Resolving ambiguity in the process of query translation is crucial to cross-language information retrieval (CLIR), given the short length of queries. This problem is even more challenging when only a bilingual dictionary is available, which is the focus of our work described here. In this paper, we will present a statistical framework for dictionary-based CLIR that estimates the translation probabilities of query words based on the monolingual word co-occurrence statistics. In addition, we will present two realizations of the proposed framework, i.e., the “maximum coherence model” and the “spectral query-translation model,” that exploit different metrics for the coherence measurement between a translation of a query word and the theme of the entire query. Compared to previous work on dictionary-based CLIR, the proposed framework is advantageous in three aspects: (1) Translation probabilities are calculated explicitly to capture the uncertainty in translating queries; (2) translations of all query words are estimated simultaneously rather than independently; and (3) the formulated problem can be solved efficiently with a unique optimal solution. Empirical studies with Chinese--English cross-language information retrieval using TREC datasets have shown that the proposed models achieve a relative 10%--50% improvement, compared to other approaches that also exploit word co-occurrence statistics for query translation disambiguation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {360–387},
numpages = {28},
keywords = {maximum coherence, cross-language information retrieval, graph partitioning, co-occurrence statistics}
}

@article{10.1145/1236181.1236186,
author = {Li, Baoli and Li, Wenjie and Lu, Qin},
title = {Topic Tracking with Time Granularity Reasoning},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236186},
doi = {10.1145/1236181.1236186},
abstract = {Temporal information is an important attribute of a topic, and a topic usually exists in a limited period. Therefore, many researchers have explored the utilization of temporal information in topic detection and tracking (TDT). They use either a story's publication time or temporal expressions in text to derive temporal relatedness between two stories or a story and a topic. However, past research neglects the fact that people tend to express a time with different granularities as time lapses. Based on a careful investigation of temporal information in news streams, we propose a new strategy with time granularity reasoning for utilizing temporal information in topic tracking. A set of topic times, which as a whole represent the temporal attribute of a topic, are distinguished from others in the given on-topic stories. The temporal relatedness between a story and a topic is then determined by the highest coreference level between each time in the story and each topic time where the coreference level between a test time and a topic time is inferred from the two times themselves, their granularities, and the time distance between the topic time and the publication time of the story where the test time appears. Furthermore, the similarity value between an incoming story and a topic, that is the likelihood that a story is on-topic, can be adjusted only when the new story is both temporally and semantically related to the target topic. Experiments on two different TDT corpora show that our proposed method could make good use of temporal information in news stories, and it consistently outperforms the baseline centroid algorithm and other algorithms which consider temporal relatedness.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {388–412},
numpages = {25},
keywords = {Time granularity, time reasoning, topic detection and tracking, event tracking, topic tracking}
}

@article{10.1145/1236181.1236183,
author = {Nie, Jian-Yun and Cao, Guihong and Bai, Jing},
title = {Inferential Language Models for Information Retrieval},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1236181.1236183},
doi = {10.1145/1236181.1236183},
abstract = {Language modeling (LM) has been widely used in IR in recent years. An important operation in LM is smoothing of the document language model. However, the current smoothing techniques merely redistribute a portion of term probability according to their frequency of occurrences only in the whole document collection. No relationships between terms are considered and no inference is involved. In this article, we propose several inferential language models capable of inference using term relationships. The inference operation is carried out through a semantic smoothing either on the document model or query model, resulting in document or query expansion. The proposed models implement some of the logical inference capabilities proposed in the previous studies on logical models, but with necessary simplifications in order to make them tractable. They are a good compromise between inference power and efficiency. The models have been tested on several TREC collections, both in English and Chinese. It is shown that the integration of term relationships into the language modeling framework can consistently improve the retrieval effectiveness compared with the traditional language models. This study shows that language modeling is a suitable framework to implement basic inference operations in IR effectively.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {296–322},
numpages = {27},
keywords = {inferential model, inference, document expansion, Query expansion}
}

@article{10.1145/1194936.1194940,
author = {Ye, Patrick and Baldwin, Timothy},
title = {Semantic Role Labeling of Prepositional Phrases},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194940},
doi = {10.1145/1194936.1194940},
abstract = {We propose a method for labelling prepositional phrases according to two different semantic role classifications, as contained in the Penn treebank and the CoNLL 2004 Semantic Role Labeling data set. Our results illustrate the difficulties in determining preposition semantics, but also demonstrate the potential for PP semantic role labelling to improve the performance of a holistic semantic role labelling system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {228–244},
numpages = {17},
keywords = {Preposition, Semantic Role}
}

@article{10.1145/1194936.1194937,
author = {Dale, Robert},
title = {Introduction to the Special Section: Extended Best Papers from IJCNLP 2005},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194937},
doi = {10.1145/1194936.1194937},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {183–184},
numpages = {2},
keywords = {Natural language processing}
}

@article{10.1145/1194936.1194939,
author = {Gao, Jianfeng and Suzuki, Hisami and Yuan, Wei},
title = {An Empirical Study on Language Model Adaptation},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194939},
doi = {10.1145/1194936.1194939},
abstract = {This article presents an empirical study of four techniques for adapting language models, including a maximum a posteriori (MAP) method and three discriminative training models, in the application of Japanese Kana-Kanji conversion. We compare the performance of these methods from various angles by adapting the baseline model to four adaptation domains. In particular, we attempt to interpret the results in terms of the character error rate (CER) by correlating them with the characteristics of the adaptation domain, measured by using the information-theoretic notion of cross entropy. We show that such a metric correlates well with the CER performance of the adaptation methods, and also show that the discriminative methods are not only superior to a MAP-based method in achieving larger CER reduction, but also in having fewer side effects and being more robust against the similarity between background and adaptation domains.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {209–227},
numpages = {19},
keywords = {statistical language modeling, entropy, Asian language text input, domain adaption, discriminative training}
}

@article{10.1145/1194936.1194938,
author = {Oh, Jong-Hoon and Choi, Key-Sun and Isahara, Hitoshi},
title = {A Machine Transliteration Model Based on Correspondence between Graphemes and Phonemes},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194938},
doi = {10.1145/1194936.1194938},
abstract = {Machine transliteration is an automatic method for converting words in one language into phonetically equivalent ones in another language. There has been growing interest in the use of machine transliteration to assist machine translation and information retrieval. Three types of machine transliteration models---grapheme-based, phoneme-based, and hybrid---have been proposed. Surprisingly, there have been few reports of efforts to utilize the correspondence between source graphemes and source phonemes, although this correspondence plays an important role in machine transliteration. Furthermore, little work has been reported on ways to dynamically handle source graphemes and phonemes. In this paper, we propose a transliteration model that dynamically uses both graphemes and phonemes, particularly the correspondence between them. With this model, we have achieved better performance---improvements of about 15 to 41% in English-to-Korean transliteration and about 16 to 44% in English-to-Japanese transliteration---than has been reported for other models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {185–208},
numpages = {24},
keywords = {Machine transliteration, machine translation, natural language processing, grapheme and phoneme, information retrieval}
}

@article{10.1145/1194936.1194941,
author = {Chung, Tze Leung and Luk, Robert Wing Pong and Wong, Kam Fai and Kwok, Kui Lam and Lee, Dik Lun},
title = {Adapting Pivoted Document-Length Normalization for Query Size: Experiments in Chinese and English},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194941},
doi = {10.1145/1194936.1194941},
abstract = {The vector space model (VSM) is one of the most widely used information retrieval (IR) models in both academia and industry. It was less effective at the Chinese ad hoc retrieval tasks than other retrieval models in the NTCIR-3 evaluation workshop, but comparable to those in the NTCIR-4 and NTCIR-5 workshops. We do not know whether the lower level performance was due to the VSM's inherent deficiencies or to a less effective normalization of document length. Hence we evaluated the VSM with various pivoted normalizations of document length using the NTCIR-3 collection for confirmation. We found that VSM's retrieval effectiveness with pivoted normalization was comparable to other competitive retrieval models (for example, 2-Poisson), and that VSM's retrieval speed with pivoted normalization was similar to competitive retrieval models (2-Poisson). We proposed a novel adaptive scheme that automatically estimates the (near) best parameters for pivoted document-length normalization based on query size; the new normalization is called adaptive pivoted document-length normalization. This scheme achieved good retrieval effectiveness, sometimes for short (title) queries and sometimes for long queries, without manually adjusting parameter values. We found that unique, adaptive pivoted normalization can enhance fixed pivoted normalizations for different test collections (TREC-5 and TREC-6). We also evaluated the VSM with the adaptive pivoted normalization using the pseudo-relevance feedback (PRF) and found that this type of VSM performs similarly to the competitive retrieval models (2-Poisson) with PRF. Hence, we conclude that the VSM with unique (adaptive) pivoted document-length normalization is effective for Chinese IR and that its retrieval effectiveness is comparable to that of other competitive retrieval models with or without PRF for the reference test collections used in this evaluation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {245–263},
numpages = {19},
keywords = {Chinese information retrieval, indexing strategies, pivoted normalization}
}

@article{10.1145/1194936.1194942,
author = {Matsumura, Atsushi and Takasu, Atsuhiro and Adachi, Jun},
title = {Effect of Relationships between Words on Japanese Information Retrieval},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194942},
doi = {10.1145/1194936.1194942},
abstract = {Two Japanese-language information retrieval (IR) methods that enhance retrieval effectiveness by utilizing the relationships between words are proposed. The first method uses dependency relationships between words in a sentence. The second method uses proximity relationships, particularly information about the ordered co-occurrence of words in a sentence, to approximate the dependency relationships between them. A Structured Index has been constructed for these two methods, which represents the dependency relationships between words in a sentence as a set of binary trees. The Structured Index is created by morphological analysis and dependency analysis based on simple template matching and compound noun analysis derived from word statistics. Through retrieval experiments using the Japanese test collection for information retrieval systems (NTCIR-1, the NACSIS Test Collection for IR systems), it is shown that these two methods offer superior retrieval effectiveness compared with the TF--IDF method, and are effective with different databases and diverse search topics sets. There is little difference in retrieval effectiveness between these two methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {264–289},
numpages = {26},
keywords = {test collection, morphological analysis, natural language processing, Structured Index, co-occurrence, Compound noun analysis, phrases, dependency relationships, NTCIR, proximity operation, information retrieval}
}

@article{10.1145/1165255.1165256,
author = {Carpuat, Marine and Fung, Pascale and Ngai, Grace},
title = {Aligning Word Senses Using Bilingual Corpora},
year = {2006},
issue_date = {June 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1165255.1165256},
doi = {10.1145/1165255.1165256},
abstract = {The growing importance of multilingual information retrieval and machine translation has made multilingual ontologies extremely valuable resources. Since the construction of an ontology from scratch is a very expensive and time-consuming undertaking, it is attractive to consider ways of automatically aligning monolingual ontologies, which already exist for many of the world's major languages. Previous research exploited similarity in the structure of the ontologies to align, or manually created bilingual resources. These approaches cannot be used to align ontologies with vastly different structures and can only be applied to much studied language pairs for which expensive resources are already available. In this paper, we propose a novel approach to align the ontologies at the node level: Given a concept represented by a particular word sense in one ontology, our task is to find the best corresponding word sense in the second language ontology. To this end, we present a language-independent, corpus-based method that borrows from techniques used in information retrieval and machine translation. We show its efficiency by applying it to two very different ontologies in very different languages: the Mandarin Chinese HowNet and the American English WordNet. Moreover, we propose a methodology to measure bilingual corpora comparability and show that our method is robust enough to use noisy nonparallel bilingual corpora efficiently, when clean parallel corpora are not available.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {89–120},
numpages = {32},
keywords = {information retrieval, nonparallel corpora, multilingual ontologies, Machine translation}
}

@article{10.1145/1165255.1165258,
author = {Shirado, Tamotsu and Marumoto, Satoko and Murata, Masaki and Isahara, Hitoshi},
title = {Using Japanese Honorific Expressions: A Psychological Study},
year = {2006},
issue_date = {June 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1165255.1165258},
doi = {10.1145/1165255.1165258},
abstract = {We investigated, via experiment, knowledge of normative honorific expressions as used in textbooks and in practice by people. Forty subjects divided into four groups according to age (younger/older) and gender (male/female) participated in the experiments. The results show that knowledge about the use of normative honorific expressions in textbooks is similar to that demonstrated by the younger subject groups, but differed from that of the older subject groups. The knowledge of the older subjects was more complex than that shown in textbooks or demonstrated by the younger subjects. A model that can identify misuse of honorific expressions in sentences is the framework for this investigation. The model is minimal, but could represent 76% to 92% of the subjects' knowledge regarding each honorific element. This model will be useful in the development of computer-aided systems to help teach how honorific expressions should be used.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {146–164},
numpages = {19},
keywords = {misuse, honorific expressions, Japanese}
}

@article{10.1145/1165255.1165257,
author = {Lee, Chun-Jen and Chang, Jason S. and Jang, Jyh-Shing R.},
title = {Alignment of Bilingual Named Entities in Parallel Corpora Using Statistical Models and Multiple Knowledge Sources},
year = {2006},
issue_date = {June 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1165255.1165257},
doi = {10.1145/1165255.1165257},
abstract = {Named entity (NE) extraction is one of the fundamental tasks in natural language processing (NLP). Although many studies have focused on identifying NEs within monolingual documents, aligning NEs in bilingual documents has not been investigated extensively due to the complexity of the task. In this article we introduce a new approach to aligning bilingual NEs in parallel corpora by incorporating statistical models with multiple knowledge sources. In our approach, we model the process of translating an English NE phrase into a Chinese equivalent using lexical translation/transliteration probabilities for word translation and alignment probabilities for word reordering. The method involves automatically learning phrase alignment and acquiring word translations from a bilingual phrase dictionary and parallel corpora, and automatically discovering transliteration transformations from a training set of name-transliteration pairs. The method also involves language-specific knowledge functions, including handling abbreviations, recognizing Chinese personal names, and expanding acronyms. At runtime, the proposed models are applied to each source NE in a pair of bilingual sentences to generate and evaluate the target NE candidates; the source and target NEs are then aligned based on the computed probabilities. Experimental results demonstrate that the proposed approach, which integrates statistical models with extra knowledge sources, is highly feasible and offers significant improvement in performance compared to our previous work, as well as the traditional approach of IBM Model 4.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {121–145},
numpages = {25},
keywords = {named entity alignment, Named entity, phrase translation, parallel corpora, transliteration}
}

@article{10.1145/1165255.1165259,
author = {Wu, Chung-Hsien and Chuang, Ze-Jing and Lin, Yu-Chung},
title = {Emotion Recognition from Text Using Semantic Labels and Separable Mixture Models},
year = {2006},
issue_date = {June 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1165255.1165259},
doi = {10.1145/1165255.1165259},
abstract = {This study presents a novel approach to automatic emotion recognition from text. First, emotion generation rules (EGRs) are manually deduced from psychology to represent the conditions for generating emotion. Based on the EGRs, the emotional state of each sentence can be represented as a sequence of semantic labels (SLs) and attributes (ATTs); SLs are defined as the domain-independent features, while ATTs are domain-dependent. The emotion association rules (EARs) represented by SLs and ATTs for each emotion are automatically derived from the sentences in an emotional text corpus using the a priori algorithm. Finally, a separable mixture model (SMM) is adopted to estimate the similarity between an input sentence and the EARs of each emotional state. Since some features defined in this approach are domain-dependent, a dialog system focusing on the students' daily expressions is constructed, and only three emotional states, happy, unhappy, and neutral, are considered for performance evaluation. According to the results of the experiments, given the domain corpus, the proposed approach is promising, and easily ported into other domains.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {165–183},
numpages = {19},
keywords = {Emotion extraction}
}

@article{10.1145/1131348.1131350,
author = {Park, Kyung-Mi and Kim, Seon-Ho and Rim, Hae-Chang and Hwang, Young-Sook},
title = {ME-Based Biomedical Named Entity Recognition Using Lexical Knowledge},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131350},
doi = {10.1145/1131348.1131350},
abstract = {In this paper, we present a two-phase biomedical NE-recognition method based on a ME model: we first recognize biomedical terms and then assign appropriate semantic classes to the recognized terms. In the two-phase NE-recognition method, the performance of the term-recognition phase is very important, because the semantic classification is performed on the region identified at the recognition phase. In this study, in order to improve the performance of term recognition, we try to incorporate lexical knowledge into pre- and postprocessing of the term-recognition phase. In the preprocessing step, we use domain-salient words as lexical knowledge obtained by corpus comparison. In the postprocessing step, we utilize χ2-based collocations gained from Medline corpus. In addition, we use morphological patterns extracted from the training data as features for learning the ME-based classifiers. Experimental results show that the performance of NE-recognition can be improved by utilizing such lexical knowledge.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {4–21},
numpages = {18},
keywords = {preprocessing, maximum-entropy model, postprocessing, semantic classification, collocations, salient words, Biomedical term recognition, morphological patterns}
}

@article{10.1145/1131348.1131352,
author = {Kim, Jung-Jae and Park, Jong C.},
title = {Extracting Contrastive Information from Negation Patterns in Biomedical Literature},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131352},
doi = {10.1145/1131348.1131352},
abstract = {Expressions of negation in the biomedical literature often encode information of contrast as a means for explaining significant differences between the objects that are so contrasted. We show that such information gives additional insights into the nature of the structures and/or biological functions of these objects, leading to valuable knowledge for subcategorization of protein families by the properties that the involved proteins do not have in common. Based on the observation that the expressions of negation employ mostly predictable syntactic structures that can be characterized by subclausal coordination and by clause-level parallelism, we present a system that extracts such contrastive information by identifying those syntactic structures with natural language processing techniques and with additional linguistic resources for semantics. The implemented system shows the performance of 85.7% precision and 61.5% recall, including 7.7% partial recall, or an F score of 76.6. We apply the system to the biological interactions as extracted by our biomedical information-extraction system in order to enrich proteome databases with contrastive information.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {44–60},
numpages = {17},
keywords = {Information extraction, biomedical literature, contrastive information}
}

@article{10.1145/1131348.1131353,
author = {Kim, Eunju and Song, Yu and Lee, Cheongjae and Kim, Kyoungduk and Lee, Gary Geunbae and Yi, Byoung-Kee and Cha, Jeongwon},
title = {Two-Phase Learning for Biological Event Extraction and Verification},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131353},
doi = {10.1145/1131348.1131353},
abstract = {Many previous biological event-extraction systems were based on hand-crafted rules which were specifically tuned to a specific biological application domain. But manually constructing and tuning the rules are time-consuming processes and make the systems less portable. So supervised machine-learning methods were developed to generate the extraction rules automatically, but accepting the trade-off between precision and recall (high recall with low precision, and vice versa) is a barrier to improving performance. To make matters worse, a text in the biological domain is more complex because it often contains more than two biological events in a sentence, and one event in a noun chunk can be an entity for the other event. As a result, there are as yet no systems that give a good performance in extracting events in biological domains by using supervised machine learning.To overcome the limitations of previous systems and the complexity of biological texts, we present the following new ideas. First, we adopted a supervised machine-learning method to reduce the human effort in making extraction rules in order to obtain a highly domain-portable system. Second, we overcame the classical trade-off between precision and recall by using an event component verification method. Thus, machine learning occurs in two phases in our architecture. In the first phase, the system focuses on improving recall in extracting events between biological entities during a supervised machine-learning period. After extracting the biological events with automatically learned rules, in the second phase the system removes incorrect biological events by verifying the extracted event components with a maximum entropy (ME) classification method. In other words, the system targets for high recall in the first phase and tries to achieve high precision with a classifier in the second phase. Finally, we improved a supervised machine-learning algorithm so that it could learn a rule in a noun chunk and a rule extending throughout a sentence at two different levels, separately, for nested biological events.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {61–73},
numpages = {13},
keywords = {two-level supervised machine learning, Biological event extraction, event component verification}
}

@article{10.1145/1131348.1131351,
author = {Nenadi\'{c}, Goran and Ananiadou, Sophia},
title = {Mining Semantically Related Terms from Biomedical Literature},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131351},
doi = {10.1145/1131348.1131351},
abstract = {Discovering links and relationships is one of the main challenges in biomedical research, as scientists are interested in uncovering entities that have similar functions, take part in the same processes, or are coregulated. This article discusses the extraction of such semantically related entities (represented by domain terms) from biomedical literature. The method combines various text-based aspects, such as lexical, syntactic, and contextual similarities between terms. Lexical similarities are based on the level of sharing of word constituents. Syntactic similarities rely on expressions (such as term enumerations and conjunctions) in which a sequence of terms appears as a single syntactic unit. Finally, contextual similarities are based on automatic discovery of relevant contexts shared among terms. The approach is evaluated using the Genia resources, and the results of experiments are presented. Lexical and syntactic links have shown high precision and low recall, while contextual similarities have resulted in significantly higher recall with moderate precision. By combining the three metrics, we achieved F measures of 68% for semantically related terms and 37% for highly related entities.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {22–43},
numpages = {22},
keywords = {biomedical literature, term similarities, contextual patterns, text mining}
}

@article{10.1145/1131348.1131354,
author = {Mima, Hideki and Ananiadou, Sophia and Matsushima, Katsumori},
title = {Terminology-Based Knowledge Mining for New Knowledge Discovery},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131354},
doi = {10.1145/1131348.1131354},
abstract = {In this article we present an integrated knowledge-mining system for the domain of biomedicine, in which automatic term recognition, term clustering, information retrieval, and visualization are combined. The primary objective of this system is to facilitate knowledge acquisition from documents and aid knowledge discovery through terminology-based similarity calculation and visualization of automatically structured knowledge. This system also supports the integration of different types of databases and simultaneous retrieval of different types of knowledge. In order to accelerate knowledge discovery, we also propose a visualization method for generating similarity-based knowledge maps. The method is based on real-time terminology-based knowledge clustering and categorization and allows users to observe real-time generated knowledge maps, graphically. Lastly, we discuss experiments using the GENIA corpus to assess the practicality and applicability of the system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {74–88},
numpages = {15},
keywords = {terminology, visualization, biomedicine, structuring knowledge, Automatic term recognition, natural language processing}
}

@article{10.1145/1131348.1131349,
author = {Park, Jong C. and Lee, Gary Geunbae and Wong, Limsoon},
title = {AUTHOR: Text Mining and Management in Biomedicine},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1131348.1131349},
doi = {10.1145/1131348.1131349},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–3},
numpages = {3}
}

@article{10.1145/1113308.1113312,
author = {Iida, Ryu and Inui, Kentaro and Matsumoto, Yuji},
title = {Anaphora Resolution by Antecedent Identification Followed by Anaphoricity Determination},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113312},
doi = {10.1145/1113308.1113312},
abstract = {We propose a machine learning-based approach to noun-phrase anaphora resolution that combines the advantages of previous learning-based models while overcoming their drawbacks. Our anaphora resolution process reverses the order of the steps in the classification-then-search model proposed by Ng and Cardie [2002b], inheriting all the advantages of that model. We conducted experiments on resolving noun-phrase anaphora in Japanese. The results show that with the selection-then-classification-based modifications, our proposed model outperforms earlier learning-based approaches.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {417–434},
numpages = {18},
keywords = {Anaphora resolution, anaphoricity determination, antecedent identification}
}

@article{10.1145/1113308.1113310,
author = {Doi, Takao and Yamamoto, Hirofumi and Sumita, Eiichiro},
title = {Example-Based Machine Translation Using Efficient Sentence Retrieval Based on Edit-Distance},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113310},
doi = {10.1145/1113308.1113310},
abstract = {An Example-Based Machine Translation (EBMT) system, whose translation example unit is a sentence, can produce an accurate and natural translation if translation examples similar enough to an input sentence are retrieved. Such a system, however, suffers from the problem of narrow coverage. To reduce the problem, a large-scale parallel corpus is required and, therefore, an efficient method is needed to retrieve translation examples from a large-scale corpus. The authors propose an efficient retrieval method for a sentence-wise EBMT using edit-distance. The proposed retrieval method efficiently retrieves the most similar sentences using the measure of edit-distance without omissions. The proposed method employs search-space division, word graphs, and an A* search algorithm. The performance of the EBMT was evaluated through Japanese-to-English translation experiments using a bilingual corpus comprising hundreds of thousands of sentences from a travel conversation domain. The EBMT system achieved a high-quality translation ability by using a large corpus and also achieved efficient processing by using the proposed retrieval method.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {377–399},
numpages = {23},
keywords = {edit-distance, word graph, Example-based machine translation, A* search, example retrieval}
}

@article{10.1145/1113308.1113311,
author = {Tomiura, Yoichi and Tanaka, Shosaku and Hitaka, Toru},
title = {Estimating Satisfactoriness of Selectional Restriction from Corpus without a Thesaurus},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113311},
doi = {10.1145/1113308.1113311},
abstract = {A selectional restriction specifies what combinations of words are semantically valid in a particular syntactic construction. This is one of the basic and important pieces of knowledge in natural language processing and has been used for syntactic and word sense disambiguation. In the case of acquiring the selectional restriction for many combinations of words from a corpus, it is necessary to estimate whether or not a word combination that is not observed in the corpus satisfies the selectional restriction. This paper proposes a new method for estimating the degree of satisfaction of the selectional restriction for a word combination from a tagged corpus, based on the multiple regression model. The independent variables of this model correspond to modifiers. Unlike a conventional multiple regression analysis, the independent variables are also parameters to be learned. We experiment on estimating the degree of satisfaction of the selectional restriction for Japanese word combinations 〈noun, postpositional-particle, verb〉. The experimental results indicate that our method estimates the degree of satisfaction of a word combination not very well observed in the corpus, and that the accuracy of syntactic disambiguation using the co-occurrencies estimated by our method is higher than using co-occurrence probabilities smoothed by previous methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {400–416},
numpages = {17},
keywords = {multiple regression model, syntactic disambiguation, similarity between words with respect to co-occurrence, Co-occurrence of word combination}
}

@article{10.1145/1113308.1113314,
author = {Ma, Qiang and Tanaka, Katsumi},
title = {Topic-Structure-Based Complementary Information Retrieval and Its Application},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113314},
doi = {10.1145/1113308.1113314},
abstract = {A great deal of technology has been developed to help people access the information they require. With advances in the availability of information, information-seeking activities are becoming more sophisticated. This means that information technology must move to the next stage, i.e., enable users to acquire information from multiple perspectives to satisfy diverse needs. For instance, with the spread of digital broadcasting and broadband Internet connection services, infrastructure for the integration of TV programs and the Internet has been developed that enables users to acquire information from different media at the same time to improve information quality and the level of detail. In this paper, we propose a novel content-based join model for data streams (closed captions of videos or TV programs) and Web pages based on the concept of topic structures. We then propose a mechanism based on this model for retrieving complementary Web pages to augment the content of video or television programs. One of the most notable features of this complementary retrieval mechanism is that the retrieved information is not just similar to the video or TV program, but also provides additional information. In addition, we introduce an application system called WebTelop, which augments the content of TV programs in real time by using complementary Web pages. We also describe some experimental results.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {475–503},
numpages = {29},
keywords = {topic structure, complementary information retrieval, content fusion, Information complementation}
}

@article{10.1145/1113308.1113309,
author = {Sakai, Tetsuya and Matsumoto, Yuji},
title = {Introduction to the Special Issue: Recent Advances in Information Processing and Access for Japanese},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113309},
doi = {10.1145/1113308.1113309},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {375–376},
numpages = {2}
}

@article{10.1145/1113308.1113313,
author = {Inui, Takashi and Inui, Kentaro and Matsumoto, Yuji},
title = {Acquiring Causal Knowledge from Text Using the Connective Marker <i>Tame</i>},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1113308.1113313},
doi = {10.1145/1113308.1113313},
abstract = {In this paper, we deal with automatic knowledge acquisition from text, specifically the acquisition of causal relations. A causal relation is the relation existing between two events such that one event causes (or enables) the other event, such as “hard rain causes flooding” or “taking a train requires buying a ticket.” In previous work these relations have been classified into several types based on a variety of points of view. In this work, we consider four types of causal relations---cause, effect, precond(ition) and means---mainly based on agents' volitionality, as proposed in the research field of discourse understanding. The idea behind knowledge acquisition is to use resultative connective markers, such as “because,” “but,” and “if” as linguistic cues. However, there is no guarantee that a given connective marker always signals the same type of causal relation. Therefore, we need to create a computational model that is able to classify samples according to the causal relation. To examine how accurately we can automatically acquire causal knowledge, we attempted an experiment using Japanese newspaper articles, focusing on the resultative connective “tame.” By using machine-learning techniques, we achieved 80% recall with over 95% precision for the cause, precond, and means relations, and 30% recall with 90% precision for the effect relation. Furthermore, the classification results suggest that one can expect to acquire over 27,000 instances of causal relations from 1 year of Japanese newspaper articles.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {435–474},
numpages = {40},
keywords = {connective marker, volitionality, Causal relation}
}

@article{10.1145/1111667.1111672,
author = {Mori, Tatsunori and Nozawa, Masanori and Asada, Yoshiaki},
title = {Multi-Answer-Focused Multi-Document Summarization Using a Question-Answering Engine},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111672},
doi = {10.1145/1111667.1111672},
abstract = {In recent years, answer-focused summarization has gained attention as a technology complementary to information retrieval and question answering. In order to realize multi-document summarization focused by multiple questions, we propose a method to calculate sentence importance using scores, for responses to multiple questions, generated by a Question-Answering engine. Further, we describe the integration of this method with a generic multi-document summarization system. The evaluation results demonstrate that the performance of the proposed method is better than not only several baselines but also other participants' systems at the evaluation workshop NTCIR4 TSC3 Formal Run. However, it should be noted that some of the other systems do not use the information of questions.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {305–320},
numpages = {16},
keywords = {Information gain ratio, question-answering engine, maximal marginal relevance}
}

@article{10.1145/1111667.1111670,
author = {Isozaki, Hideki},
title = {An Analysis of a High-Performance Japanese Question Answering System},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111670},
doi = {10.1145/1111667.1111670},
abstract = {Twenty-five Japanese Question Answering systems participated in NTCIR QAC2 subtask 1. Of these, our system SAIQA-QAC2 performed the best: MRR = 0.607. SAIQA-QAC2 is an improvement on our previous system SAIQA-Ii that achieved MRR = 0.46 for QAC1. We mainly improved the answer-type determination module and the retrieval module. In general, a fine-grained answer taxonomy improves QA performance but it is difficult to build an accurate answer extraction module for the fine-grained taxonomy because Machine Learning methods require a huge training corpus and hand-crafted rules are hard to maintain. Therefore, we built a fine-grained system by using a coarse-grained named entity recognizer and a Japanese lexicon “Nihongo Goi-taikei.” Our experiments show that named entity/numerical expression recognition and word sense-based answer extraction mainly contributed to the performance. In addition, we developed a new proximity-based document retrieval module that performs better than BM25. We also compared its performance with MultiText, a conventional proximity-based retrieval method developed for QA.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {263–279},
numpages = {17},
keywords = {document retrieval, Question answering}
}

@article{10.1145/1111667.1111671,
author = {Mori, Tatsunori},
title = {Japanese Question-Answering System Using A* Search and Its Improvement},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111671},
doi = {10.1145/1111667.1111671},
abstract = {We have proposed a method to introduce A* search control in a sentential matching mechanism for Japanese question-answering systems in order to reduce the turnaround time while maintaining the accuracy of the answers. Using this method, preprocessing need not be performed on a document database and we may use any information retrieval systems by writing a simple wrapper program. However, the disadvantage is that the accuracy is not sufficiently high and the mean reciprocal rank (MRR) is approximately 0.3 in NTCIR3 QAC1, an evaluation workshop for question-answering systems. In order to improve the accuracy, we propose several measures of the degree of sentence matching and a variant of a voting method. Both of them can be integrated with our system of controlled search. Using these techniques, the system achieves a higher MRR of 0.5 in the evaluation workshop NTCIR4 QAC2.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {280–304},
numpages = {25},
keywords = {dependency vectors, question answering, pseudo voting method, A* search, sentence chaining}
}

@article{10.1145/1111667.1111674,
author = {Yoshioka, Masaharu and Haraguchi, Makoto},
title = {On a Combination of Probabilistic and Boolean Ir Models for WWW Document Retrieval},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111674},
doi = {10.1145/1111667.1111674},
abstract = {Even though a Boolean query can express the information need precisely enough to select relevant documents, it is not easy to construct an appropriate Boolean query that covers all relevant documents. To utilize a Boolean query effectively, a mechanism to retrieve as many as possible relevant documents is therefore required. In accordance with this requirement, we propose a method for modifying a given Boolean query by using information from a relevant document set. The retrieval results, however, may deteriorate if some important query terms are removed by this reformulation. A further mechanism is thus required in order to use other query terms that are useful for finding more relevant documents, but are not strictly required in relevant documents. To meet this requirement, we propose a new method that combines the probabilistic IR and the Boolean IR models. We also introduce a new IR system---called appropriate Boolean query reformulation for information retrieval (ABRIR)---based on these two methods and the Okapi system. ABRIR uses both a word index and a phrase index formed from combinations of two adjacent noun words. The effectiveness of these two methods was confirmed according to the NTCIR-4 Web test collection.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {340–356},
numpages = {17},
keywords = {Boolean IR model, probabilistic IR model}
}

@article{10.1145/1111667.1111675,
author = {Lingpeng, Yang and Donghong, Ji and Li, Tang and Zhengyu, Niu},
title = {Chinese Information Retrieval Based on Terms and Relevant Terms},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111675},
doi = {10.1145/1111667.1111675},
abstract = {In this article we describe our approach to Chinese information retrieval, where a query is a short natural language description. First, we use automatically extracted short terms from document sets to build indexes and use the short terms in both the query and documents to do initial retrieval. Next, we use long terms extracted from the document collection to reorder the top N retrieved documents to improve precision. Finally, we acquire the relevant terms of the short terms from the Internet and the top retrieved documents and use them to do query expansion. Experiments on the NTCIR-4 CLIR Chinese SLIR sub-collection show that document reranking can both improve the retrieval performance on its own and make a significant contribution to query expansion. The experiments also show that the extended query expansion proposed in this article is more effective than the standard Rocchio query expansion.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {357–374},
numpages = {18},
keywords = {document re-ranking, relevant term, information retrieval, term clustering, Term extraction, query expansion}
}

@article{10.1145/1111667.1111673,
author = {Okazaki, Naoaki and Matsuo, Yutaka and Ishizuka, Mitsuru},
title = {Improving Chronological Ordering of Sentences Extracted from Multiple Newspaper Articles},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111673},
doi = {10.1145/1111667.1111673},
abstract = {It is necessary to determine a proper arrangement of extracted sentences to generate a well-organized summary from multiple documents. This paper describes our Multi-Document Summarization (MDS) system for TSC-3. It specifically addresses an approach to coherent sentence ordering for MDS. An impediment to the use of chronological ordering, which is widely used by conventional summarization system, is that it arranges sentences without considering the presupposed information of each sentence. We propose a method to improve chronological ordering by resolving precedent information of arranging sentences. Combining the refinement algorithm with topical segmentation and chronological ordering, we address our experiments and metrics to test the effectiveness of MDS tasks. Results demonstrate that the proposed method significantly improves chronological sentence ordering. At the end of the paper, we also report an outline/evaluation of important sentence extraction and redundant clause elimination integrated in our MDS system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {321–339},
numpages = {19},
keywords = {order, arrange, sentence ordering, coherence, Multi-document summarization}
}

@article{10.1145/1111667.1111669,
author = {Kato, Tsuneaki and Fukumoto, Jun'ichi and Masui, Fumito and Kando, Noriko},
title = {Are Open-Domain Question Answering Technologies Useful for Information Access Dialogues?---An Empirical Study and a Proposal of a Novel Challenge},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111669},
doi = {10.1145/1111667.1111669},
abstract = {There are strong expectations for the use of question answering technologies in information access dialogues, such as for information gathering and browsing. In this paper, we empirically examine what kinds of abilities are needed for question answering systems in such situations, and propose a challenge for evaluating those abilities objectively and quantitatively. We also show that existing technologies have the potential to address this challenge. From the empirical study, we found that questions that have values and names as answers account for a majority in realistic information-gathering situations and that those sequences of questions contain a wide range of reference expressions and are sometimes complicated by the inclusion of subdialogues and focus shifts. The challenge proposed is not only novel as an evaluation of the handling of information access dialogues, but also includes several valuable ideas such as categorization and characterization of information access dialogues, and introduces three measures to evaluate various aspects in addressing list-type questions and reference test sets for evaluating context-processing ability in isolation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {243–262},
numpages = {20},
keywords = {Question answering, evaluation, information access dialogue}
}

@article{10.1145/1111667.1111668,
author = {Nakagawa, Hiroshi and Mori, Tatsunori and Kando, Noriko},
title = {Preface to the Special Issues on NTCIR-4},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1111667.1111668},
doi = {10.1145/1111667.1111668},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {237–242},
numpages = {6}
}

@article{10.1145/1105696.1105697,
author = {Zhang, Ying and Vines, Phil and Zobel, Justin},
title = {Chinese OOV Translation and Post-Translation Query Expansion in Chinese--English Cross-Lingual Information Retrieval},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105697},
doi = {10.1145/1105696.1105697},
abstract = {Cross-lingual information retrieval allows users to query mixed-language collections or to probe for documents written in an unfamiliar language. A major difficulty for cross-lingual information retrieval is the detection and translation of out-of-vocabulary (OOV) terms; for OOV terms in Chinese, another difficulty is segmentation. At NTCIR-4, we explored methods for translation and disambiguation for OOV terms when using a Chinese query on an English collection. We have developed a new segmentation-free technique for automatic translation of Chinese OOV terms using the web. We have also investigated the effects of distance factor and window size when using a hidden Markov model to provide disambiguation. Our experiments show these methods significantly improve effectiveness; in conjunction with our post-translation query expansion technique, effectiveness approaches that of monolingual retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {57–77},
numpages = {21},
keywords = {post-translation query expansion, web mining, CLIR, query translations, OOV terms, HMM, mutual information, translation disambiguation}
}

@article{10.1145/1105696.1105699,
author = {Sakai, Tetsuya and Manabe, Toshihiko and Koyama, Makoto},
title = {Flexible Pseudo-Relevance Feedback via Selective Sampling},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105699},
doi = {10.1145/1105696.1105699},
abstract = {Although Pseudo-Relevance Feedback (PRF) is a widely used technique for enhancing average retrieval performance, it may actually hurt performance for around one-third of a given set of topics. To enhance the reliability of PRF, Flexible PRF has been proposed, which adjusts the number of pseudo-relevant documents and/or the number of expansion terms for each topic. This paper explores a new, inexpensive Flexible PRF method, called Selective Sampling, which is unique in that it can skip documents in the initial ranked output to look for more “novel” pseudo-relevant documents. While Selective Sampling is only comparable to Traditional PRF in terms of average performance and reliability, per-topic analyses show that Selective Sampling outperforms Traditional PRF almost as often as Traditional PRF outperforms Selective Sampling. Thus, treating the top P documents as relevant is often not the best strategy. However, predicting when Selective Sampling outperforms Traditional PRF appears to be as difficult as predicting when a PRF method fails. For example, our per-topic analyses show that even the proportion of truly relevant documents in the pseudo-relevant set is not necessarily a good performance predictor.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {111–135},
numpages = {25},
keywords = {Pseudo-relevance feedback, flexible pseudo-relevance feedback, selective sampling}
}

@article{10.1145/1105696.1105701,
author = {Savoy, Jacques},
title = {Comparative Study of Monolingual and Multilingual Search Models for Use with Asian Languages},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105701},
doi = {10.1145/1105696.1105701},
abstract = {Based on the NTCIR-4 test-collection, our first objective is to present an overview of the retrieval effectiveness of nine vector-space and two probabilistic models that perform monolingual searches in the Chinese, Japanese, Korean, and English languages. Our second goal is to analyze the relative merits of the various automated and freely available toolsto translate the English-language topics into Chinese, Japanese, or Korean, and then submit the resultant query in order to retrieve pertinent documents written in one of the three Asian languages. We also demonstrate how bilingual searches could be improved by applying both the combined query translation strategies and data-fusion approaches. Finally, we address basic problems related to multilingual searches, in which queries written in English are used to search documents written in the English, Chinese, Japanese, and Korean languages.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {163–189},
numpages = {27},
keywords = {natural language processing with Asian languages, search engines with Asian languages, Korean language, Japanese language, cross-language information retrieval, Chinese language, results-merging, Multilingual information retrieval}
}

@article{10.1145/1105696.1105700,
author = {Kwok, Kui Lam and Choi, Sora and Dinstl, Norbert},
title = {Rich Results from Poor Resources: NTCIR-4 Monolingual and Cross-Lingual Retrieval of Korean Texts Using Chinese and English},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105700},
doi = {10.1145/1105696.1105700},
abstract = {We report on Korean monolingual, Chinese-Korean English-as-pivot bilingual, and Chinese-English bilingual CLIR experiments using MT software augmented with Web-based entity-oriented translation as resources in the NTCIR-4 environment. Simple stemming is helpful in improving bigram indexing for Korean retrieval. For word indexing, keeping nouns only is preferable. Web-based translation reduces untranslated terms left over after MT and substantially improves CLIR results. Translation concatenation is found to consistently improve CLIR effectiveness, while combining a retrieval list from bigram and word indexing is also helpful. A method to disambiguate multiple MT outputs using a log likelihood ratio threshold was tested. Depending on the nature of the title or description queries, bigram only or a retrieval combination, or relaxed or rigid evaluations, direct bilingual CLIR returned an average precision of 71--79% (English-Korean) and 76--84% (Chinese-English) of the corresponding Korean-Korean and English-English monolingual results. Using English as a pivot in Chinese-Korean CLIR provides about 55--65% the effectiveness that Korean alone does. Entity/terminology translation at the pivot language stage accounts for a large portion of this deficiency. A topic with comparatively worse Chinese-English bilingual result does not necessarily mean that it will continue to under-perform (after further transitive Korean translation) at the Korean retrieval level.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {136–162},
numpages = {27},
keywords = {Web-based entity-oriented translation, bigram indexing, Chinese-English-Korean pivot CLIR, translation disambiguation, Chinese-Korean CLIR}
}

@article{10.1145/1105696.1105702,
author = {Mase, Hisao and Matsubayashi, Tadataka and Ogawa, Yuichi and Iwayama, Makoto and Oshio, Tadaaki},
title = {Proposal of Two-Stage Patent Retrieval Method Considering the Claim Structure},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105702},
doi = {10.1145/1105696.1105702},
abstract = {The importance of patents is increasing in global society. In preparing a patent application, it is essential to search for related patents that may invalidate the invention. However, it is time-consuming to identify them among the millions of patents. This article proposes a patent-retrieval method that considers a claim structure for a more accurate search for invalidity. This method uses a claim text as input; it consists of two retrieval stages. In stage 1, general text analysis and retrieval methods are applied to improve recall. In stage 2, the top N documents retrieved in stage 1 are rearranged to improve precision by applying text analysis and retrieval methods using the claim structure. Our two-stage retrieval introduces five precision-oriented analysis and retrieval methods: query-term extraction from a portion of a claim text that describes the characteristics of a claim; query term-weighting without term frequency; query term-weighting with “measurement terms”; text retrieval using only claims as a target; and calculating the relevant score by “partially” adding scores in stage 2 to those in stage 1. Evaluation results using test sets of the NTCIR4 Patent Retrieval Task show that our methods are effective, though the degree of the effectiveness varies depending on the test sets.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {190–206},
numpages = {17},
keywords = {term weighting, Patent retrieval, term extraction, claim structure, relevant score calculation}
}

@article{10.1145/1105696.1105698,
author = {Qu, Yan and Hull, David A. and Grefenstette, Gregory and Evans, David A. and Ishikawa, Motoko and Nara, Setsuko and Ueda, Toshiya and Noda, Daisuke and Arita, Kousaku and Funakoshi, Yuki and Matsuda, Hiroshi},
title = {Towards Effective Strategies for Monolingual and Bilingual Information Retrieval: Lessons Learned from NTCIR-4},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105698},
doi = {10.1145/1105696.1105698},
abstract = {At the NTCIR-4 workshop, Justsystem Corporation (JSC) and Clairvoyance Corporation (CC) collaborated in the cross-language retrieval task (CLIR). Our goal was to evaluate the performance and robustness of our recently developed commercial-grade CLIR systems for English and Asian languages. The main contribution of this article is the investigation of different strategies, their interactions in both monolingual and bilingual retrieval tasks, and their respective contributions to operational retrieval systems in the context of NTCIR-4. We report results of Japanese and English monolingual retrieval and results of Japanese-to-English bilingual retrieval. In monolingual retrieval analysis, we examine two special properties of the NTCIR experimental design (two levels of relevance and identical queries in multiple languages) and explore how they interact with strategies of our retrieval system, including pseudo-relevance feedback, multi-word term down-weighting, and term weight merging strategies. Our analysis shows that the choice of language (English or Japanese) does not have a significant impact on retrieval performance. Query expansion is slightly more effective with relaxed judgments than with rigid judgments. For better retrieval performance, weights of multi-word terms should be lowered. In the bilingual retrieval analysis, we aim to identify robust strategies that are effective when used alone and when used in combination with other strategies. We examine cross-lingual specific strategies such as translation disambiguation and translation structuring, as well as general strategies such as pseudo-relevance feedback and multi-word term down-weighting. For shorter title topics, pseudo-relevance feedback is a major performance enhancer, but translation structuring affects retrieval performance negatively when used alone or in combination with other strategies. All experimented strategies improve retrieval performance for the longer description topics, with pseudo-relevance feedback and translation structuring as the major contributors.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {78–110},
numpages = {33},
keywords = {Monolingual information retrieval, NTCIR, comparison, cross-language information retrieval}
}

@article{10.1145/1105696.1105853,
author = {Fujita, Sumio},
title = {Revisiting Document Length Hypotheses: A Comparative Study of Japanese Newspaper and Patent Retrieval},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105853},
doi = {10.1145/1105696.1105853},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {207–235},
numpages = {29}
}

@article{10.1145/1066078.1066080,
author = {Murata, Masaki and Utiyama, Masao and Uchimoto, Kiyotaka and Isahara, Hitoshi and Ma, Qing},
title = {Correction of Errors in a Verb Modality Corpus for Machine Translation with a Machine-Learning Method},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1066078.1066080},
doi = {10.1145/1066078.1066080},
abstract = {In recent years, various types of tagged corpora have been constructed and much research using tagged corpora has been done. However, tagged corpora contain errors, which impedes the progress of research. Therefore, the correction of errors in corpora is an important research issue. In this study we investigate the correction of such errors, which we call corpus correction. Using machine-learning methods, we applied corpus correction to a verb modality corpus for machine translation. We used the maximum-entropy and decision-list methods as machine-learning methods. We compared several kinds of methods for corpus correction in our experiments, and determined which is most effective by using a statistical test. We obtained several noteworthy findings: (1) Precision was almost the same for both detection and correction, so it is more convenient to do both correction and detection, rather than detection only. (2) In general, the maximum-entropy method worked better than the decision-list method; but the two methods had almost the same precision for the top 50 pieces of extracted data when closed data was used. (3) In terms of precision, the use of closed data was better than the use of open data; however, in terms of the total number of extracted errors, the use of open data was better than the use of closed data. Based on our analysis of these results, we developed a good method for corpus correction. We confirmed the effectiveness of our method by carrying out experiments on machine translation. As corpus-based machine translation continues to be developed, the corpus correction we discuss in this article should prove to be increasingly significant.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {18–37},
numpages = {20},
keywords = {machine learning, modality corpus, machine translation, corpus correction}
}

@article{10.1145/1066078.1066081,
author = {Hendessi, F. and Ghayoori, A. and Gulliver, T. A.},
title = {A Speech Synthesizer for Persian Text Using a Neural Network with a Smooth Ergodic HMM},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1066078.1066081},
doi = {10.1145/1066078.1066081},
abstract = {The feasibility of converting text into speech using an inexpensive computer with minimal memory is of great interest. Speech synthesizers have been developed for many popular languages (e.g., English, Chinese, Spanish, French, etc.), but designing a speech synthesizer for a language is largely dependant on the language structure. In this article, we develop a Persian synthesizer that includes an innovative text analyzer module. In the synthesizer, the text is segmented into words and after preprocessing, a neural network is passed over each word. In addition to preprocessing, a new model (SEHMM) is used as a postprocessor to compensate for errors generated by the neural network. The performance of the proposed model is verified and the intelligibility of the synthetic speech is assessed via listening tests.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {38–52},
numpages = {15},
keywords = {Hidden Markov model, TD-PSOLA}
}

@article{10.1145/1066078.1066079,
author = {Wu, Chung-Hsien and Yeh, Jui-Feng and Chen, Ming-Jun},
title = {Domain-Specific FAQ Retrieval Using Independent Aspects},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1066078.1066079},
doi = {10.1145/1066078.1066079},
abstract = {This investigation presents an approach to domain-specific FAQ (frequently-asked question) retrieval using independent aspects. The data analysis classifies the questions in the collected QA (question-answer) pairs into ten question types in accordance with question stems. The answers in the QA pairs are then paragraphed and clustered using latent semantic analysis and the K-means algorithm. For semantic representation of the aspects, a domain-specific ontology is constructed based on WordNet and HowNet. A probabilistic mixture model is then used to interpret the query and QA pairs based on independent aspects; hence the retrieval process can be viewed as the maximum likelihood estimation problem. The expectation-maximization (EM) algorithm is employed to estimate the optimal mixing weights in the probabilistic mixture model. Experimental results indicate that the proposed approach outperformed the FAQ-Finder system in medical FAQ retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–17},
numpages = {17},
keywords = {question-answering, probabilistic mixture model, ontology, natural language processing, latent semantic analysis, information retrieval, FAQ retrieval}
}

@article{10.1145/1039621.1039622,
author = {Myaeng, Sung Hyon},
title = {Introduction to the Special Issue on Computer Processing of Oriental Languages},
year = {2004},
issue_date = {December 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1039621.1039622},
doi = {10.1145/1039621.1039622},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {213},
numpages = {1}
}

@article{10.1145/1039621.1039625,
author = {Zhang, Le and Zhu, Jingbo and Yao, Tianshun},
title = {An Evaluation of Statistical Spam Filtering Techniques},
year = {2004},
issue_date = {December 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1039621.1039625},
doi = {10.1145/1039621.1039625},
abstract = {This paper evaluates five supervised learning methods in the context of statistical spam filtering. We study the impact of different feature pruning methods and feature set sizes on each learner's performance using cost-sensitive measures. It is observed that the significance of feature selection varies greatly from classifier to classifier. In particular, we found support vector machine, AdaBoost, and maximum entropy model are top performers in this evaluation, sharing similar characteristics: not sensitive to feature selection strategy, easily scalable to very high feature dimension, and good performances across different datasets. In contrast, naive Bayes, a commonly used classifier in spam filtering, is found to be sensitive to feature selection methods on small feature set, and fails to function well in scenarios where false positives are penalized heavily. The experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams (such as λ = 999), so as to maintain a better-than-baseline performance. An interesting finding is the effect of mail headers on spam filtering, which is often ignored in previous studies. Experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only. This implies that message headers can be reliable and powerfully discriminative feature sources for spam filtering.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {243–269},
numpages = {27},
keywords = {text categorization, Spam filtering}
}

@article{10.1145/1039621.1039624,
author = {Kim, Pyung and Myaeng, Sung Hyon},
title = {Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking},
year = {2004},
issue_date = {December 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1039621.1039624},
doi = {10.1145/1039621.1039624},
abstract = {Temporal information plays an important role in natural language processing (NLP) applications such as information extraction, discourse analysis, automatic summarization, and question-answering. In the topic detection and tracking (TDT) area, the temporal information often used is the publication date of a message, which is readily available but limited in its usefulness. We developed a relatively simple NLP method for extracting temporal information from Korean news articles, with the goal of improving performance of TDT tasks. To extract temporal information, we make use of finite state automata and a lexicon containing timerevealing vocabulary. Extracted information is converted into a canonicalized representation of a time point or a time duration. We first evaluated and investigated the extraction and canonicalization methods for their accuracy and the extent to which temporal information extracted as such can help TDT tasks. The experimental results show that time information extracted from the text does indeed help to significantly improve both precision and recall.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {227–242},
numpages = {16},
keywords = {temporal information extraction, event detection and tracking}
}

@article{10.1145/1039621.1039623,
author = {Baoli, Li and Qin, Lu and Shiwen, Yu},
title = {An Adaptive <i>k</i>-Nearest Neighbor Text Categorization Strategy},
year = {2004},
issue_date = {December 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1039621.1039623},
doi = {10.1145/1039621.1039623},
abstract = {k is the most important parameter in a text categorization system based on the k-nearest neighbor algorithm (kNN). To classify a new document, the k-nearest documents in the training set are determined first. The prediction of categories for this document can then be made according to the category distribution among the k nearest neighbors. Generally speaking, the class distribution in a training set is not even; some classes may have more samples than others. The system's performance is very sensitive to the choice of the parameter k. And it is very likely that a fixed k value will result in a bias for large categories, and will not make full use of the information in the training set. To deal with these problems, an improved kNN strategy, in which different numbers of nearest neighbors for different categories are used instead of a fixed number across all categories, is proposed in this article. More samples (nearest neighbors) will be used to decide whether a test document should be classified in a category that has more samples in the training set. The numbers of nearest neighbors selected for different categories are adaptive to their sample size in the training set. Experiments on two different datasets show that our methods are less sensitive to the parameter k than the traditional ones, and can properly classify documents belonging to smaller classes with a large k. The strategy is especially applicable and promising for cases where estimating the parameter k via cross-validation is not possible and the class distribution of a training set is skewed.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {215–226},
numpages = {12},
keywords = {k-nearest neighbor algorithm, machine learning, text categorization, text classification}
}

@article{10.1145/1037811.1037812,
author = {Huang, Chien-Chung and Chuang, Shui-Lung and Chien, Lee-Feng},
title = {Using a Web-Based Categorization Approach to Generate Thematic Metadata from Texts},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1037811.1037812},
doi = {10.1145/1037811.1037812},
abstract = {Conventional tools for automatic metadata creation mostly extract named entities or text segments from texts and annotate them with information about persons, locations, dates, and so on. However, this kind of entity type information is often insufficient for machines to understand the facts contained in the texts, thus precluding the possibility of implementing more advanced, intelligent applications, such as concept-based search. In this work, we try to create more refined thematic metadata inherent in texts. Based on Web resource mining, our approach acquires training corpora necessary to describe both the thematic categories and the metadata extracted from the texts. The approach then finds the corresponding relationships among them by means of categorization and thus generates thematic metadata for the textual data. Experimental results confirm the potential and wide adaptability of our approach.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {190–212},
numpages = {23},
keywords = {Categorization, metadata, Web mining}
}

@article{10.1145/1037811.1037813,
author = {Li, Yujia and Lee, Tan and Qian, Yao},
title = {Analysis and Modeling of F0 Contours for Cantonese Text-to-Speech},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1037811.1037813},
doi = {10.1145/1037811.1037813},
abstract = {For the generation of highly natural synthetic speech, the control of prosody is of primary importance. The fundamental frequency (F0) is one of the most important components of speech prosody. This research investigates the variation of F0 in continuous Cantonese speech, with the goal of establishing an effective mechanism of prosody control in Cantonese text-to-speech (TTS) applications. Cantonese is a commonly used Chinese dialect that is well known for being rich in tones. This article describes a simple yet effective approach to the analysis and modeling of F0. The surface F0 contour of a continuous Cantonese utterance is considered to be the combination of a global component--phrase-level intonation curve, and local components--syllable-level tone contoursA novel method of F0 normalization is proposed to separate the local components from the global one. As a result, the variation in tone contours is greatly reduced. Statistical analysis is performed for the phrase curves and context-dependent tone contours that are extracted from a large corpus of 1,200 utterances. Specifically, the analysis is focused on co-articulated tone contours for disyllabic words, cross-word contours, and phrase-initial tone contours. Based on the results of the analysis, a template-based model for F0 generation is established and integrated with a Cantonese TTS system. Subjective listening tests show that the proposed model significantly improves the naturalness of the output speech.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {169–180},
numpages = {12},
keywords = {tones, Text-to-speech, Chinese dialects, prosody, fundamental frequency}
}

@article{10.1145/1034780.1034781,
author = {Gao, Jianfeng and Lin, Chin-Yew},
title = {Introduction to the Special Issue on Statistical Language Modeling},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034781},
doi = {10.1145/1034780.1034781},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {87–93},
numpages = {7},
keywords = {Statistical language modeling, source-channel models, n-gram models, discriminative training}
}

@article{10.1145/1034780.1034784,
author = {Chen, Berlin and Wang, Hsin-Min and Lee, Lin-Shan},
title = {A Discriminative HMM/N-Gram-Based Retrieval Approach for Mandarin Spoken Documents},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034784},
doi = {10.1145/1034780.1034784},
abstract = {In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {128–145},
numpages = {18},
keywords = {syllable-level Indexing features, Mandarin spoken documents, Hidden Markov models}
}

@article{10.1145/1034780.1034783,
author = {Linares, Diego and Bened\'{\i}, Jos\'{e}-Miguel and S\'{a}nchez, Joan-Andreu},
title = {A Hybrid Language Model Based on a Combination of <i>N</i>-Grams and Stochastic Context-Free Grammars},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034783},
doi = {10.1145/1034780.1034783},
abstract = {In this paper, a hybrid language model is defined as a combination of a word-based <i>n</i>-gram, which is used to capture the local relations between words, and a category-based stochastic context-free grammar (SCFG) with a word distribution into categories, which is defined to represent the long-term relations between these categories. The problem of unsupervised learning of a SCFG in General Format and in Chomsky Normal Form by means of estimation algorithms is studied. Moreover, a bracketed version of the classical estimation algorithm based on the Earley algorithm is proposed. This paper also explores the use of SCFGs obtained from a treebank corpus as initial models for the estimation algorithms. Experiments on the UPenn Treebank corpus are reported. These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {113–127},
numpages = {15},
keywords = {stochastic context-free grammar, Language model}
}

@article{10.1145/1034780.1034782,
author = {Kim, Woosung and Khudanpur, Sanjeev},
title = {Lexical Triggers and Latent Semantic Analysis for Cross-Lingual Language Model Adaptation},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034782},
doi = {10.1145/1034780.1034782},
abstract = {In-domain texts for estimating statistical language models are not easily found for most languages of the world. We present two techniques to take advantage of in-domain text resources in other languages. First, we extend the notion of <i>lexical triggers</i>, which have been used monolingually for language model adaptation, to the cross-lingual problem, permitting the construction of sharper language models for a target-language document by drawing statistics from related documents in a resource-rich language. Next, we show that <i>cross-lingual latent semantic analysis</i> is similarly capable of extracting useful statistics for language modeling. Neither technique requires explicit translation capabilities between the two languages! We demonstrate significant reductions in both perplexity and word error rate on a Mandarin speech recognition task by using these techniques.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {94–112},
numpages = {19},
keywords = {latent semantic analysis, multilingual processing, language model adaptation, lexical trigger, Automatic speech recognition, statistical language modeling}
}

@article{10.1145/1034780.1034786,
author = {Fung, Pascale and Ngai, Grace and Yang, Yongsheng and Chen, Benfeng},
title = {A Maximum-Entropy Chinese Parser Augmented by Transformation-Based Learning},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034786},
doi = {10.1145/1034780.1034786},
abstract = {Parsing, the task of identifying syntactic components, e.g., noun and verb phrases, in a sentence, is one of the fundamental tasks in natural language processing. Many natural language applications such as spoken-language understanding, machine translation, and information extraction, would benefit from, or even require, high accuracy parsing as a preprocessing step.Even though most state-of-the-art statistical parsers were initially constructed for parsing in English, most of them are not language-specific, in that they do not rely on properties of the language that are specific to English. Therefore, construction of a parser in a given language becomes a matter of retraining the statistical parameters with a Treebank in the corresponding language.The development of the Chinese treebank [Xia et al. 2000] spurred the construction of parsers for Chinese. However, Chinese as a language poses some unique problems for the development of a statistical parser, the most apparent being word segmentation. Since words in written Chinese are not delimited in the same way as in Western languages, the first problem that needs to be solved before an existing statistical method can be applied to Chinese is to identify the word boundaries. This is a step that is neglected by most pre-existing Chinese parsers, which assume that the input data has already been pre-segmented.This article describes a character-based statistical parser, which gives the best performance to-date on the Chinese treebank data. We augment an existing maximum entropy parser with transformation-based learning, creating a parser that can operate at the character level. We present experiments that show that our parser achieves results that are close to those achievable under perfect word segmentation conditions.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {159–168},
numpages = {10},
keywords = {Parsing for Chinese, transformation-based learning, POS tagging, maximum entropy, chunking and parsing for Chinese}
}

@article{10.1145/1034780.1034785,
author = {Nguyen, Minh Le and Horiguchi, Susumu and Shimazu, Akira and Ho, Bao Tu},
title = {Example-Based Sentence Reduction Using the Hidden Markov Model},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1034780.1034785},
doi = {10.1145/1034780.1034785},
abstract = {Sentence reduction is the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. All previous methods required a syntax parser before sentences could be reduced; hence it was difficult to apply them to a language with no reliable parser. In this article we propose two new sentence-reduction algorithms that do not use syntactic parsing for the input sentence. The first algorithm, based on the template-translation learning algorithm, one of example-based machine-translation methods, works quite well in reducing sentences, but its computational complexity can be exponential in certain cases. The second algorithm, an extension of the template--translation algorithm via innovative employment of the Hidden Markov model, which uses the set of template rules learned from examples, can overcome this computation problem. Experiments show that the proposed algorithms achieve acceptable results in comparison to sentence reduction done by humans.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {146–158},
numpages = {13},
keywords = {Sentence reduction, HMM-based sentence reduction, example-based sentence reduction}
}

@article{10.1145/1017068.1017070,
author = {Han, Benjamin and Lavie, Alon},
title = {A Framework for Resolution of Time in Natural Language},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1017068.1017070},
doi = {10.1145/1017068.1017070},
abstract = {Automatic extraction and reasoning over temporal properties in natural language discourse has not had wide use in practical systems due to its demand for a rich and compositional, yet inference-friendly, representation of time. Motivated by our study of temporal expressions from the Penn Treebank corpora, we address the problem by proposing a two-level constraint-based framework for processing and reasoning over temporal information in natural language. Within this framework, temporal expressions are viewed as partial assignments to the variables of an underlying calendar constraint system, and multiple expressions together describe a temporal constraint-satisfaction problem (TCSP). To support this framework, we designed a typed formal language for encoding natural language expressions. The language can cope with phenomena such as under-specification and granularity change. The constraint problems can be solved using various constraint propagation and search methods, and the solutions can then be used to answer a wide range of time-related queries.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {11–32},
numpages = {22},
keywords = {computational semantics, temporal information processing, temporal reasoning, constraint solving, knowledge representation}
}

@article{10.1145/1017068.1017072,
author = {Jang, Seok Bae and Baldwin, Jennifer and Mani, Inderjeet},
title = {Automatic TIMEX2 Tagging of Korean News},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1017068.1017072},
doi = {10.1145/1017068.1017072},
abstract = {This article reports on a temporal tagger for Korean based on a Korean extension of the TIDES TIMEX2 guidelines. The extension, which primarily addresses the idiosyncrasies of Korean morphology, shows high inter-annotator reliability (0.893 F-measure for tag extent) when applied to a corpus of Korean newspaper articles. A machine-learning approach based on rote learning from a human-edited, automatically-derived dictionary of temporal expressions is compared with a second approach that adds manual patterns, and a third onethat tries to learn the patterns. Results for the first two are promising (0.87 F-measure for tag extent). Overall, the article shows that rote learning approaches can be very useful when language-specific features such as morphology are taken into account.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {51–65},
numpages = {15},
keywords = {time, Korean, temporal expressions, temporal information}
}

@article{10.1145/1017068.1017073,
author = {Hobbs, Jerry R. and Pan, Feng},
title = {An Ontology of Time for the Semantic Web},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1017068.1017073},
doi = {10.1145/1017068.1017073},
abstract = {In connection with the DAML project for bringing about the Semantic Web, an ontology of time is being developed for describing the temporal content of Web pages and the temporal properties of Web services. This ontology covers topological properties of instants and intervals, measures of duration, and the meanings of clock and calendar terms.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {66–85},
numpages = {20},
keywords = {temporal information, time, duration, clock and calendar, semantic web, ontology, time zone, temporal relation}
}

@article{10.1145/1017068.1017071,
author = {Schilder, Frank},
title = {Extracting Meaning from Temporal Nouns and Temporal Prepositions},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1017068.1017071},
doi = {10.1145/1017068.1017071},
abstract = {This article provides a compositional semantics for temporal nouns and temporal prepositions that are annotated as temporal prepositional phrases or noun phrases by an automatic tagging system (e.g., last Monday, on Dec. 1st, for three weeks or before Christmas). Current temporal tagging systems rely on an ad-hoc-representation for temporal date and time expressions, but the more demanding tasks of temporal question-answering and automatic text summarization require a sound logical derivation and representation of temporal expressions. Our proposal draws from two formal accounts of temporal prepositional phrases by Pratt and Francez [2001] and von Stechow [2002b], and is realized within an automatic temporal tagging system for German newspaper articles.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {33–50},
numpages = {18},
keywords = {temporal reference in discourse, temporal reasoning, semantics for temporal expressions and temporal prepositions, temporal information, information extraction}
}

@article{10.1145/1017068.1017069,
author = {Mani, Inderjeet and Pustejovsky, James and Sundheim, Beth},
title = {Introduction to the Special Issue on Temporal Information Processing},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1017068.1017069},
doi = {10.1145/1017068.1017069},
abstract = {Time is a key dimension of our information space, with many applications standing to benefit from exploiting it. This special issue is devoted to temporal information processing for natural language as well as temporal reasoning. We begin by describing some of the ways time is expressed in natural language, and the particular requirements this imposes on information processing systems. We then provide an overview of current annotation-based approaches to temporal information extraction, followed by an introduction to the relevant literature on temporal reasoning. We also provide brief synopses of the articles in this issue, situating them in a broader context. We end with administrative remarks about the creation of this special issue.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–10},
numpages = {10}
}

@article{10.1145/1007551.1007553,
author = {Kim, Harksoo and Seo, Jungyun},
title = {Resolution of Referring Expressions in a Korean Multimodal Dialogue System},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1007551.1007553},
doi = {10.1145/1007551.1007553},
abstract = {Referring expressions in multimodal dialogues have different aspects compared to those in language-only dialogues. They often refer to the items signified by either a gesture or visual means. In this article we classify referring expressions into two types (i.e., a deictic reference and an anaphoric reference), and propose two general methods to resolve these referring expressions. One method is a simple mapping algorithm that can find items referred with/without pointing gestures on a screen. The other is the centering algorithm with a dual cache model, to which Walker's centering algorithm is extended for a multimodal dialogue system. The extended algorithm is appropriate for resolving various anaphoric references in a multimodal dialogue. In the experiments, the proposed system correctly resolved 376 out of 405 referring expressions in 40 dialogues (0.54 referring expressions per utterance) showing 92.84 percent correctness.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {324–337},
numpages = {14},
keywords = {anaphoric reference, reference resolution, multimodal algorithm, dual cache model, deictic reference}
}

@article{10.1145/1007551.1007552,
author = {Kang, Mi-Young and Yoon, Aesun and Kwon, Hyuk-Chul},
title = {Improving Partial Parsing Based on Error-Pattern Analysis for a Korean Grammar-Checker},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1007551.1007552},
doi = {10.1145/1007551.1007552},
abstract = {The main aim of this work is to make improvements to Mirine 2.2, a Korean grammar-checker that takes Korean language properties into account and to develop a system that satisfies user expectations. Particular attention is given to processing Korean texts that may contain users' grammatical and practical errors. In order to treat these errors efficiently, the system includes methods based on asymmetric relations from which partial parsing and the potential governing relationship are derived, implying the starting point for checking, the direction for parsing, and the limits of the scope for parsing.To organize partial parsing efficiently, the system requires an appropriate knowledge base. As its essential prerequisites, this study (a) considers the factors from which the various error types encountered while parsing various Korean texts arise; (b) extracts general patterns from the linguistic or extra-linguistic factors obtained in this manner; and (c) demonstrates how the system, based on a linguistic analysis, procures an adequate knowledge base for partial parsing to satisfy end-users. Mirine 2.2 achieves an F-measure of about 0.80 in detecting unknown erroneous words; an F-measure of about 0.98 when not considering unknown words; and 98.94% precision in correcting erroneous words.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {301–323},
numpages = {23},
keywords = {partial parsing, parsing, error-pattern analysis, Korean grammar checker, text preprocessing, parsing triggering condition, linguistic analysis, language}
}

@article{10.1145/979872.979876,
author = {He, Daqing and Oard, Douglas W. and Wang, Jianqiang and Luo, Jun and Demner-Fushman, Dina and Darwish, Kareem and Resnik, Philip and Khudanpur, Sanjeev and Nossal, Michael and Subotin, Michael and Leuski, Anton},
title = {Making MIRACLEs: Interactive Translingual Search for Cebuano and Hindi},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979876},
doi = {10.1145/979872.979876},
abstract = {Searching is inherently a user-centered process; people pose the questions for which machines seek answers, and ultimately people judge the degree to which retrieved documents meet their needs. Rapid development of interactive systems that use queries expressed in one language to search documents written in another poses five key challenges: (1) interaction design, (2) query formulation, (3) cross-language search, (4) construction of translated summaries, and (5) machine translation. This article describes the design of MIRACLE, an easily extensible system based on English queries that has previously been used to search French, German, and Spanish documents, and explains how the capabilities of MIRACLE were rapidly extended to accommodate Cebuano and Hindi. Evaluation results for the cross-language search component are presented for both languages, along with results from a brief full-system interactive experiment with Hindi. The article concludes with some observations on directions for further research on interactive cross-language information retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {219–244},
numpages = {26},
keywords = {Cross-language information retrieval, Machine translation, Interactive information retrieval}
}

@article{10.1145/979872.979874,
author = {Sekine, Satoshi and Grishman, Ralph},
title = {Hindi-English Cross-Lingual Question-Answering System},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979874},
doi = {10.1145/979872.979874},
abstract = {We developed a cross-lingual, question-answering (CLQA) system for Hindi and English. It accepts questions in English, finds candidate answers in Hindi newspapers, and translates the answer candidates into English along with the context surrounding each answer. The system was developed as part of the surprise language exercise (SLE) within the TIDES program.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {181–192},
numpages = {12},
keywords = {Hindi}
}

@article{10.1145/979872.979880,
author = {Maynard, Diana and Tablan, Valentin and Bontcheva, Kalina and Cunningham, Hamish},
title = {Rapid Customization of an Information Extraction System for a Surprise Language},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979880},
doi = {10.1145/979872.979880},
abstract = {This paper describes the rapid adaptation for surprise languages of a flexible and robust Information Extraction system based on GATE, a portable Natural Language Processing infrastructure. Our experiences show that even without a native speaker and in the absence of training data, we can quickly customize the system to a new language. We adapted the default English system for the Cebuano language in 10 days, achieving an F measure of 77.5%.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {295–300},
numpages = {6},
keywords = {Information Extraction, named entity recognition, language agility}
}

@article{10.1145/979872.979877,
author = {Leuski, Anton and Lin, Chin-Yew and Zhou, Liang and Germann, Ulrich and Och, Franz Josef and Hovy, Eduard},
title = {Cross-Lingual C*ST*RD: English Access to Hindi Information},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979877},
doi = {10.1145/979872.979877},
abstract = {We present C*ST*RD, a cross-language information delivery system that supports cross-language information retrieval, information space visualization and navigation, machine translation, and text summarization of single documents and clusters of documents. C*ST*RD was assembled and trained within 1 month, in the context of DARPA's Surprise Language Exercise, that selected as source a heretofore unstudied language, Hindi. Given the brief time, we could not create deep Hindi capabilities for all the modules, but instead experimented with combining shallow Hindi capabilities, or even English-only modules, into one integrated system. Various possible configurations, with different tradeoffs in processing speed and ease of use, enable the rapid deployment of C*ST*RD to new languages under various conditions.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {245–269},
numpages = {25},
keywords = {headline generation, Cross-language information retrieval, Hindi-to-English machine translation, single- and multi-document text summarization, information retrieval and information space navigation}
}

@article{10.1145/979872.979878,
author = {Dorr, Bonnie and Zajic, David and Schwartz, Richard},
title = {Cross-Language Headline Generation for Hindi},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979878},
doi = {10.1145/979872.979878},
abstract = {This paper presents new approaches to headline generation for English newspaper texts, with an eye toward the production of document surrogates for document selection in cross-language information retrieval. This task is difficult because the user must make decisions about relevance based on (often poor) translations of retrieved documents. To facilitate the decision-making process we need translations that can be assessed rapidly and accurately; our approach is to provide an English headline for the non-English document. We describe two approaches to headline generation and their application to the recent DARPA TIDES-2003 Surprise Language Exercise for Hindi. For comparison, we also implemented an alternative method for surrogate generation: a system that produces topic lists for (Hindi) articles. We present the results of a series of experiments comparing each of these approaches. We demonstrate in both automatic and human evaluations that our linguistically motivated approach outperforms two other surrogate-generation methods: a statistical system and a topic discovery system.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {270–289},
numpages = {20}
}

@article{10.1145/979872.979875,
author = {Ma, Huanfeng and Doermann, David},
title = {Adaptive Hindi OCR Using Generalized Hausdorff Image Comparison},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979875},
doi = {10.1145/979872.979875},
abstract = {We present an adaptive Hindi OCR implemented as part of a rapidly retargetable language tool effort. The system includes: script identification, character segmentation, training sample creation, and character recognition. In script identification, Hindi words are identified from bilingual or multilingual documents based on features of the Devanagari script or using Support Vector Machines. Identified words are then segmented into individual characters in the next step, where the composite characters are identified and further segmented based on the structural properties of the script and statistical information. Segmented characters are recognized using generalized Hausdorff image comparison (GHIC) and postprocessing is applied to improve the performance. The OCR system, which was designed and implemented in one month, was applied to a complete Hindi--English bilingual dictionary and a set of ideal images extracted from Hindi documents in PDF format. Experimental results show the recognition accuracy can reach 88% for noisy images and 95% for ideal images. The presented method can also be extended to design OCR systems for different scripts.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {193–218},
numpages = {26},
keywords = {script identification, document processing, generalized Hausdorff image comparison, Optical character recognition (OCR)}
}

@article{10.1145/979872.979879,
author = {Li, Wei and McCallum, Andrew},
title = {Rapid Development of Hindi Named Entity Recognition Using Conditional Random Fields and Feature Induction},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979879},
doi = {10.1145/979872.979879},
abstract = {This paper describes our application of conditional random fields with feature induction to a Hindi named entity recognition task. With only five days development time and little knowledge of this language, we automatically discover relevant features by providing a large array of lexical tests and using feature induction to automatically construct the features that most increase conditional likelihood. In an effort to reduce overfitting, we use a combination of a Gaussian prior and early stopping based on the results of 10-fold cross validation.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {290–294},
numpages = {5},
keywords = {feature induction, conditional random fields, Extraction}
}

@article{10.1145/979872.979873,
author = {May, Jonathan and Brunstein, Ada and Natarajan, Prem and Weischedel, Ralph},
title = {Surprise! What's in a Cebuano or Hindi Name?},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/979872.979873},
doi = {10.1145/979872.979873},
abstract = {Empirical results are presented for creating training data and training a statistical name learning algorithm on Cebuano and Hindi in roughly three weeks time. The empirical study compares performance in a compressed time frame against performance of the same statistical language model in English (where there was no compressed time frame). Rapid development of several co-reference heuristics in Hindi are also described, and co-reference performance in Hindi is compared to previously developed English techniques.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {169–180},
numpages = {12},
keywords = {Extraction, Hindi, Cebuano}
}

@article{10.1145/974740.974748,
author = {Xu, Jinxi and Weischedel, Ralph},
title = {Cross-Lingual Retrieval for Hindi},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974748},
doi = {10.1145/974740.974748},
abstract = {In this paper we describe the evaluation results of applying a cross-lingual retrieval model to retrieve Hindi documents relevant to an English query. Though the technique has been previously applied and evaluated for retrieving Chinese and Arabic documents given an English query, what is new about these experiments is porting the model to Hindi in two weeks' time.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {164–168},
numpages = {5},
keywords = {cross-lingual retrieval, Hindi}
}

@article{10.1145/974740.974742,
author = {Allan, James and Lavrenko, Victor and Connell, Margaret E.},
title = {A Month to Topic Detection and Tracking in Hindi},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974742},
doi = {10.1145/974740.974742},
abstract = {We describe the one-month (June 2003) effort to create a topic detection and tracking (TDT) system to support news stories in Hindi. The University of Massachusetts submitted results for three different TDT tasks in the DARPA surprise language evaluation. The official task was topic tracking, but we also provided results for the new event detection and topic detection (clustering) tasks. Our approach to all three tasks was based on the vector-space model of information retrieval. We also describe the process we used to create the relevance judgments used to evaluate the system. Results suggest that topic tracking effectiveness is comparable to that of TDT tracking systems in other languages. Results for clustering and new event detection indicate that parameter settings for those tasks are sensitive to the language being used.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {85–100},
numpages = {16},
keywords = {TDT, Hindi, topic detection and tracking, surprise language}
}

@article{10.1145/974740.974744,
author = {Dorr, Bonnie J. and Ayan, Necip Fazil and Habash, Nizar and Madnani, Nitin and Hwa, Rebecca},
title = {Rapid Porting of DUSTer to Hindi},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974744},
doi = {10.1145/974740.974744},
abstract = {The frequent occurrence of divergences—structural differences between languages---presents a great challenge for statistical word-level alignment and machine translation. This paper describes the adaptation of DUSTer, a divergence unraveling package, to Hindi during the DARPA TIDES-2003 Surprise Language Exercise. We show that it is possible to port DUSTer to Hindi in under 3 days.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {118–123},
numpages = {6},
keywords = {Divergences, machine translation}
}

@article{10.1145/974740.974741,
author = {Oard, Douglas W.},
title = {The Surprise Language Exercises},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974741},
doi = {10.1145/974740.974741},
abstract = {For ten days in March and twenty-nine days in June of 2003, sixteen teams in two nations sought to develop language technologies for two previously unanticipated languages; Cebuano and Hindi. This introduction to a pair of special issues explains the motivation for those exercises, the approaches that were tried, and some of the lessons that were learned.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {79–84},
numpages = {6},
keywords = {language parsing and understanding, text analysis, Cross-language information retrieval, machine translation, information extraction, summarization}
}

@article{10.1145/974740.974747,
author = {Lavie, Alon and Vogel, Stephan and Levin, Lori and Peterson, Erik and Probst, Katharina and Llitj\'{o}s, Ariadna Font and Reynolds, Rachel and Carbonell, Jaime and Cohen, Richard},
title = {Experiments with a Hindi-to-English Transfer-Based MT System under a Miserly Data Scenario},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974747},
doi = {10.1145/974740.974747},
abstract = {We describe an experiment designed to evaluate the capabilities of our trainable transfer-based (Xfer) machine translation approach, as applied to the task of Hindi-to-English translation, and trained under an extremely limited data scenario. We compare the performance of the Xfer approach with two corpus-based approaches---Statistical MT (SMT) and Example-based MT (EBMT)---under the limited data scenario. The results indicate that the Xfer system significantly outperforms both EBMT and SMT in this scenario. Results also indicate that automatically learned transfer rules are effective in improving translation performance, compared with a baseline word-to-word translation version of the system. Xfer system performance with a limited number of manually written transfer rules is, however, still better than the current automatically inferred rules. Furthermore, a "multiengine" version of our system that combined the output of the Xfer and SMT systems and optimizes translation selection outperformed both individual systems.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {143–163},
numpages = {21},
keywords = {multiengine machine translation, example-based machine translation, limited data resources, Evaluation, Hindi, transfer rules, statistical translation, machine learning}
}

@article{10.1145/974740.974743,
author = {Strassel, Stephanie and Maxwell, Mike and Cieri, Christopher},
title = {Linguistic Resource Creation for Research and Technology Development: A Recent Experiment},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974743},
doi = {10.1145/974740.974743},
abstract = {Advances in statistical machine learning encourage language-independent approaches to linguistic technology development. Experiments in "porting" technologies to handle new natural languages have revealed a great potential for multilingual computing, but also a frustrating lack of linguistic resources for most languages. Recent efforts to address the lack of available resources have focused either on intensive resource development for a small number of languages or development of technologies for rapid porting. The Linguistic Data Consortium recently participated in an experiment falling primarily under the first approach, the surprise language exercise. This article describes linguistic resource creation within this context, including the overall methodology for surveying and collecting language resources, as well as details of the resources developed during the exercise. The article concludes with discussion of a new approach to solving the problem of limited linguistic resources, one that has recently proven effective in identifying core linguistic resources for less common studied languages.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {101–117},
numpages = {17},
keywords = {Hindi, translingual information access technology, machine translation, information retrieval, Cebuano, crosslanguage, text analysis, linguistic resources, information extraction, summarization, Machine translation, language parsing and understanding}
}

@article{10.1145/974740.974745,
author = {Huang, Fei and Vogel, Stephan and Waibel, Alex},
title = {Extracting Named Entity Translingual Equivalence with Limited Resources},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974745},
doi = {10.1145/974740.974745},
abstract = {In this article we present an automatic approach to extracting Hindi-English (H-E) Named Entity (NE) translingual equivalences from bilingual parallel corpora. In the absence of a Hindi NE tagger or H-E translation dictionary, this approach adapts a Chinese-English (C-E) surface string transliteration model for H-E NE extraction. The model is initially trained using automatically extracted C-E NE pairs, then iteratively updated based on newly extracted H-E NE pairs. For each English person and location NE in each sentence pair, this approach searches for its Hindi correspondence with minimum transliteration cost and constructs an H-E NE list from the bilingual corpus. Experiments show that this approach extracted 1000 H-E NE pairs with a precision of 91.8%.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {124–129},
numpages = {6},
keywords = {transliteration, information extraction, machine translation, Named entity translation}
}

@article{10.1145/974740.974746,
author = {Larkey, Leah S. and Connell, Margaret E. and Abduljaleel, Nasreen},
title = {Hindi CLIR in Thirty Days},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/974740.974746},
doi = {10.1145/974740.974746},
abstract = {As participants in the TIDES Surprise language exercise, researchers at the University of Massachusetts helped collect Hindi--English resources and developed a cross-language information retrieval system. Components included normalization, stop-word removal, transliteration, structured query translation, and language modeling using a probabilistic dictionary derived from a parallel corpus. Existing technology was successfully applied to Hindi. The biggest stumbling blocks were collection of parallel English and Hindi text and dealing with numerous proprietary encodings.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {130–142},
numpages = {13},
keywords = {evaluation, cross-language, cross-lingual information retrieval, Hindi}
}

@article{10.1145/964161.964164,
author = {Lee, Yue-Shi},
title = {Task Adaptation in Stochastic Language Model for Chinese Homophone Disambiguation},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/964161.964164},
doi = {10.1145/964161.964164},
abstract = {The runtime application domain has a great effect on the performance of practical corpus-based applications. Previous smoothing techniques and class-based and similarity-based models could not handle the dynamic status perfectly. In this paper, an adaptive learning algorithm is proposed for task adaptation that best fits the runtime application domain in applying Chinese homophone disambiguation. The proposed algorithm is first formulated by a neural network model and then generalized to avoid the problem of slow convergence. The resulting techniques are greatly simplified and robust. The experimental results demonstrate the effects of the learning algorithm from a generic domain to a specific one. A methodology is also presented to show how these techniques can be extended to various language models and corpus-based applications.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {49–62},
numpages = {14},
keywords = {task adaptation, Adaptive learning, language model, runtime application domain, neural network, Chinese homophone disambiguation}
}

@article{10.1145/964161.964162,
author = {Lo, Wai-Kit and Meng, Helen and Ching, P. C.},
title = {Cross-Language Spoken Document Retrieval Using HMM-Based Retrieval Model with Multi-Scale Fusion},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/964161.964162},
doi = {10.1145/964161.964162},
abstract = {Cross-language spoken document retrieval (CL-SDR) is the technology that facilitates automatic retrieval of relevant information from a collection of spoken documents in a language that is different from that used in the queries. Information sources that are in different languages can then be retrieved automatically with CL-SDR, and the number of searchable information sources will increase significantly. The HMM-based retrieval model is a probabilistic formulation for the retrieval problem. Extensions to this retrieval model can be made by taking advantage of its probabilistic nature. Specifically, we have incorporated the translation component to make it possible to perform cross-language information retrieval (CLIR). In addition, this HMM-based CLIR retrieval model is also extended for retrieval at subword scales.In this work the extended HMM-based retrieval model has been applied to an English-Mandarin CL-SDR task, which is to search the Mandarin spoken document collection with English queries at word and subword scales. Retrieval results obtained from these indexing scales are then fused for multi-scale CL-SDR. Experimental results demonstrate that improvement in CL-SDR retrieval performance can be achieved by fusion of word and subword scales.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–26},
numpages = {26},
keywords = {Cross-language information retrieval, Spoken document retrieval, Multi-scale data fusion}
}

@article{10.1145/964161.964163,
author = {Shi, Daming and Damper, Robert I. and Gunn, Steve R.},
title = {Offline Handwritten Chinese Character Recognition by Radical Decomposition},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/964161.964163},
doi = {10.1145/964161.964163},
abstract = {Offline handwritten Chinese character recognition is a very hard pattern-recognition problem of considerable practical importance. Two popular approaches are to extract features holistically from the character image or to decompose characters structurally into component parts---usually strokes. Here we take a novel approach, that of decomposing into radicals on the basis of image information (i.e., without first decomposing into strokes). During training, 60 examples of each radical were represented by "landmark" points, labeled semiautomatically, with radicals in different characteristic positions treated as distinctly different radicals. Kernel principal-component analysis then captured the main (nonlinear) variations around the mean radical. During the recognition, the dynamic tunneling algorithm was used to search for optimal shape parameters in terms of chamfer distance minimization. Considering character composition as a Markov process in which up to four radicals are combined in some assumed sequential order, we can recognize complete, hierarchically-composed characters by using the Viterbi algorithm. This gave a character recognition rate of 93.5% characters correct (writer-independent) on a test set of 430,800 characters from 2,154 character classes composed of 200 radical categories, which is comparable to the best reported results in the literature. Although the initial semiautomatic landmark labeling is time consuming, the decomposition approach is theoretically well-motivated and allows the different sources of variability in Chinese handwriting to be handled separately and by the most appropriate means--either learned from example data or incorporated as prior knowledge. Hence, high generalizability is obtained from small amounts of training data, and only simple prior knowledge needs to be incorporated, thus promising robust recognition performance. As such, there is very considerable potential for further development and improvement in the direction of larger character sets and less constrained writing conditions.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {27–48},
numpages = {22},
keywords = {Chinese computing, offline character recognition, active shape modeling, Viterbi decoding}
}

@article{10.1145/964161.964165,
author = {Shieh, Jiann-Cherng},
title = {An Efficient Accessing Technique for Taiwanese Phonetic Transcriptions},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/964161.964165},
doi = {10.1145/964161.964165},
abstract = {Recently, the Taiwan government has been enthusiastically promoting the study of the languages of her native inhabitants, including Taiwanese. Hence the focus of our research is to design and develop an efficient retrieval technique for phonetic transcriptions. This new technique will make possible widespread utilization of Taiwanese, e.g., in PDA (mobile phone applications). In this paper we propose a minimal perfect hashing function for the 3028 existing Taiwanese phonetic transcriptions. Compared to the hashing designs based on the Chinese remainder theorem for various data sets, the proposed design is shown to be superior in space utilization.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {63–77},
numpages = {15},
keywords = {Chinese remainder theorem, Taiwanese phonetic transcriptions, Hashing function design, minimal perfect hashing function}
}

@article{10.1145/795458.795460,
author = {Jin, Honglan and Wong, Kam-Fai},
title = {A Chinese Dictionary Construction Algorithm for Information Retrieval},
year = {2002},
issue_date = {December 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/795458.795460},
doi = {10.1145/795458.795460},
abstract = {In this article we propose a method for constructing, from raw Chinese text, a statistics-based automatic dictionary. The method makes use of local statistical information (i.e., data within a document) to identify and discard repeated string patterns, which, at an earlier stage, were substrings of legitimate words. Global statistical information (which exists throughout the entire corpus) and contextual constraints are then used for further filtering. The method can be used to alleviate the out-of-vocabulary (OOV) problem, which is commonly found in dictionary-based natural language information-processing applications, e.g., word segmentation. It can handle text corpora dynamically and, further, it does not impose any strict requirements on the size and quality of the training corpora. Based on our method, we constructed Chinese dictionaries from different Chinese corpora. We then applied the words in the constructed dictionaries to indexing in information retrieval (IR). Retrieval performance using such indexes was compared to the same, but based on indexes produced by static dictionaries. Three Chinese corpora using various character-encoding schemes and language styles were used in the experiments. The results show that retrieval using indexes based on the constructed dictionary is effective. This implies that fully automatic Chinese dictionary construction based on dynamic data sources, e.g., from the Internet, for the purposes of IR is feasible. Drawing on the experiment, we were able to make some interesting observations: (1) using only a portion of a dictionary is enough to produce good retrieval performance, e.g., a dictionary consisting of only the 500 highest-frequency strings extracted from the NTCIR 2 Chinese corpus produced as good a retrieval result as using a more complete dictionary with over 100K entries; and (2) complete word segmentation is not a strict requirement for achieving practical information retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {281–296},
numpages = {16},
keywords = {automatic word extraction, Chinese information retrieval, dictionary construction}
}

@article{10.1145/795458.795461,
author = {Li, Yuanxiang and Ding, Xiaoqing and Tan, Chew Lim},
title = {Combining Character-Based Bigrams with Word-Based Bigrams in Contextual Postprocessing for Chinese Script Recognition},
year = {2002},
issue_date = {December 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/795458.795461},
doi = {10.1145/795458.795461},
abstract = {It is crucial to use contextual information to improve the recognition accuracy of Chinese script in an offline, handwritten Chinese character-recognition system. However, with the increase in the number of candidates given by a character recognizer, contextual postprocessing using a word-based bigram is time-consuming. This article presents a novel contextual postprocessing method that integrates character-based bigram postprocessing with word-based bigram postprocessing in light of the complementary action between Chinese characters and Chinese words. On the basis of isolated character recognition, character-based bigram postprocessing using a forward-backward search is first executed on a big candidate set, which improves both the accuracy and efficiency of the candidate set (the cumulative accuracy of the top ten candidates is greatly boosted). Then, to further improve accuracy, word-based bigram postprocessing (WBP) is executed on a small candidate set. This method obtains high accuracy while paying attention to postprocessing speed at the same time. Experimental results for three Chinese scripts (about 66,000 characters in total) demonstrate the effectiveness of our method: character-based bigram postprocessing improves accuracy from 81.58% to 94.50%, and the cumulative accuracy of the top ten candidates rises from 94.33% to 98.25%. After WBP, 95.75% accuracy is achieved, which is equivalent to the accuracy of WBP executed on a big candidate set. However, our method is more than 100 times faster than that of WBP.},
journal = {ACM Transactions on Asian Language Information Processing},
month = dec,
pages = {297–309},
numpages = {13},
keywords = {forward-backward search, contextual post-processing, Chinese character recognition, efficiency of candidate set, statistical language model}
}

@article{10.1145/772755.772759,
author = {Suzuki, Izumi and Mikami, Yoshiki and Ohsato, Ario and Chubachi, Yoshihide},
title = {A Language and Character Set Determination Method Based on N-Gram Statistics},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/772755.772759},
doi = {10.1145/772755.772759},
abstract = {An N-gram-based language, script, and encoding scheme-detection method is introduced in this article. The method detects language, script, and encoding schemes using a target text document encoded by computer by checking how many byte sequences of the target match the byte sequences that can appear in the texts belonging to a language, script, and encoding scheme. This detection mechanism is different from conventional N-gram-based methods in that its threshold for any category is uniquely predetermined. The method was originally created for a survey of web pages conducted to find how many web pages are written in a particular language, script, and encoding scheme. The requirement is that the method must be able to respond to either "correct answer" or "unable to detect" where "unable to detect" includes "other than registered." There are some minor problems with this method, but its effectiveness as a language, script, and encoding scheme-detection method has been confirmed by experiments.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {269–278},
numpages = {10},
keywords = {Unicode, corpus-based analysis, natural languages, N-gram, text categorization, character set, local language site}
}

@article{10.1145/772755.772756,
author = {Li, Wenjie and Wong, Kam-Fai},
title = {A Word-Based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/772755.772756},
doi = {10.1145/772755.772756},
abstract = {Conventional information extraction systems cannot effectively mine temporal information. For example, users' queries on how one event is related to another in time could not be handled effectively. For this reason, it is important to capture and deduce temporal knowledge associated with the relevant events. It is generally acknowledged that information extraction cannot be isolated from natural language processing. As Chinese has no tenses, conventional means for finding temporal references based on verb forms no longer apply. In this article we present an approach for formulating and discovering temporal relations in Chinese. A set of rules is devised to map the combinational effects of the temporal indicators (also known as temporal markers, gathered from various grammatical categories) in a sentence to its corresponding temporal relation. To evaluate the proposed algorithm, experiments were conducted using a set of news reports and the results look promising. Problem discussions are also provided. Through this work, we hope to open up new doors for future research in Chinese temporal information extraction and processing.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {173–206},
numpages = {34},
keywords = {Chinese language processing, temporal information processing, temporal relationship discovery}
}

@article{10.1145/772755.772758,
author = {Luk, Robert W. P. and Kwok, K. L.},
title = {A Comparison of Chinese Document Indexing Strategies and Retrieval Models},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/772755.772758},
doi = {10.1145/772755.772758},
abstract = {With the advent of the Internet and intranets, substantial interest is being shown in Asian language information retrieval; especially in Chinese, which is a good example of an Asian ideographic language (other examples include Japanese and Korean). Since, in this type of language, spaces do not delimit words, an important issue is which index terms should be extracted from documents. This issue also has wider implications for indexing other languages such as agglutinating languages (e.g., Finnish and Turkish), archaic ideographic languages like Egyptian hieroglyphs, and other types of information such as data stored in genomic databases. Although comparisons of indexing strategies for Chinese documents have been made, almost all of them are based on a single retrieval model. This article compares the performance of various combinations of indexing strategies (i.e., character, word, short-word, bigram, and Pircs indexing) and retrieval models (i.e., vector space, 2-Poisson, logistic regression, and Pircs models). We determine which model (and its parameters) achieves the (near) best retrieval effectiveness without relevance feedback, and compare it with the open evaluations (i.e., TREC and NTCIR) for both long and title queries. In addition, we describe a more extensive investigation of retrieval efficiency. In particular, the storage cost of word indexing is only slightly more than character indexing, and bigram indexing is about double the storage cost of other indexing strategies. The retrieval time typically varies linearly with the number of unique terms in the query, which is supported by correlation values above 90%. The Pircs retrieval system achieves robust and good retrieval performance, but it appears to be the slowest method, whereas vector space models were not very effective in retrieval, but were able to respond quickly. For robust, near-best retrieval effectiveness, without considering storage overhead, the 2-Poisson model using bigram indexing appears to be a good compromise between retrieval effectiveness and efficiency for both long and title queries.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {225–268},
numpages = {44},
keywords = {Chinese information retrieval, comparison, indexing strategies}
}

@article{10.1145/772755.772757,
author = {Lee, Jin-Seok and Kim, Byeongchang and Lee, Gary Geunbae},
title = {Automatic Corpus-Based Tone and Break-Index Prediction Using K-ToBI Representation},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/772755.772757},
doi = {10.1145/772755.772757},
abstract = {In this article we present a prosody generation architecture based on K-ToBI (Korean Tone and Break Index) representation. ToBI is a multitier representation system based on linguistic knowledge that transcribes events in an utterance. The TTS (Text-To-Speech) system, which adopts ToBI as an intermediate representation, is known to exhibit higher flexibility, modularity, and domain/task portability compared to the direct prosody generation TTS systems. However, for practical-level performance, the cost of corpus preparation is very expensive because the ToBI labeled corpus is constructed manually by many prosody experts, and normally requires large amounts of data for statistical prosody modeling. Unlike previous ToBI-based systems, this article proposes a new method, which transcribes the K-ToBI labels in Korean speech completely automatically. We develop automatic corpus-based K-ToBI labeling tools and prediction methods based on several lexico-syntactic linguistic features for decision-tree induction. We demonstrate the performance of F0 generation from automatically predicted K-ToBI labels, and confirm that the performance is reasonably comparable to state-of-the-art direct prosody generation methods and previous ToBI-based methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {207–224},
numpages = {18},
keywords = {intonation, text-to-speech system, prosodic phrase, phrase break, pitch, prosody, K-ToBI}
}

@article{10.1145/568954.568957,
author = {Murata, Masaki and Ma, Qing and Isahara, Hitoshi},
title = {Comparison of Three Machine-Learning Methods for Thai Part-of-Speech Tagging},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/568954.568957},
doi = {10.1145/568954.568957},
abstract = {The elastic-input neuro-tagger and hybrid tagger, combined with a neural network and Brill's error-driven learning, have already been proposed to construct a practical tagger using as little training data as possible. When a small Thai corpus is used for training, these taggers have tagging accuracies of, respectively, 94.4% and 95.5% (accounting only for the ambiguous words that relate to the parts of speech). In this study, in order to construct more accurate taggers, we developed new tagging methods using three different machine-learning approaches: the decision list, maximum entropy, and the support vector machine methods. We then performed tagging experiments using them. Our results show that the support vector machine method has the best precision (96.1%), and that it is capable of improving the accuracy of tagging in the Thai language. The improvement in accuracy was also confirmed by using a statistical test (a sign test). Finally, we examined theoretically all these methods in an effort to determine how the improvements were achieved. We found that the improvements were due to our use of word information, which is helpful for tagging, and a support vector machine that performed well.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {145–158},
numpages = {14},
keywords = {decision list method, maximum entropy method, POS tagging, machine learning, lexical information, support vector machine}
}

@article{10.1145/568954.568958,
author = {Lu, Wen-Hsiang and Chien, Lee-Feng and Lee, Hsi-Jian},
title = {Translation of Web Queries Using Anchor Text Mining},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/568954.568958},
doi = {10.1145/568954.568958},
abstract = {This article presents an approach to automatically extracting translations of Web query terms through mining of Web anchor texts and link structures. One of the existing difficulties in cross-language information retrieval (CLIR) and Web search is the lack of appropriate translations of new terminology and proper names. The proposed approach successfully exploits the anchor-text resources and reduces the existing difficulties of query term translation. Many query terms that cannot be obtained in general-purpose translation dictionaries are, therefore, extracted.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {159–172},
numpages = {14},
keywords = {machine translation, web mining, comparable corpora, cross-language information retrieval, parallel corpora, anchor text mining}
}

@article{10.1145/568954.568956,
author = {Meng, Helen and Luk, Po-Chui and Xu, Kui and Weng, Fuliang},
title = {GLR Parsing with Multiple Grammars for Natural Language Queries},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/568954.568956},
doi = {10.1145/568954.568956},
abstract = {This article presents an approach for parsing natural language queries that integrates multiple subparsers and subgrammars, in contrast to the traditional single grammar and parser approach. In using LR(k) parsers for natural language processing, we are faced with the problem of rapid growth in parsing table sizes as the number of grammar rules increases. We propose to partition the grammar into multiple subgrammars, each having its own parsing table and parser. Grammar partitioning helps reduce the overall parsing table size when compared to using a single grammar. We used the GLR parser with an LR(1) parsing table in our framework because GLR parsers can handle ambiguity in natural language. A parser composition technique then combines the parsers' outputs to produce an overall parse that is the same as the output parse of single parser. Two different strategies were used for parser composition: (i) parser composition by cascading; and (ii) parser composition with predictive pruning.Our experiments were conducted with natural language queries from the ATIS (Air Travel Information Service) domain. We have manually translated the ATIS-3 corpora into Chinese, and consequently we could experiment with grammar partitioning on parallel linguistic corpora. For English, the unpartitioned ATIS grammar has 72,869 states in its parsing table, while the partitioned English grammar has 3,350 states in total. For Chinese, grammar partitioning reduced the overall parsing table size from 29,734 states to 3,894 states. Both results show that grammar partitioning greatly economizes on the overall parsing table size. Language understanding performances were also examined. Parser composition imparts a robust parsing capability in our framework, and hence obtains a higher understanding performance when compared to using a single GLR parser.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {123–144},
numpages = {22},
keywords = {parser composition, grammar partitioning, generalized LR parsing, lattice with multiple granularities}
}

@article{10.1145/568954.568955,
author = {Chen, Hsin-Hsi and Lin, Chi-Ching and Lin, Wen-Cheng},
title = {Building a Chinese-English Wordnet for Translingual Applications},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/568954.568955},
doi = {10.1145/568954.568955},
abstract = {A WordNet-like linguistic resource is useful, but difficult to construct. This article proposes a method to integrate five linguistic resources, including English/Chinese sense-tagged corpora, English/Chinese thesauruses, and a bilingual dictionary. Chinese words are mapped into WordNet. A Chinese WordNet and a Chinese-English WordNet are derived by following the structures of WordNet. Experiments with Chinese-English information retrieval are developed to evaluate the applicability of the Chinese-English WordNet. The best model achieves 0.1010 average precision, 69.23% of monolingual information retrieval. It also gains a 10.02% increase relative to a model that resolves translation ambiguity and target polysemy problems together.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {103–122},
numpages = {20},
keywords = {word sense disambiguation, bilingual wordnet, translingual application, thesaurus construction, sense tagging, cross language information retrieval}
}

@article{10.1145/595576.595577,
author = {Wong, Kam-Fai and Tsujii, Jun'ichi},
title = {Prologue},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/595576.595577},
doi = {10.1145/595576.595577},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–2},
numpages = {2}
}

@article{10.1145/595576.595581,
author = {Lee, Tan and Lau, Wai and Wong, Y. W. and Ching, P. C.},
title = {Using Tone Information in Cantonese Continuous Speech Recognition},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/595576.595581},
doi = {10.1145/595576.595581},
abstract = {In Chinese languages, tones carry important information at various linguistic levels. This research is based on the belief that tone information, if acquired accurately and utilized effectively, contributes to the automatic speech recognition of Chinese. In particular, we focus on the Cantonese dialect, which is spoken by tens of millions of people in Southern China and Hong Kong. Cantonese is well known for its complicated tone system, which makes automatic tone recognition very difficult. This article describes an effective approach to explicit tone recognition of Cantonese in continuously spoken utterances. Tone feature vectors are derived, on a short-time basis, to characterize the syllable-wide patterns of F0 (fundamental frequency) and energy movements. A moving-window normalization technique is proposed to reduce the tone-irrelevant fluctuation of F0 and energy features. Hidden Markov models are employed for context-dependent acoustic modeling of different tones. A tone recognition accuracy of 66.4% has been achieved in the speaker-independent case. The recognized tone patterns are then utilized to assist Cantonese large-vocabulary continuous speech recognition (LVCSR) via a lattice expansion approach. Experimental results show that reliable tone information helps to improve the overall performance of LVCSR.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {83–102},
numpages = {20},
keywords = {Chinese dialects, F0 normalization, knowledge integration, speech recognition, tone recognition}
}

@article{10.1145/595576.595578,
author = {Gao, Jianfeng and Goodman, Joshua and Li, Mingjing and Lee, Kai-Fu},
title = {Toward a Unified Approach to Statistical Language Modeling for Chinese},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/595576.595578},
doi = {10.1145/595576.595578},
abstract = {This article presents a unified approach to Chinese statistical language modeling (SLM). Applying SLM techniques like trigram language models to Chinese is challenging because (1) there is no standard definition of words in Chinese; (2) word boundaries are not marked by spaces; and (3) there is a dearth of training data. Our unified approach automatically and consistently gathers a high-quality training data set from the Web, creates a high-quality lexicon, segments the training data using this lexicon, and compresses the language model, all by using the maximum likelihood principle, which is consistent with trigram model training. We show that each of the methods leads to improvements over standard SLM, and that the combined method yields the best pinyin conversion result reported.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {3–33},
numpages = {31},
keywords = {perplexity, domain adaptation, backoff, Chinese language, lexicon, statistical language modeling, character error rate, pruning, word segmentation, Chinese pinyin-to-character conversion, smoothing, n-gram model}
}

@article{10.1145/595576.595580,
author = {Kim, Byeongchang and Lee, Gary Geunbae and Lee, Jong-Hyeok},
title = {Morpheme-Based Grapheme to Phoneme Conversion Using Phonetic Patterns and Morphophonemic Connectivity Information},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/595576.595580},
doi = {10.1145/595576.595580},
abstract = {Both dictionary-based and rule-based methods on grapheme-to-phoneme conversion have their own advantages and limitations. For example, a large sized phonetic dictionary and complex morphophonemic rules are required for the dictionary-based method and the LTS (letter to sound) rule-based method itself cannot model the complete morphophonemic constraints.This paper describes a grapheme-to-phoneme conversion method for Korean using a dictionary-based and rule-based hybrid method with a phonetic pattern dictionary and CCV (consonant consonant vowel) LTS (letter to sound) rules. The phonetic pattern dictionary, standing for the dictionary-based method, contains entries in the form of a morpheme pattern and its phonetic pattern. The patterns represent candidate phonological changes in left and right boundaries of morphemes. Obviously, the CCV LTS rules stand for the rule-based method. The rules are in charge of grapheme-to-phoneme conversion within morphemes.The conversion method consists of mainly two steps including morpheme to phoneme conversion and morphophonemic connectivity check, and two preprocessing steps including phrase break prediction and morpheme normalization. Phrase break prediction presumes phrase breaks using the stochastic method on part-of-speech (POS) information. Morpheme normalization is to replace non-Korean symbols with their corresponding standard Korean graphemes. In the morpheme-phoneticizing module, each morpheme in the phrase is converted into phonetic patterns by looking it up in the phonetic pattern dictionary. Graphemes within a morpheme are grouped into CCV units and converted into phonemes by the CCV LTS rules. The morphophonemic connectivity table supports grammaticality checking of the two adjacent phonetic morphemes.In experiments with a non-Korean symbol free corpus of 4,973 sentences, we achieved a 99.98% grapheme-to-phoneme conversion performance rate and a 99.0% sentence conversion performance rate. With a broadcast news corpus of 621 sentences, 99.7% of the graphemes and 86.6% of the sentences are correctly converted. The full Korean TTS (Text-to-Speech) system is now being implemented using this conversion method.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {65–82},
numpages = {18},
keywords = {CCV LTS rule, phonetic pattern dictionary, grapheme-to-phoneme conversion, text-to-speech system, morphophonemic modeling}
}

@article{10.1145/595576.595579,
author = {Lai, Yu-Sheng and Wu, Chung-Hsien},
title = {Meaningful Term Extraction and Discriminative Term Selection in Text Categorization via Unknown-Word Methodology},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/595576.595579},
doi = {10.1145/595576.595579},
abstract = {In this article, an approach based on unknown words is proposed for meaningful term extraction and discriminative term selection in text categorization. For meaningful term extraction, a phrase-like unit (PLU)-based likelihood ratio is proposed to estimate the likelihood that a word sequence is an unknown word. On the other hand, a discriminative measure is proposed for term selection and is combined with the PLU-based likelihood ratio to determine the text category. We conducted several experiments on a news corpus, called MSDN. The MSDN corpus is collected from an online news Website maintained by the Min-Sheng Daily News, Taiwan. The corpus contains 44,675 articles with over 35 million words. The experimental results show that the system using a simple classifier achieved 95.31% accuracy. When using a state-of-the-art classifier, kNN, the average accuracy is 96.40%, outperforming all the other systems evaluated on the same collection, including the traditional term-word by kNN (88.52%); sleeping-experts (82.22%); sparse phrase by four-word sleeping-experts (86.34%); and Boolean combinations of words by RIPPER (87.54%). A proposed purification process can effectively reduce the dimensionality of the feature space from 50,576 terms in the word-based approach to 19,865 terms in the unknown word-based approach. In addition, more than 80% of automatically extracted terms are meaningful. Experiments also show that the proportion of meaningful terms extracted from training data is relative to the classification accuracy in outside testing.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {34–64},
numpages = {31},
keywords = {meaningful term extraction, AC-machine, text categorization, n-gram, text indexing, dimensionality reduction, phrase-like unit, discriminability, sparse data problem, discriminative term selection, unknown word detection, term adaptation, inconsistency problem, term purification, vector space modeling}
}

