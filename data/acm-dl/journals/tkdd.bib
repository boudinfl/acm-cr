@article{10.1145/3399712,
author = {Mohotti, Wathsala Anupama and Nayak, Richi},
title = {Efficient Outlier Detection in Text Corpus Using Rare Frequency and Ranking},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3399712},
doi = {10.1145/3399712},
abstract = {Outlier detection in text data collections has become significant due to the need of finding anomalies in the myriad of text data sources. High feature dimensionality, together with the larger size of these document collections, presents a need for developing accurate outlier detection methods with high efficiency. Traditional outlier detection methods face several challenges including data sparseness, distance concentration, and the presence of a larger number of sub-groups when dealing with text data. In this article, we propose to address these issues by developing novel concepts such as presenting documents with the rare document frequency, finding ranking-based neighborhood for similarity computation, and identifying sub-dense local neighborhoods in high dimensions. To improve the proposed primary method based on rare document frequency, we present several novel ensemble approaches using the ranking concept to reduce the false identifications while finding the higher number of true outliers. Extensive empirical analysis shows that the proposed method and its ensemble variations improve the quality of outlier detection in document repositories as well as they are found scalable compared to the relevant benchmarking methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {71},
numpages = {30},
keywords = {high dimensional data, term-weighting, Outlier detection, ranking function, k-occurrences}
}

@article{10.1145/3399711,
author = {Xu, Jiarong and Luo, Yifan and Tao, Jianrong and Fan, Changjie and Zhao, Zhou and Lu, Jiangang},
title = {NGUARD+: An Attention-Based Game Bot Detection Framework via Player Behavior Sequences},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3399711},
doi = {10.1145/3399711},
abstract = {Game bots are automated programs that assist cheating users, leading to an imbalance in the game ecosystem and the collapse of user interest. Online games provide immersive gaming experience and attract many loyal fans. However, game bots have proliferated in volume and method, evolving with the real-world detection methods and showing strong diversity, leaving game bot detection efforts extremely difficult. Existing game bot detection techniques mostly rely on handcrafted features or time-series based features instead of fully utilizing player behavior sequences. In this regard, a more reasonable way should be learning user patterns from player behavior sequences when facing the fast-changing nature of game bots. Here we propose a general game bot detection framework for massively multiplayer online role playing games termed NGUARD+ (denoting NetEase Games’ Guard), which captures user patterns in order to identify game bots from player behavior sequences. NGUARD+ mainly employs attention-based methods to automatically differentiate game bots from humans. We provide a combination of supervised and unsupervised methods for game bot detection to detect game bots and new type of game bots even when the labels of game bots are limited. Specifically, we propose the following two variants for attention-based sequence modeling: Attention based Bidirectional Long Short-Term Memory Networks (ABLSTM) and Hierarchical Self-Attention Network (HSAN) as our supervised models. ABLSTM is keen on inducing certain inductive biases which makes learning more reasonable as well as capturing local dependency and global information, while HSAN could handle much longer behavior sequences with less memory and higher computational efficiency. Experiments conducted on a real-world dataset show that NGUARD+ can achieve remarkable performance improvement compared to traditional methods. Moreover, NGUARD+ can reveal outstanding robustness for game bots in mutated patterns and even in completely unseen patterns.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {65},
numpages = {24},
keywords = {transfer learning, auto-iteration mechanism, attention mechanism, sequence modeling, Game bot detection}
}

@article{10.1145/3406243,
author = {Paudel, Ramesh and Eberle, William},
title = {An Approach For Concept Drift Detection in a Graph Stream Using Discriminative Subgraphs},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3406243},
doi = {10.1145/3406243},
abstract = {The emergence of mining complex networks like social media, sensor networks, and the world-wide-web has attracted considerable research interest. In a streaming scenario, the concept to be learned can change over time. However, while there has been some research done for detecting concept drift in traditional data streams, little work has been done on addressing concept drift in data represented as a graph. We propose a novel unsupervised concept-drift detection method on graph streams called Discriminative Subgraph-based Drift Detector (DSDD). The methodology starts by discovering discriminative subgraphs for each graph in the stream. We then compute the entropy of the window based on the distribution of discriminative subgraphs with respect to the graphs and then use the direct density-ratio estimation approach for detecting concept drift in the series of entropy values obtained by moving one step forward in the sliding window. The effectiveness of the proposed method is demonstrated through experiments using artificial and real-world datasets and its performance is evaluated by comparing against related baseline methods. Similarly, the usefulness of the proposed concept drift detection approach is studied by incorporating it in a popular graph stream classification algorithm and studying the impact of drift detection in classification accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {70},
numpages = {25},
keywords = {Concept drift detection, graph stream, graph stream classification}
}

@article{10.1145/3412371,
author = {Li, Shuangyin and Zhang, Yu and Pan, Rong},
title = {Bi-Directional Recurrent Attentional Topic Model},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3412371},
doi = {10.1145/3412371},
abstract = {In a document, the topic distribution of a sentence depends on both the topics of its neighbored sentences and its own content, and it is usually affected by the topics of the neighbored sentences with different weights. The neighbored sentences of a sentence include the preceding sentences and the subsequent sentences. Meanwhile, it is natural that a document can be treated as a sequence of sentences. Most existing works for Bayesian document modeling do not take these points into consideration. To fill this gap, we propose a bi-Directional Recurrent Attentional Topic Model (bi-RATM) for document embedding. The bi-RATM not only takes advantage of the sequential orders among sentences but also uses the attention mechanism to model the relations among successive sentences. To support to the bi-RATM, we propose a bi-Directional Recurrent Attentional Bayesian Process (bi-RABP) to handle the sequences. Based on the bi-RABP, bi-RATM fully utilizes the bi-directional sequential information of the sentences in a document. Online bi-RATM is proposed to handle large-scale corpus. Experiments on two corpora show that the proposed model outperforms state-of-the-art methods on document modeling and classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {74},
numpages = {30},
keywords = {topic modeling, Bi-directional recurrent attentions, recurrent attentional Bayesian process}
}

@article{10.1145/3406600,
author = {Hu, Renjun and Liu, Yanchi and Li, Yanyan and Zhou, Jingbo and Ma, Shuai and Xiong, Hui},
title = {Exploiting User Preference and Mobile Peer Influence for Human Mobility Annotation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3406600},
doi = {10.1145/3406600},
abstract = {Human mobility annotation aims to assign mobility records the corresponding visiting Point-of-Interests (POIs). It is one of the most fundamental problems for understanding human mobile behaviors. In literature, many efforts have been devoted to annotating mobility records in a pointwise or trajectory-wise manner. However, the user preference factor is not fully explored and, worse still, the mobile peer influence factor has never been integrated. To this end, in this article, we propose a novel framework, named JEPPI, to jointly exploit user preference and mobile peer influence to tackle the problem. In our JEPPI, we first unify the two distinct factors in a behavior-driven user-POI graph. This graph enables us to model user preference with user-POI visiting relationships, and model two types of mobile peer influence with co-location and co-visiting peer relationships, respectively. Moreover, we devise an equivalence-emphasizing metric to reduce redundancy in the second-order co-visiting peer influence. In addition, a mutual augmentation learning approach is proposed to preserve the latent structures of various factors exploited. Notably, our learning approach preserves all factors in a shared representation space such that user preference is learned with mobile peer influence being considered at the same time, and vice versa. In this way, the different factors are mutually augmented and semantically integrated to enhance human mobility annotation. Finally, using two large-scale real-world datasets, we conduct extensive experiments to demonstrate the superiority of our approach compared with the state-of-the-art annotation methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {67},
numpages = {18},
keywords = {mobile analysis, Point-of-Interest, Human mobility annotation, network embedding}
}

@article{10.1145/3406242,
author = {Zamzami, Nuha and Bouguila, Nizar},
title = {Probabilistic Modeling for Frequency Vectors Using a Flexible Shifted-Scaled Dirichlet Distribution Prior},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3406242},
doi = {10.1145/3406242},
abstract = {Burstiness and overdispersion phenomena of count vectors pose significant challenges in modeling such data accurately. While the dependency assumption of the multinomial distribution causes its failure to model frequency vectors in several machine learning and data mining applications, researchers found that by extending the multinomial distribution to the Dirichlet Compound multinomial (DCM), both phenomena modeling can be addressed. However, Dirichlet distribution is not the best choice, as a prior, given its negative-correlation and equal-confidence requirements. Thus, we propose to use a flexible generalization of the Dirichlet distribution, namely, the shifted-scaled Dirichlet, as a prior to the multinomial, which grants the model a capability to better fit real data, and we call the new model the Multinomial Shifted-Scaled Dirichlet (MSSD). Given that the likelihood function plays a key role in statistical inference, e.g., in maximum likelihood estimation and Fisher information matrix investigation, we propose to improve the efficiency of computing the MSSD log-likelihood by approximating its function based on Bernoulli polynomials where the log-likelihood function is computed using the proposed mesh algorithm. Moreover, given the sparsity and high-dimensionality nature of count vectors, we propose to improve its computation efficiency by approximating the novel MSSD as a member of the exponential family of distribution, which we call EMSSD. The clustering is based on mixture models, and for learning a model, selection approach is seamlessly integrated with the estimation of the parameters. The merits of the proposed approach are validated via challenging real-world applications such as hate speech detection in Twitter, real-time recognition of criminal action, and anomaly detection in crowded scenes. Results reveal that the proposed clustering frameworks offer a good compromise between other state-of-the-art techniques and outperform other approaches previously used for frequency vectors modeling. Besides, comparing to the MSSD, the approximation EMSSD has reduced the computational complexity in high-dimensional feature spaces.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {69},
numpages = {35},
keywords = {exponential approximation, Count data, mixture models, SVM kernels, online learning}
}

@article{10.1145/3406241,
author = {Nikolakopoulos, Athanasios N. and Karypis, George},
title = {Boosting Item-Based Collaborative Filtering via Nearly Uncoupled Random Walks},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3406241},
doi = {10.1145/3406241},
abstract = {Item-based models are among the most popular collaborative filtering approaches for building recommender systems. Random walks can provide a powerful tool for harvesting the rich network of interactions captured within these models. They can exploit indirect relations between the items, mitigate the effects of sparsity, ensure wider itemspace coverage, as well as increase the diversity of recommendation lists. Their potential however, can be hindered by the tendency of the walks to rapidly concentrate towards the central nodes of the graph, thereby significantly restricting the range of K-step distributions that can be exploited for personalized recommendations. In this work, we introduce RecWalk; a novel random walk-based method that leverages the spectral properties of nearly uncoupled Markov chains to provably lift this limitation and prolong the influence of users’ past preferences on the successive steps of the walk—thereby allowing the walker to explore the underlying network more fruitfully. A comprehensive set of experiments on real-world datasets verify the theoretically predicted properties of the proposed approach and indicate that they are directly linked to significant improvements in top-n recommendation accuracy. They also highlight RecWalk’s potential in providing a framework for boosting the performance of item-based models. RecWalk achieves state-of-the-art top-n recommendation quality outperforming several competing approaches, including recently proposed methods that rely on deep neural networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {64},
numpages = {26},
keywords = {nearly uncoupled markov chains, Collaborative filtering, random walks, Top-N recommendation}
}

@article{10.1145/3399661,
author = {Guo, Jianxiong and Wu, Weili},
title = {Influence Maximization: Seeding Based on Community Structure},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3399661},
doi = {10.1145/3399661},
abstract = {Influence maximization problem attempts to find a small subset of nodes in a social network that makes the expected influence maximized, which has been researched intensively before. Most of the existing literature focus only on maximizing total influence, but it ignores whether the influential distribution is balanced through the network. Even though the total influence is maximized, but gathered in a certain area of social network. Sometimes, this is not advisable. In this article, we propose a novel seeding strategy based on community structure, and formulate the Influence Maximization with Community Budget (IMCB) problem. In this problem, the number of seed nodes in each community is under the cardinality constraint, which can be classified as the problem of monotone submodular maximization under the matroid constraint. To give a satisfactory solution for IMCB problem under the triggering model, we propose the IMCB-Framework, which is inspired by the idea of continuous greedy process and pipage rounding, and derive the best approximation ratio for this problem. In IMCB-Framework, we adopt sampling techniques to overcome the high complexity of continuous greedy. Then, we propose a simplified pipage rounding algorithm, which reduces the complexity of IMCB-Framework further. Finally, we conduct experiments on three real-world datasets to evaluate the correctness and effectiveness of our proposed algorithms, as well as the advantage of IMCB-Framework against classical greedy method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {66},
numpages = {22},
keywords = {social network, Influence maximization, community structure, approximation algorithm, matorid, IMCB-Framework, continuous greedy}
}

@article{10.1145/3410448,
author = {Savva, Fotis and Anagnostopoulos, Christos and Triantafillou, Peter and Kolomvatsos, Kostas},
title = {Large-Scale Data Exploration Using Explanatory Regression Functions},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3410448},
doi = {10.1145/3410448},
abstract = {Analysts wishing to explore multivariate data spaces, typically issue queries involving selection operators, i.e., range or equality predicates, which define data subspaces of potential interest. Then, they use aggregation functions, the results of which determine a subspace’s interestingness for further exploration and deeper analysis. However, Aggregate Query (AQ) results are scalars and convey limited information and explainability about the queried subspaces for enhanced exploratory analysis. Analysts have no way of identifying how these results are derived or how they change w.r.t query (input) parameter values. We address this shortcoming by aiding analysts to explore and understand data subspaces by contributing a novel explanation mechanism based on machine learning. We explain AQ results using functions obtained by a three-fold joint optimization problem which assume the form of explainable piecewise-linear regression functions. A key feature of the proposed solution is that the explanation functions are estimated using past executed queries. These queries provide a coarse grained overview of the underlying aggregate function (generating the AQ results) to be learned. Explanations for future, previously unseen AQs can be computed without accessing the underlying data and can be used to further explore the queried data subspaces, without issuing more queries to the backend analytics engine. We evaluate the explanation accuracy and efficiency through theoretically grounded metrics over real-world and synthetic datasets and query workloads.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {76},
numpages = {33},
keywords = {aggregate query explanation, range query explanation, Explainability, data exploration}
}

@article{10.1145/3408313,
author = {Zhang, Chen and Hoi, Steven C. H. and Tsung, Fugee},
title = {Time-Warped Sparse Non-Negative Factorization for Functional Data Analysis},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3408313},
doi = {10.1145/3408313},
abstract = {This article proposes a novel time-warped sparse non-negative factorization method for functional data analysis. The proposed method on the one hand guarantees the extracted basis functions and their coefficients to be positive and interpretable, and on the other hand is able to handle weakly correlated functions with different features. Furthermore, the method incorporates time warping into factorization and hence allows the extracted basis functions of different samples to have temporal deformations. An efficient framework of estimation algorithms is proposed based on a greedy variable selection approach. Numerical studies together with case studies on real-world data demonstrate the efficacy and applicability of the proposed methodology.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {72},
numpages = {23},
keywords = {multivariate functional data, Non-negative functional factorization, sparse representation, time warping}
}

@article{10.1145/3403934,
author = {Pang, Guansong and Cao, Longbing},
title = {Heterogeneous Univariate Outlier Ensembles in Multidimensional Data},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3403934},
doi = {10.1145/3403934},
abstract = {In outlier detection, recent major research has shifted from developing univariate methods to multivariate methods due to the rapid growth of multidimensional data. However, one typical issue of this paradigm shift is that many multidimensional data often mainly contains univariate outliers, in which many features are actually irrelevant. In such cases, multivariate methods are ineffective in identifying such outliers due to the potential biases and the curse of dimensionality brought by irrelevant features. Those univariate outliers might be well detected by applying univariate outlier detectors in individually relevant features. However, it is very challenging to choose a right univariate detector for each individual feature since different features may take very different probability distributions. To address this challenge, we introduce a novel Heterogeneous Univariate Outlier Ensembles (HUOE) framework and its instance ZDD to synthesize a set of heterogeneous univariate outlier detectors as base learners to build heterogeneous ensembles that are optimized for each individual feature. Extensive results on 19 real-world datasets and a collection of synthetic datasets show that ZDD obtains 5%–14% average AUC improvement over four state-of-the-art multivariate ensembles and performs substantially more robustly w.r.t. irrelevant features.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {68},
numpages = {27},
keywords = {Outlier detection, multidimensional data, outlier ensemble, anomaly detection, univariate outlier, heterogeneous data}
}

@article{10.1145/3394046,
author = {Matheny, Michael and Xie, Dong and Phillips, Jeff M.},
title = {Scalable Spatial Scan Statistics for Trajectories},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3394046},
doi = {10.1145/3394046},
abstract = {We define several new models for how to define anomalous regions among enormous sets of trajectories. These are based on spatial scan statistics, and identify a geometric region which captures a subset of trajectories which are significantly different in a measured characteristic from the background population. The model definition depends on how much a geometric region is contributed to by some overlapping trajectory. This contribution can be the full trajectory, proportional to the length within the spatial region, or dependent on the flux across the boundary of that spatial region. Our methods are based on and significantly extend a recent two-level sampling approach which provides high accuracy at enormous scales of data. We support these new models and algorithms with extensive experiments on millions of trajectories and also theoretical guarantees.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {73},
numpages = {24},
keywords = {discrepancy, range spaces, trajectories, Spatial scan statistics}
}

@article{10.1145/3412364,
author = {Ma, Qian and Gu, Yu and Lee, Wang-Chien and Yu, Ge and Liu, Hongbo and Wu, Xindong},
title = {REMIAN: Real-Time and Error-Tolerant Missing Value Imputation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3412364},
doi = {10.1145/3412364},
abstract = {Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {77},
numpages = {38},
keywords = {poor-quality streaming data, Missing value, real-time imputation}
}

@article{10.1145/3409478,
author = {Guo, Jipeng and Sun, Yanfeng and Gao, Junbin and Hu, Yongli and Yin, Baocai},
title = {Robust Adaptive Linear Discriminant Analysis with Bidirectional Reconstruction Constraint},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3409478},
doi = {10.1145/3409478},
abstract = {Linear discriminant analysis (LDA) is a well-known supervised method for dimensionality reduction in which the global structure of data can be preserved. The classical LDA is sensitive to the noises, and the projection direction of LDA cannot preserve the main energy. This article proposes a novel feature extraction model with l2,1 norm constraint based on LDA, termed as RALDA. This model preserves within-class local structure in the latent subspace according to the label information. To reduce information loss, it learns a projection matrix and an inverse projection matrix simultaneously. By introducing an implicit variable and matrix norm transformation, the alternating direction multiple method with updating variables is designed to solve the RALDA model. Moreover, both computational complexity and weak convergence property of the proposed algorithm are investigated. The experimental results on several public databases have demonstrated the effectiveness of our proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {75},
numpages = {20},
keywords = {adaptive self-learning weights, face recognition, image classification, Linear discriminant analysis, bidirectional reconstruction constraint}
}

@article{10.1145/3407901,
author = {Xiong, Hui and Lin, Chih-Jen},
title = {Introduction to the Special Issue on the Best Papers from KDD 2018},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3407901},
doi = {10.1145/3407901},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {51e},
numpages = {2}
}

@article{10.1145/3361559,
author = {Zhao, Peilin and Wang, Dayong and Wu, Pengcheng and Hoi, Steven C. H.},
title = {A Unified Framework for Sparse Online Learning},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3361559},
doi = {10.1145/3361559},
abstract = {The amount of data in our society has been exploding in the era of big data. This article aims to address several open challenges in big data stream classification. Many existing studies in data mining literature follow the batch learning setting, which suffers from low efficiency and poor scalability. To tackle these challenges, we investigate a unified online learning framework for the big data stream classification task. Different from the existing online data stream classification techniques, we propose a unified Sparse Online Classification (SOC) framework. Based on SOC, we derive a second-order online learning algorithm and a cost-sensitive sparse online learning algorithm, which could successfully handle online anomaly detection tasks with the extremely unbalanced class distribution. As the performance evaluation, we analyze the theoretical bounds of the proposed algorithms and conduct an extensive set of experiments. The encouraging experimental results demonstrate the efficacy of the proposed algorithms over the state-of-the-art techniques on multiple data stream classification tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {59},
numpages = {20},
keywords = {cost-sensitive learning, sparse learning, Online learning, classification}
}

@article{10.1145/3397191,
author = {Rossi, Ryan A. and Jin, Di and Kim, Sungchul and Ahmed, Nesreen K. and Koutra, Danai and Lee, John Boaz},
title = {On Proximity and Structural Role-Based Embeddings in Networks: Misconceptions, Techniques, and Applications},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3397191},
doi = {10.1145/3397191},
abstract = {Structural roles define sets of structurally similar nodes that are more similar to nodes inside the set than outside, whereas communities define sets of nodes with more connections inside the set than outside. Roles based on structural similarity and communities based on proximity are fundamentally different but important complementary notions. Recently, the notion of structural roles has become increasingly important and has gained a lot of attention due to the proliferation of work on learning representations (node/edge embeddings) from graphs that preserve the notion of roles. Unfortunately, recent work has sometimes confused the notion of structural roles and communities (based on proximity) leading to misleading or incorrect claims about the capabilities of network embedding methods. As such, this article seeks to clarify the misconceptions and key differences between structural roles and communities, and formalize the general mechanisms (e.g., random walks and feature diffusion) that give rise to community- or role-based structural embeddings. We theoretically prove that embedding methods based on these mechanisms result in either community- or role-based structural embeddings. These mechanisms are typically easy to identify and can help researchers quickly determine whether a method preserves community- or role-based embeddings. Furthermore, they also serve as a basis for developing new and improved methods for community- or role-based structural embeddings. Finally, we analyze and discuss applications and data characteristics where community- or role-based embeddings are most appropriate.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {63},
numpages = {37},
keywords = {graphlets, positions, proximity, node embeddings, network representation learning, structural embeddings, proximity-based node embeddings, structural node embeddings, structural similarity, role discovery, Role-based structural embedding, roles, community-based embedding}
}

@article{10.1145/3402448,
author = {Ceccarello, Matteo and Pietracaprina, Andrea and Pucci, Geppino},
title = {A General Coreset-Based Approach to Diversity Maximization under Matroid Constraints},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3402448},
doi = {10.1145/3402448},
abstract = {Diversity maximization is a fundamental problem in web search and data mining. For a given dataset S of n elements, the problem requires to determine a subset of S containing k≪n “representatives” which maximize some diversity function expressed in terms of pairwise distances, where distance models dissimilarity. An important variant of the problem prescribes that the solution satisfy an additional orthogonal requirement, which can be specified as a matroid constraint (i.e., a feasible solution must be an independent set of size k of a given matroid). While unconstrained diversity maximization admits efficient coreset-based strategies for several diversity functions, known approaches dealing with the additional matroid constraint apply only to one diversity function (sum of distances), and are based on an expensive, inherently sequential, local search over the entire input dataset. We devise the first coreset-based algorithms for diversity maximization under matroid constraints for various diversity functions, together with efficient sequential, MapReduce, and Streaming implementations. Technically, our algorithms rely on the construction of a small coreset, that is, a subset of S containing a feasible solution which is no more than a factor 1−ɛ away from the optimal solution for S. While our algorithms are fully general, for the partition and transversal matroids, if ɛ is a constant in (0,1) and S has bounded doubling dimension, the coreset size is independent of n and it is small enough to afford the execution of a slow sequential algorithm to extract a final, accurate, solution in reasonable time. Extensive experiments show that our algorithms are accurate, fast, and scalable, and therefore they are capable of dealing with the large input instances typical of the big data scenario.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {60},
numpages = {27},
keywords = {coresets, Diversity maximization, approximation algorithms, matroids, MapReduce, streaming, doubling spaces}
}

@article{10.1145/3399671,
author = {Wang, Tingting and Duan, Lei and Dong, Guozhu and Bao, Zhifeng},
title = {Efficient Mining of Outlying Sequence Patterns for Analyzing Outlierness of Sequence Data},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3399671},
doi = {10.1145/3399671},
abstract = {Recently, a lot of research work has been proposed in different domains to detect outliers and analyze the outlierness of outliers for relational data. However, while sequence data is ubiquitous in real life, analyzing the outlierness for sequence data has not received enough attention. In this article, we study the problem of mining outlying sequence patterns in sequence data addressing the question: given a query sequence s in a sequence dataset D, the objective is to discover sequence patterns that will indicate the most unusualness (i.e., outlierness) of s compared against other sequences. Technically, we use the rank defined by the average probabilistic strength (aps) of a sequence pattern in a sequence to measure the outlierness of the sequence. Then a minimal sequence pattern where the query sequence is ranked the highest is defined as an outlying sequence pattern. To address the above problem, we present OSPMiner, a heuristic method that computes aps by incorporating several pruning techniques. Our empirical study using both real and synthetic data demonstrates that OSPMiner is effective and efficient.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {62},
numpages = {26},
keywords = {average probabilistic strength, outlierness analysis, sequence mining, Outlying sequence pattern}
}

@article{10.1145/3399660,
author = {Nguyen, Hung and Wang, Xuejian and Akoglu, Leman},
title = {End-to-End Continual Rare-Class Recognition with Emerging Novel Subclasses},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3399660},
doi = {10.1145/3399660},
abstract = {Given a labeled dataset that contains a rare (or minority) class containing of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? The setting is different from traditional classification in that instances from novel minority subclasses might continually emerge over time—and hence is often referred as continual, life-long, or open-world classification. We introduce RaRecognize, which (i) estimates a general decision boundary between the rare class and the majority class, (ii) learns to recognize the individual rare subclasses that exist within the training data, as well as (iii) flags instances from previously unseen rare subclasses as newly emerging (i.e., novel). The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii), thus distinguishes minority from the majority without overly tuning to what is only seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurring as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Overall, we build an end-to-end system which consists of (1) a representation learning component that transforms data instances into suitable vector inputs; (2) a continual classifier that labels incoming instances as majority (not of interest), rare recurrent, or rare emerging; and (3) a clustering component that groups the rare emerging instances into novel subclasses for expert vetting and model re-training. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain documents related to corporate-risk and (natural and man-made) disasters as rare classes.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {61},
numpages = {28},
keywords = {rare class recognition, open-world classification, generalization to novel classes, classification with emerging classes, Continual learning}
}

@article{10.1145/3364320,
author = {Huai, Mengdi and Miao, Chenglin and Li, Yaliang and Suo, Qiuling and Su, Lu and Zhang, Aidong},
title = {Learning Distance Metrics from Probabilistic Information},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3364320},
doi = {10.1145/3364320},
abstract = {The goal of metric learning is to learn a good distance metric that can capture the relationships among instances, and its importance has long been recognized in many fields. An implicit assumption in the traditional settings of metric learning is that the associated labels of the instances are deterministic. However, in many real-world applications, the associated labels come naturally with probabilities instead of deterministic values, which makes it difficult for the existing metric-learning methods to work well in these applications. To address this challenge, in this article, we study how to effectively learn the distance metric from datasets that contain probabilistic information, and then propose several novel metric-learning mechanisms for two types of probabilistic labels, i.e., the instance-wise probabilistic label and the group-wise probabilistic label. Compared with the existing metric-learning methods, our proposed mechanisms are capable of learning distance metrics directly from the probabilistic labels with high accuracy. We also theoretically analyze the proposed mechanisms and conduct extensive experiments on real-world datasets to verify the desirable properties of these mechanisms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {53},
numpages = {33},
keywords = {Metric learning, distance measure, probabilistic labels}
}

@article{10.1145/3397503,
author = {Zhou, Xu and Li, Kenli and Yang, Zhibang and Gao, Yunjun and Li, Keqin},
title = {Efficient Approaches to k Representative G-Skyline Queries},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3397503},
doi = {10.1145/3397503},
abstract = {The G-Skyline (GSky) query is a powerful tool to analyze optimal groups in decision support. Compared with other group skyline queries, it releases users from providing an aggregate function. Besides, it can get much comprehensive results without overlooking some important results containing non-skylines. However, it is hard for the users to make sensible choices when facing so many results the GSky query returns, especially over a large, high-dimensional dataset or with a large group size. In this article, we investigate k representative G-Skyline (kGSky) queries to obtain a manageable size of optimal groups. The kGSky query can also inherit the advantage of the GSky query; its results are representative and diversified. Next, we propose three exact algorithms with novel techniques including an upper bound pruning, a grouping strategy, a layered optimum strategy, and a hybrid strategy to efficiently process the kGSky query. Consider these exact algorithms have high time complexity and the precise results are not necessary in many applications. We further develop two approximate algorithms to trade off some accuracy for efficiency. Extensive experiments on both real and synthetic datasets demonstrate the efficiency, scalability, and accuracy of the proposed algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {58},
numpages = {27},
keywords = {Approximate algorithms, Progressive algorithms, data management, group skyline query}
}

@article{10.1145/3374915,
author = {Zhu, Hongyuan and Liu, Qi and Yuan, Nicholas Jing and Zhang, Kun and Zhou, Guang and Chen, Enhong},
title = {Pop Music Generation: From Melody to Multi-Style Arrangement},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3374915},
doi = {10.1145/3374915},
abstract = {Music plays an important role in our daily life. With the development of deep learning and modern generation techniques, researchers have done plenty of works on automatic music generation. However, due to the special requirements of both melody and arrangement, most of these methods have limitations when applying to multi-track music generation. Some critical factors related to the quality of music are not well addressed, such as chord progression, rhythm pattern, and musical style. In order to tackle the problems and ensure the harmony of multi-track music, in this article, we propose an end-to-end melody and arrangement generation framework to generate a melody track with several accompany tracks played by some different instruments. To be specific, we first develop a novel Chord based Rhythm and Melody Cross-Generation Model to generate melody with a chord progression. Then, we propose a Multi-Instrument Co-Arrangement Model based on multi-task learning for multi-track music arrangement. Furthermore, to control the musical style of arrangement, we design a Multi-Style Multi-Instrument Co-Arrangement Model to learn the musical style with adversarial training. Therefore, we can not only maintain the harmony of the generated music but also control the musical style for better utilization. Extensive experiments on a real-world dataset demonstrate the superiority and effectiveness of our proposed models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {54},
numpages = {31},
keywords = {melody and arrangement generation, multi-task joint learning, Harmony evaluation, Music generation, musical style}
}

@article{10.1145/3360048,
author = {Xiao, Keli and Ye, Zeyang and Zhang, Lihao and Zhou, Wenjun and Ge, Yong and Deng, Yuefan},
title = {Multi-User Mobile Sequential Recommendation for Route Optimization},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3360048},
doi = {10.1145/3360048},
abstract = {We enhance the mobile sequential recommendation (MSR) model and address some critical issues in existing formulations by proposing three new forms of the MSR from a multi-user perspective. The multi-user MSR (MMSR) model searches optimal routes for multiple drivers at different locations while disallowing overlapping routes to be recommended. To enrich the properties of pick-up points in the problem formulation, we additionally consider the pick-up capacity as an important feature, leading to the following two modified forms of the MMSR: MMSR-m and MMSR-d. The MMSR-m sets a maximum pick-up capacity for all urban areas, while the MMSR-d allows the pick-up capacity to vary at different locations. We develop a parallel framework based on the simulated annealing to numerically solve the MMSR problem series. Also, a push-point method is introduced to improve our algorithms further for the MMSR-m and the MMSR-d, which can handle the route optimization in more practical ways. Our results on both real-world and synthetic data confirmed the superiority of our problem formulation and solutions under more demanding practical scenarios over several published benchmarks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {52},
numpages = {28},
keywords = {parallel computing, Mobile sequential recommendation, potential traveling distance, simulated annealing, trajectory data analysis}
}

@article{10.1145/3350488,
author = {Zhang, Ping and Bao, Zhifeng and Li, Yuchen and Li, Guoliang and Zhang, Yipeng and Peng, Zhiyong},
title = {Towards an Optimal Outdoor Advertising Placement: When a Budget Constraint Meets Moving Trajectories},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3350488},
doi = {10.1145/3350488},
abstract = {In this article, we propose and study the problem of trajectory-driven influential billboard placement: given a set of billboards U (each with a location and a cost), a database of trajectories T, and a budget L, we find a set of billboards within the budget to influence the largest number of trajectories. One core challenge is to identify and reduce the overlap of the influence from different billboards to the same trajectories, while keeping the budget constraint into consideration. We show that this problem is NP-hard and present an enumeration based algorithm with (1-1/e) approximation ratio. However, the enumeration would be very costly when |U| is large. By exploiting the locality property of billboards’ influence, we propose a partition-based framework PartSel. PartSel partitions U into a set of small clusters, computes the locally influential billboards for each cluster, and merges them to generate the global solution. Since the local solutions can be obtained much more efficiently than the global one, PartSel would reduce the computation cost greatly; meanwhile it achieves a non-trivial approximation ratio guarantee. Then we propose a LazyProbe method to further prune billboards with low marginal influence, while achieving the same approximation ratio as PartSel. Next, we propose a branch-and-bound method to eliminate unnecessary enumerations in both PartSel and LazyProbe, as well as an aggregated index to speed up the computation of marginal influence. Experiments on real datasets verify the efficiency and effectiveness of our methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {51},
numpages = {32},
keywords = {trajectory, influence maximization, Outdoor advertising}
}

@article{10.1145/3394053,
author = {Marques, Henrique O. and Campello, Ricardo J. G. B. and Sander, J\"{o}rg and Zimek, Arthur},
title = {Internal Evaluation of Unsupervised Outlier Detection},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3394053},
doi = {10.1145/3394053},
abstract = {Although there is a large and growing literature that tackles the unsupervised outlier detection problem, the unsupervised evaluation of outlier detection results is still virtually untouched in the literature. The so-called internal evaluation, based solely on the data and the assessed solutions themselves, is required if one wants to statistically validate (in absolute terms) or just compare (in relative terms) the solutions provided by different algorithms or by different parameterizations of a given algorithm in the absence of labeled data. However, in contrast to unsupervised cluster analysis, where indexes for internal evaluation and validation of clustering solutions have been conceived and shown to be very useful, in the outlier detection domain, this problem has been notably overlooked. Here we discuss this problem and provide a solution for the internal evaluation of outlier detection results. Specifically, we describe an index called Internal, Relative Evaluation of Outlier Solutions (IREOS) that can evaluate and compare different candidate outlier detection solutions. Initially, the index is designed to evaluate binary solutions only, referred to as top-n outlier detection results. We then extend IREOS to the general case of non-binary solutions, consisting of outlier detection scorings. We also statistically adjust IREOS for chance and extensively evaluate it in several experiments involving different collections of synthetic and real datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {47},
numpages = {42},
keywords = {validation, unsupervised evaluation, Outlier detection}
}

@article{10.1145/3385655,
author = {Constantinou, Anthony C.},
title = {Learning Bayesian Networks with the Saiyan Algorithm},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385655},
doi = {10.1145/3385655},
abstract = {Some structure learning algorithms have proven to be effective in reconstructing hypothetical Bayesian Network graphs from synthetic data. However, in their mission to maximise a scoring function, many become conservative and minimise edges discovered. While simplicity is desired, the output is often a graph that consists of multiple independent subgraphs that do not enable full propagation of evidence. While this is not a problem in theory, it can be a problem in practice. This article examines a novel unconventional associational heuristic called Saiyan, which returns a directed acyclic graph that enables full propagation of evidence. Associational heuristics are not expected to perform well relative to sophisticated constraint-based and score-based learning approaches. Moreover, forcing the algorithm to connect all data variables implies that the forced edges will not be correct at the rate of those identified unrestrictedly. Still, synthetic and real-world experiments suggest that such a heuristic can be competitive relative to some of the well-established constraint-based, score-based and hybrid learning algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {44},
numpages = {21},
keywords = {structure learning, graphical models, Bayesian networks, directed acyclic graphs}
}

@article{10.1145/3396608,
author = {Das, Shubhomoy and Wong, Weng-Keen and Dietterich, Thomas and Fern, Alan and Emmott, Andrew},
title = {Discovering Anomalies by Incorporating Feedback from an Expert},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3396608},
doi = {10.1145/3396608},
abstract = {Unsupervised anomaly detection algorithms search for outliers and then predict that these outliers are the anomalies. When deployed, however, these algorithms are often criticized for high false-positive and high false-negative rates. One main cause of poor performance is that not all outliers are anomalies and not all anomalies are outliers. In this article, we describe the Active Anomaly Discovery (AAD) algorithm, which incorporates feedback from an expert user that labels a queried data instance as an anomaly or nominal point. This feedback is intended to adjust the anomaly detector so that the outliers it discovers are more in tune with the expert user’s semantic understanding of the anomalies.The AAD algorithm is based on a weighted ensemble of anomaly detectors. When it receives a label from the user, it adjusts the weights on each individual ensemble member such that the anomalies rank higher in terms of their anomaly score than the outliers. The AAD approach is designed to operate in an interactive data exploration loop. In each iteration of this loop, our algorithm first selects a data instance to present to the expert as a potential anomaly and then the expert labels the instance as an anomaly or as a nominal data point. When it receives the instance label, the algorithm updates its internal model and the loop continues until a budget of B queries is spent. The goal of our approach is to maximize the total number of true anomalies in the B instances presented to the expert. We show that the AAD method performs well and in some cases doubles the number of true anomalies found compared to previous methods. In addition we present approximations that make the AAD algorithm much more computationally efficient while maintaining a desirable level of performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {49},
numpages = {32},
keywords = {user feedback, optimization, anomaly detection, Active learning}
}

@article{10.1145/3396238,
author = {Zhu, Xiaofeng and Zhang, Shichao and Zhu, Yonghua and Zheng, Wei and Yang, Yang},
title = {Self-Weighted Multi-View Fuzzy Clustering},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3396238},
doi = {10.1145/3396238},
abstract = {Since the data in each view may contain distinct information different from other views as well as has common information for all views in multi-view learning, many multi-view clustering methods have been designed to use these information (including the distinct information for each view and the common information for all views) to improve the clustering performance. However, previous multi-view clustering methods cannot effectively detect these information so that difficultly outputting reliable clustering models. In this article, we propose a fuzzy, sparse, and robust multi-view clustering method to consider all kinds of relations among the data (such as view importance, view stability, and view diversity), which can effectively extract both distinct information and common information as well as balance these two kinds of information. Moreover, we devise an alternating optimization algorithm to solve the resulting objective function as well as prove that our proposed algorithm achieves fast convergence. It is noteworthy that existing multi-view clustering methods only consider a part of the relations, and thus are a special case of our proposed framework. Experimental results on synthetic datasets and real datasets show that our proposed method outperforms the state-of-the-art clustering methods in terms of evaluation metrics of clustering such as clustering accuracy, normalized mutual information, purity, and adjusted rand index.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {48},
numpages = {17},
keywords = {Multi-view clustering, fuzzy clustering, sparse learning}
}

@article{10.1145/3394520,
author = {Z\"{u}gner, Daniel and Borchert, Oliver and Akbarnejad, Amir and G\"{u}nnemann, Stephan},
title = {Adversarial Attacks on Graph Neural Networks: Perturbations and Their Patterns},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3394520},
doi = {10.1145/3394520},
abstract = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, little is known about their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g., the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we present a study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node’s features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain, we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given. For the first time, we successfully identify important patterns of adversarial attacks on graph neural networks (GNNs) — a first step towards being able to detect adversarial attacks on GNNs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {57},
numpages = {31},
keywords = {adversarial attacks, Relational data, poisoning attacks, graph neural networks}
}

@article{10.1145/3385653,
author = {Riondato, Matteo and Vandin, Fabio},
title = {MiSoSouP: Mining Interesting Subgroups with Sampling and Pseudodimension},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385653},
doi = {10.1145/3385653},
abstract = {We present MiSoSouP, a suite of algorithms for extracting high-quality approximations of the most interesting subgroups, according to different popular interestingness measures, from a random sample of a transactional dataset. We describe a new formulation of these measures as functions of averages, that makes it possible to approximate them using sampling. We then discuss how pseudodimension, a key concept from statistical learning theory, relates to the sample size needed to obtain an high-quality approximation of the most interesting subgroups. We prove an upper bound on the pseudodimension of the problem at hand, which depends on characteristic quantities of the dataset and of the language of patterns of interest. This upper bound then leads to small sample sizes. Our evaluation on real datasets shows that MiSoSouP outperforms state-of-the-art algorithms offering the same guarantees, and it vastly speeds up the discovery of subgroups w.r.t.&nbsp;analyzing the whole dataset.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {56},
numpages = {31},
keywords = {Pattern mining, statistical learning theory}
}

@article{10.1145/3385652,
author = {Mautz, Dominik and Ye, Wei and Plant, Claudia and B\"{o}hm, Christian},
title = {Non-Redundant Subspace Clusterings with Nr-Kmeans and Nr-DipMeans},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385652},
doi = {10.1145/3385652},
abstract = {A huge object collection in high-dimensional space can often be clustered in more than one way, for instance, objects could be clustered by their shape or alternatively by their color. Each grouping represents a different view of the dataset. The new research field of non-redundant clustering addresses this class of problems. In this article, we follow the approach that different, non-redundant k-means-like clusterings may exist in different, arbitrarily oriented subspaces of the high-dimensional space. We assume that these subspaces (and optionally a further noise space without any cluster structure) are orthogonal to each other. This assumption enables a particularly rigorous mathematical treatment of the non-redundant clustering problem and thus a particularly efficient algorithm, which we call Nr-Kmeans (for non-redundant k-means). The superiority of our algorithm is demonstrated both theoretically, as well as in extensive experiments. Further, we propose an extension of Nr-Kmeans that harnesses Hartigan’s dip test to identify the number of clusters for each subspace automatically.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {55},
numpages = {24},
keywords = {non-redundant, subspace, Clustering, k-means}
}

@article{10.1145/3396607,
author = {Xu, Yuanbo and Yang, Yongjian and Wang, En and Han, Jiayu and Zhuang, Fuzhen and Yu, Zhiwen and Xiong, Hui},
title = {Neural Serendipity Recommendation: Exploring the Balance between Accuracy and Novelty with Sparse Explicit Feedback},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3396607},
doi = {10.1145/3396607},
abstract = {Recommender systems have been playing an important role in providing personalized information to users. However, there is always a trade-off between accuracy and novelty in recommender systems. Usually, many users are suffering from redundant or inaccurate recommendation results. To this end, in this article, we put efforts into exploring the hidden knowledge of observed ratings to alleviate this recommendation dilemma. Specifically, we utilize some basic concepts to define a concept, Serendipity, which is characterized by high-satisfaction and low-initial-interest. Based on this concept, we propose a two-phase recommendation problem which aims to strike a balance between accuracy and novelty achieved by serendipity prediction and personalized recommendation. Along this line, a Neural Serendipity Recommendation (NSR) method is first developed by combining Muti-Layer Percetron and Matrix Factorization for serendipity prediction. Then, a weighted candidate filtering method is designed for personalized recommendation. Finally, extensive experiments on real-world data demonstrate that NSR can achieve a superior serendipity by a 12% improvement in average while maintaining stable accuracy compared with state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {50},
numpages = {25},
keywords = {Serendipity, muti-layer percetron, recommender system, matrix factorization}
}

@article{10.1145/3397188,
author = {Zhu, Xiaofeng and Zhang, Shichao and Zhang, Jilian and Li, Yonggang and Lu, Guangquan and Yang, Yang},
title = {Sparse Graph Connectivity for Image Segmentation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3397188},
doi = {10.1145/3397188},
abstract = {It has been demonstrated that the segmentation performance is highly dependent on both subspace preservation and graph connectivity. In the literature, the full connectivity method linearly represents each data point (e.g.,&nbsp;a pixel in one image) by all data points for achieving subspace preservation, while the sparse connectivity method was designed to linearly represent each data point by a set of data points for achieving graph connectivity. However, previous methods only focused on either subspace preservation or graph connectivity. In this article, we propose a Sparse Graph Connectivity (SGC) method for image segmentation to automatically learn the affinity matrix from the low-dimensional space of original data, which aims at simultaneously achieving subspace preservation and graph connectivity. To do this, the proposed SGC simultaneously learns a self-representation affinity matrix for subspace preservation and a sparse affinity matrix for graph connectivity, from the intrinsic low-dimensional feature space of high-dimensional original data. Meanwhile, the self-representation affinity matrix is pushed to be similar to the sparse affinity as well as be the final segmentation results. Experimental result on synthetic and real-image datasets showed that our SGC method achieved the best segmentation performance, compared to state-of-the-art segmentation methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {46},
numpages = {19},
keywords = {clustering, similarity measurement, sparse learning, Image segmentation}
}

@article{10.1145/3385416,
author = {Sun, Bintao and Chan, T.-H. Hubert and Sozio, Mauro},
title = {Fully Dynamic Approximate K-Core Decomposition in Hypergraphs},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385416},
doi = {10.1145/3385416},
abstract = {In this article, we design algorithms to maintain approximate core values in dynamic hypergraphs. This notion has been well studied for normal graphs in both static and dynamic setting. We generalize the problem to hypergraphs when edges can be inserted or deleted by an adversary.We consider two dynamic scenarios. In the first case, there are only insertions; and in the second case, there can be both insertions and deletions. In either case, the update time is poly-logarithmic in the number of nodes, with the insertion-only case boasting a better approximation ratio. We also perform extensive experiments on large real-world datasets, which demonstrate the accuracy and efficiency of our algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {39},
numpages = {21},
keywords = {hypergraphs, Core values, dynamic network}
}

@article{10.1145/3385414,
author = {Chen, Cen and Li, Kenli and Teo, Sin G. and Zou, Xiaofeng and Li, Keqin and Zeng, Zeng},
title = {Citywide Traffic Flow Prediction Based on Multiple Gated Spatio-Temporal Convolutional Neural Networks},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385414},
doi = {10.1145/3385414},
abstract = {Traffic flow prediction is crucial for public safety and traffic management, and remains a big challenge because of many complicated factors, e.g., multiple spatio-temporal dependencies, holidays, and weather. Some work leveraged 2D convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) to explore spatial relations and temporal relations, respectively, which outperformed the classical approaches. However, it is hard for these work to model spatio-temporal relations jointly. To tackle this, some studies utilized LSTMs to connect high-level layers of CNNs, but left the spatio-temporal correlations not fully exploited in low-level layers. In this work, we propose novel spatio-temporal CNNs to extract spatio-temporal features simultaneously from low-level to high-level layers, and propose a novel gated scheme to control the spatio-temporal features that should be propagated through the hierarchy of layers. Based on these, we propose an end-to-end framework, multiple gated spatio-temporal CNNs (MGSTC), for citywide traffic flow prediction. MGSTC can explore multiple spatio-temporal dependencies through multiple gated spatio-temporal CNN branches, and combine the spatio-temporal features with external factors dynamically. Extensive experiments on two real traffic datasets demonstrates that MGSTC outperforms other state-of-the-art baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {42},
numpages = {23},
keywords = {Traffic prediction, spatio-temporal analysis, CNNs, traffic flow prediction}
}

@article{10.1145/3387162,
author = {Zhu, Tianyu and Liu, Guannan and Chen, Guoqing},
title = {Social Collaborative Mutual Learning for Item Recommendation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3387162},
doi = {10.1145/3387162},
abstract = {Recommender Systems (RSs) provide users with item choices based on their preferences reflected in past interactions and become important tools to alleviate the information overload problem for users. However, in real-world scenarios, the user–item interaction matrix is generally sparse, leading to the poor performance of recommendation methods. To cope with this problem, social information is introduced into these methods in several ways, such as regularization, ensemble, and sampling. However, these strategies to use social information have their limitations. The regularization and ensemble strategies may suffer from the over-smoothing problem, while the sampling-based strategy may be affected by the overfitting problem. To overcome the limitations of the previous efforts, a novel social recommendation model, namely, Social Collaborative Mutual Learning (SCML), is proposed in this article. SCML combines the item-based CF model with the social CF model by two well-designed mutual regularization strategies. The embedding-level mutual regularization forces the user representations in two models to be close, and the output-level mutual regularization matches the distributions of the predictions in two models. Extensive experiments on three public datasets show that SCML significantly outperforms the baseline methods and the proposed mutual regularization strategies can embrace the advantages of the item-based CF model and the social CF model to improve the recommendation performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {43},
numpages = {19},
keywords = {collaborative filtering, social network, Recommender systems, mutual learning}
}

@article{10.1145/3385654,
author = {Balasubramaniam, Thirunavukarasu and Nayak, Richi and Yuen, Chau},
title = {Efficient Nonnegative Tensor Factorization via Saturating Coordinate Descent},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385654},
doi = {10.1145/3385654},
abstract = {With the advancements in computing technology and web-based applications, data are increasingly generated in multi-dimensional form. These data are usually sparse due to the presence of a large number of users and fewer user interactions. To deal with this, the Nonnegative Tensor Factorization (NTF) based methods have been widely used. However existing factorization algorithms are not suitable to process in all three conditions of size, density, and rank of the tensor. Consequently, their applicability becomes limited. In this article, we propose a novel fast and efficient NTF algorithm using the element selection approach. We calculate the element importance using Lipschitz continuity and propose a saturation point-based element selection method that chooses a set of elements column-wise for updating to solve the optimization problem. Empirical analysis reveals that the proposed algorithm is scalable in terms of tensor size, density, and rank in comparison to the relevant state-of-the-art algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {40},
numpages = {28},
keywords = {Nonnegative tensor factorization, saturating coordinate descent, element selection, coordinate descent, pattern mining, recommender systems}
}

@article{10.1145/3385530,
author = {Kong, Xiangjie and Zhang, Jun and Zhang, Da and Bu, Yi and Ding, Ying and Xia, Feng},
title = {The Gene of Scientific Success},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385530},
doi = {10.1145/3385530},
abstract = {This article elaborates how to identify and evaluate causal factors to improve scientific impact. Currently, analyzing scientific impact can be beneficial to various academic activities including funding application, mentor recommendation, discovering potential cooperators, and the like. It is universally acknowledged that high-impact scholars often have more opportunities to receive awards as an encouragement for their hard work. Therefore, scholars spend great efforts in making scientific achievements and improving scientific impact during their academic life. However, what are the determinate factors that control scholars’ academic success? The answer to this question can help scholars conduct their research more efficiently. Under this consideration, our article presents and analyzes the causal factors that are crucial for scholars’ academic success. We first propose five major factors including article-centered factors, author-centered factors, venue-centered factors, institution-centered factors, and temporal factors. Then, we apply recent advanced machine learning algorithms and jackknife method to assess the importance of each causal factor. Our empirical results show that author-centered and article-centered factors have the highest relevancy to scholars’ future success in the computer science area. Additionally, we discover an interesting phenomenon that the h-index of scholars within the same institution or university are actually very close to each other.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {41},
numpages = {19},
keywords = {Scientific impact, feature selection, machine learning, academic networks}
}

@article{10.1145/3384203,
author = {Zhang, Si and Tong, Hanghang and Tang, Jie and Xu, Jiejun and Fan, Wei},
title = {Incomplete Network Alignment: Problem Definitions and Fast Solutions},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3384203},
doi = {10.1145/3384203},
abstract = {Networks are prevalent in many areas and are often collected from multiple sources. However, due to the veracity characteristics, more often than not, networks are incomplete. Network alignment and network completion have become two fundamental cornerstones behind a wealth of high-impact graph mining applications. The state-of-the-art have been addressing these two tasks in parallel. That is, most of the existing network alignment methods have implicitly assumed that the topology of the input networks for alignment are perfectly known a priori, whereas the existing network completion methods admit either a single network (i.e., matrix completion) or multiple aligned networks (e.g., tensor completion). In this article, we argue that network alignment and completion are inherently complementary with each other, and hence propose to jointly address them so that the two tasks can mutually benefit from each other. We formulate the problem from the optimization perspective, and propose an effective algorithm (iNeAt) to solve it. The proposed method offers two distinctive advantages. First (Alignment accuracy), our method benefits from the higher-quality input networks while mitigates the effect of the incorrectly inferred links introduced by the completion task itself. Second (Alignment efficiency), thanks to the low-rank structure of the complete networks and the alignment matrix, the alignment process can be significantly accelerated. We perform extensive experiments which show that (1) the network completion can significantly improve the alignment accuracy, i.e., up to 30% over the baseline methods; (2) the network alignment can in turn help recover more missing edges than the baseline methods; and (3) our method achieves a good balance between the running time and the accuracy, and scales with a provable linear complexity in both time and space.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {38},
numpages = {26},
keywords = {low rank, Incomplete network alignment, network completion}
}

@article{10.1145/3391298,
author = {Wang, Changping and Wang, Chaokun and Wang, Zheng and Ye, Xiaojun and Yu, Philip S.},
title = {Edge2vec: Edge-Based Social Network Embedding},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3391298},
doi = {10.1145/3391298},
abstract = {Graph embedding, also known as network embedding and network representation learning, is a useful technique which helps researchers analyze information networks through embedding a network into a low-dimensional space. However, existing graph embedding methods are all node-based, which means they can just directly map the nodes of a network to low-dimensional vectors while the edges could only be mapped to vectors indirectly. One important reason is the computational cost, because the number of edges is always far greater than the number of nodes. In this article, considering an important property of social networks, i.e., the network is sparse, and hence the average degree of nodes is bounded, we propose an edge-based graph embedding (edge2vec) method to map the edges in social networks directly to low-dimensional vectors. Edge2vec takes both the local and the global structure information of edges into consideration to preserve structure information of embedded edges as much as possible. To achieve this goal, edge2vec first ingeniously combines the deep autoencoder and Skip-gram model through a well-designed deep neural network. The experimental results on different datasets show edge2vec benefits from the direct mapping in preserving the structure information of edges.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {45},
numpages = {24},
keywords = {Graph embedding, deep autoencoder, Edge2vec, skip-gram}
}

@article{10.1145/3385656,
author = {Tuomo, Alasalmi and Suutala, Jaakko and R\"{o}ning, Juha and Koskim\"{a}ki, Heli},
title = {Better Classifier Calibration for Small Datasets},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385656},
doi = {10.1145/3385656},
abstract = {Classifier calibration does not always go hand in hand with the classifier’s ability to separate the classes. There are applications where good classifier calibration, i.e., the ability to produce accurate probability estimates, is more important than class separation. When the amount of data for training is limited, the traditional approach to improve calibration starts to crumble. In this article, we show how generating more data for calibration is able to improve calibration algorithm performance in many cases where a classifier is not naturally producing well-calibrated outputs and the traditional approach fails. The proposed approach adds computational cost but considering that the main use case is with small datasets this extra computational cost stays insignificant and is comparable to other methods in prediction time. From the tested classifiers, the largest improvement was detected with the random forest and naive Bayes classifiers. Therefore, the proposed approach can be recommended at least for those classifiers when the amount of data available for training is limited and good calibration is essential.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {34},
numpages = {19},
keywords = {Calibration, small datasets, overfitting}
}

@article{10.1145/3380744,
author = {Akhtar, Md Shad and Chauhan, Dushyant Singh and Ekbal, Asif},
title = {A Deep Multi-Task Contextual Attention Framework for Multi-Modal Affect Analysis},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3380744},
doi = {10.1145/3380744},
abstract = {Multi-modal affect analysis (e.g., sentiment and emotion analysis) is an interdisciplinary study and has been an emerging and prominent field in Natural Language Processing and Computer Vision. The effective fusion of multiple modalities (e.g., text, acoustic, or visual frames) is a non-trivial task, as these modalities, often, carry distinct and diverse information, and do not contribute equally. The issue further escalates when these data contain noise. In this article, we study the concept of multi-task learning for multi-modal affect analysis and explore a contextual inter-modal attention framework that aims to leverage the association among the neighboring utterances and their multi-modal information. In general, sentiments and emotions have inter-dependence on each other (e.g., anger → negative or happy → positive). In our current work, we exploit the relatedness among the participating tasks in the multi-task framework. We define three different multi-task setups, each having two tasks, i.e., sentiment 8 emotion classification, sentiment classification 8 sentiment intensity prediction, and emotion classificati on 8 emotion intensity prediction. Our evaluation of the proposed system on the CMU-Multi-modal Opinion Sentiment and Emotion Intensity benchmark dataset suggests that, in comparison with the single-task learning framework, our multi-task framework yields better performance for the inter-related participating tasks. Further, comparative studies show that our proposed approach attains state-of-the-art performance for most of the cases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {32},
numpages = {27},
keywords = {sentiment analysis, Multi-task learning, multi-modal analysis, inter-modal attention, emotion intensity prediction, sentiment intensity prediction, emotion analysis}
}

@article{10.1145/3389433,
author = {Abd-Elaziz, M. M. and El-Bakry, Hazem M. and Elfetouh, Ahmed Abou and Elzeiny, Amira},
title = {Enhanced Data Mining Technique to Measure Satisfaction Degree of Social Media Users of Xeljanz Drug},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3389433},
doi = {10.1145/3389433},
abstract = {In the recent times, social media has become important in the field of health care as a major resource of valuable health information. Social media can provide massive amounts of data in real-time through user interaction, and this data can be analysed to reflect the harms and benefits of treatment by using the personal health experiences of users to improve health outcomes. In this study, we propose an enhanced data mining framework for analysing user opinions on Twitter and on a health-care forum. The proposed framework measures the degree of satisfaction of consumers regarding the drug Xeljanz, which is used to treat rheumatoid arthritis. The proposed framework is based on seven steps distributed in two phases. The first phase involves aggregating data related to the drug Xeljanz. This data is pre-processed to produce a list of words with a term frequency-inverse document frequency score. The word list is then classified into the following three categories: positive, negative and neutral. The second phase involves modelling social media posts using network analysis, identifying sub-graphs, calculating average opinions and detecting influential users. The results showed 77.3% user satisfaction with Xeljanz. Positive opinions were especially pronounced among users who switched to Xeljanz based on advice from a physician. Negative opinions of Xeljanz typically pertained to the high cost of the drug.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {33},
numpages = {13},
keywords = {Social media, health care, Xeljanz, rheumatoid arthritis, user opinion}
}

@article{10.1145/3372408,
author = {Ermi\c{s}, Beyza and Cemgundefinedl, A. Taylan},
title = {Data Sharing via Differentially Private Coupled Matrix Factorization},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372408},
doi = {10.1145/3372408},
abstract = {We address the privacy-preserving data-sharing problem in a distributed multiparty setting. In this setting, each data site owns a distinct part of a dataset and the aim is to estimate the parameters of a statistical model conditioned on the complete data without any site revealing any information about the individuals in their own parts. The sites want to maximize the utility of the collective data analysis while providing privacy guarantees for their own portion of the data as well as for each participating individual. Our first contribution is to classify these different privacy requirements as (i) site-level and (ii) user-level differential privacy and present formal privacy guarantees for these two cases under the model of differential privacy. To satisfy a stronger form of differential privacy, we use a variant of differential privacy which is local differential privacy where the sensitive data is perturbed with a randomized response mechanism prior to the estimation. In this study, we assume that the data instances that are partitioned between several parties are arranged as matrices. A natural statistical model for this distributed scenario is coupled matrix factorization. We present two generic frameworks for privatizing Bayesian inference for coupled matrix factorization models that are able to guarantee proposed differential privacy notions based on the privacy requirements of the model. To privatize Bayesian inference, we first exploit the connection between differential privacy and sampling from a Bayesian posterior via stochastic gradient Langevin dynamics and then derive an efficient coupled matrix factorization method. In the local privacy context, we propose two models that have an additional privatization mechanism to achieve a stronger measure of privacy and introduce a Gibbs sampling based algorithm. We demonstrate that the proposed methods are able to provide good prediction accuracy on synthetic and real datasets while adhering to the introduced privacy constraints.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {28},
numpages = {27},
keywords = {stochastic gradient Langevin dynamics (SGLD), distributed data, local differential privacy, Markov Chain Monte Carlo (MCMC), Differential privacy, collective matrix factorization}
}

@article{10.1145/3385730,
author = {Amornbunchornvej, Chainarong and Berger-Wolf, Tanya},
title = {Framework for Inferring Following Strategies from Time Series of Movement Data},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385730},
doi = {10.1145/3385730},
abstract = {How do groups of individuals achieve consensus in movement decisions? Do individuals follow their friends, the one predetermined leader, or whomever just happens to be nearby? To address these questions computationally, we formalize Coordination Strategy Inference Problem. In this setting, a group of multiple individuals moves in a coordinated manner toward a target path. Each individual uses a specific strategy to follow others (e.g., nearest neighbors, pre-defined leaders, and preferred friends). Given a set of time series that includes coordinated movement and a set of candidate strategies as inputs, we provide the first methodology (to the best of our knowledge) to infer whether each individual uses local-agreement system or dictatorship-like strategy to achieve movement coordination at the group level. We evaluate and demonstrate the performance of the proposed framework by predicting directions of movement of an individual in a group in both simulated datasets as well as in two real-world datasets: a school of fish and a troop of baboons. Moreover, since there is no prior methodology for inferring individual-level strategies, we compare our framework with the state-of-the-art approach for the task of classification of group-level-coordination models. Results show that our approach is highly accurate in inferring correct strategies in simulated datasets even in complicated mixed strategy settings, which no existing method can infer. In the task of classification of group-level-coordination models, our framework performs better than the state-of-the-art approach in all datasets. Animal data experiments show that fish, as expected, follow their neighbors, while baboons have a preference to follow specific individuals. Our methodology generalizes to arbitrary time series data of real numbers, beyond movement data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {35},
numpages = {22},
keywords = {Model selection, leadership, coordination, time series, data science}
}

@article{10.1145/3385415,
author = {Sun, Heli and He, Fang and Huang, Jianbin and Sun, Yizhou and Li, Yang and Wang, Chenyu and He, Liang and Sun, Zhongbin and Jia, Xiaolin},
title = {Network Embedding for Community Detection in Attributed Networks},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3385415},
doi = {10.1145/3385415},
abstract = {Community detection aims to partition network nodes into a set of clusters, such that nodes are more densely connected to each other within the same cluster than other clusters. For attributed networks, apart from the denseness requirement of topology structure, the attributes of nodes in the same community should also be homogeneous. Network embedding has been proved extremely useful in a variety of tasks, such as node classification, link prediction, and graph visualization, but few works dedicated to unsupervised embedding of node features specified for clustering task, which is vital for community detection and graph clustering. By post-processing with clustering algorithms like k-means, most existing network embedding methods can be applied to clustering tasks. However, the learned embeddings are not designed for clustering task, they only learn topological and attributed information of networks, and no clustering-oriented information is explored. In this article, we propose an algorithm named Network Embedding for node Clustering (NEC) to learn network embedding for node clustering in attributed graphs. Specifically, the presented work introduces a framework that simultaneously learns graph structure-based representations and clustering-oriented representations together. The framework consists of the following three modules: graph convolutional autoencoder module, soft modularity maximization module, and self-clustering module. Graph convolutional autoencoder module learns node embeddings based on topological structure and node attributes. We introduce soft modularity, which can be easily optimized using gradient descent algorithms, to exploit the community structure of networks. By integrating clustering loss and embedding loss, NEC can jointly optimize node cluster labels assignment and learn representations that keep local structure of network. This model can be effectively optimized using stochastic gradient algorithm. Empirical experiments on real-world networks and synthetic networks validate the feasibility and effectiveness of our algorithm on community detection task compared with network embedding based methods and traditional community detection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {36},
numpages = {25},
keywords = {Community detection, representation learning, attributed network, network embedding}
}

@article{10.1145/3379984,
author = {Lappas, Theodoros},
title = {Mining Career Paths from Large Resume Databases: Evidence from IT Professionals},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3379984},
doi = {10.1145/3379984},
abstract = {The emergence of online professional platforms, such as LinkedIn and Indeed, has led to unprecedented volumes of rich resume data that have revolutionized the study of careers. One of the most prevalent problems in this space is the extraction of prototype career paths from a workforce. Previous research has consistently relied on a two-step approach to tackle this problem. The first step computes the pairwise distances between all the career sequences in the database. The second step uses the distance matrix to create clusters, with each cluster representing a different prototype path. As we demonstrate in this work, this approach faces two significant challenges when applied on large resume databases. First, the overwhelming diversity of job titles in the modern workforce prevents the accurate evaluation of distance between career sequences. Second, the clustering step of the standard approach leads to highly heterogeneous clusters, due to its inability to handle categorical sequences and sensitivity to outliers. This leads to non-representative centroids and spurious prototype paths that do not accurately represent the actual groups in the workforce. Our work addresses these two challenges and has practical implications for the numerous researchers and practitioners working on the analysis of career data across domains.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {37},
numpages = {38},
keywords = {career paths, data mining, Career data, clustering}
}

@article{10.1145/3377939,
author = {Liu, Bang and Han, Fred X. and Niu, Di and Kong, Linglong and Lai, Kunfeng and Xu, Yu},
title = {Story Forest: Extracting Events and Telling Stories from Breaking News},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3377939},
doi = {10.1145/3377939},
abstract = {Extracting events accurately from vast news corpora and organize events logically is critical for news apps and search engines, which aim to organize news information collected from the Internet and present it to users in the most sensible forms. Intuitively speaking, an event is a group of news documents that report the same news incident possibly in different ways. In this article, we describe our experience of implementing a news content organization system at Tencent to discover events from vast streams of breaking news and to evolve news story structures in an online fashion. Our real-world system faces unique challenges in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we (1) need to accurately and quickly extract distinguishable events from massive streams of long text documents, and (2) must develop the structures of event stories in an online manner, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest, a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. A core novelty of our Story Forest system is EventX, a semi-supervised scheme to extract events from massive Internet news corpora. EventX relies on a two-layered, graph-based clustering procedure to group documents into fine-grained events. We conducted extensive evaluations based on (1) 60 GB of real-world Chinese news data, (2) a large Chinese Internet news dataset that contains 11,748 news articles with truth event labels, and (3) the 20 News Groups English dataset, through detailed pilot user experience studies. The results demonstrate the superior capabilities of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {31},
numpages = {28},
keywords = {community detection, EventX, document clustering, news articles organization, Story forest}
}

@article{10.1145/3380928,
author = {Yang, Yu and Mao, Xiangbo and Pei, Jian and He, Xiaofei},
title = {Continuous Influence Maximization},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3380928},
doi = {10.1145/3380928},
abstract = {Imagine we are introducing a new product through a social network, where we know for each user in the network the function of purchase probability with respect to discount. Then, what discounts should we offer to those social network users so that, under a predefined budget, the adoption of the product is maximized in expectation? Although influence maximization has been extensively explored, this appealing practical problem still cannot be answered by the existing influence maximization methods. In this article, we tackle the problem systematically. We formulate the general continuous influence maximization problem, investigate the essential properties, and develop a general coordinate descent algorithmic framework as well as the engineering techniques for practical implementation. Our investigation does not assume any specific influence model and thus is general and principled. At the same time, using the most popularly adopted triggering model as a concrete example, we demonstrate that more efficient methods are feasible under specific influence models. Our extensive empirical study on four benchmark real-world networks with synthesized purchase probability curves clearly illustrates that continuous influence maximization can improve influence spread significantly with very moderate extra running time comparing to the classical influence maximization methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {29},
numpages = {38},
keywords = {Viral marketing, budget allocation, influence maximization}
}

@article{10.1145/3375398,
author = {Ostovar, Alireza and Leemans, Sander J. J. and Rosa, Marcello La},
title = {Robust Drift Characterization from Event Streams of Business Processes},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375398},
doi = {10.1145/3375398},
abstract = {Process workers may vary the normal execution of a business process to adjust to changes in their operational environment, e.g., changes in workload, season, or regulations. Changes may be simple, such as skipping an individual activity, or complex, such as replacing an entire procedure with another. Over time, these changes may negatively affect process performance; hence, it is important to identify and understand them early on. As such, a number of techniques have been developed to detect process drifts, i.e., statistically significant changes in process behavior, from process event logs (offline) or event streams (online). However, detecting a drift without characterizing it, i.e., without providing explanations on its nature, is not enough to help analysts understand and rectify root causes for process performance issues. Existing approaches for drift characterization are limited to simple changes that affect individual activities. This article contributes an efficient, accurate, and noise-tolerant automated method for characterizing complex drifts affecting entire process fragments. The method, which works both offline and online, relies on two cornerstone techniques, one to automatically discover process trees from event streams (logs) and the other to transform process trees using a minimum number of change operations. The operations identified are then translated into natural language statements to explain the change behind a drift. The method has been extensively evaluated on artificial and real-life datasets, and against a state-of-the-art baseline method. The results from one of the real-life datasets have also been validated with a process stakeholder.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {30},
numpages = {57},
keywords = {Concept drift, process mining, drift characterization, process drift, business process management, business process}
}

@article{10.1145/3375399,
author = {Lee, Kwang Hee and Kim, Myoung Ho},
title = {Linearization of Dependency and Sampling for Participation-Based Betweenness Centrality in Very Large B-Hypergraphs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375399},
doi = {10.1145/3375399},
abstract = {A B-hypergraph consisting of nodes and directed hyperedges is a generalization of the directed graph. A directed hyperedge in the B-hypergraph represents a relation from a set of source nodes to a single destination node. We suggest one possible definition of betweenness centrality (BC) in B-hypergraphs, called Participation-based BC (PBC). A PBC score of a node is computed based on the number of the shortest paths in which the node participates. This score can be expressed in terms of dependency on the set of its outgoing hyperedges. In this article, we focus on developing efficient computation algorithms for PBC. We first present an algorithm called ePBC for computing exact PBC scores of nodes, which has a cubic-time complexity. This algorithm, however, can be used for only small-sized B-hypergraphs because of its cubic-time complexity, so we propose linearized PBC (ℓPBC) that is an approximation method of ePBC. ℓPBC that comes with a guaranteed upper bound on its error, uses a linearization of dependency on a set of hyperedges. ℓPBC improves the computing time of ePBC by an order of magnitude (i.e., it requires a quadratic time) while maintaining a high accuracy. ℓPBC works well on small to medium-sized B-hypergraphs, but is not scalable enough for a very large B-hypergraph with more than a million hyperedges. To cope with such a very large B-hypergraph, we propose a very fast heuristic sampling-based method called sampling-based ℓPBC (sℓPBC). We show through extensive experiments that ℓPBC and sℓPBC can efficiently estimate PBC scores in various real-world B-hypergraphs with a reasonably good precision@k. The experimental results show that sℓPBC works efficiently even for a very large B-hypergraph.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {25},
numpages = {41},
keywords = {B-hypergraph, rademacher complexity, inclusion-exclusion principle, betweenness centrality, Directed hypergraph}
}

@article{10.1145/3373839,
author = {Tang, Lei and Liu, Zihang and Zhao, Yaling and Duan, Zongtao and Jia, Jingchi},
title = {Efficient Ridesharing Framework for Ride-Matching via Heterogeneous Network Embedding},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3373839},
doi = {10.1145/3373839},
abstract = {Ridesharing has attracted increasing attention in recent years, and combines the flexibility and speed of private cars with the reduced cost of fixed-line systems to benefit alleviating traffic pressure. A major issue in ridesharing is the accurate assignment of passengers to drivers, and how to maximize the number of rides shared between people being assigned to different drivers has become an increasingly popular research topic. There are two major challenges facing ride-matching: scalability and sparsity. Here, we show that network embedding drives the optimal matches between drivers and riders. Contrary to existing approaches that merely depend on the proximity between passengers and drivers, we employ a heterogeneous network to learn the latent semantics from different choices in two types of ridesharing, and extract features in terms of user trajectories and sentiment. A novel framework for ridesharing, RShareForm, which encodes not only the objects but also a variety of semantic relationships between them, is proposed. This article extends the existing skip-gram model to incorporate meta-paths over a proposed heterogeneous network. It allows diverse features to be used to search for similar participants and then ranks them to improve the quality of ride-matching. Extensive experiments on a large-scale dataset from DiDi in Chengdu, China show that by leveraging heterogeneous network embedding with meta paths, RShareForm can significantly improve the accuracy of identifying the participants for ridesharing over existing methods, including both meta-path guided similarity search methods and variants of embedding methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {27},
numpages = {24},
keywords = {skip-gram, heterogeneous network, Ridesharing matching, meta-path, network embedding}
}

@article{10.1145/3374919,
author = {Bian, Jiang and Xiong, Haoyi and Fu, Yanjie and Huan, Jun and Guo, Zhishan},
title = {MP<sup>2</sup>SDA: Multi-Party Parallelized Sparse Discriminant Learning},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3374919},
doi = {10.1145/3374919},
abstract = {Sparse Discriminant Analysis (SDA) has been widely used to improve the performance of classical Fisher’s Linear Discriminant Analysis in supervised metric learning, feature selection, and classification. With the increasing needs of distributed data collection, storage, and processing, enabling the Sparse Discriminant Learning to embrace the multi-party distributed computing environments becomes an emerging research topic. This article proposes a novel multi-party SDA algorithm, which can learn SDA models effectively without sharing any raw data and basic statistics among machines. The proposed algorithm (1) leverages the direct estimation of SDA to derive a distributed loss function for the discriminant learning, (2) parameterizes the distributed loss function with local/global estimates through bootstrapping, and (3) approximates a global estimation of linear discriminant projection vector by optimizing the “distributed bootstrapping loss function” with gossip-based stochastic gradient descent. Experimental results on both synthetic and real-world benchmark datasets show that our algorithm can compete with the aggregated SDA with similar performance, and significantly outperforms the most recent distributed SDA in terms of accuracy and F1-score.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {26},
numpages = {22},
keywords = {Sparse discriminant analysis, multi-party, distributed, parallelized}
}

@article{10.1145/3369873,
author = {Hua, Ting and Lu, Chang-Tien and Choo, Jaegul and Reddy, Chandan K.},
title = {Probabilistic Topic Modeling for Comparative Analysis of Document Collections},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369873},
doi = {10.1145/3369873},
abstract = {Probabilistic topic models, which can discover hidden patterns in documents, have been extensively studied. However, rather than learning from a single document collection, numerous real-world applications demand a comprehensive understanding of the relationships among various document sets. To address such needs, this article proposes a new model that can identify the common and discriminative aspects of multiple datasets. Specifically, our proposed method is a Bayesian approach that represents each document as a combination of common topics (shared across all document sets) and distinctive topics (distributions over words that are exclusive to a particular dataset). Through extensive experiments, we demonstrate the effectiveness of our method compared with state-of-the-art models. The proposed model can be useful for “comparative thinking” analysis in real-world document collections.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {24},
numpages = {27},
keywords = {text mining, Probabilistic topic modeling}
}

@article{10.1145/3373086,
author = {Guo, Yuan and Sun, Yu and Wu, Kai and Jiang, Kerong},
title = {New Algorithms of Feature Selection and Big Data Assignment for CBR System Integrated by Bayesian Network},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3373086},
doi = {10.1145/3373086},
abstract = {Under big data, the integrated system of case-based reasoning and Bayesian network has exhibited great advantage in implementing the intelligence of engineering application in many domains. To further improve the performance of the hybrid system, this article proposes Probability Change Measurement of Solution Parameters (PCMSP)–Half-Division-Cross (HDC) method, which includes two algorithms, namely PCMSP and HDC algorithm. PCMSP algorithm can select principal problem features according to their effects upon all solution features measured by calculating the weighted relative probability (RP) change of all solution features caused by each problem feature. PCMSP algorithm can perfectly work under big data no matter how complex the data types are and how huge the data size is. HDC algorithm is used to assign the computation task of big data to enhance the efficiency of the integrated system. HDC algorithm assigns big data by grouping all the problem parameters into many small sub-groups and then distributing the data which covers the same sub-group of problem parameters to a slave node. HDC algorithm can guarantee enough efficiency of the integrated system under big data no matter how large the number of problem parameters is. Finally, lots of experiments are executed to validate the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {23},
numpages = {20},
keywords = {CBR, integrated system, Feature selection, big data}
}

@article{10.1145/3371153,
author = {Dai, Chenglong and Pi, Dechang and Becker, Stefanie I. and Wu, Jia and Cui, Lin and Johnson, Blake},
title = {CenEEGs: Valid EEG Selection for Classification},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3371153},
doi = {10.1145/3371153},
abstract = {This article explores valid brain electroencephalography (EEG) selection for EEG classification with different classifiers, which has been rarely addressed in previous studies and is mostly ignored by existing EEG processing methods and applications. Importantly, traditional selection methods are not able to select valid EEG signals for different classifiers. This article focuses on a source control-based valid EEG selection to reduce the impact of invalid EEG signals and aims to improve EEG-based classification performance for different classifiers. We propose a novel centroid-based EEG selection approach named CenEEGs, which uses a scale-and-shift-invariance similarity metric to measure similarities of EEG signals and then applies a globally optimal centroid strategy to select valid EEG signals with respect to a similarity threshold. A detailed comparison with several state-of-the-art time series selection methods by using standard criteria on 8 EEG datasets demonstrates the efficacy and superiority of CenEEGs for different classifiers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {18},
numpages = {25},
keywords = {classification, Electroencephalography (EEG) selection, EEG similarity, centroid searching}
}

@article{10.1145/3375392,
author = {Shin, Kijung and Oh, Sejoon and Kim, Jisu and Hooi, Bryan and Faloutsos, Christos},
title = {Fast, Accurate and Provable Triangle Counting in Fully Dynamic Graph Streams},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375392},
doi = {10.1145/3375392},
abstract = {Given a stream of edge additions and deletions, how can we estimate the count of triangles in it? If we can store only a subset of the edges, how can we obtain unbiased estimates with small variances?Counting triangles (i.e., cliques of size three) in a graph is a classical problem with applications in a wide range of research areas, including social network analysis, data mining, and databases. Recently, streaming algorithms for triangle counting have been extensively studied since they can naturally be used for large dynamic graphs. However, existing algorithms cannot handle edge deletions or suffer from low accuracy.Can we handle edge deletions while achieving high accuracy? We propose ThinkD, which accurately estimates the counts of global triangles (i.e., all triangles) and local triangles associated with each node in a fully dynamic graph stream with additions and deletions of edges. Compared to its best competitors, ThinkD is (a) Accurate: up to 4.3\texttimes{} more accurate within the same memory budget, (b) Fast: up to 2.2\texttimes{} faster for the same accuracy requirements, and (c) Theoretically sound: always maintaining estimates with zero bias (i.e., the difference between the true triangle count and the expected value of its estimate) and small variance. As an application, we use ThinkD to detect suddenly emerging dense subgraphs, and we show its advantages over state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {12},
numpages = {39},
keywords = {edge deletions, local triangles, Triangle counting}
}

@article{10.1145/3366022,
author = {Han, Huimei and Zhu, Xingquan and Li, Ying},
title = {Generalizing Long Short-Term Memory Network for Deep Learning from Generic Data},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3366022},
doi = {10.1145/3366022},
abstract = {Long Short-Term Memory (LSTM) network, a popular deep-learning model, is particularly useful for data with temporal correlation, such as texts, sequences, or time series data, thanks to its well-sought after recurrent network structures designed to capture temporal correlation. In this article, we propose to generalize LSTM to generic machine-learning tasks where data used for training do not have explicit temporal or sequential correlation. Our theme is to explore feature correlation in the original data and convert each instance into a synthetic sentence format by using a two-gram probabilistic language model. More specifically, for each instance represented in the original feature space, our conversion first seeks to horizontally align original features into a sequentially correlated feature vector, resembling to the letter coherence within a word. In addition, a vertical alignment is also carried out to create multiple time points and simulate word sequential order in a sentence (i.e., word correlation). The two dimensional horizontal-and-vertical alignments not only ensure feature correlations are maximally utilized, but also preserve the original feature values in the new representation. As a result, LSTM model can be utilized to achieve good classification accuracy, even if the underlying data do not have temporal or sequential dependency. Experiments on 20 generic datasets show that applying LSTM to generic data can improve the classification accuracy, compared to conventional machine-learning methods. This research opens a new opportunity for LSTM deep learning to be broadly applied to generic machine-learning tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {13},
numpages = {28},
keywords = {feature learning, long short-term memory, Deep learning, classification}
}

@article{10.1145/3378537,
author = {Li, Pei-Zhen and Huang, Ling and Wang, Chang-Dong and Lai, Jian-Huang and Huang, Dong},
title = {Community Detection by Motif-Aware Label Propagation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3378537},
doi = {10.1145/3378537},
abstract = {Community detection (or graph clustering) is crucial for unraveling the structural properties of complex networks. As an important technique in community detection, label propagation has shown the advantage of finding a good community structure with nearly linear time complexity. However, despite the progress that has been made, there are still several important issues that have not been properly addressed. First, the label propagation typically proceeds over the lower order structure of the network and only the direct one-hop connections between nodes are taken into consideration. Unfortunately, the higher order structure that may encode design principle of the network and be crucial for community detection is neglected under this regime. Second, the stability of the identified community structure may also be seriously affected by the inherent randomness in the label propagation process. To tackle the above issues, this article proposes a Motif-Aware Weighted Label Propagation method for community detection. We focus on triangles within the network, but our technique extends to other kinds of motifs as well. Specifically, the motif-based higher order structure mining is conducted to capture structural characteristics of the network. First, the motif of interest (locally meaningful pattern) is identified, and then, the motif-based hypergraph can be constructed to encode the higher order connections. To further utilize the structural information of the network, a re-weighted network is designed, which unifies both the higher order structure and the original lower order structure. Accordingly, a novel voting strategy termed NaS (considering both <underline>N</underline>umber <underline>a</underline>nd <underline>S</underline>trength of connections) is proposed to update node labels during the label propagation process. In this way, the random label selection can be effectively eliminated, yielding more stable community structures. Experimental results on multiple real-world datasets have shown the superiority of the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {22},
numpages = {19},
keywords = {higher order structure, Community detection, motifs, label propagation}
}

@article{10.1145/3372409,
author = {Concas, Francesco and Xu, Pengfei and Hoque, Mohammad A. and Lu, Jiaheng and Tarkoma, Sasu},
title = {Multiple Set Matching with Bloom Matrix and Bloom Vector},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372409},
doi = {10.1145/3372409},
abstract = {Bloom Filter is a space-efficient probabilistic data structure for checking the membership of elements in a set. Given multiple sets, a standard Bloom Filter is not sufficient when looking for the items to which an element or a set of input elements belong. An example case is searching for documents with keywords in a large text corpus, which is essentially a multiple set matching problem where the input is single or multiple keywords, and the result is a set of possible candidate documents. This article solves the multiple set matching problem by proposing two efficient Bloom Multifilters called Bloom Matrix and Bloom Vector, which generalize the standard Bloom Filter. Both structures are space-efficient and answer queries with a set of identifiers for multiple set matching problems. The space efficiency can be optimized according to the distribution of labels among multiple sets: Uniform and Zipf. Bloom Vector efficiently exploits the Zipf distribution of data for further space reduction. Indeed, both structures are much more space-efficient compared with the state-of-the-art, Bloofi. The results also highlight that a Lookup operation on Bloom Matrix is significantly faster than on Bloom Vector and Bloofi.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {21},
numpages = {21},
keywords = {multiple sets, Bloom filter, big data, uniform distribution, Bloomier filter, Zipf distribution}
}

@article{10.1145/3375394,
author = {Yan, Xiaoqiang and Lou, Zhengzheng and Hu, Shizhe and Ye, Yangdong},
title = {Multi-Task Information Bottleneck Co-Clustering for Unsupervised Cross-View Human Action Categorization},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375394},
doi = {10.1145/3375394},
abstract = {The widespread adoption of low-cost cameras generates massive amounts of videos recorded from different viewpoints every day. To cope with this vast amount of unlabeled and heterogeneous data, a new multi-task information bottleneck co-clustering (MIBC) approach is proposed to automatically categorize human actions in collections of unlabeled cross-view videos. Our motivation is that, if a learning action category from each view is seen as a single task, it is reasonable to assume that the tasks of learning action patterns from the videos recorded by multiple cameras are dependent and inter-related, since the actions of the same subjects synchronously recorded from different camera viewpoints are complementary to each other. MIBC aims to transfer the shared view knowledge across multiple tasks (i.e., camera viewpoints) to boost the performance of each task. Specifically, MIBC involves the following two parts: (1) extracting action categories for each task by independently maintaining its own relevant information, and (2) allowing the feature representations of all tasks to be compressed into a common feature space, which is utilized to capture the relatedness of multiple tasks and transfer the shared knowledge across different camera viewpoints. These two parts of MIBC work simultaneously and can be solved in a novel co-clustering mechanism. Our experimental evaluation on several cross-view action collections shows that the MIBC algorithm outperforms the existing state-of-the-art baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {15},
numpages = {23},
keywords = {information bottleneck, view knowledge transfer, multi-task clustering, Cross-view action categorization}
}

@article{10.1145/3369871,
author = {Liu, Shuai and Song, Guojie and Huang, Wenhao},
title = {Real-Time Transportation Prediction Correction Using Reconstruction Error in Deep Learning},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369871},
doi = {10.1145/3369871},
abstract = {In online complex systems such as transportation system, an important work is real-time traffic prediction. Due to the data shift, data model inconsistency, and sudden change of traffic patterns (like transportation accident), the prediction result derived from an offline-built model would be unreliable. Retraining the model is usually not time affordable for online prediction, especially when the prediction model is very complex and costs a lot of training time (for example, deep neural networks). A real-time prediction correction strategy would be of great value under this situation. Traditionally, the prediction correction usually relies on the prediction error in several previous time intervals. They assume that the error pattern is similar in the current time interval, so that it is time-delayed to some extent. In this article, we propose the prediction correction strategy using the reconstruction error in the deep neural network. The reconstruction error can reflect the model’s ability on feature representation and then determine the fitness of an input data to the model. We first build the relationship between reconstruction error and prediction error. From the perspective of the prediction interval, we demonstrate that the reconstruction error is in positive relation with the prediction interval. Thus the prediction result is more reliable when the reconstruction error is smaller. Then we propose two mechanisms of real-time prediction correction using the reconstruction error. The data driven prediction correction approach selects several training instances with similar reconstruction errors to the current instance and using their average prediction error in correcting the prediction result. The model-driven approach builds several component deep neural networks in training. The component training set for each network is selected according to the reconstruction error of training instances. For a predicting instance, it first computes the reconstruction error of the sample in each component network and then averages the results by the reconstruction error and prediction interval. The model-driven approach is actually a reconstruction error-based deep neural network ensemble approach. Finally, a series of experiments demonstrated that reconstruction error based prediction correction approaches are effective in several prediction problems in transportation including traffic flow prediction on road, traffic flow prediction in entrance and exit station and travel time prediction. Besides the high overall accuracy, our approach can also provide many observations of using the reconstruction error in transportation prediction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {17},
numpages = {20},
keywords = {traffic flow prediction, Deep learning, reconstruction error, travel time prediction, prediction correction}
}

@article{10.1145/3369438,
author = {Lin, Bo and Luo, Wei and Luo, Zhiling and Wang, Bo and Deng, Shuiguang and Yin, Jianwei and Zhou, Mengchu},
title = {Bradykinesia Recognition in Parkinson’s Disease via Single RGB Video},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369438},
doi = {10.1145/3369438},
abstract = {Parkinson’s disease is a progressive nervous system disorder afflicting millions of patients. Among its motor symptoms, bradykinesia is one of the cardinal manifestations. Experienced doctors are required for the clinical diagnosis of bradykinesia, but sometimes they also miss subtle changes, especially in early stages of such disease. Therefore, developing auxiliary diagnostic methods that can automatically detect bradykinesia has received more and more attention. In this article, we employ a two-stage framework for bradykinesia recognition based on the video of patient movement. First, convolution neural networks are trained to localize keypoints in each video frame. These time-varying coordinates form motion trajectories that represent the whole movement. From the trajectory, we then propose novel measurements, namely stability, completeness, and self-similarity, to quantify different motor behaviors. We also propose a periodic motion model called PMNet. An encoder--decoder structure is applied to learn a low dimensional representation of a motion process. The compressed motion process and quantified motor behaviors are combined as inputs to a fully-connected neural network. Different from the traditional means, our solution extends the application scenario outside the hospital and can be easily transplanted to conduct similar tasks. A commonly used clinical assessment is served as a case study. Experimental results based on real-world data validate the effectiveness of our approach for bradykinesia recognition.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {16},
numpages = {19},
keywords = {Parkinson’s disease, Bradykinesia, time sequence analysis, computer vision, RGB video}
}

@article{10.1145/3375393,
author = {Lin, Chi-Chun and Chuang, Kun-Ta and Wu, Wush Chi-Hsuan and Chen, Ming-Syan},
title = {Budget-Constrained Real-Time Bidding Optimization: Multiple Predictors Make It Better},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375393},
doi = {10.1145/3375393},
abstract = {In this article, we pursue a better solution for the promising problem, i.e., the bidding strategy design, in the real-time bidding (RTB) advertising (AD) environment. Under the budget constraint, the design of an optimal strategy for bidding on each incoming impression opportunity targets at acquiring as many clicks as possible during an AD campaign. State-of-the-art bidding algorithms rely on a single predictor, the clickthrough rate predictor, to calculate the bidding value for each impression. This provides reasonable performance if the predictor has appropriate accuracy in predicting the probability of user clicking. However, the classical methods usually fail to capture optimal results since the predictor accuracy is limited. We improve the situation by accomplishing an additional winning price predictor in the bidding process. In this article, an algorithm combining powers of multiple prediction models is developed. It emerges from an analogy to the online stochastic knapsack problem, and the efficiency of the algorithm is also theoretically analyzed. Experiments conducted on real world RTB datasets show that the proposed solution performs better with regard to both number of clicks achieved and effective cost per click in many different settings of budget constraints.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {14},
numpages = {27},
keywords = {bidding strategy design, Real-time bidding, demand-side platform, bidding with click and winning price predictors}
}

@article{10.1145/3372407,
author = {Dong, Jialin and Yang, Kai and Shi, Yuanming},
title = {Ranking from Crowdsourced Pairwise Comparisons via Smoothed Riemannian Optimization},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372407},
doi = {10.1145/3372407},
abstract = {Social Internet of Things has recently become a promising paradigm for augmenting the capability of humans and devices connected in the networks to provide services. In social Internet of Things network, crowdsourcing that collects the intelligence of the human crowd has served as a powerful tool for data acquisition and distributed computing. To support critical applications (e.g., a recommendation system and assessing the inequality of urban perception), in this article, we shall focus on the collaborative ranking problems for user preference prediction from crowdsourced pairwise comparisons. Based on the Bradley--Terry--Luce (BTL) model, a maximum likelihood estimation (MLE) is proposed via low-rank approach in order to estimate the underlying weight/score matrix, thereby predicting the ranking list for each user. A novel regularized formulation with the smoothed surrogate of elementwise infinity norm is proposed in order to address the unique challenge of the coupled the non-smooth elementwise infinity norm constraint and non-convex low-rank constraint in the MLE problem. We solve the resulting smoothed rank-constrained optimization problem via developing the Riemannian trust-region algorithm on quotient manifolds of fixed-rank matrices, which enjoys the superlinear convergence rate. The admirable performance and algorithmic advantages of the proposed method over the state-of-the-art algorithms are demonstrated via numerical results. Moreover, the proposed method outperforms state-of-the-art algorithms on large collaborative filtering datasets in both success rate of inferring preference and normalized discounted cumulative gain.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {19},
numpages = {26},
keywords = {smoothed matrix manifold optimization, crowdsourced data, low-rank optimization, Ranking, social Internet of Things, pairwise comparison}
}

@article{10.1145/3372406,
author = {Xu, Yanan and Shen, Yanyan and Zhu, Yanmin and Yu, Jiadi},
title = {AR<sup>2</sup>Net: An Attentive Neural Approach for Business Location Selection with Satellite Data and Urban Data},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372406},
doi = {10.1145/3372406},
abstract = {Business location selection is crucial to the success of businesses. Traditional approaches like manual survey investigate multiple factors, such as foot traffic, neighborhood structure, and available workforce, which are typically hard to measure. In this article, we propose to explore both satellite data (e.g., satellite images and nighttime light data) and urban data for business location selection tasks of various businesses. We extract discriminative features from the two kinds of data and perform empirical analysis to evaluate the correlation between extracted features and the business popularity of locations. A novel neural network approach named R2Net is proposed to learn deep interactions among features and predict the business popularity of locations. The proposed approach is trained with a regression-and-ranking combined loss function to preserve accurate popularity estimation and the ranking order of locations simultaneously. To support the location selection for multiple businesses, we propose an approach named AR2Net with three attention modules, which enable the approach to focus on different latent features according to business types. Comprehensive experiments on a real-world dataset demonstrate that the satellite features are effective and our models outperform the state-of-the-art methods in terms of four metrics.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {20},
numpages = {28},
keywords = {satellite images, Satellite data, nighttime light, business location selection}
}

@article{10.1145/3369870,
author = {Nie, Feiping and Wang, Zheng and Wang, Rong and Wang, Zhen and Li, Xuelong},
title = {Adaptive Local Linear Discriminant Analysis},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369870},
doi = {10.1145/3369870},
abstract = {Dimensionality reduction plays a significant role in high-dimensional data processing, and Linear Discriminant Analysis (LDA) is a widely used supervised dimensionality reduction approach. However, a major drawback of LDA is that it is incapable of extracting the local structure information, which is crucial for handling multimodal data. In this article, we propose a novel supervised dimensionality reduction method named Adaptive Local Linear Discriminant Analysis (ALLDA), which adaptively learns a k-nearest neighbors graph from data themselves to extract the local connectivity of data. Furthermore, the original high-dimensional data usually contains noisy and redundant features, which has a negative impact on the evaluation of neighborships and degrades the subsequent classification performance. To address this issue, our method learns the similarity matrix and updates the subspace simultaneously so that the neighborships can be evaluated in the optimal subspaces where the noises have been removed. Through the optimal graph embedding, the underlying sub-manifolds of data in intra-class can be extracted precisely. Meanwhile, an efficient iterative optimization algorithm is proposed to solve the minimization problem. Promising experimental results on synthetic and real-world datasets are provided to evaluate the effectiveness of proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {9},
numpages = {19},
keywords = {linear discriminant analysis, Supervised dimensionality reduction, local connectivity, optimal graph embedding}
}

@article{10.1145/3362158,
author = {Angiulli, Fabrizio},
title = {CFOF: A Concentration Free Measure for Anomaly Detection},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3362158},
doi = {10.1145/3362158},
abstract = {We present a novel notion of outlier, called the Concentration Free Outlier Factor, or CFOF. As a main contribution, we formalize the notion of concentration of outlier scores and theoretically prove that CFOF does not concentrate in the Euclidean space for any arbitrary large dimensionality. To the best of our knowledge, there are no other proposals of data analysis measures related to the Euclidean distance for which it has been provided theoretical evidence that they are immune to the concentration effect. We determine the closed form of the distribution of CFOF scores in arbitrarily large dimensionalities and show that the CFOF score of a point depends on its squared norm standard score and on the kurtosis of the data distribution, thus providing a clear and statistically founded characterization of this notion. Moreover, we leverage this closed form to provide evidence that the definition does not suffer of the hubness problem affecting other measures in high dimensions. We prove that the number of CFOF outliers coming from each cluster is proportional to cluster size and kurtosis, a property that we call semi-locality. We leverage theoretical findings to shed lights on properties of well-known outlier scores. Indeed, we determine that semi-locality characterizes existing reverse nearest neighbor-based outlier definitions, thus clarifying the exact nature of their observed local behavior. We also formally prove that classical distance-based and density-based outliers concentrate both for bounded and unbounded sample sizes and for fixed and variable values of the neighborhood parameter. We introduce the fast-CFOF algorithm for detecting outliers in large high-dimensional dataset. The algorithm has linear cost, supports multi-resolution analysis, and is embarrassingly parallel. Experiments highlight that the technique is able to efficiently process huge datasets and to deal even with large values of the neighborhood parameter, to avoid concentration, and to obtain excellent accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {4},
numpages = {53},
keywords = {huge datasets, high-dimensional data, Outlier detection, curse of dimensionality, kurtosis, hubness, concentration of distances}
}

@article{10.1145/3369799,
author = {Lu, Xinjiang and Yu, Zhiwen and Liu, Chuanren and Liu, Yanchi and Xiong, Hui and Guo, Bin},
title = {Inferring Lifetime Status of Point-of-Interest: A Multitask Multiclass Approach},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369799},
doi = {10.1145/3369799},
abstract = {A Point-of-Interest (POI) refers to a specific location that people may find useful or interesting. In modern cities, a large number of POIs emerge, grow, stabilize for a period, then finally disappear. The stages (e.g., emerge and grow) in this process are called lifetime statuses of a POI. While a large body of research has been devoted to identifying and recommending POIs, there are few studies on inferring the lifetime status of POIs. Indeed, the predictive analytics of POI lifetime status can be valuable for various tasks, such as urban planning, business site selection, and real estate appraisal. In this article, we propose a multitask learning approach, named inferring POI lifetime status, to inferring the POI lifetime status with multifaceted data sources. Specifically, we first define three types of POI lifetime status, i.e., booming, decaying, and stable. Then, we formulate a serial classification problem to predict the sequential/successive lifetime statuses of POIs over time. Leveraging geographical data and human mobility data, we examine and integrate three aspects of features related to the prosperity of POIs, i.e., region popularity, region demands, and peer competitiveness. Next, as the booming/decaying POIs are relatively rare in our data, we perform stable class decomposition to alleviate the imbalance between stable POIs and booming/decaying POIs. Finally, we develop a POI lifetime status classifier by exploiting the multitask learning framework as well as the multiclass kernel-based vector machines. We perform extensive experiments using large-scale and real-world datasets of New York City. The experimental results validate the effectiveness of our approach to automatically inferring POI lifetime status.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {10},
numpages = {27},
keywords = {lifetime status, urban computing, Point-of-Interest, multitask learning, multiclass classification}
}

@article{10.1145/3365673,
author = {Mitra, Sayantan and Hasanuzzaman, Mohammed and Saha, Sriparna},
title = {A Unified Multi-View Clustering Algorithm Using Multi-Objective Optimization Coupled with Generative Model},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3365673},
doi = {10.1145/3365673},
abstract = {There is a large body of works on multi-view clustering that exploit multiple representations (or views) of the same input data for better convergence. These multiple views can come from multiple modalities (image, audio, text) or different feature subsets. Obtaining one consensus partitioning after considering different views is usually a non-trivial task. Recently, multi-objective based multi-view clustering methods have suppressed the performance of single objective based multi-view clustering techniques. One key problem is that it is difficult to select a single solution from a set of alternative partitionings generated by multi-objective techniques on the final Pareto optimal front. In this article, we propose a novel multi-objective based multi-view clustering framework that overcomes the problem of selecting a single solution in multi-objective based techniques. In particular, our proposed framework has three major components as follows: (i) multi-view based multi-objective algorithm, Multiview-AMOSA, for initial clustering of data points; (ii) a generative model for generating a combined solution having probabilistic labels; and (iii) K-means algorithm for obtaining the final labels. As the first component, we have adopted a recently developed multi-view based multi-objective clustering algorithm to generate different possible consensus partitionings of a given dataset taking into account different views. A generative model is coupled with the first component to generate a single consensus partitioning after considering multiple solutions. It exploits the latent subsets of the non-dominated solutions obtained from the multi-objective clustering algorithm and combines them to produce a single probabilistic labeled solution. Finally, a simple clustering algorithm, namely K-means, is applied on the generated probabilistic labels to obtain the final cluster labels. Experimental validation of our proposed framework is carried out over several benchmark datasets belonging to three different domains; UCI datasets, multi-view datasets, search result clustering datasets, and patient stratification datasets. Experimental results show that our proposed framework achieves an improvement of around 2%--4% over different evaluation metrics in all the four domains in comparison to state-of-the art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {2},
numpages = {31},
keywords = {multi-view clustering, generative model, Multi-objective clustering, search result clustering}
}

@article{10.1145/3369872,
author = {Galimberti, Edoardo and Bonchi, Francesco and Gullo, Francesco and Lanciano, Tommaso},
title = {Core Decomposition in Multilayer Networks: Theory, Algorithms, and Applications},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369872},
doi = {10.1145/3369872},
abstract = {Multilayer networks are a powerful paradigm to model complex systems, where multiple relations occur between the same entities. Despite the keen interest in a variety of tasks, algorithms, and analyses in this type of network, the problem of extracting dense subgraphs has remained largely unexplored so far.As a first step in this direction, in this work, we study the problem of core decomposition of a multilayer network. Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting, we devise three algorithms, which differ in the way they visit the core lattice and in their pruning techniques. We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks.We then move a step forward and study the problem of extracting the inner-most (also known as maximal) cores, i.e., the cores that are not dominated by any other core in terms of their core index in all the layers. inner-most cores are typically orders of magnitude less than all the cores. Motivated by this, we devise an algorithm that effectively exploits the maximality property and extracts inner-most cores directly, without first computing a complete decomposition. This allows for a consistent speed up over a na\"{\i}ve method that simply filters out non-inner-most ones from all the cores.Finally, we showcase the multilayer core-decomposition tool in a variety of scenarios and problems. We start by considering the problem of densest-subgraph extraction in multilayer networks. We introduce a definition of multilayer densest subgraph that tradesoff between high density and number of layers in which the high density holds, and exploit multilayer core decomposition to approximate this problem with quality guarantees. As further applications, we show how to utilize multilayer core decomposition to speed-up the extraction of frequent cross-graph quasi-cliques and to generalize the community-search problem to the multilayer setting.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {11},
numpages = {40},
keywords = {cliques and quasi-cliques, dense-subgraph extraction, core decomposition, Graph mining, multilayer networks, community search}
}

@article{10.1145/3365677,
author = {Kuang, Kun and Cui, Peng and Li, Bo and Jiang, Meng and Wang, Yashen and Wu, Fei and Yang, Shiqiang},
title = {Treatment Effect Estimation via Differentiated Confounder Balancing and Regression},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3365677},
doi = {10.1145/3365677},
abstract = {Treatment effect plays an important role on decision making in many fields, such as social marketing, healthcare, and public policy. The key challenge on estimating treatment effect in the wild observational studies is to handle confounding bias induced by imbalance of the confounder distributions between treated and control units. Traditional methods remove confounding bias by re-weighting units with supposedly accurate propensity score estimation under the unconfoundedness assumption. Controlling high-dimensional variables may make the unconfoundedness assumption more plausible, but poses new challenge on accurate propensity score estimation. One strand of recent literature seeks to directly optimize weights to balance confounder distributions, bypassing propensity score estimation. But existing balancing methods fail to do selection and differentiation among the pool of a large number of potential confounders, leading to possible underperformance in many high-dimensional settings. In this article, we propose a data-driven Differentiated Confounder Balancing (DCB) algorithm to jointly select confounders, differentiate weights of confounders and balance confounder distributions for treatment effect estimation in the wild high-dimensional settings. Besides, under some settings with heavy confounding bias, in order to further reduce the bias and variance of estimated treatment effect, we propose a Regression Adjusted Differentiated Confounder Balancing (RA-DCB) algorithm based on our DCB algorithm by incorporating outcome regression adjustment. The synergistic learning algorithms we proposed are more capable of reducing the confounding bias in many observational studies. To validate the effectiveness of our DCB and RA-DCB algorithms, we conduct extensive experiments on both synthetic and real-world datasets. The experimental results clearly demonstrate that our algorithms outperform the state-of-the-art methods. By incorporating regression adjustment, our RA-DCB algorithm achieves more precise estimation on treatment effect than DCB algorithm, especially under the settings with heavy confounding bias. Moreover, we show that the top features ranked by our algorithm generate accurate prediction of online advertising effect.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {6},
numpages = {25},
keywords = {confounding bias, regression adjustment, Treatment effect estimation, differentiated confounder balancing}
}

@article{10.1145/3361739,
author = {Ni, Li and Luo, Wenjian and Zhu, Wenjie and Hua, Bei},
title = {Local Overlapping Community Detection},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3361739},
doi = {10.1145/3361739},
abstract = {Local community detection refers to finding the community that contains the given node based on local information, which becomes very meaningful when global information about the network is unavailable or expensive to acquire. Most studies on local community detection focus on finding non-overlapping communities. However, many real-world networks contain overlapping communities like social networks. Given an overlapping node that belongs to multiple communities, the problem is to find communities to which it belongs according to local information. We propose a framework for local overlapping community detection. The framework has three steps. First, find nodes in multiple communities to which the given node belongs. Second, select representative nodes from nodes obtained above, which tends to be in different communities. Third, discover the communities to which these representative nodes belong. In addition, to demonstrate the effectiveness of the framework, we implement six versions of this framework. Experimental results demonstrate that the six implementation versions outperform the other algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {3},
numpages = {25},
keywords = {Social network, local overlapping community detection, local community detection, community detection}
}

@article{10.1145/3370912,
author = {Queiroz-Sousa, Paulo Orlando and Salgado, Ana Carolina},
title = {A Review on OLAP Technologies Applied to Information Networks},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3370912},
doi = {10.1145/3370912},
abstract = {Many real systems produce network data or highly interconnected data, which can be called information networks. These information networks form a critical component in modern information infrastructure, constituting a large graph data volume. The analysis of information network data covers several technological areas, among them OLAP technologies. OLAP is a technology that enables multi-dimensional and multi-level analysis on a large volume of data, providing aggregated data visualizations with different perspectives. This article presents a literature review on the main applications of OLAP technology in the analysis of information network data. To achieve such goal, it shows a systematic review to list the works that apply OLAP technologies in graph data. It defines seven comparison criteria (Materialization, Network, Selection, Aggregation, Model, OLAP Operations, Analytics) to qualify the works found based on their functionalities. The works are analyzed according to each criterion and discussed to identify trends and challenges in the application of OLAP in the information network.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {8},
numpages = {25},
keywords = {OLAP, data warehousing, cube, Graph-database, information network, graph analytics}
}

@article{10.1145/3365676,
author = {Arifuzzaman, Shaikh and Khan, Maleq and Marathe, Madhav},
title = {Fast Parallel Algorithms for Counting and Listing Triangles in Big Graphs},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3365676},
doi = {10.1145/3365676},
abstract = {Big graphs (networks) arising in numerous application areas pose significant challengesfor graph analysts as these graphs grow to billions of nodes and edges and are prohibitively large to fit in the main memory. Finding the number of triangles in a graph is an important problem in the mining and analysis of graphs. In this article, we present two efficient MPI-based distributed memory parallel algorithms for counting triangles in big graphs. The first algorithm employs overlapping partitioning and efficient load balancing schemes to provide a very fast parallel algorithm. The algorithm scales well to networks with billions of nodes and can compute the exact number of triangles in a network with 10 billion edges in 16 minutes. The second algorithm divides the network into non-overlapping partitions leading to a space-efficient algorithm. Our results on both artificial and real-world networks demonstrate a significant space saving with this algorithm. We also present a novel approach that reduces communication cost drastically leading the algorithm to both a space- and runtime-efficient algorithm. Further, we demonstrate how our algorithms can be used to list all triangles in a graph and compute clustering coefficients of nodes. Our algorithm can also be adapted to a parallel approximation algorithm using an edge sparsification method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {5},
numpages = {34},
keywords = {clustering-coefficient, graph mining, social networks, Triangle-counting, massive networks, parallel algorithms}
}

@article{10.1145/3366633,
author = {Md. Jan, Zohaib and Verma, Brijesh},
title = {Evolutionary Classifier and Cluster Selection Approach for Ensemble Classification},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3366633},
doi = {10.1145/3366633},
abstract = {Ensemble classifiers improve the classification performance by combining several classifiers using a suitable fusion methodology. Many ensemble classifier generation methods have been developed that allowed the training of multiple classifiers on a single dataset. As such random subspace is a common methodology utilized by many state-of-the-art ensemble classifiers that generate random subsamples from the input data and train classifiers on different subsamples. Real-world datasets have randomness and noise in them, therefore not all randomly generated samples are suitable for training. In this article, we propose a novel particle swarm optimization-based approach to optimize the random subspace to generate an ensemble classifier. We first generate a random subspace by incrementally clustering input data and then optimize all generated data clusters. On all optimized data clusters, a set of classifiers is trained and added to the pool. The pool of classifiers is then optimized and an optimized ensemble classifier is generated. The proposed approach is tested on 12 benchmark datasets from the UCI repository and results are compared with current state-of-the-art ensemble classifier approaches. A statistical significance test is also conducted and an analysis is presented.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {7},
numpages = {18},
keywords = {clustering, particle swarm optimization, Multiple classifier systems}
}

@article{10.1145/3365672,
author = {Hasan, Md Kamrul and Pal, Christopher},
title = {A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3365672},
doi = {10.1145/3365672},
abstract = {We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning metaparameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach is as follows: (1) more robust to outliers compared to the logistic and hinge losses; (2) outperforms comparable logistic and max margin models on larger scale benchmark problems; (3) when combined with Gaussian–Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and (4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {1},
numpages = {28},
keywords = {classification, probabilistic models, Machine learning, logistic regression, supervised learning}
}

@article{10.1145/3364226,
author = {Zhou, Yu and Huang, Jianbin and Sun, Heli and Sun, Yizhou and Qiao, Shaojie and Wambura, Stephen},
title = {Recurrent Meta-Structure for Robust Similarity Measure in Heterogeneous Information Networks},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3364226},
doi = {10.1145/3364226},
abstract = {Similarity measure is one of the fundamental task in heterogeneous information network (HIN) analysis. It has been applied to many areas, such as product recommendation, clustering, and Web search. Most of the existing metrics can provide personalized services for users by taking a meta-path or meta-structure as input. However, these metrics may highly depend on the user-specified meta-path or meta-structure. In addition, users must know how to select an appropriate meta-path or meta-structure. In this article, we propose a novel similarity measure in HINs, called Recurrent Meta-Structure (RecurMS)-based Similarity (RMSS). The RecurMS as a schematic structure in HINs provides a unified framework for integrating all of the meta-paths and meta-structures, and can be constructed automatically by means of repetitively traversing the network schema. In order to formalize the semantics, the RecurMS is decomposed into several recurrent meta-paths and recurrent meta-trees, and we then define the commuting matrices of the recurrent meta-paths and meta-trees. All of these commuting matrices are combined together according to different weights. We propose two kinds of weighting strategies to determine the weights. The first is called the local weighting strategy that depends on the sparsity of the commuting matrices, and the second is called the global weighting strategy that depends on the strength of the commuting matrices. As a result, RMSS is defined by means of the weighted summation of the commuting matrices. Note that RMSS can also provide personalized services for users by means of the weights of the recurrent meta-paths and meta-trees. Experimental evaluations show that the proposed RMSS is robust and outperforms the existing metrics in terms of ranking and clustering task.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {64},
numpages = {33},
keywords = {Heterogeneous information network, meta-path, meta-structure, similarity}
}

@article{10.1145/3362695,
author = {Veloso, Br\'{a}ulio M. and Assun\c{c}\~{a}o, Renato M. and Ferreira, Anderson A. and Ziviani, Nivio},
title = {In Search of a Stochastic Model for the E-News Reader},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3362695},
doi = {10.1145/3362695},
abstract = {E-news readers have increasingly at their disposal a broad set of news articles to read. Online newspaper sites use recommender systems to predict and to offer relevant articles to their users. Typically, these recommender systems do not leverage users’ reading behavior. If we know how the topics-reads change in a reading session, we may lead to fine-tuned recommendations, for example, after reading a certain number of sports items, it may be counter-productive to keep recommending other sports news. The motivation for this article is the assumption that understanding user behavior when reading successive online news articles can help in developing better recommender systems. We propose five categories of stochastic models to describe this behavior depending on how the previous reading history affects the future choices of topics. We instantiated these five classes with many different stochastic processes covering short-term memory, revealed-preference, cumulative advantage, and geometric sojourn models. Our empirical study is based on large datasets of E-news from two online newspapers. We collected data from more than 13 million users who generated more than 23 million reading sessions, each one composed by the successive clicks of the users on the posted news. We reduce each user session to the sequence of reading news topics. The models were fitted and compared using the Akaike Information Criterion and the Brier Score. We found that the best models are those in which the user moves through topics influenced only by their most recent readings. Our models were also better to predict the next reading than the recommender systems currently used in these journals showing that our models can improve user satisfaction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {65},
numpages = {27},
keywords = {online newspapers, stochastic models, Modeling user behavior}
}

@article{10.1145/3363571,
author = {Wu, Jimmy Ming-Tai and Lin, Jerry Chun-Wei and Tamrakar, Ashish},
title = {High-Utility Itemset Mining with Effective Pruning Strategies},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363571},
doi = {10.1145/3363571},
abstract = {High-utility itemset mining is a popular data mining problem that considers utility factors, such as quantity and unit profit of items besides frequency measure from the transactional database. It helps to find the most valuable and profitable products/items that are difficult to track by using only the frequent itemsets. An item might have a high-profit value which is rare in the transactional database and has a tremendous importance. While there are many existing algorithms to find high-utility itemsets (HUIs) that generate comparatively large candidate sets, our main focus is on significantly reducing the computation time with the introduction of new pruning strategies. The designed pruning strategies help to reduce the visitation of unnecessary nodes in the search space, which reduces the time required by the algorithm. In this article, two new stricter upper bounds are designed to reduce the computation time by refraining from visiting unnecessary nodes of an itemset. Thus, the search space of the potential HUIs can be greatly reduced, and the mining procedure of the execution time can be improved. The proposed strategies can also significantly minimize the transaction database generated on each node. Experimental results showed that the designed algorithm with two pruning strategies outperform the state-of-the-art algorithms for mining the required HUIs in terms of runtime and number of revised candidates. The memory usage of the designed algorithm also outperforms the state-of-the-art approach. Moreover, a multi-thread concept is also discussed to further handle the problem of big datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {58},
numpages = {22},
keywords = {pruning strategy, high-utility itemset, multiple threads, HUIM}
}

@article{10.1145/3363574,
author = {Lee, John Boaz and Rossi, Ryan A. and Kim, Sungchul and Ahmed, Nesreen K. and Koh, Eunyee},
title = {Attention Models in Graphs: A Survey},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363574},
doi = {10.1145/3363574},
abstract = {Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large—with many complex patterns—and noisy, which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate “attention” into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {62},
numpages = {25},
keywords = {deep learning, graph attention survey, Attention mechanism, graph attention}
}

@article{10.1145/3363570,
author = {Liu, Shenghua and Shen, Huawei and Zheng, Houdong and Cheng, Xueqi and Liao, Xiangwen},
title = {CT LIS: Learning Influences and Susceptibilities through Temporal Behaviors},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363570},
doi = {10.1145/3363570},
abstract = {How to quantify influences between users, seeing that social network users influence each other in their temporal behaviors? Previous work has directly defined an independent model parameter to capture the interpersonal influence between each pair of users. To do so, these models need a parameter for each pair of users, which results in high-dimensional models becoming easily trapped into the overfitting problem. However, such models do not consider how influences depend on each other if influences are sent from the same user or if influences are received by the same user. Therefore, we propose a model that defines parameters for every user with a latent influence vector and a susceptibility vector, opposite to define influences on user pairs. Such low-dimensional representations naturally cause the interpersonal influences involving the same user to be coupled with each other, thus reducing the model’s complexity. Additionally, the model can easily consider the temporal information and sentimental polarities of users’ messages. Finally, we conduct extensive experiments on two real-world Microblog datasets, showing that our model with such representations achieves best performance on three prediction tasks, compared to the state-of-the-art and pair-wise baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {57},
numpages = {21},
keywords = {User behaviors, susceptibility, influence, time series}
}

@article{10.1145/3363573,
author = {Roseberry, Martha and Krawczyk, Bartosz and Cano, Alberto},
title = {Multi-Label Punitive KNN with Self-Adjusting Memory for Drifting Data Streams},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363573},
doi = {10.1145/3363573},
abstract = {In multi-label learning, data may simultaneously belong to more than one class. When multi-label data arrives as a stream, the challenges associated with multi-label learning are joined by those of data stream mining, including the need for algorithms that are fast and flexible, able to match both the speed and evolving nature of the stream. This article presents a punitive k nearest neighbors algorithm with a self-adjusting memory (MLSAMPkNN) for multi-label, drifting data streams. The memory adjusts in size to contain only the current concept and a novel punitive system identifies and penalizes errant data examples early, removing them from the window. By retaining and using only data that are both current and beneficial, MLSAMPkNN is able to adapt quickly and efficiently to changes within the data stream while still maintaining a low computational complexity. Additionally, the punitive removal mechanism offers increased robustness to various data-level difficulties present in data streams, such as class imbalance and noise. The experimental study compares the proposal to 24 algorithms using 30 real-world and 15 artificial multi-label data streams on six multi-label metrics, evaluation time, and memory consumption. The superior performance of the proposed method is validated through non-parametric statistical analysis, proving both high accuracy and low time complexity. MLSAMPkNN is a versatile classifier, capable of returning excellent performance in diverse stream scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {60},
numpages = {31},
keywords = {nearest neighbor, concept drift, data stream, Multi-label classification}
}

@article{10.1145/3363572,
author = {Henzgen, Sascha and H\"{u}llermeier, Eyke},
title = {Mining Rank Data},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363572},
doi = {10.1145/3363572},
abstract = {The problem of frequent pattern mining has been studied quite extensively for various types of data, including sets, sequences, and graphs. Somewhat surprisingly, another important type of data, namely rank data, has received very little attention in data mining so far. In this article, we therefore address the problem of mining rank data, that is, data in the form of rankings (total orders) of an underlying set of items. More specifically, two types of patterns are considered, namely frequent rankings and dependencies between such rankings in the form of association rules. Algorithms for mining frequent rankings and frequent closed rankings are proposed and tested experimentally, using both synthetic and real data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {59},
numpages = {36},
keywords = {rank data, association rules, Data mining, frequent pattern mining}
}

@article{10.1145/3363575,
author = {Yang, Wangdong and Li, Kenli and Li, Keqin},
title = {A Pipeline Computing Method of SpTV for Three-Order Tensors on CPU and GPU},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363575},
doi = {10.1145/3363575},
abstract = {Tensors have drawn a growing attention in many applications, such as physics, engineering science, social networks, recommended systems. Tensor decomposition is the key to explore the inherent intrinsic data relationship of tensor. There are many sparse tensor and vector multiplications (SpTV) in tensor decomposition. We analyze a variety of storage formats of sparse tensors and develop a piecewise compression strategy to improve the storage efficiency of large sparse tensors. This compression strategy can avoid storing a large number of empty slices and empty fibers in sparse tensors, and thus the storage space is significantly reduced. A parallel algorithm for the SpTV based on the high-order compressed format based on slices is designed to greatly improve its computing performance on graphics processing unit. Each tensor is cut into multiple slices to form a series of sparse matrix and vector multiplications, which form the pipelined parallelism. The transmission time of the slices can be hidden through pipelined parallel to further optimize the performance of the SpTV.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {63},
numpages = {27},
keywords = {Pipeline, tensor and vector multiplication, tensor, slices}
}

@article{10.1145/3350487,
author = {Zhu, Peisong and Chen, Zhuang and Zheng, Haojie and Qian, Tieyun},
title = {Aspect Aware Learning for Aspect Category Sentiment Analysis},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3350487},
doi = {10.1145/3350487},
abstract = {Aspect category sentiment analysis (ACSA) is an underexploited subtask in aspect level sentiment analysis. It aims to identify the sentiment of predefined aspect categories. The main challenge in ACSA comes from the fact that the aspect category may not occur in the sentence in most of the cases. For example, the review “they have delicious sandwiches” positively talks about the aspect category “food” in an implicit manner.In this article, we propose a novel aspect aware learning (AAL) framework for ACSA tasks. Our key idea is to exploit the interaction between the aspect category and the contents under the guidance of both sentiment polarity and predefined categories. To this end, we design a two-way memory network for integrating AAL into the framework of sentiment classification. We further present two algorithms to incorporate the potential impacts of aspect categories. One is to capture the correlations between aspect terms and the aspect category like “sandwiches” and “food.” The other is to recognize the aspect category for sentiment representations like “food” for “delicious.” We conduct extensive experiments on four SemEval datasets. The results reveal the essential role of AAL in ACSA by achieving the state-of-the-art performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {55},
numpages = {21},
keywords = {Aspect category sentiment analysis, aspect aware learning, memory network}
}

@article{10.1145/3359554,
author = {Lei, Yu and Li, Wenjie},
title = {Interactive Recommendation with User-Specific Deep Reinforcement Learning},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3359554},
doi = {10.1145/3359554},
abstract = {In this article, we study a multi-step interactive recommendation problem for explicit-feedback recommender systems. Different from the existing works, we propose a novel user-specific deep reinforcement learning approach to the problem. Specifically, we first formulate the problem of interactive recommendation for each target user as a Markov decision process (MDP). We then derive a multi-MDP reinforcement learning task for all involved users. To model the possible relationships (including similarities and differences) between different users’ MDPs, we construct user-specific latent states by using matrix factorization. After that, we propose a user-specific deep Q-learning (UDQN) method to estimate optimal policies based on the constructed user-specific latent states. Furthermore, we propose Biased UDQN (BUDQN) to explicitly model user-specific information by employing an additional bias parameter when estimating the Q-values for different users. Finally, we validate the effectiveness of our approach by comprehensive experimental results and analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {61},
numpages = {15},
keywords = {deep Q-learning, Interactive recommendation, deep reinforcement learning}
}

@article{10.1145/3355563,
author = {Wang, Yuandong and Lin, Xuelian and Wei, Hua and Wo, Tianyu and Huang, Zhou and Zhang, Yong and Xu, Jie},
title = {A Unified Framework with Multi-Source Data for Predicting Passenger Demands of Ride Services},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3355563},
doi = {10.1145/3355563},
abstract = {Ride-hailing applications have been offering convenient ride services for people in need. However, such applications still suffer from the issue of supply-demand disequilibrium, which is a typical problem for traditional taxi services. With effective predictions on passenger demands, we can alleviate the disequilibrium by pre-dispatching, dynamic pricing or avoiding dispatching cars to zero-demand areas. Existing studies of demand predictions mainly utilize limited data sources, trajectory data, or orders of ride services or both of them, which also lacks a multi-perspective consideration. In this article, we present a unified framework with a new combined model and a road-network-based spatial partition to leverage multi-source data and model the passenger demands from temporal, spatial, and zero-demand-area perspectives. In addition, our framework realizes offline training and online predicting, which can satisfy the real-time requirement more easily. We analyze and evaluate the performance of our combined model using the actual operational data from UCAR. The experimental results indicate that our model outperforms baselines on both Mean Absolute Error and Root Mean Square Error on average.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {56},
numpages = {24},
keywords = {demand prediction, Spatio-temporal data, ride service, ride-hailing application}
}

@article{10.1145/3344721,
author = {Zhou, Xiren and Chen, Huanhuan and Li, Jinlong},
title = {Probabilistic Mixture Model for Mapping the Underground Pipes},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3344721},
doi = {10.1145/3344721},
abstract = {Buried pipes beneath our city are blood vessels that feed human civilization through the supply of water, gas, electricity, and so on, and mapping the buried pipes has long been addressed as an issue. In this article, a suitable coordinate of the detected area is established, the noisy Ground Penetrating Radar (GPR) and Global Positioning System (GPS) data are analyzed and normalized, and the pipeline is described mathematically. Based on these, the Probabilistic Mixture Model is proposed to map the buried pipes, which takes discrete noisy GPR and GPS data as the input and the accurate pipe locations and directions as the output. The proposed model consists of the Preprocessing, the Pipe Fitting algorithm, the Classification Fitting Expectation Maximization (CFEM) algorithm, and the Angle-limited Hough (Al-Hough) transform. The direction information of the detecting point is added into the measuring of the distance from the point to nearby pipelines, to handle some areas where the pipes are intersected or difficult to classify. The Expectation Maximization (EM) algorithm is upgraded to CFEM algorithm that is able to classify detecting points into different classes, and connect and fit multiple points in each class to get accurate pipeline locations and directions, and the Al-Hough transform provides reliable initializations for CFEM, to some extent, ensuring the convergence of the proposed model. The experimental results on the simulated and real-world datasets demonstrate the effectiveness of the proposed model.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {47},
numpages = {26},
keywords = {pipeline mapping, buried utility location, Data mining, probabilistic model}
}

@article{10.1145/3340847,
author = {Xu, Yanan and Zhu, Yanmin and Shen, Yanyan and Yu, Jiadi},
title = {Fine-Grained Air Quality Inference with Remote Sensing Data and Ubiquitous Urban Data},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3340847},
doi = {10.1145/3340847},
abstract = {Air quality has gained much attention in recent years and is of great importance to protecting people’s health. Due to the influence of multiple factors, the limited air quality monitoring stations deployed in cities are unable to provide fine-grained air quality information. One cost-effective way is to infer air quality with records from existing monitoring stations. However, the severe data sparsity problem (e.g., only 0.2% data are known) leads to the failure of most inference methods. We observe that remote sensing data are of high quality and have a strong correlation with the air quality. Therefore, we propose to integrate remote sensing data and ubiquitous urban data for the air quality inference. But there are two main challenges, i.e., data heterogeneity and incompleteness of the remote sensing data. To address the challenges, we propose a two-stage approach. In the first stage, we infer and predict air quality conditions of some places leveraging the remote sensing data and meteorological data with two proposed ANN-based methods, respectively. This stage significantly alleviates the data sparsity problem. In the second stage, the records and estimated air quality data are put in a tensor. A tensor decomposition method is applied to complete the tensor. The features extracted from urban data are classified into the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints to further address the data sparsity problem. In addition, an iterative training framework is proposed to improve the inference performance. Experiments on a real-world dataset show that our approach outperforms state-of-the-art methods, such as U-Air.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {46},
numpages = {27},
keywords = {air quality inference, neural network, AOT, Remote sensing, tensor decomposition}
}

@article{10.1145/3340848,
author = {Xue, Yu and Xue, Bing and Zhang, Mengjie},
title = {Self-Adaptive Particle Swarm Optimization for Large-Scale Feature Selection in Classification},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3340848},
doi = {10.1145/3340848},
abstract = {Many evolutionary computation (EC) methods have been used to solve feature selection problems and they perform well on most small-scale feature selection problems. However, as the dimensionality of feature selection problems increases, the solution space increases exponentially. Meanwhile, there are more irrelevant features than relevant features in datasets, which leads to many local optima in the huge solution space. Therefore, the existing EC methods still suffer from the problem of stagnation in local optima on large-scale feature selection problems. Furthermore, large-scale feature selection problems with different datasets may have different properties. Thus, it may be of low performance to solve different large-scale feature selection problems with an existing EC method that has only one candidate solution generation strategy (CSGS). In addition, it is time-consuming to find a suitable EC method and corresponding suitable parameter values for a given large-scale feature selection problem if we want to solve it effectively and efficiently. In this article, we propose a self-adaptive particle swarm optimization (SaPSO) algorithm for feature selection, particularly for large-scale feature selection. First, an encoding scheme for the feature selection problem is employed in the SaPSO. Second, three important issues related to self-adaptive algorithms are investigated. After that, the SaPSO algorithm with a typical self-adaptive mechanism is proposed. The experimental results on 12 datasets show that the solution size obtained by the SaPSO algorithm is smaller than its EC counterparts on all datasets. The SaPSO algorithm performs better than its non-EC and EC counterparts in terms of classification accuracy not only on most training sets but also on most test sets. Furthermore, as the dimensionality of the feature selection problem increases, the advantages of SaPSO become more prominent. This highlights that the SaPSO algorithm is suitable for solving feature selection problems, particularly large-scale feature selection problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {50},
numpages = {27},
keywords = {classification, large-scale, particle swarm optimization, self-adaptive, Feature selection}
}

@article{10.1145/3340804,
author = {Jiang, Fei and Yin, Guosheng and Dominici, Francesca},
title = {Bayesian Model Selection Approach to Multiple Change-Points Detection with Non-Local Prior Distributions},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3340804},
doi = {10.1145/3340804},
abstract = {We propose a Bayesian model selection (BMS) boundary detection procedure using non-local prior distributions for a sequence of data with multiple systematic mean changes. By using the non-local priors in the BMS framework, the BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. Further, we speedup the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. From both theoretical and numerical perspectives, we show that the non-local inverse moment prior leads to the fastest convergence rate in identifying the true change points on the boundaries. Extensive simulation studies are conducted to compare the BMS with existing methods, and our method is illustrated with application to the magnetic resonance imaging guided radiation therapy data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {48},
numpages = {17},
keywords = {moment prior, marginal likelihood, parallel computing, inverse moment prior, Bayes factor, bayesian model selection, local prior}
}

@article{10.1145/3356584,
author = {Wang, Yun and Song, Guojie and Du, Lun and Lu, Zhicong},
title = {Real-Time Estimation of the Urban Air Quality with Mobile Sensor System},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3356584},
doi = {10.1145/3356584},
abstract = {Recently, real-time air quality estimation has attracted more and more attention from all over the world, which is close to our daily life. With the prevalence of mobile sensors, there is an emerging way to monitor the air quality with mobile sensors on vehicles. Compared with traditional expensive monitor stations, mobile sensors are cheaper and more abundant, but observations from these sensors have unstable spatial and temporal distributions, which results in the existing model could not work very well on this type of data. In this article, taking advantage of air quality data from mobile sensors, we propose an real-time urban air quality estimation method based on the Gaussian Process Regression for air pollution of the unmonitored areas, pivoting on the diffusion effect and the accumulation effect of air pollution. In order to meet the real-time demands, we propose a two-layer ensemble learning framework and a self-adaptivity mechanism to improve computational efficiency and adaptivity. We evaluate our model with real data from mobile sensor system located in Beijing, China. And the experiments show that our proposed model is superior to the state-of-the-art spatial regression methods in both precision and time performances.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {49},
numpages = {9},
keywords = {Air quality real-time estimation, ensemble learning, mobile sensors, gaussian process regression}
}

@article{10.1145/3344719,
author = {Bergamini, Elisabetta and Borassi, Michele and Crescenzi, Pierluigi and Marino, Andrea and Meyerhenke, Henning},
title = {Computing Top-<i>k</i> Closeness Centrality Faster in Unweighted Graphs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3344719},
doi = {10.1145/3344719},
abstract = {Given a connected graph G=(V,E), where V denotes the set of nodes and E the set of edges of the graph, the length (that is, the number of edges) of the shortest path between two nodes v and w is denoted by d(v,w). The closeness centrality of a vertex v is then defined as n=1/Σw ∈ V d(v,w), where n=|V|. This measure is widely used in the analysis of real-world complex networks, and the problem of selecting the k most central vertices has been deeply analyzed in the last decade. However, this problem is computationally not easy, especially for large networks: in the first part of the article, we prove that it is not solvable in time O(|E|2=ϵ) on directed graphs, for any constant ϵ &gt; 0, under reasonable complexity assumptions. Furthermore, we propose a new algorithm for selecting the k most central nodes in a graph: we experimentally show that this algorithm improves significantly both the textbook algorithm, which is based on computing the distance between all pairs of vertices, and the state of the art. For example, we are able to compute the top k nodes in few dozens of seconds in real-world networks with millions of nodes and edges. Finally, as a case study, we compute the 10 most central actors in the Internet Movie Database (IMDB) collaboration network, where two actors are linked if they played together in a movie, and in the Wikipedia citation network, which contains a directed edge from a page p to a page q if p contains a link to q.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {53},
numpages = {40},
keywords = {complex networks, Centrality, closeness}
}

@article{10.1145/3340708,
author = {He, Kun and Shi, Pan and Bindel, David and Hopcroft, John E.},
title = {Krylov Subspace Approximation for Local Community Detection in Large Networks},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3340708},
doi = {10.1145/3340708},
abstract = {Community detection is an important information mining task to uncover modular structures in large networks. For increasingly common large network datasets, global community detection is prohibitively expensive, and attention has shifted to methods that mine local communities,&nbsp; i.e.,&nbsp;identifying all latent members of a particular community from a few labeled seed members. To address such semi-supervised mining task, we systematically develop a local spectral (LOSP) subspace-based community detection method, called LOSP. We define a family of LOSP subspaces based on Krylov subspaces, and seek a sparse indicator for the target community via an ℓ1 norm minimization over the Krylov subspace. Variants of LOSP depend on type of random walks with different diffusion speeds, type of random walks, dimension of the LOSP subspace, and step of diffusions. The effectiveness of the proposed LOSP approach is theoretically analyzed based on Rayleigh quotients, and it is experimentally verified on a wide variety of real-world networks across social, production, and biological domains, as well as on an extensive set of synthetic LFR benchmark datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {52},
numpages = {30},
keywords = {Krylov subspace, spectral clustering, Rayleigh quotient, sparse linear coding, Local community detection}
}

@article{10.1145/3344720,
author = {Crescenzi, Valter and Merialdo, Paolo and Qiu, Disheng},
title = {Hybrid Crowd-Machine Wrapper Inference},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3344720},
doi = {10.1145/3344720},
abstract = {Wrapper inference deals in generating programs to extract data from Web pages. Several supervised and unsupervised wrapper inference approaches have been proposed in the literature. On one hand, unsupervised approaches produce erratic wrappers: whenever the sources do not satisfy underlying assumptions of the inference algorithm, their accuracy is compromised. On the other hand, supervised approaches produce accurate wrappers, but since they need training data, their scalability is limited. The recent advent of crowdsourcing platforms has opened new opportunities for supervised approaches, as they make possible the production of large amounts of training data with the support of workers recruited online. Nevertheless, involving human workers has monetary costs. We present an original hybrid crowd-machine wrapper inference system that offers the benefits of both approaches exploiting the cooperation of crowd workers and unsupervised algorithms. Based on a principled probabilistic model that estimates the quality of wrappers, humans workers are recruited only when unsupervised wrapper induction algorithms are not able to produce sufficiently accurate solutions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {51},
numpages = {43},
keywords = {data extraction, wrapper inference, Crowdsourcing}
}

@article{10.1145/3344210,
author = {Tatti, Nikolaj},
title = {Density-Friendly Graph Decomposition},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3344210},
doi = {10.1145/3344210},
abstract = {Decomposing a graph into a hierarchical structure via k-core analysis is a standard operation in any modern graph-mining toolkit. k-core decomposition is a simple and efficient method that allows to analyze a graph beyond its mere degree distribution. More specifically, it is used to identify areas in the graph of increasing centrality and connectedness, and it allows to reveal the structural organization of the graph.Despite the fact that k-core analysis relies on vertex degrees, k-cores do not satisfy a certain, rather natural, density property. Simply put, the most central k-core is not necessarily the densest subgraph. This inconsistency between k-cores and graph density provides the basis of our study.We start by defining what it means for a subgraph to be locally dense, and we show that our definition entails a nested chain decomposition of the graph, similar to the one given by k-cores, but in this case the components are arranged in order of increasing density. We show that such a locally dense decomposition for a graph G=(V,E) can be computed in polynomial time. The running time of the exact decomposition algorithm is O(|V|2|E|) but is significantly faster in practice. In addition, we develop a linear-time algorithm that provides a factor-2 approximation to the optimal locally dense decomposition. Furthermore, we show that the k-core decomposition is also a factor-2 approximation, however, as demonstrated by our experimental evaluation, in practice k-cores have different structure than locally dense subgraphs, and as predicted by the theory, k-cores are not always well-aligned with graph density.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {54},
numpages = {29},
keywords = {max-flow, fractional programming, Locally-dense graph decomposition}
}

@article{10.1145/3322126,
author = {Ben-Gal, Irad and Weinstock, Shahar and Singer, Gonen and Bambos, Nicholas},
title = {Clustering Users by Their Mobility Behavioral Patterns},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3322126},
doi = {10.1145/3322126},
abstract = {The immense stream of data from mobile devices during recent years enables one to learn more about human behavior and provide mobile phone users with personalized services. In this work, we identify clusters of users who share similar mobility behavioral patterns. We analyze trajectories of semantic locations to find users who have similar mobility “lifestyle,” even when they live in different areas. For this task, we propose a new grouping scheme that is called Lifestyle-Based Clustering (LBC). We represent the mobility movement of each user by a Markov model and calculate the Jensen–Shannon distances among pairs of users. The pairwise distances are represented by a similarity matrix, which is used for the clustering. To validate the unsupervised clustering task, we develop an entropy-based clustering measure, namely, an index that measures the homogeneity of mobility patterns within clusters of users. The analysis is validated on a real-world dataset that contains location-movements of 50,000 cellular phone users that were analyzed over a two-month period.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {45},
numpages = {28},
keywords = {clustering evaluation, Clustering trajectories}
}

@article{10.1145/3333030,
author = {Qiang, Jipeng and Chen, Ping and Ding, Wei and Wang, Tong and Xie, Fei and Wu, Xindong},
title = {Heterogeneous-Length Text Topic Modeling for Reader-Aware Multi-Document Summarization},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3333030},
doi = {10.1145/3333030},
abstract = {More and more user comments like Tweets are available, which often contain user concerns. In order to meet the demands of users, a good summary generating from multiple documents should consider reader interests as reflected in reader comments. In this article, we focus on how to generate a summary from multi-document documents by considering reader comments, named as reader-aware multi-document summarization (RA-MDS). We present an innovative topic-based method for RA-MDA, which exploits latent topics to obtain the most salient and lessen redundancy summary from multiple documents. Since finding latent topics for RA-MDS is a crucial step, we also present a Heterogeneous-length Text Topic Modeling (HTTM) to extract topics from the corpus that includes both news reports and user comments, denoted as heterogeneous-length texts. In this case, the latent topics extract by HTTM cover not only important aspects of the event, but also aspects that attract reader interests. Comparisons on summary benchmark datasets also confirm that the proposed RA-MDS method is effective in improving the quality of extracted summaries. In addition, experimental results demonstrate that the proposed topic modeling method outperforms existing topic modeling algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {42},
numpages = {21},
keywords = {LDA, multi-document summarization, Topic modeling, heterogeneous-length text}
}

@article{10.1145/3332168,
author = {Amelkin, Victor and Bogdanov, Petko and Singh, Ambuj K.},
title = {A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332168},
doi = {10.1145/3332168},
abstract = {Analysis of opinion dynamics in social networks plays an important role in today’s life. For predicting users’ political preference, it is particularly important to be able to analyze the dynamics of competing polar opinions, such as pro-Democrat vs. pro-Republican. While observing the evolution of polar opinions in a social network over time, can we tell when the network evolved abnormally? Furthermore, can we predict how the opinions of the users will change in the future? To answer such questions, it is insufficient to study individual user behavior, since opinions can spread beyond users’ ego-networks. Instead, we need to consider the opinion dynamics of all users simultaneously and capture the connection between the individuals’ behavior and the global evolution pattern of the social network.In this work, we introduce the Social Network Distance (SND)—a distance measure that quantifies the likelihood of evolution of one snapshot of a social network into another snapshot under a chosen model of polar opinion dynamics. SND has a rich semantics of a transportation problem, yet, is computable in time linear in the number of users and, as such, is applicable to large-scale online social networks. In our experiments with synthetic and Twitter data, we demonstrate the utility of our distance measure for anomalous event detection. It achieves a true positive rate of 0.83, twice as high as that of alternatives. The same predictions presented in precision-recall space show that SND retains perfect precision for recall up to 0.2. Its precision then decreases while maintaining more than 2-fold improvement over alternatives for recall up to 0.95. When used for opinion prediction in Twitter data, SND’s accuracy is 75.6%, which is 7.5% higher than that of the next best method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {38},
numpages = {34},
keywords = {Social network, model-driven analysis, competing opinions, time-series, transportation problem, opinion prediction, minimum-cost network flow, polarization, anomaly detection, opinion dynamics, earth mover’s distance, distance measure, wasserstein metric, polar opinions}
}

@article{10.1145/3332185,
author = {Comito, Carmela and Forestiero, Agostino and Pizzuti, Clara},
title = {Bursty Event Detection in Twitter Streams},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332185},
doi = {10.1145/3332185},
abstract = {Social media, in recent years, have become an invaluable source of information for both public and private organizations to enhance the comprehension of people interests and the onset of new events. Twitter, especially, allows a fast spread of news and events happening real time that can contribute to situation awareness during emergency situations, but also to understand trending topics of a period. The article proposes an online algorithm that incrementally groups tweet streams into clusters. The approach summarizes the examined tweets into the cluster centroid by maintaining a number of textual and temporal features that allow the method to effectively discover groups of interest on particular themes. Experiments on messages posted by users addressing different issues, and a comparison with state-of-the-art approaches show that the method is capable to detect discussions regarding topics of interest, but also to distinguish bursty events revealed by a sudden spreading of attention on messages published by users.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {41},
numpages = {28},
keywords = {online clustering, bursty event, Twitter, event detection}
}

@article{10.1145/3332183,
author = {Chen, Haoran and Li, Jinghua and Gao, Junbin and Sun, Yanfeng and Hu, Yongli and Yin, Baocai},
title = {Maximally Correlated Principal Component Analysis Based on Deep Parameterization Learning},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332183},
doi = {10.1145/3332183},
abstract = {Dimensionality reduction is widely used to deal with high-dimensional data. As a famous dimensionality reduction method, principal component analysis (PCA) aiming at finding the low dimension feature of original data has made great successes, and many improved PCA algorithms have been proposed. However, most algorithms based on PCA only consider the linear correlation of data features. In this article, we propose a novel dimensionality reduction model called maximally correlated PCA based on deep parameterization learning (MCPCADP), which takes nonlinear correlation into account in the deep parameterization framework for the purpose of dimensionality reduction. The new model explores nonlinear correlation by maximizing Ky-Fan norm of the covariance matrix of nonlinearly mapped data features. A new BP algorithm for model optimization is derived. In order to assess the proposed method, we conduct experiments on both a synthetic database and several real-world databases. The experimental results demonstrate that the proposed algorithm is comparable to several widely used algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {39},
numpages = {17},
keywords = {deep parameterization learning, classification, back propagation, Maximally correlated principal component analysis}
}

@article{10.1145/3333028,
author = {Sajadmanesh, Sina and Bazargani, Sogol and Zhang, Jiawei and Rabiee, Hamid R.},
title = {Continuous-Time Relationship Prediction in Dynamic Heterogeneous Information Networks},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3333028},
doi = {10.1145/3333028},
abstract = {Online social networks, World Wide Web, media, and technological networks, and other types of so-called information networks are ubiquitous nowadays. These information networks are inherently heterogeneous and dynamic. They are heterogeneous as they consist of multi-typed objects and relations, and they are dynamic as they are constantly evolving over time. One of the challenging issues in such heterogeneous and dynamic environments is to forecast those relationships in the network that will appear in the future. In this article, we try to solve the problem of continuous-time relationship prediction in dynamic and heterogeneous information networks. This implies predicting the time it takes for a relationship to appear in the future, given its features that have been extracted by considering both heterogeneity and temporal dynamics of the underlying network. To this end, we first introduce a feature extraction framework that combines the power of meta-path-based modeling and recurrent neural networks to effectively extract features suitable for relationship prediction regarding heterogeneity and dynamicity of the networks. Next, we propose a supervised non-parametric approach, called Non-Parametric Generalized Linear Model (Np-Glm), which infers the hidden underlying probability distribution of the relationship building time given its features. We then present a learning algorithm to train Np-Glm and an inference method to answer time-related queries. Extensive experiments conducted on synthetic data and three real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate the effectiveness of Np-Glm in solving continuous-time relationship prediction problem vis-\`{a}-vis competitive baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {44},
numpages = {31},
keywords = {recurrent neural network, social network analysis, non-parametric modeling, Link prediction, heterogeneous network, autoencoder}
}

@article{10.1145/3332522,
author = {Li, Qingyang and Yu, Zhiwen and Guo, Bin and Xu, Huang and Lu, Xinjiang},
title = {Housing Demand Estimation Based on Express Delivery Data},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332522},
doi = {10.1145/3332522},
abstract = {Housing demand estimation is an important topic in the field of economic research. It is beneficial and helpful for various applications including real estate market regulation and urban planning, and therefore is crucial for both real estate investors and government administrators. Meanwhile, given the rapid development of the express industry, abundant useful information is embedded in express delivery records, which is helpful for researchers in profiling urban life patterns. The express delivery behaviors of the residents in a residential community can reflect the housing demand to some extent. Although housing demand has been analyzed in previous studies, its estimation has not been very good, and the subject remains under explored. To this end, in this article, we propose a systematic housing demand estimation method based on express delivery data. First, the express delivery records are aggregated on the community scale with the use of clustering methods, and the missing values in the records are completed. Then, various features are extracted from a less sparse dataset considering both the probability of residential mobility and the attractiveness of residential communities. In addition, given that the correlations between different districts can influence the performances of the inference model, the commonalities and differences of different districts are considered. After obtaining the features and correlations between different districts being obtained, the housing demand is estimated by using a multi-task learning method based on neural networks. The experimental results for real-world data show that the proposed model is effective at estimating the housing demand at the residential community level.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {43},
numpages = {25},
keywords = {Housing demand, express delivery data, multi-task learning}
}

@article{10.1145/3332146,
author = {Gallardo, Laura Fern\'{a}ndez and Sanchez-Iborra, Ramon},
title = {On the Impact of Voice Encoding and Transmission on the Predictions of Speaker Warmth and Attractiveness},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332146},
doi = {10.1145/3332146},
abstract = {Modern human-computer interaction systems may not only be based on interpreting natural language but also on detecting speaker interpersonal characteristics in order to determine dialog strategies. This may be of high interest in different fields such as telephone marketing or automatic voice-based interactive services. However, when such systems encounter signals transmitted over a communication network instead of clean speech, e.g., in call centers, the speaker characterization accuracy might be impaired by the degradations caused in the speech signal by the encoding and communication processes. This article addresses a binary classification of high versus low warm--attractive speakers over different channel and encoding conditions. The ground truth is derived from ratings given to clean speech extracted from an extensive subjective test. Our results show that, under the considered conditions, the AMR-WB+ codec permits good levels of classification accuracy, comparable to the classification with clean, non-degraded speech. This is especially notable for the case of a Random Forest-based classifier, which presents the best performance among the set of evaluated algorithms. The impact of different packet loss rates has been examined, whereas jitter effects have been found to be negligible.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {40},
numpages = {17},
keywords = {predictive modeling, transmission channels, Speaker characteristics, speech processing}
}

@article{10.1145/3326919,
author = {Dornaika, Fadi},
title = {Active Two Phase Collaborative Representation Classifier},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3326919},
doi = {10.1145/3326919},
abstract = {The Sparse Representation Classifier, the Collaborative Representation Classifier (CRC), and the Two Phase Test Sample Sparse Representation (TPTSSR) classifier were introduced in recent times. All these frameworks are supervised and passive in the sense that they cannot benefit from unlabeled data samples. In this paper, inspired by active learning paradigms, we introduce an active CRC that can be used by these frameworks. More precisely, we are interested in the TPTSSR framework due to its good performance and its reasonable computational cost. Our proposed Active Two Phase Collaborative Representation Classifier (ATPCRC) starts by predicting the label of the available unlabeled samples. At testing stage, two coding processes are carried out separately on the set of originally labeled samples and the whole set (original and predicted label). The two types of class-wise reconstruction errors are blended in order to decide the class of any test image. Experiments conducted on four public image datasets show that the proposed ATPCRC can outperform the classic TPTSSR as well as many state-of-the-art methods that exploit label and unlabeled data samples.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {36},
numpages = {10},
keywords = {active learning, Two phase collaborative representation classifier, semi-supervised learning, image categorization}
}

@article{10.1145/3332932,
author = {Yang, Wenmain and Wang, Kun and Ruan, Na and Gao, Wenyuan and Jia, Weijia and Zhao, Wei and Liu, Nan and Zhang, Yunyong},
title = {Time-Sync Video Tag Extraction Using Semantic Association Graph},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332932},
doi = {10.1145/3332932},
abstract = {Time-sync comments (TSCs) reveal a new way of extracting the online video tags. However, such TSCs have lots of noises due to users’ diverse comments, introducing great challenges for accurate and fast video tag extractions. In this article, we propose an unsupervised video tag extraction algorithm named Semantic Weight-Inverse Document Frequency (SW-IDF). Specifically, we first generate corresponding semantic association graph (SAG) using semantic similarities and timestamps of the TSCs. Second, we propose two graph cluster algorithms, i.e., dialogue-based algorithm and topic center-based algorithm, to deal with the videos with different density of comments. Third, we design a graph iteration algorithm to assign the weight to each comment based on the degrees of the clustered subgraphs, which can differentiate the meaningful comments from the noises. Finally, we gain the weight of each word by combining Semantic Weight (SW) and Inverse Document Frequency (IDF). In this way, the video tags are extracted automatically in an unsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based algorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in high-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments; while SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122 MAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density comments. It has a better performance than the state-of-the-art unsupervised algorithms in both F1-score and MAP.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {37},
numpages = {24},
keywords = {semantic association graph, crowdsourced time-sync comments, video tagging, keywords extraction, Multimedia retrieval}
}

@article{10.1145/3326060,
author = {Huang, Yourong and Xiao, Zhu and Yu, Xiaoyou and Wang, Dong and Havyarimana, Vincent and Bai, Jing},
title = {Road Network Construction with Complex Intersections Based on Sparsely Sampled Private Car Trajectory Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3326060},
doi = {10.1145/3326060},
abstract = {A road network is a critical aspect of both urban planning and route recommendation. This article proposes an efficient approach to build a fine-grained road network based on sparsely sampled private car trajectory data under complex urban environment. In order to resolve difficulties introduced by low sampling rate trajectory data, we concentrate sample points around intersections by utilizing the turning characteristics from the large-scale trajectory data to ensure the accuracy of the detection of intersections and road segments. In front of complex road networks including many complex intersections, such as the overpasses and underpasses, we first layer intersections into major and minor one, and then propose a simplified representation of intersections and corresponding computable model based on the features of roads, which can significantly improve the accuracy of detected road networks, especially for the complex intersections. In order to construct fine-grained road networks, we distinguish various types of intersections using direction information and detected turning limit. To the best of our knowledge, our road network building method is the first time to give fine-grained road networks based on low-sampling rate private car trajectory data, especially able to infer the location of complex intersections and its connections to other intersections. Last but not the least, we propose an effective parameter selection process for the Density-Based Spatial Clustering of Applications with Noise based clustering algorithm, which is used to implement the reliable intersection detection. Extensive evaluations are conducted based on a real-world trajectory dataset from 1,345 private cars in Futian district, Shenzhen city of China. The results demonstrate the effectiveness of the proposed method. The constructed road network matches close to the one from a public editing map OpenStreetMap, especially the location of the road intersections and road segments, which achieves 92.2% intersections within 20m and 91.6% road segments within 8m.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {35},
numpages = {28},
keywords = {road networks, private cars, Trajectory data}
}

@article{10.1145/3320277,
author = {Zhang, Mingyue and Wei, Xuan and Guo, Xunhua and Chen, Guoqing and Wei, Qiang},
title = {Identifying Complements and Substitutes of Products: A Neural Network Framework Based on Product Embedding},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320277},
doi = {10.1145/3320277},
abstract = {Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {29},
keywords = {product embedding, substitutes, online reviews, product recommendation, product relationship, Complements}
}

@article{10.1145/3328795,
author = {Iqbal, Mohsin and Karim, Asim and Kamiran, Faisal},
title = {Balancing Prediction Errors for Robust Sentiment Classification},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3328795},
doi = {10.1145/3328795},
abstract = {Sentiment classification is a popular text mining task in which textual content (e.g., a message) is assigned a polarity label (typically positive or negative) reflecting the sentiment expressed in it. Sentiment classification is used widely in applications like customer feedback analysis where robustness and correctness of results are critical. In this article, we highlight that prediction accuracy alone is not sufficient for assessing the performance of a sentiment classifier; it is also important that the classifier is not biased toward positive or negative polarity, thus distorting the distribution of positive and negative messages in the predictions. We propose a measure, called Polarity Bias Rate, for quantifying this bias in a sentiment classifier. Second, we present two methods for removing this bias in the predictions of unsupervised and supervised sentiment classifiers. Our first method, called Bias-Aware Thresholding (BAT), shifts the decision boundary to control the bias in the predictions. Motivated from cost-sensitive learning, BAT is easily applicable to both lexicon-based unsupervised and supervised classifiers. Our second method, called Balanced Logistic Regression (BLR) introduces a bias-remover constraint into the standard logistic regression model. BLR is an automatic bias-free supervised sentiment classifier.We evaluate our methods extensively on seven real-world datasets. The experiments involve two lexicon-based and two supervised sentiment classifiers and include evaluation on multiple train-test data sizes. The results show that bias is controlled effectively in predictions. Furthermore, prediction accuracy is also increased in many cases, thus enhancing the robustness of sentiment classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {33},
numpages = {21},
keywords = {Sentiment analysis, fairness in learning, lexicon-based methods, bias-aware sentiment analysis, supervised methods}
}

@article{10.1145/3321517,
author = {Ju, Fujiao and Sun, Yanfeng and Gao, Junbin and Antolovich, Michael and Dong, Junliang and Yin, Baocai},
title = {Tensorizing Restricted Boltzmann Machine},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3321517},
doi = {10.1145/3321517},
abstract = {Restricted Boltzmann machine (RBM) is a famous model for feature extraction and can be used as an initializer for neural networks. When applying the classic RBM to multidimensional data such as 2D/3D tensors, one needs to vectorize such as high-order data. Vectorizing will result in dimensional disaster and valuable spatial information loss. As RBM is a model with fully connected layers, it requires a large amount of memory. Therefore, it is difficult to use RBM with high-order data on low-end devices. In this article, to utilize classic RBM on tensorial data directly, we propose a new tensorial RBM model parameterized by the tensor train format (TTRBM). In this model, both visible and hidden variables are in tensorial form, which are connected by a parameter matrix in tensor train format. The biggest advantage of the proposed model is that TTRBM can obtain comparable performance compared with the classic RBM with much fewer model parameters and faster training process. To demonstrate the advantages of TTRBM, we conduct three real-world applications, face reconstruction, handwritten digit recognition, and image super-resolution in the experiments.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {30},
numpages = {16},
keywords = {Tensor, tensor train format, restricted Boltzmann machine}
}

@article{10.1145/3314105,
author = {Zhang, Xuchao and Lei, Shuo and Zhao, Liang and Boedihardjo, Arnold P. and Lu, Chang-Tien},
title = {Robust Regression via Heuristic Corruption Thresholding and Its Adaptive Estimation Variation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314105},
doi = {10.1145/3314105},
abstract = {The presence of data noise and corruptions has recently invoked increasing attention on robust least-squares regression (RLSR), which addresses this fundamental problem that learns reliable regression coefficients when response variables can be arbitrarily corrupted. Until now, the following important challenges could not be handled concurrently: (1) rigorous recovery guarantee of regression coefficients, (2) difficulty in estimating the corruption ratio parameter, and (3) scaling to massive datasets. This article proposes a novel Robust regression algorithm via Heuristic Corruption Thresholding (RHCT) that concurrently addresses all the above challenges. Specifically, the algorithm alternately optimizes the regression coefficients and estimates the optimal uncorrupted set via heuristic thresholding without a pre-defined corruption ratio parameter until its convergence. Moreover, to improve the efficiency of corruption estimation in large-scale data, a Robust regression algorithm via Adaptive Corruption Thresholding (RACT) is proposed to determine the size of the uncorrupted set in a novel adaptive search method without iterating data samples exhaustively. In addition, we prove that our algorithms benefit from strong guarantees analogous to those of state-of-the-art methods in terms of convergence rates and recovery guarantees. Extensive experiments demonstrate that the effectiveness of our new methods is superior to that of existing methods in the recovery of both regression coefficients and uncorrupted sets, with very competitive efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {28},
numpages = {22},
keywords = {adaptive search, adversarial data corruption, discrete optimization, hard thresholding, Robust regression}
}

@article{10.1145/3314107,
author = {Gan, Wensheng and Lin, Jerry Chun-Wei and Fournier-Viger, Philippe and Chao, Han-Chieh and Yu, Philip S.},
title = {A Survey of Parallel Sequential Pattern Mining},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314107},
doi = {10.1145/3314107},
abstract = {With the growing popularity of shared resources, large volumes of complex data of different types are collected automatically. Traditional data mining algorithms generally have problems and challenges including huge memory cost, low processing speed, and inadequate hard disk space. As a fundamental task of data mining, sequential pattern mining (SPM) is used in a wide variety of real-life applications. However, it is more complex and challenging than other pattern mining tasks, i.e., frequent itemset mining and association rule mining, and also suffers from the above challenges when handling the large-scale data. To solve these problems, mining sequential patterns in a parallel or distributed computing environment has emerged as an important issue with many applications. In this article, an in-depth survey of the current status of parallel SPM (PSPM) is investigated and provided, including detailed categorization of traditional serial SPM approaches, and state-of-the art PSPM. We review the related work of PSPM in details including partition-based algorithms for PSPM, apriori-based PSPM, pattern-growth-based PSPM, and hybrid algorithms for PSPM, and provide deep description (i.e., characteristics, advantages, disadvantages, and summarization) of these parallel approaches of PSPM. Some advanced topics for PSPM, including parallel quantitative/weighted/utility SPM, PSPM from uncertain data and stream data, hardware acceleration for PSPM, are further reviewed in details. Besides, we review and provide some well-known open-source software of PSPM. Finally, we summarize some challenges and opportunities of PSPM in the big data era.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {25},
numpages = {34},
keywords = {data mining, Data science, parallelism, sequential pattern, big data}
}

@article{10.1145/3314202,
author = {Wang, Zheng and Ye, Xiaojun and Wang, Chaokun and Yu, Philip S.},
title = {Feature Selection via Transferring Knowledge Across Different Classes},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314202},
doi = {10.1145/3314202},
abstract = {The problem of feature selection has attracted considerable research interest in recent years. Supervised information is capable of significantly improving the quality of selected features. However, existing supervised feature selection methods all require that classes in the labeled data (source domain) and unlabeled data (target domain) to be identical, which may be too restrictive in many cases. In this article, we consider a more challenging cross-class setting where the classes in these two domains are related but different, which has rarely been studied before. We propose a cross-class knowledge transfer feature selection framework which transfers the cross-class knowledge from the source domain to guide target domain feature selection. Specifically, high-level descriptions, i.e., attributes, are used as the bridge for knowledge transfer. To further improve the quality of the selected features, our framework jointly considers the tasks of cross-class knowledge transfer and feature selection. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {22},
numpages = {29},
keywords = {dimension reduction, Feature selection, supervision transfer}
}

@article{10.1145/3310227,
author = {Zhou, Yao and Ying, Lei and He, Jingrui},
title = {Multi-Task Crowdsourcing via an Optimization Framework},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3310227},
doi = {10.1145/3310227},
abstract = {The unprecedented amounts of data have catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. One crucial challenge in crowdsourcing is the diverse worker quality, which determines the accuracy of the label information provided by such workers. Motivated by the observations that same set of tasks are typically labeled by the same set of workers, we studied their behaviors across multiple related tasks and proposed an optimization framework for learning from task and worker dual heterogeneity. The proposed method uses a weight tensor to represent the workers’ behaviors across multiple tasks, and seeks to find the optimal solution of the tensor by exploiting its structured information. Then, we propose an iterative algorithm to solve the optimization problem and analyze its computational complexity. To infer the true label of an example, we construct a worker ensemble based on the estimated tensor, whose decisions will be weighted using a set of entropy weight. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed. Moreover, we extend the learning framework to accommodate to the multi-class setting. Finally, we test the performance of our framework on several datasets, and demonstrate its superiority over state-of-the-art techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {27},
numpages = {26},
keywords = {entropy ensemble, tensor representation, crowdsourcing, optimization, Multi-task learning}
}

@article{10.1145/3320482,
author = {Liu, Chenyang and Cao, Jian and Feng, Shanshan},
title = {Leveraging Kernel-Incorporated Matrix Factorization for App Recommendation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320482},
doi = {10.1145/3320482},
abstract = {The ever-increasing number of smartphone applications (apps) available on different app markets poses a challenge for personalized app recommendation. Conventional collaborative filtering-based recommendation methods suffer from sparse and binary user-app implicit feedback, which results in poor performance in discriminating user-app preferences. In this article, we first propose two kernel incorporated probabilistic matrix factorization models, which introduce app-categorical information to constrain the user and app latent features to be similar to their neighbors in the latent space. The two models are solved by Stochastic Gradient Descent with a user-oriented negative sampling scheme. To further improve the recommendation performance, we construct pseudo user-app ratings based on user-app usage information, and propose a novel kernelized non-negative matrix factorization by incorporating non-negative constraints on latent factors to predict user-app preferences. This model also leverages user--user and app--app similarities with regard to app-categorical information to mine the latent geometric structure in the pseudo-rating space. Adopting the Karush--Kuhn--Tucker conditions, a Multiplicative Updating Rules based optimization is proposed for model learning, and the convergence is proved by introducing an auxiliary function. The experimental results on a real user-app installation usage dataset show the comparable performance of our models with the state-of-the-art baselines in terms of two ranking-oriented evaluation metrics.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {31},
numpages = {27},
keywords = {App recommendation, kernel function, non-negative matrix factorization, probabilistic matrix factorization}
}

@article{10.1145/3320489,
author = {Dehghan, Mahdi and Abin, Ahmad Ali},
title = {Translations Diversification for Expert Finding: A Novel Clustering-Based Approach},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320489},
doi = {10.1145/3320489},
abstract = {Expert finding is the task of retrieving and ranking knowledgeable people in the subject of user’s query. It is a well-studied problem that has attracted the attention of many researchers. The most important challenge in expert finding is to determine the similarity between query words and documents authored by candidate experts. One of the most important challenges in Information Retrieval (IR) community is the issue of vocabulary gap between queries and documents. In this study, a translation model based on words clustering in two query and co-occurrence spaces is proposed to overcome this problem. First, the words that are semantically close, are clustered in a query space and then each cluster in this space are clustered again in a co-occurrence space. Representatives of each cluster in the co-occurrence space are considered as a diverse subset of the parent cluster. By this method, the query translations are expected to be diversified in the query space. Next, a probabilistic model, that is based on the belonging degree of word to cluster and similarity of cluster to query in the query space, is used to consider the problem of vocabulary gap. Finally, the corresponding translations to each query are used in conjunction with a combination model for expert finding. Experiments on Stack Overflow dataset show the effectiveness of the proposed method for expert finding.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {32},
numpages = {20},
keywords = {translation diversification, Expert finding, data clustering, translation model, Stack Overflow}
}

@article{10.1145/3314204,
author = {Mahmoudi, Amin and Yaakub, Mohd Ridzwan and Bakar, Azuraliza Abu},
title = {The Relationship between Online Social Network Ties and User Attributes},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314204},
doi = {10.1145/3314204},
abstract = {The distance between users has an effect on the formation of social network ties, but it is not the only or even the main factor. Knowing all the features that influence such ties is very important for many related domains such as location-based recommender systems and community and event detection systems for online social networks (OSNs). In recent years, researchers have analyzed the role of user geo-location in OSNs. Researchers have also attempted to determine the probability of friendships being established based on distance, where friendship is not only a function of distance. However, some important features of OSNs remain unknown. In order to comprehensively understand the OSN phenomenon, we also need to analyze users’ attributes. Basically, an OSN functions according to four main user properties: user geo-location, user weight, number of user interactions, and user lifespan. The research presented here sought to determine whether the user mobility pattern can be used to predict users’ interaction behavior. It also investigated whether, in addition to distance, the number of friends (known as user weight) interferes in social network tie formation. To this end, we analyzed the above-stated features in three large-scale OSNs. We found that regardless of a high degree freedom in user mobility, the fraction of the number of outside activities over the inside activity is a significant fraction that helps us to address the user interaction behavior. To the best of our knowledge, research has not been conducted elsewhere on this issue. We also present a high-resolution formula in order to improve the friendship probability function.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {26},
numpages = {15},
keywords = {tie formation, Distance, online social network, IO-fraction, user weight}
}

@article{10.1145/3314106,
author = {Wang, Zhitao and Chen, Chengyao and Li, Wenjie},
title = {Information Diffusion Prediction with Network Regularized Role-Based User Representation Learning},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314106},
doi = {10.1145/3314106},
abstract = {In this article, we aim at developing a user representation learning model to solve the information diffusion prediction problem in social media. The main idea is to project the diffusion users into a continuous latent space as the role-based (sender and receiver) representations, which capture unique diffusion characteristics of users. The model learns the role-based representations based on a cascade modeling objective that aims at maximizing the likelihood of observed cascades, and employs the matrix factorization objective of reconstructing structural proximities as a regularization on representations. By jointly embedding the information of cascades and network, the learned representations are robust on different diffusion data. We evaluate the proposed model on three real-world datasets. The experimental results demonstrate the better performance of the proposed model than state-of-the-art diffusion embedding and network embedding models and other popular graph-based methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {29},
numpages = {23},
keywords = {diffusion role, network regularization, representation learning, Information diffusion}
}

@article{10.1145/3314203,
author = {Hong, Junyuan and Li, Yang and Chen, Huanhuan},
title = {Variant Grassmann Manifolds: A Representation Augmentation Method for Action Recognition},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314203},
doi = {10.1145/3314203},
abstract = {In classification tasks, classifiers trained with finite examples might generalize poorly to new data with unknown variance. For this issue, data augmentation is a successful solution where numerous artificial examples are added to training sets. In this article, we focus on the data augmentation for improving the accuracy of action recognition, where action videos are modeled by linear dynamical systems and approximately represented as linear subspaces. These subspace representations lie in a non-Euclidean space, named Grassmann manifold, containing points as orthonormal matrixes. It is our concern that poor generalization may result from the variance of manifolds when data come from different sources or classes. Thus, we introduce infinitely many variant Grassmann manifolds (VGM) subject to a known distribution, then represent each action video as different Grassmann points leading to augmented representations. Furthermore, a prior based on the stability of subspace bases is introduced, so the manifold distribution can be adaptively determined, balancing discrimination and representation. Experimental results of multi-class and multi-source classification show that VGM softmax classifiers achieve lower test error rates compared to methods with a single manifold.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {23},
numpages = {23},
keywords = {data augmentation, action recognition, Representation learning, Grassmann manifold}
}

@article{10.1145/3319911,
author = {Guo, Yumeng and Chung, Fulai and Li, Guozheng and Wang, Jiancong and Gee, James C.},
title = {Leveraging Label-Specific Discriminant Mapping Features for Multi-Label Learning},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3319911},
doi = {10.1145/3319911},
abstract = {As an important machine learning task, multi-label learning deals with the problem where each sample instance (feature vector) is associated with multiple labels simultaneously. Most existing approaches focus on manipulating the label space, such as exploiting correlations between labels and reducing label space dimension, with identical feature space in the process of classification. One potential drawback of this traditional strategy is that each label might have its own specific characteristics and using identical features for all label cannot lead to optimized performance. In this article, we propose an effective algorithm named LSDM, i.e., leveraging label-specific discriminant mapping features for multi-label learning, to overcome the drawback. LSDM sets diverse ratio parameter values to conduct cluster analysis on the positive and negative instances of identical label. It reconstructs label-specific feature space which includes distance information and spatial topology information. Our experimental results show that combining these two parts of information in the new feature representation can better exploit the clustering results in the learning process. Due to the problem of diverse combinations for identical label, we employ simplified linear discriminant analysis to efficiently excavate optimal one for each label and perform classification by querying the corresponding results. Comparison with the state-of-the-art algorithms on a total of 20 benchmark datasets clearly manifests the competitiveness of LSDM.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {24},
numpages = {23},
keywords = {multi-label learning, Machine learning, label specific features}
}

@article{10.1145/3309712,
author = {Cadena, Jose and Chen, Feng and Vullikanti, Anil},
title = {Near-Optimal and Practical Algorithms for Graph Scan Statistics with Connectivity Constraints},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3309712},
doi = {10.1145/3309712},
abstract = {One fundamental task in network analysis is detecting “hotspots” or “anomalies” in the network; that is, detecting subgraphs where there is significantly more activity than one would expect given historical data or some baseline process. Scan statistics is one popular approach used for anomalous subgraph detection. This methodology involves maximizing a score function over all connected subgraphs, which is a challenging computational problem. A number of heuristics have been proposed for these problems, but they do not provide any quality guarantees. Here, we propose a framework for designing algorithms for optimizing a large class of scan statistics for networks, subject to connectivity constraints. Our algorithms run in time that scales linearly on the size of the graph and depends on a parameter we call the “effective solution size,” while providing rigorous approximation guarantees. In contrast, most prior methods have super-linear running times in terms of graph size. Extensive empirical evidence demonstrates the effectiveness and efficiency of our proposed algorithms in comparison with state-of-the-art methods. Our approach improves on the performance relative to all prior methods, giving up to over 25% increase in the score. Further, our algorithms scale to networks with up to a million nodes, which is 1--2 orders of magnitude larger than all prior applications.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {20},
numpages = {33},
keywords = {parameterized complexity, anomalous subgraph detection, Scan statistics, graph anomaly detection}
}

@article{10.1145/3309541,
author = {Jiang, Bingbing and Li, Chang and Rijke, Maarten De and Yao, Xin and Chen, Huanhuan},
title = {Probabilistic Feature Selection and Classification Vector Machine},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3309541},
doi = {10.1145/3309541},
abstract = {Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency due to the incapability of eliminating irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection algorithm that adopts truncated Gaussian distributions as both sample and feature priors. The proposed algorithm, called probabilistic feature selection and classification vector machine (PFCVMLP) is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP. By tightening the bound, the importance of feature selection is demonstrated.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {21},
numpages = {27},
keywords = {EEG emotion recognition, Feature selection, probabilistic classification model, sparse Bayesian learning, supervised learning}
}

@article{10.1145/3301304,
author = {Katib, Anas and Rao, Praveen and Barnard, Kobus and Kamhoua, Charles},
title = {Fast Approximate Score Computation on Large-Scale Distributed Data for Learning Multinomial Bayesian Networks},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3301304},
doi = {10.1145/3301304},
abstract = {In this article, we focus on the problem of learning a Bayesian network over distributed data stored in a commodity cluster. Specifically, we address the challenge of computing the scoring function over distributed data in an efficient and scalable manner, which is a fundamental task during learning. While exact score computation can be done using the MapReduce-style computation, our goal is to compute approximate scores much faster with probabilistic error bounds and in a scalable manner. We propose a novel approach, which is designed to achieve the following: (a) decentralized score computation using the principle of gossiping; (b) lower resource consumption via a probabilistic approach for maintaining scores using the properties of a Markov chain; and (c) effective distribution of tasks during score computation (on large datasets) by synergistically combining well-known hashing techniques. We conduct theoretical analysis of our approach in terms of convergence speed of the statistics required for score computation, and memory and network bandwidth consumption. We also discuss how our approach is capable of efficiently recomputing scores when new data are available. We conducted a comprehensive evaluation of our approach and compared with the MapReduce-style computation using datasets of different characteristics on a 16-node cluster. When the MapReduce-style computation provided exact statistics for score computation, it was nearly 10 times slower than our approach. Although it ran faster on randomly sampled datasets than on the entire datasets, it performed worse than our approach in terms of accuracy. Our approach achieved high accuracy (below 6% average relative error) in estimating the statistics for approximate score computation on all the tested datasets. In conclusion, it provides a feasible tradeoff between computation time and accuracy for fast approximate score computation on large-scale distributed data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {14},
numpages = {40},
keywords = {gossip algorithms, bayesian networks, structure learning, distributed data, Approximate score computation}
}

@article{10.1145/3300230,
author = {Ma, Liang and Srivatsa, Mudhakar and Cansever, Derya and Yan, Xifeng and Kase, Sue and Vanni, Michelle},
title = {Performance Bounds of Decentralized Search in Expert Networks for Query Answering},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3300230},
doi = {10.1145/3300230},
abstract = {Expert networks are formed by a group of expert-professionals with different specialties to collaboratively resolve specific queries posted to the network. In such networks, when a query reaches an expert who does not have sufficient expertise, this query needs to be routed to other experts for further processing until it is completely solved; therefore, query answering efficiency is sensitive to the underlying query routing mechanism being used. Among all possible query routing mechanisms, decentralized search, operating purely on each expert’s local information without any knowledge of network global structure, represents the most basic and scalable routing mechanism, which is applicable to any network scenarios even in dynamic networks. However, there is still a lack of fundamental understanding of the efficiency of decentralized search in expert networks. In this regard, we investigate decentralized search by quantifying its performance under a variety of network settings. Our key findings reveal the existence of network conditions, under which decentralized search can achieve significantly short query routing paths (i.e., between O(log n) and O(log2 n) hops, n: total number of experts in the network). Based on such theoretical foundation, we further study how the unique properties of decentralized search in expert networks are related to the anecdotal small-world phenomenon. In addition, we demonstrate that decentralized search is robust against estimation errors introduced by misinterpreting the required expertise levels. The developed performance bounds, confirmed by real datasets, are able to assist in predicting network performance and designing complex expert networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {18},
numpages = {23},
keywords = {Expert networks, decentralized search, query answering, theory, performance bounds}
}

@article{10.1145/3310254,
author = {Jha, Kishlay and Xun, Guangxu and Gopalakrishnan, Vishrawas and Zhang, Aidong},
title = {DWE-Med: Dynamic Word Embeddings for Medical Domain},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3310254},
doi = {10.1145/3310254},
abstract = {Recent advances in unsupervised language processing methods have created an opportunity to exploit massive text corpora for developing high-quality vector space representation (also known as word embeddings) of words. Towards this direction, practitioners have developed and applied several data driven embedding models with quite good rate of success. However, a drawback of these models lies in their premise of static context; wherein, the meaning of a word is assumed to remain the same over the period of time. This is limiting because it is known that the semantic meaning of a concept evolves over time. While such semantic drifts are routinely observed in almost all the domains; their effect is acute in domain such as biomedicine, where the semantic meaning of a concept changes relatively fast. To address this, in this study, we aim to learn temporally aware vector representation of medical concepts from the timestamped text data, and in doing so provide a systematic approach to formalize the problem. More specifically, a dynamic word embedding based model that jointly learns the temporal characteristics of medical concepts and performs across time-alignment is proposed. Apart from capturing the evolutionary characteristics in an optimal manner, the model also factors in the implicit medical properties useful for a variety of bio-medical applications. Empirical studies conducted on two important bio-medical use cases validates the effectiveness of the proposed approach and suggests that the model not only learns quality embeddings but also facilitates intuitive trajectory visualizations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {19},
numpages = {21},
keywords = {word embeddings, temporal dynamics, Biomedical domain}
}

@article{10.1145/3301303,
author = {Gao, Xiaofeng and Cao, Zhenhao and Li, Sha and Yao, Bin and Chen, Guihai and Tang, Shaojie},
title = {Taxonomy and Evaluation for Microblog Popularity Prediction},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3301303},
doi = {10.1145/3301303},
abstract = {As social networks become a major source of information, predicting the outcome of information diffusion has appeared intriguing to both researchers and practitioners. By organizing and categorizing the joint efforts of numerous studies on popularity prediction, this article presents a hierarchical taxonomy and helps to establish a systematic overview of popularity prediction methods for microblog. Specifically, we uncover three lines of thoughts: the feature-based approach, time-series modelling, and the collaborative filtering approach and analyse them, respectively. Furthermore, we also categorize prediction methods based on their underlying rationale: whether they attempt to model the motivation of users or monitor the early responses. Finally, we put these prediction methods to test by performing experiments on real-life data collected from popular social networks Twitter and Weibo. We compare the methods in terms of accuracy, efficiency, timeliness, robustness, and bias.As far as we are concerned, there is no precedented survey aimed at microblog popularity prediction at the time of submission. By establishing a taxonomy and evaluation for the first time, we hope to provide an in-depth review of state-of-the-art prediction methods and point out directions for further research. Our evaluations show that time-series modelling has the advantage of high accuracy and the ability to improve over time. The feature-based methods using only temporal features performs nearly as well as using all possible features, producing average results. This suggests that temporal features do have strong predictive power and that power is better exploited with time-series models. On the other hand, this implies that we know little about the future popularity of an item before it is posted, which may be the focus of further research.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {15},
numpages = {40},
keywords = {evaluation, Social network, popularity prediction, taxonomy}
}

@article{10.1145/3301300,
author = {Teinemaa, Irene and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria},
title = {Outcome-Oriented Predictive Process Monitoring: Review and Benchmark},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3301300},
doi = {10.1145/3301300},
abstract = {Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible categorical outcomes—e.g., Will the customer complain or not? Will an order be delivered, canceled, or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures, and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering 24&nbsp;predictive process monitoring tasks based on nine real-life event logs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {17},
numpages = {57},
keywords = {Business process, predictive monitoring, sequence classification}
}

@article{10.1145/3301302,
author = {Yan, Ruidong and Li, Yi and Wu, Weili and Li, Deying and Wang, Yongcai},
title = {Rumor Blocking through Online Link Deletion on Social Networks},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3301302},
doi = {10.1145/3301302},
abstract = {In recent years, social networks have become important platforms for people to disseminate information. However, we need to take effective measures such as blocking a set of links to control the negative rumors spreading over the network. In this article, we propose a Rumor Spread Minimization (RSM) problem, i.e., we remove an edge set from network such that the rumor spread is minimized. We first prove the objective function of RSM problem is not submodular. Then, we propose both submodular lower-bound and upper-bound of the objective function. Next, we develop a heuristic algorithm to approximate the objective function. Furthermore, we reformulate our objective function as the DS function (the Difference of Submodular functions). Finally, we conduct experiments on real-world datasets to evaluate our proposed method. The experiment results show that the upper and lower bounds are very close, which indicates the good quality of them. And, the proposed method outperforms the comparison methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {16},
numpages = {26},
keywords = {rumor blocking, Social network, non-submodularity, approximation algorithm}
}

@article{10.1145/3299876,
author = {Barton, Tomas and Bruna, Tomas and Kordik, Pavel},
title = {Chameleon 2: An Improved Graph-Based Clustering Algorithm},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299876},
doi = {10.1145/3299876},
abstract = {Traditional clustering algorithms fail to produce human-like results when confronted with data of variable density, complex distributions, or in the presence of noise. We propose an improved graph-based clustering algorithm called Chameleon 2, which overcomes several drawbacks of state-of-the-art clustering approaches. We modified the internal cluster quality measure and added an extra step to ensure algorithm robustness. Our results reveal a significant positive impact on the clustering quality measured by Normalized Mutual Information on 32 artificial datasets used in the clustering literature. This significant improvement is also confirmed on real-world datasets.The performance of clustering algorithms such as DBSCAN is extremely parameter sensitive, and exhaustive manual parameter tuning is necessary to obtain a meaningful result. All hierarchical clustering methods are very sensitive to cutoff selection, and a human expert is often required to find the true cutoff for each clustering result. We present an automated cutoff selection method that enables the Chameleon 2 algorithm to generate high-quality clustering in autonomous mode.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {10},
numpages = {27},
keywords = {Cluster analysis, clustering, graph clustering, pattern recognition}
}

@article{10.1145/3298786,
author = {Leeuwen, Matthijs van and Chau, Polo and Vreeken, Jilles and Shahaf, Dafna and Faloutsos, Christos},
title = {Addendum to the Special Issue on Interactive Data Exploration and Analytics (TKDD, Vol. 12, Iss. 1): Introduction by the Guest Editors},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3298786},
doi = {10.1145/3298786},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {13},
numpages = {2}
}

@article{10.1145/3299886,
author = {Wang, Huan and Wu, Jia and Hu, Wenbin and Wu, Xindong},
title = {Detecting and Assessing Anomalous Evolutionary Behaviors of Nodes in Evolving Social Networks},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299886},
doi = {10.1145/3299886},
abstract = {Based on the performance of entire social networks, anomaly analysis for evolving social networks generally ignores the otherness of the evolutionary behaviors of different nodes, such that it is difficult to precisely identify the anomalous evolutionary behaviors of nodes (AEBN). Assuming that a node's evolutionary behavior that generates and removes edges normally follows stable evolutionary mechanisms, this study focuses on detecting and assessing AEBN, whose evolutionary mechanisms deviate from their past mechanisms, and proposes a link prediction detection (LPD) method and a matrix perturbation assessment (MPA) method. LPD describes a node's evolutionary behavior by fitting its evolutionary mechanism, and designs indexes for edge generation and removal to evaluate the extent to which the evolutionary mechanism of a node's evolutionary behavior can be fitted by a link prediction algorithm. Furthermore, it detects AEBN by quantifying the differences among behavior vectors that characterize the node's evolutionary behaviors in different periods. In addition, MPA considers AEBN as a perturbation of the social network structure, and quantifies the effect of AEBN on the social network structure based on matrix perturbation analysis. Extensive experiments on eight disparate real-world networks demonstrate that analyzing AEBN from the perspective of evolutionary mechanisms is important and beneficial.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {12},
numpages = {24},
keywords = {detection, Anomalous evolutionary behaviors of nodes, perturbation analysis, assessment, social network}
}

@article{10.1145/3299877,
author = {Murai, Fabricio and Ribeiro, Bruno and Towlsey, Don and Wang, Pinghui},
title = {Characterizing Directed and Undirected Networks via Multidimensional Walks with Jumps},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299877},
doi = {10.1145/3299877},
abstract = {Estimating distributions of node characteristics (labels) such as number of connections or citizenship of users in a social network via edge and node sampling is a vital part of the study of complex networks. Due to its low cost, sampling via a random walk (RW) has been proposed as an attractive solution to this task. Most RW methods assume either that the network is undirected or that walkers can traverse edges regardless of their direction. Some RW methods have been designed for directed networks where edges coming into a node are not directly observable. In this work, we propose Directed Unbiased Frontier Sampling (DUFS), a sampling method based on a large number of coordinated walkers, each starting from a node chosen uniformly at random. It applies to directed networks with invisible incoming edges because it constructs, in real time, an undirected graph consistent with the walkers trajectories, and its use of random jumps to prevent walkers from being trapped. DUFS generalizes previous RW methods and is suited for undirected networks and to directed networks regardless of in-edge visibility. We also propose an improved estimator of node label distribution that combines information from initial walker locations with subsequent RW observations. We evaluate DUFS, compare it to other RW methods, investigate the impact of its parameters on estimation accuracy and provide practical guidelines for choosing them. In estimating out-degree distributions, DUFS yields significantly better estimates of the head of the distribution than other methods, while matching or exceeding estimation accuracy of the tail. Last, we show that DUFS outperforms uniform sampling when estimating distributions of node labels of the top 10% largest degree nodes, even when sampling a node uniformly has the same cost as RW steps.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {11},
numpages = {33},
keywords = {graph sampling, Complex networks, random walks, directed networks}
}

@article{10.1145/3299875,
author = {Sahoo, Doyen and Hoi, Steven C. H. and Li, Bin},
title = {Large Scale Online Multiple Kernel Regression with Application to Time-Series Prediction},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299875},
doi = {10.1145/3299875},
abstract = {Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks as follows: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume that a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these drawbacks, this work presents a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fashion, and dynamically explore a pool of multiple diverse kernels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selection. The OMKR problem is more challenging than regular kernel-based regression tasks since we have to on-the-fly determine both the optimal kernel-based regressor for each individual kernel and the best combination of the multiple kernel regressors. We propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks including application to AR, ARMA, and ARIMA time series. We develop novel approaches to make OMKR scalable for large datasets, to counter the problems arising from an unbounded number of support vectors. We also explore the effect of kernel combination at prediction level and at the representation level. Finally, we conduct extensive experiments to evaluate the empirical performance on both real-world regression and times series prediction tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {9},
numpages = {33},
keywords = {Online learning, multiple kernel regression, large-scale kernel learning, time-series prediction}
}

@article{10.1145/3278607,
author = {Song, Qingquan and Ge, Hancheng and Caverlee, James and Hu, Xia},
title = {Tensor Completion Algorithms in Big Data Analytics},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3278607},
doi = {10.1145/3278607},
abstract = {Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {6},
numpages = {48},
keywords = {Tensor, tensor completion, dynamic data analysis, multilinear data analysis, big data analytics, tensor factorization, tensor decomposition}
}

@article{10.1145/3285954,
author = {Chen, Hung-Hsuan and Chen, Pu},
title = {Differentiating Regularization Weights -- A Simple Mechanism to Alleviate Cold Start in Recommender Systems},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3285954},
doi = {10.1145/3285954},
abstract = {Matrix factorization (MF) and its extended methodologies have been studied extensively in the community of recommender systems in the last decade. Essentially, MF attempts to search for low-ranked matrices that can (1) best approximate the known rating scores, and (2) maintain low Frobenius norm for the low-ranked matrices to prevent overfitting. Since the two objectives conflict with each other, the common practice is to assign the relative importance weights as the hyper-parameters to these objectives. The two low-ranked matrices returned by MF are often interpreted as the latent factors of a user and the latent factors of an item that would affect the rating of the user on the item. As a result, it is typical that, in the loss function, we assign a regularization weight λp on the norms of the latent factors for all users, and another regularization weight λq on the norms of the latent factors for all the items. We argue that such a methodology probably over-simplifies the scenario. Alternatively, we probably should assign lower constraints to the latent factors associated with the items or users that reveal more information, and set higher constraints to the others. In this article, we systematically study this topic. We found that such a simple technique can improve the prediction results of the MF-based approaches based on several public datasets. Specifically, we applied the proposed methodology on three baseline models -- SVD, SVD++, and the NMF models. We found that this technique improves the prediction accuracy for all these baseline models. Perhaps more importantly, this technique better predicts the ratings on the long-tail items, i.e., the items that were rated/viewed/purchased by few users. This suggests that this approach may partially remedy the cold-start issue. The proposed method is very general and can be easily applied on various recommendation models, such as Factorization Machines, Field-aware Factorization Machines, Factorizing Personalized Markov Chains, Prod2Vec, Behavior2Vec, and so on. We release the code for reproducibility. We implemented a Python package that integrates the proposed regularization technique with the SVD, SVD++, and the NMF model. The package can be accessed at https://github.com/ncu-dart/rdf.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {8},
numpages = {22},
keywords = {SVD, collaborative filtering, cold start, matrix factorization, SVD++, long tail, Recommender systems}
}

@article{10.1145/3230666,
author = {Siddiqui, Md Amran and Fern, Alan and Dietterich, Thomas G. and Wong, Weng-Keen},
title = {Sequential Feature Explanations for Anomaly Detection},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3230666},
doi = {10.1145/3230666},
abstract = {In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation’s quality is related to the number of features that must be revealed to attain confidence. In this article, we first formulate the problem of optimizing SFEs for a particular density-based anomaly detector. We then present both greedy algorithms and an optimal algorithm, based on branch-and-bound search, for optimizing SFEs. Finally, we provide a large scale quantitative evaluation of these algorithms using a novel framework for evaluating explanations. The results show that our algorithms are quite effective and that our best greedy algorithm is competitive with optimal solutions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {1},
numpages = {22},
keywords = {anomaly explanation, explanation evaluation, anomaly interpretation, Anomaly detection}
}

@article{10.1145/3278606,
author = {Tao, Zhiqiang and Liu, Hongfu and Li, Sheng and Ding, Zhengming and Fu, Yun},
title = {Robust Spectral Ensemble Clustering via Rank Minimization},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3278606},
doi = {10.1145/3278606},
abstract = {Ensemble Clustering (EC) is an important topic for data cluster analysis. It targets to integrate multiple Basic Partitions (BPs) of a particular dataset into a consensus partition. Among previous works, one promising and effective way is to transform EC as a graph partitioning problem on the co-association matrix, which is a pair-wise similarity matrix summarized by all the BPs in essence. However, most existing EC methods directly utilize the co-association matrix, yet without considering various noises (e.g., the disagreement between different BPs and the outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix, and thus mislead the final graph partitioning process. To address this challenge, we propose a novel Robust Spectral Ensemble Clustering (RSEC) algorithm in this article. Specifically, we learn low-rank representation (LRR) for the co-association matrix to uncover its cluster structure and handle the noises, and meanwhile, we perform spectral clustering with the learned representation to seek for a consensus partition. These two steps are jointly proceeded within a unified optimization framework. In particular, during the optimizing process, we leverage consensus partition to iteratively enhance the block-diagonal structure of LRR, in order to assist the graph partitioning. To solve RSEC, we first formulate it by using nuclear norm as a convex proxy to the rank function. Then, motivated by the recent advances in non-convex rank minimization, we further develop a non-convex model for RSEC and provide it a solution by the majorization--minimization Augmented Lagrange Multiplier algorithm. Experiments on 18 real-world datasets demonstrate the effectiveness of our algorithm compared with state-of-the-art methods. Moreover, several impact factors on the clustering performance of our approach are also explored extensively.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {4},
numpages = {25},
keywords = {co-association matrix, non-convex relaxation, Ensemble clustering, low-rank representation, spectral clustering}
}

@article{10.1145/3281631,
author = {Moghaz, Dror and Hacohen-Kerner, Yaakov and Gabbay, Dov},
title = {Text Mining for Evaluating Authors' Birth and Death Years},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3281631},
doi = {10.1145/3281631},
abstract = {This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in&nbsp;an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {7},
numpages = {24},
keywords = {undated documents, knowledge discovery, time analysis, key-phrases, text and data mining, Hebrew-Aramaic documents, Temporal-data}
}

@article{10.1145/3274670,
author = {Lagr\'{e}e, Paul and Capp\'{e}, Olivier and Cautis, Bogdan and Maniu, Silviu},
title = {Algorithms for Online Influencer Marketing},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3274670},
doi = {10.1145/3274670},
abstract = {Influence maximization is the problem of finding influential users, or nodes, in a graph so as to maximize the spread of information. It has many applications in advertising and marketing on social networks. In this article, we study a highly generic version of influence maximization, one of optimizing influence campaigns by sequentially selecting “spread seeds” from a set of influencers, a small subset of the node population, under the hypothesis that, in a given campaign, previously activated nodes remain persistently active. This problem is in particular relevant for an important form of online marketing, known as influencer marketing, in which the marketers target a sub-population of influential people, instead of the entire base of potential buyers. Importantly, we make no assumptions on the underlying diffusion model, and we work in a setting where neither a diffusion network nor historical activation data are available. We call this problem online influencer marketing with persistence (in short, OIMP). We first discuss motivating scenarios and present our general approach. We introduce an estimator on the influencers’ remaining potential – the expected number of nodes that can still be reached from a given influencer – and justify its strength to rapidly estimate the desired value, relying on real data gathered from Twitter. We then describe a novel algorithm, GT-UCB, relying on probabilistic upper confidence bounds on the remaining potential. We show that our approach leads to high-quality spreads on both simulated and real datasets. Importantly, it is orders of magnitude faster than state-of-the-art influence maximization methods, making it possible to deal with large-scale online scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {3},
numpages = {30},
keywords = {online learning, information diffusion, influence maximization, Influencer marketing, multi-armed bandits, online social networks}
}

@article{10.1145/3281632,
author = {Jaysawal, Bijay Prasad and Huang, Jen-Wei},
title = {PSP-AMS: Progressive Mining of Sequential Patterns Across Multiple Streams},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3281632},
doi = {10.1145/3281632},
abstract = {Sequential pattern mining is used to find frequent data sequences over time. When sequential patterns are generated, the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences. Progressive sequential pattern mining aims to find the most up-to-date sequential patterns given that obsolete items will be deleted from the sequences. When sequences come with multiple data streams, it is difficult to maintain and update the current sequential patterns. Even worse, when we consider the sequences across multiple streams, previous methods cannot efficiently compute the frequent sequential patterns. In this work, we propose an efficient algorithm PSP-AMS to address this problem. PSP-AMS uses a novel data structure PSP-MS-tree to insert new items, update current items, and delete obsolete items. By maintaining a PSP-MS-tree, PSP-AMS efficiently finds the frequent sequential patterns across multiple streams. The experimental results show that PSP-AMS significantly outperforms previous algorithms for mining of progressive sequential patterns across multiple streams on synthetic data as well as real data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {5},
numpages = {23},
keywords = {across-streams sequential patterns, multiple data streams, Progressive mining, sequential patterns, across data streams}
}

@article{10.1145/3267106,
author = {Liu, Xiaoming and Shen, Chao and Guan, Xiaohong and Zhou, Yadong},
title = {D<span class="smallcaps SmallerCapital">igger</span>: Detect Similar Groups in Heterogeneous Social Networks},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3267106},
doi = {10.1145/3267106},
abstract = {People participate in multiple online social networks, e.g., Facebook, Twitter, and Linkedin, and these social networks with heterogeneous social content and user relationship are named as heterogeneous social networks. Group structure widely exists in heterogeneous social networks, which reveals the evolution of human cooperation. Detecting similar groups in heterogeneous networks has a great significance for many applications, such as recommendation system and spammer detection, using the wealth of group information. Although promising, this novel problem encounters a variety of technical challenges, including incomplete data, high time complexity, and ground truth. To address the research gap and technical challenges, we take advantage of a ratio-cut optimization function to model this novel problem by the linear mixed-effects method and graph spectral theory. Based on this model, we propose an efficient algorithm called Digger to detect the similar groups in the large graphs. Digger consists of three steps, including measuring user similarity, construct a matching graph, and detecting similar groups. We adopt several strategies to lower the computational cost and detail the basis of labeling the ground truth. We evaluate the effectiveness and efficiency of our algorithm on five different types of online social networks. The extensive experiments show that our method achieves 0.693, 0.783, and 0.735 in precision, recall, and F1-measure, which significantly surpass the state-of-arts by 24.4%, 15.3%, and 20.7%, respectively. The results demonstrate that our proposal can detect similar groups in heterogeneous networks effectively.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {2},
numpages = {27},
keywords = {graph spectral, linear mixed-effects, Heterogeneous networks, detecting similar groups}
}

@article{10.1145/3184455,
author = {Wen, Xidao and Lin, Yu-Ru and Pelechrinis, Konstantinos},
title = {Event Analytics via Discriminant Tensor Factorization},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3184455},
doi = {10.1145/3184455},
abstract = {Analyzing the impact of disastrous events has been central to understanding and responding to crises. Traditionally, the assessment of disaster impact has primarily relied on the manual collection and analysis of surveys and questionnaires as well as the review of authority reports. This can be costly and time-consuming, whereas a timely assessment of an event’s impact is critical for crisis management and humanitarian operations. In this work, we formulate the impact discovery as the problem to identify the shared and discriminative subspace via tensor factorization due to the multi-dimensional nature of mobility data. Existing work in mining the shared and discriminative subspaces typically requires the predefined number of either type of them. In the context of event impact discovery, this could be impractical, especially for those unprecedented events. To overcome this, we propose a new framework, called “PairFac,” that jointly factorizes the multi-dimensional data to discover the latent mobility pattern along with its associated discriminative weight. This framework does not require splitting the shared and discriminative subspaces in advance and at the same time automatically captures the persistent and changing patterns from multi-dimensional behavioral data. Our work has important applications in crisis management and urban planning, which provides a timely assessment of impacts of major events in the urban environment.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {72},
numpages = {38},
keywords = {Urban computing, event analytics, data mining, tensor factorization}
}

@article{10.1145/3264745,
author = {Chen, Chaochao and Chang, Kevin Chen-Chuan and Li, Qibing and Zheng, Xiaolin},
title = {Semi-Supervised Learning Meets Factorization: Learning to Recommend with Chain Graph Model},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3264745},
doi = {10.1145/3264745},
abstract = {Recently, latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predict missing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which is mainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this article, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field. The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {73},
numpages = {24},
keywords = {latent factor model, chain graph model, data sparsity, Semi-supervised learning}
}

@article{10.1145/3237191,
author = {Park, Ha-Myung and Silvestri, Francesco and Pagh, Rasmus and Chung, Chin-Wan and Myaeng, Sung-Hyon and Kang, U},
title = {Enumerating Trillion Subgraphs On Distributed Systems},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3237191},
doi = {10.1145/3237191},
abstract = {How can we find patterns from an enormous graph with billions of vertices and edges? The subgraph enumeration, which is to find patterns from a graph, is an important task for graph data analysis with many applications, including analyzing the social network evolution, measuring the significance of motifs in biological networks, observing the dynamics of Internet, and so on. Especially, the triangle enumeration, a special case of the subgraph enumeration, where the pattern is a triangle, has many applications such as identifying suspicious users in social networks, detecting web spams, and finding communities. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time.In this article, we propose scalable methods for enumerating trillion subgraphs on distributed systems. We first propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. We also propose PSE (Pre-partitioned Subgraph Enumeration), a generalized version of PTE for enumerating subgraphs that match an arbitrary query graph. Experimental results show that PTE provides 79 times faster performance than recent distributed algorithms on real-world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges. Furthermore, PSE successfully enumerates 265 trillion clique subgraphs with 4 vertices from a subdomain hyperlink network, showing 47 times faster performance than the state of the art distributed subgraph enumeration algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {71},
numpages = {30},
keywords = {scalable algorithm, Triangle enumeration, graph algorithm, network analysis, distributed algorithm, big data, subgraph enumeration}
}

@article{10.1145/3241063,
author = {Huang, Xiao and Li, Jundong and Zou, Na and Hu, Xia},
title = {A General Embedding Framework for Heterogeneous Information Learning in Large-Scale Networks},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3241063},
doi = {10.1145/3241063},
abstract = {Network analysis has been widely applied in many real-world tasks, such as gene analysis and targeted marketing. To extract effective features for these analysis tasks, network embedding automatically learns a low-dimensional vector representation for each node, such that the meaningful topological proximity is well preserved. While the embedding algorithms on pure topological structure have attracted considerable attention, in practice, nodes are often abundantly accompanied with other types of meaningful information, such as node attributes, second-order proximity, and link directionality. A general framework for incorporating the heterogeneous information into network embedding could be potentially helpful in learning better vector representations. However, it remains a challenging task to jointly embed the geometrical structure and a distinct type of information due to the heterogeneity. In addition, the real-world networks often contain a large number of nodes, which put demands on the scalability of the embedding algorithms. To bridge the gap, in this article, we propose a general embedding framework named Heterogeneous Information Learning in Large-scale networks (HILL) to accelerate the joint learning. It enables the simultaneous node proximity assessing process to be done in a distributed manner by decomposing the complex modeling and optimization into many simple and independent sub-problems. We validate the significant correlation between the heterogeneous information and topological structure, and illustrate the generalizability of HILL by applying it to perform attributed network embedding and second-order proximity learning. A variation is proposed for link directionality modeling. Experimental results on real-world networks demonstrate the effectiveness and efficiency of HILL.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {70},
numpages = {24},
keywords = {Data mining, distributed processing, heterogeneity, network embedding}
}

@article{10.1145/3233227,
author = {He, Xinran and Kempe, David},
title = {Stability and Robustness in Influence Maximization},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3233227},
doi = {10.1145/3233227},
abstract = {In the well-studied Influence Maximization problem, the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. A large body of recent work has justified research on Influence Maximization models and algorithms with their potential to create societal or economic value. However, in order to live up to this potential, the algorithms must be robust to large amounts of noise, for they require quantitative estimates of the influence, which individuals exert on each other; ground truth for such quantities is inaccessible, and even decent estimates are very difficult to obtain.We begin to address this concern formally. First, we exhibit simple inputs on which even very small estimation errors may mislead every algorithm into highly suboptimal solutions. Motivated by this observation, we propose the Perturbation Interval model as a framework to characterize the stability of Influence Maximization against noise in the inferred diffusion network. Analyzing the susceptibility of specific instances to estimation errors leads to a clean algorithmic question, which we term the Influence Difference Maximization problem. However, the objective function of Influence Difference Maximization is NP-hard to approximate within a factor of O(n1−ϵ) for any ϵ &gt; 0.Given the infeasibility of diagnosing instability algorithmically, we focus on finding influential users robustly across multiple diffusion settings. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions. The algorithm’s goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al.&nbsp;can be used to approximate the optimum robust influence to within a factor of 1−1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world datasets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world datasets; all algorithms perform fairly well.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {66},
numpages = {34},
keywords = {Social networks, robust optimization, stability analysis, influence maximization, submodular optimization, information diffusion}
}

@article{10.1145/3230668,
author = {Liu, Xiaoli and Cao, Peng and Gon\c{c}alves, Andr\'{e} R. and Zhao, Dazhe and Banerjee, Arindam},
title = {Modeling Alzheimer’s Disease Progression with Fused Laplacian Sparse Group Lasso},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3230668},
doi = {10.1145/3230668},
abstract = {Alzheimer’s disease (AD), the most common type of dementia, not only imposes a huge financial burden on the health care system, but also a psychological and emotional burden on patients and their families. There is thus an urgent need to infer trajectories of cognitive performance over time and identify biomarkers predictive of the progression. In this article, we propose the multi-task learning with fused Laplacian sparse group lasso model, which can identify biomarkers closely related to cognitive measures due to its sparsity-inducing property, and model the disease progression with a general weighted (undirected) dependency graphs among the tasks. An efficient alternative directions method of multipliers based optimization algorithm is derived to solve the proposed non-smooth objective formulation. The effectiveness of the proposed model is demonstrated by its superior prediction performance over multiple state-of-the-art methods and accurate identification of compact sets of cognition-relevant imaging biomarkers that are consistent with prior medical studies.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {65},
numpages = {35},
keywords = {multi-task learning, Alzheimer’s disease, ADMM, graph laplacian, disease progression}
}

@article{10.1145/3234943,
author = {Ying, Josh Jia-Ching and Zhang, Ji and Huang, Che-Wei and Chen, Kuan-Ta and Tseng, Vincent S.},
title = {<i>FrauDetector</i><sup>+</sup>: An Incremental Graph-Mining Approach for Efficient Fraudulent Phone Call Detection},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3234943},
doi = {10.1145/3234943},
abstract = {In recent years, telecommunication fraud has become more rampant internationally with the development of modern technology and global communication. Because of rapid growth in the volume of call logs, the task of fraudulent phone call detection is confronted with big data issues in real-world implementations. Although our previous work, FrauDetector, addressed this problem and achieved some promising results, it can be further enhanced because it focuses only on fraud detection accuracy, whereas the efficiency and scalability are not top priorities. Other known approaches for fraudulent call number detection suffer from long training times or cannot accurately detect fraudulent phone calls in real time. However, the learning process of FrauDetector is too time-consuming to support real-world application. Although we have attempted to accelerate the the learning process of FrauDetector by parallelization, the parallelized learning process, namely PFrauDetector, still cannot afford the computing cost. In this article, we propose a highly efficient incremental graph-mining-based fraudulent phone call detection approach, namely FrauDetector+, which can automatically label fraudulent phone numbers with a “fraud” tag a crucial prerequisite for distinguishing fraudulent phone call numbers from nonfraudulent ones. FrauDetector+ initially generates smaller, more manageable subnetworks from original graph and performs a parallelized weighted HITS algorithm for a significant speed increase in the graph learning module. It adopts a novel aggregation approach to generate a trust (or experience) value for each phone number (or user) based on their respective local values. After the initial procedure, we can incrementally update the trust (or experience) value for each phone number (or user) while a new fraud phone number is identified. An efficient fraud-centric hash structure is constructed to support fast real-time detection of fraudulent phone numbers in the detection module. We conduct a comprehensive experimental study based on real datasets collected through an antifraud mobile application called Whoscall. The results demonstrate a significantly improved efficiency of our approach compared with FrauDetector as well as superior performance against other major classifier-based methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {68},
numpages = {35},
keywords = {incremental learning, Telecommunication fraud, trust value mining, parallelized weighted HITS algorithm, fraudulent phone call detection}
}

@article{10.1145/3230967,
author = {Wang, Can and Chi, Chi-Hung and She, Zhong and Cao, Longbing and Stantic, Bela},
title = {Coupled Clustering Ensemble by Exploring Data Interdependence},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3230967},
doi = {10.1145/3230967},
abstract = {Clustering ensembles combine multiple partitions of data into a single clustering solution. It is an effective technique for improving the quality of clustering results. Current clustering ensemble algorithms are usually built on the pairwise agreements between clusterings that focus on the similarity via consensus functions, between data objects that induce similarity measures from partitions and re-cluster objects, and between clusters that collapse groups of clusters into meta-clusters. In most of those models, there is a strong assumption on IIDness (i.e., independent and identical distribution), which states that base clusterings perform independently of one another and all objects are also independent. In the real world, however, objects are generally likely related to each other through features that are either explicit or even implicit. There is also latent but definite relationship among intermediate base clusterings because they are derived from the same set of data. All these demand a further investigation of clustering ensembles that explores the interdependence characteristics of data. To solve this problem, a new coupled clustering ensemble (CCE) framework that works on the interdependence nature of objects and intermediate base clusterings is proposed in this article. The main idea is to model the coupling relationship between objects by aggregating the similarity of base clusterings, and the interactive relationship among objects by addressing their neighborhood domains. Once these interdependence relationships are discovered, they will act as critical supplements to clustering ensembles. We verified our proposed framework by using three types of consensus function: clustering-based, object-based, and cluster-based. Substantial experiments on multiple synthetic and real-life benchmark datasets indicate that CCE can effectively capture the implicit interdependence relationships among base clusterings and among objects with higher clustering accuracy, stability, and robustness compared to 14 state-of-the-art techniques, supported by statistical analysis. In addition, we show that the final clustering quality is dependent on the data characteristics (e.g., quality and consistency) of base clusterings in terms of sensitivity analysis. Finally, the applications in document clustering, as well as on the datasets with much larger size and dimensionality, further demonstrate the effectiveness, efficiency, and scalability of our proposed models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {63},
numpages = {38},
keywords = {interdependence, behavior interior dimensions, object, Clustering ensemble, coupling, base clustering}
}

@article{10.1145/3233185,
author = {Romero-Tris, Cristina and Meg\'{\i}as, David},
title = {Protecting Privacy in Trajectories with a User-Centric Approach},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3233185},
doi = {10.1145/3233185},
abstract = {The increased use of location-aware devices, such as smartphones, generates a large amount of trajectory data. These data can be useful in several domains, like marketing, path modeling, localization of an epidemic focus, and so on. Nevertheless, since trajectory information contains personal mobility data, improper use or publication of trajectory data can threaten users’ privacy. It may reveal sensitive details like habits of behavior, religious beliefs, and sexual preferences. Therefore, many users might be unwilling to share their trajectory data without a previous anonymization process. Currently, several proposals to address this problem can be found in the literature. These solutions focus on anonymizing data before its publication, i.e., when they are already stored in the server database. Nevertheless, we argue that this approach gives the user no control about the information she shares. For this reason, we propose anonymizing data in the users’ mobile devices, before they are sent to a third party. This article extends our previous work which was, to the best of our knowledge, the first one to anonymize data at the client side, allowing users to select the amount and accuracy of shared data. In this article, we describe an improved version of the protocol, and we include the implementation together with an analysis of the results obtained after the simulation with real trajectory data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {67},
numpages = {27},
keywords = {Trajectory anonymization, user-centric protocol, privacy}
}

@article{10.1145/3230667,
author = {Di, Mingyang and Klabjan, Diego and Sha, Long and Lucey, Patrick},
title = {Large-Scale Adversarial Sports Play Retrieval with Learning to Rank},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3230667},
doi = {10.1145/3230667},
abstract = {As teams of professional leagues are becoming more and more analytically driven, the interest in effective data management and access of sports plays has dramatically increased. In this article, we present a retrieval system that can quickly find the most relevant plays from historical games given an input query. To search through a large number of games at an interactive speed, our system is built upon a distributed framework so that each query-result pair is evaluated in parallel. We also propose a pairwise learning to rank approach to improve search ranking based on users’ clickthrough behavior. The similarity metric in training the rank function is based on automatically learnt features from a convolutional autoencoder. Finally, we showcase the efficacy of our learning to rank approach by demonstrating rank quality in a user study.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {69},
numpages = {18},
keywords = {convolutional autoencoder, Similarity measures, learning to rank}
}

@article{10.1145/3233186,
author = {Huang, Zhipeng and Cautis, Bogdan and Cheng, Reynold and Zheng, Yudian and Mamoulis, Nikos and Yan, Jing},
title = {Entity-Based Query Recommendation for Long-Tail Queries},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3233186},
doi = {10.1145/3233186},
abstract = {Query recommendation, which suggests related queries to search engine users, has attracted a lot of attention in recent years. Most of the existing solutions, which perform analysis of users’ search history (or query logs), are often insufficient for long-tail queries that rarely appear in query logs. To handle such queries, we study the use of entities found in queries to provide recommendations. Specifically, we extract entities from a query, and use these entities to explore new ones by consulting an information source. The discovered entities are then used to suggest new queries to the user. In this article, we examine two information sources: (1) a knowledge base (or KB), such as YAGO and Freebase; and (2) a click log, which contains the URLs accessed by a query user. We study how to use these sources to find new entities useful for query recommendation. We further study a hybrid framework that integrates different query recommendation methods effectively. As shown in the experiments, our proposed approaches provide better recommendations than existing solutions for long-tail queries. In addition, our query recommendation process takes less than 100ms to complete. Thus, our solution is suitable for providing online query recommendation services for search engines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {64},
numpages = {24},
keywords = {entity, Query recommendation, knowledge base}
}

@article{10.1145/3201603,
author = {Santos, Ludovic Dos and Piwowarski, Benjamin and Denoyer, Ludovic and Gallinari, Patrick},
title = {Representation Learning for Classification in Heterogeneous Graphs with Application to Social Networks},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201603},
doi = {10.1145/3201603},
abstract = {We address the task of node classification in heterogeneous networks, where the nodes are of different types, each type having its own set of labels, and the relations between nodes may also be of different types. A typical example is provided by social networks where node types may for example be users, content, or films, and relations friendship, like, authorship. Learning and performing inference on such heterogeneous networks is a recent task requiring new models and algorithms. We propose a model, Labeling Heterogeneous Network (LaHNet), a transductive approach to classification that learns to project the different types of nodes into a common latent space. This embedding is learned so as to reflect different characteristics of the problem such as the correlation between node labels, as well as the graph topology. The application focus is on social graphs, but the algorithm is general and can be used for other domains. The model is evaluated on five datasets representative of different instances of social data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {62},
numpages = {33},
keywords = {node classification, relational data, Representation learning}
}

@article{10.1145/3208351,
author = {Riondato, Matteo and Upfal, Eli},
title = {ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3208351},
doi = {10.1145/3208351},
abstract = {ABPA Ξ AΣ (ABRAXAS): Gnostic word of mystic meaning.We present ABRA, a suite of algorithms to compute and maintain probabilistically guaranteed high-quality approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, ABRA is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime number of samples, and accuracy, state-of-the-art algorithms with the same quality guarantees.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {61},
numpages = {38},
keywords = {pseudodimension, statistical learning theory, Centrality measures, uniform bounds}
}

@article{10.1145/3182382,
author = {Lines, Jason and Taylor, Sarah and Bagnall, Anthony},
title = {Time Series Classification with HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182382},
doi = {10.1145/3182382},
abstract = {A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC.We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {52},
numpages = {35},
keywords = {heterogeneous ensembles, Time series classification, meta ensembles, deep learning}
}

@article{10.1145/3201578,
author = {Bollegala, Danushka and Atanasov, Vincent and Maehara, Takanori and Kawarabayashi, Ken-Ichi},
title = {ClassiNet -- Predicting Missing Features for Short-Text Classification},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201578},
doi = {10.1145/3201578},
abstract = {Short and sparse texts such as tweets, search engine snippets, product reviews, and chat messages are abundant on the Web. Classifying such short-texts into a pre-defined set of categories is a common problem that arises in various contexts, such as sentiment classification, spam detection, and information recommendation. The fundamental problem in short-text classification is feature sparseness -- the lack of feature overlap between a trained model and a test instance to be classified. We propose ClassiNet -- a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex vi in the ClassiNet, where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge eij connecting a vertex vi to a vertex vj represents the conditional probability that given vi exists in an instance, vj also exists in the same instance.We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance x, we find similar features from ClassiNet that did not appear in x, and append those features in the representation of x. Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text. We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {55},
numpages = {29},
keywords = {feature sparseness, short-texts, Classifier networks, text classification}
}

@article{10.1145/3201408,
author = {Zhang, Ziqi and Gao, Jie and Ciravegna, Fabio},
title = {SemRe-Rank: Improving Automatic Term Extraction by Incorporating Semantic Relatedness with Personalised PageRank},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201408},
doi = {10.1145/3201408},
abstract = {Automatic Term Extraction (ATE) deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a ‘one-size-fit-all’ solution that may never exist, we propose to develop generic methods to ‘enhance’ existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness—an often overlooked venue—into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute ‘semantic importance’ scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K’s), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {57},
numpages = {41},
keywords = {Automatic term extraction, information retrieval, word embedding, information extraction, text mining, termhood, ATR, semantic relatedness, personalised pagerank, automatic term recognition, ATE}
}

@article{10.1145/3201407,
author = {Li, Peipei and Wang, Haixun and Li, Hongsong and Wu, Xindong},
title = {Employing Semantic Context for Sparse Information Extraction Assessment},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201407},
doi = {10.1145/3201407},
abstract = {A huge amount of texts available on the World Wide Web presents an unprecedented opportunity for information extraction (IE). One important assumption in IE is that frequent extractions are more likely to be correct. Sparse IE is hence a challenging task because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. However, there is limited research on sparse IE, especially in the assessment of the validity of sparse IEs. Motivated by this, we introduce a lightweight, explicit semantic approach for assessing sparse IE.1 We first use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of any semantic relationship. Second, we learn from three semantic contexts using different base classifiers to select an optimal classification model for assessing sparse extractions. Finally, experiments show that as compared with several state-of-the-art approaches, our approach can significantly improve the F-score in the assessment of sparse extractions while maintaining the efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {54},
numpages = {36},
keywords = {semantic network, isA relationship, classification, Sparse information extraction}
}

@article{10.1145/3209669,
author = {Silva, Fabr\'{\i}cio A. and Domingues, Augusto C. S. A. and Silva, Thais R. M. Braga},
title = {Discovering Mobile Application Usage Patterns from a Large-Scale Dataset},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3209669},
doi = {10.1145/3209669},
abstract = {The discovering of patterns regarding how, when, and where users interact with mobile applications reveals important insights for mobile service providers. In this work, we exploit for the first time a real and large-scale dataset representing the records of mobile application usage of 5,342 users during 2014. The data was collected by a software agent, installed at the users’ smartphones, which monitors detailed usage of applications. First, we look for general patterns of how users access some of the most popular mobile applications in terms of frequency, duration, diversity, and data traffic. Next, we mine the dataset looking for temporal patterns in terms of when and how often accesses occur. Finally, we exploit the location of each access to detect users’ points of interest and location-based communities. Based on the results, we derive a model to generate synthetic datasets of mobile application usage and evaluate solutions to predict the next application to be launched. We also discuss a series of implications of the findings regarding telecommunication services, mobile advertisements, and smart cities. This is the first time this dataset is used, and we also make it publicly available for other researchers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {59},
numpages = {36},
keywords = {pattern mining, synthetic data generation, Mobile data}
}

@article{10.1145/3201604,
author = {Hao, Shuji and Hu, Peiying and Zhao, Peilin and Hoi, Steven C. H. and Miao, Chunyan},
title = {Online Active Learning with Expert Advice},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201604},
doi = {10.1145/3201604},
abstract = {In literature, learning with expert advice methods usually assume that a learner always obtain the true label of every incoming training instance at the end of each trial. However, in many real-world applications, acquiring the true labels of all instances can be both costly and time consuming, especially for large-scale problems. For example, in the social media, data stream usually comes in a high speed and volume, and it is nearly impossible and highly costly to label all of the instances. In this article, we address this problem with active learning with expert advice, where the ground truth of an instance is disclosed only when it is requested by the proposed active query strategies. Our goal is to minimize the number of requests while training an online learning model without sacrificing the performance. To address this challenge, we propose a framework of active forecasters, which attempts to extend two fully supervised forecasters, Exponentially Weighted Average Forecaster and Greedy Forecaster, to tackle the task of online active learning (OAL) with expert advice. Specifically, we proposed two OAL with expert advice algorithms, named Active Exponentially Weighted Average Forecaster (AEWAF) and active greedy forecaster (AGF), by considering the difference of expert advices. To further improve the robustness of the proposed AEWAF and AGF algorithms in the noisy scenarios (where noisy experts exist), we also proposed two robust active learning with expert advice algorithms, named Robust Active Exponentially Weighted Average Forecaster and Robust Active Greedy Forecaster. We validate the efficacy of the proposed algorithms by an extensive set of experiments in both normal scenarios (where all of experts are comparably reliable) and noisy scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {58},
numpages = {22},
keywords = {expert advice, data streaming, Online learning, active learning}
}

@article{10.1145/3211872,
author = {Li, Feijiang and Qian, Yuhua and Wang, Jieting and Dang, Chuangyin and Liu, Bing},
title = {Cluster’s Quality Evaluation and Selective Clustering Ensemble},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3211872},
doi = {10.1145/3211872},
abstract = {Clustering ensemble has drawn much attention in recent years due to its ability to generate a high quality and robust partition result. Weighted clustering ensemble and selective clustering ensemble are two general ways to further improve the performance of a clustering ensemble method. Existing weighted clustering ensemble methods assign the same weight to each cluster in a partition of the ensemble. Since the qualities of the clusters in a partition are different, the clusters should be weighted differently. To address this issue, this article proposes a new measure to calculate the similarity between a cluster and a partition. Theoretically, this measure is effective in handling two problems in measuring the quality of a cluster, which are defined as the symmetric problem and the context meaning problem. In addition, some properties of the proposed measure are analyzed. This measure can be easily expanded to a clustering performance measure that calculates the similarity between two partitions. As a result of this measure, we propose a novel selective clustering ensemble framework, which considers the differences between the objective of the ensemble selection stage and the object of the ensemble integration stage in the selective clustering ensemble. To verify the performance of the new measure, we compare the performance of the measure with the two existing measures in weighting clusters. The experiments show that the proposed measure is more effective. To verify the performance of the novel framework, four existing state-of-the-art selective clustering ensemble frameworks are employed as references. The experiments show that the proposed framework is statistically better than the others on 17 UCI benchmark datasets, 8 document datasets, and the Olivetti Face Database.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {60},
numpages = {27},
keywords = {selective clustering ensemble, weighted clustering ensemble, Clustering ensemble, cluster quality}
}

@article{10.1145/3201577,
author = {Qin, Tian and Shangguan, Wufan and Song, Guojie and Tang, Jie},
title = {Spatio-Temporal Routine Mining on Mobile Phone Data},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201577},
doi = {10.1145/3201577},
abstract = {Mining human behaviors has always been an important subarea of Data Mining. While it provides empirical evidences to psychological/behavioral studies, it also builds the foundation of various big-data systems, which rely heavily on the prediction of human behaviors. In recent years, the ubiquitous spreading of mobile phones and the massive amount of spatio-temporal data collected from them make it possible to keep track of the daily commute behaviors of mobile subscribers and further conduct routine mining on them. In this article, we propose to model mobile subscribers’ daily commute behaviors by three levels: location trajectory, one-day pattern, and routine pattern. We develop the model Spatio-Temporal Routine Mining Model (STRMM) to characterize the generative process between these three levels. From daily trajectories, the STRMM model unsupervisedly extracts spatio-temporal routine patterns that contain two aspects of information: (1) How people’s typical commute patterns are. (2) How much their commute behaviors vary from day to day. Compared to traditional methods, STRMM takes into account the different degrees of behavioral uncertainty in different timespans of a day, yielding more realistic and intuitive results. To learn model parameters, we adopt Stochastic Expectation Maximization algorithm. Experiments are conducted on two real world datasets, and the empirical results show that the STRMM model can effectively discover hidden routine patterns of human commute behaviors and yields higher accuracy results in trajectory prediction task.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {56},
numpages = {24},
keywords = {mobile phone data, spatio-temporal pattern, Routine mining}
}

@article{10.1145/3201406,
author = {Amornbunchornvej, Chainarong and Brugere, Ivan and Strandburg-Peshkin, Ariana and Farine, Damien R. and Crofoot, Margaret C. and Berger-Wolf, Tanya Y.},
title = {Coordination Event Detection and Initiator Identification in Time Series Data},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201406},
doi = {10.1145/3201406},
abstract = {Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. In this work, we formalize the Coordination Initiator Inference Problem and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. The proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks. We demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. Our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. Our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate NASDAQ index. Our method is easily generalizable to any coordinated time series data from interacting entities.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {53},
numpages = {33},
keywords = {influence, coordination, time series, Leadership}
}

@article{10.1145/3186282,
author = {Li, Bo and Vorobeychik, Yevgeniy},
title = {Evasion-Robust Classification on Binary Domains},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3186282},
doi = {10.1145/3186282},
abstract = {The success of classification learning has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static, but make a deliberate effort to evade the classifiers. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. We first present a general approach based on mixed-integer linear programming (MILP) with constraint generation. This approach is the first to compute an optimal solution to adversarial loss minimization for two general classes of adversarial evasion models in the context of binary feature spaces. To further improve scalability and significantly generalize the scope of the MILP-based method, we propose a principled iterative retraining framework, which can be used with arbitrary classifiers and essentially arbitrary attack models. We show that the retraining approach, when it converges, minimizes an upper bound on adversarial loss. Extensive experiments demonstrate that the mixed-integer programming approach significantly outperforms several state-of-the-art adversarial learning alternatives. Moreover, the retraining framework performs nearly as well, but scales significantly better. Finally, we show that our approach is robust to misspecifications of the adversarial model.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {50},
numpages = {32},
keywords = {adversarial examples, classifier evasion, mixed-integer linear programming, robust learning, Adversarial classification}
}

@article{10.1145/3193573,
author = {Mohammadi, Majid and Atashin, Amir Ahooye and Hofman, Wout and Tan, Yaohua},
title = {Comparison of Ontology Alignment Systems Across Single Matching Task Via the McNemar’s Test},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3193573},
doi = {10.1145/3193573},
abstract = {Ontology alignment is widely used to find the correspondences between different ontologies in diverse fields. After discovering the alignments, several performance scores are available to evaluate them. The scores typically require the identified alignment and a reference containing the underlying actual correspondences of the given ontologies. The current trend in the alignment evaluation is to put forward a new score (e.g., precision, weighted precision, semantic precision, etc.) and to compare various alignments by juxtaposing the obtained scores. However, it is substantially provocative to select one measure among others for comparison. On top of that, claiming if one system has a better performance than one another cannot be substantiated solely by comparing two scalars. In this article, we propose the statistical procedures that enable us to theoretically favor one system over one another. The McNemar’s test is the statistical means by which the comparison of two ontology alignment systems over one matching task is drawn. The test applies to a 2 \texttimes{} 2 contingency table, which can be constructed in two different ways based on the alignments, each of which has their own merits/pitfalls. The ways of the contingency table construction and various apposite statistics from the McNemar’s test are elaborated in minute detail. In the case of having more than two alignment systems for comparison, the family wise error rate is expected to happen. Thus, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar’s test in the presence of multiple alignment systems. From this graph, it is readily understood if one system is better than one another or if their differences are imperceptible. The proposed statistical methodologies are applied to the systems participated in the OAEI 2016 anatomy track, and also compares several well-known similarity metrics for the same matching problem.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {51},
numpages = {18},
keywords = {McNemar’s test, family-wise error rate, Ontology alignment, OAEI, anatomy}
}

@article{10.1145/3186566,
author = {Nguyen, Minh-Tien and Tran, Duc-Vu and Nguyen, Le-Minh and Phan, Xuan-Hieu},
title = {Exploiting User Posts for Web Document Summarization},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3186566},
doi = {10.1145/3186566},
abstract = {Relevant user posts such as comments or tweets of a Web document provide additional valuable information to enrich the content of this document. When creating user posts, readers tend to borrow salient words or phrases in sentences. This can be considered as word variation. This article proposes a framework that models the word variation aspect to enhance the quality of Web document summarization. Technically, the framework consists of two steps: scoring and selection. In the first step, the social information of a Web document such as user posts is exploited to model intra-relations and inter-relations in lexical and semantic levels. These relations are denoted by a mutual reinforcement similarity graph used to score each sentence and user post. After scoring, summaries are extracted by using a ranking approach or concept-based method formulated in the form of Integer Linear Programming. To confirm the efficiency of our framework, sentence and story highlight extraction tasks were taken as a case study on three datasets in two languages, English and Vietnamese. Experimental results show that: (i) the framework can improve ROUGE-scores compared to state-of-the-art baselines of social context summarization and (ii) the combination of the two relations benefits the sentence extraction of single Web documents.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {49},
numpages = {28},
keywords = {information retrieval, summarization, ILP, social context summarization, Data mining, ranking}
}

@article{10.1145/3182181,
author = {Saha, Sriparna and Mitra, Sayantan and Kramer, Stefan},
title = {Exploring Multiobjective Optimization for Multiview Clustering},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182181},
doi = {10.1145/3182181},
abstract = {We present a new multiview clustering approach based on multiobjective optimization. In contrast to existing clustering algorithms based on multiobjective optimization, it is generally applicable to data represented by two or more views and does not require specifying the number of clusters a priori. The approach builds upon the search capability of a multiobjective simulated annealing based technique, AMOSA, as the underlying optimization technique. In the first version of the proposed approach, an internal cluster validity index is used to assess the quality of different partitionings obtained using different views. A new way of checking the compatibility of these different partitionings is also proposed and this is used as another objective function. A new encoding strategy and some new mutation operators are introduced. Finally, a new way of computing a consensus partitioning from multiple individual partitions obtained on multiple views is proposed. As a baseline and for comparison, two multiobjective based ensemble clustering techniques are proposed to combine the outputs of different simple clustering approaches. The efficacy of the proposed clustering methods is shown for partitioning several real-world datasets having multiple views. To show the practical usefulness of the method, we present results on web-search result clustering, where the task is to find a suitable partitioning of web snippets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {44},
numpages = {30},
keywords = {multiobjective optimization, simulated annealing, Multiview classification, search result clustering}
}

@article{10.1145/3182384,
author = {Liu, Hongfu and Fu, Yun},
title = {Consensus Guided Multi-View Clustering},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182384},
doi = {10.1145/3182384},
abstract = {In recent decades, tremendous emerging techniques thrive the artificial intelligence field due to the increasing collected data captured from multiple sensors. These multi-view data provide more rich information than traditional single-view data. Fusing heterogeneous information for certain tasks is a core part of multi-view learning, especially for multi-view clustering. Although numerous multi-view clustering algorithms have been proposed, most scholars focus on finding the common space of different views, but unfortunately ignore the benefits from partition level by ensemble clustering. For ensemble clustering, however, there is no interaction between individual partitions from each view and the final consensus one. To fill the gap, we propose a Consensus Guided Multi-View Clustering (CMVC) framework, which incorporates the generation of basic partitions from each view and fusion of consensus clustering in an interactive way, i.e., the consensus clustering guides the generation of basic partitions, and high quality basic partitions positively contribute to the consensus clustering as well. We design a non-trivial optimization solution to formulate CMVC into two iterative k-means clusterings by an approximate calculation. In addition, the generalization of CMVC provides a rich feasibility for different scenarios, and the extension of CMVC with incomplete multi-view clustering further validates the effectiveness for real-world applications. Extensive experiments demonstrate the advantages of CMVC over other widely used multi-view clustering methods in terms of cluster validity, and the robustness of CMVC to some important parameters and incomplete multi-view data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {42},
numpages = {21},
keywords = {ensemble clustering, utility function, Multi-view clustering}
}

@article{10.1145/3184454,
author = {Chen, Hung-Hsuan},
title = {Behavior2Vec: Generating Distributed Representations of Users’ Behaviors on Products for Recommender Systems},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3184454},
doi = {10.1145/3184454},
abstract = {Most studies on recommender systems target at increasing the click through rate, and hope that the number of orders will increase as well. We argue that clicking and purchasing an item are different behaviors. Thus, we should probably apply different strategies for different objectives, e.g., increase the click through rate, or increase the order rate. In this article, we propose to generate the distributed representations of users’ viewing and purchasing behaviors on an e-commerce website. By leveraging on the cosine distance between the distributed representations of the behaviors on items under different contexts, we can predict a user’s next clicking or purchasing item more precisely, compared to several baseline methods. Perhaps more importantly, we found that the distributed representations may help discover interesting analogies among the products. We may utilize such analogies to explain how two products are related, and eventually apply different recommendation strategies under different scenarios. We developed the Behavior2Vec library for demonstration. The library can be accessed at https://github.com/ncu-dart/behavior2vec/.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {43},
numpages = {20},
keywords = {collaborative filtering, behavior embedding, Distributed representation, learning representations, Word2Vec, matrix factorization}
}

@article{10.1145/3182383,
author = {Wu, Hao and Ning, Yue and Chakraborty, Prithwish and Vreeken, Jilles and Tatti, Nikolaj and Ramakrishnan, Naren},
title = {Generating Realistic Synthetic Population Datasets},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182383},
doi = {10.1145/3182383},
abstract = {Modern studies of societal phenomena rely on the availability of large datasets capturing attributes and activities of synthetic, city-level, populations. For instance, in epidemiology, synthetic population datasets are necessary to study disease propagation and intervention measures before implementation. In social science, synthetic population datasets are needed to understand how policy decisions might affect preferences and behaviors of individuals. In public health, synthetic population datasets are necessary to capture diagnostic and procedural characteristics of patient records without violating confidentialities of individuals. To generate such datasets over a large set of categorical variables, we propose the use of the maximum entropy principle to formalize a generative model such that in a statistically well-founded way we can optimally utilize given prior information about the data, and are unbiased otherwise. An efficient inference algorithm is designed to estimate the maximum entropy model, and we demonstrate how our approach is adept at estimating underlying data distributions. We evaluate this approach against both simulated data and US census datasets, and demonstrate its feasibility using an epidemic simulation application.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {45},
numpages = {22},
keywords = {Multivariate categorical data, maximum entropy models, synthetic population, probabilistic modeling}
}

@article{10.1145/3186586,
author = {Bressan, Marco and Chierichetti, Flavio and Kumar, Ravi and Leucci, Stefano and Panconesi, Alessandro},
title = {Motif Counting Beyond Five Nodes},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3186586},
doi = {10.1145/3186586},
abstract = {Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural algorithms based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that such algorithms are outperformed by color coding (CC)&nbsp;[2], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC; furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC’s memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that CC can push the limits of the state-of-the-art, both in terms of the size of the input graph and of that of the graphlets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {48},
numpages = {25},
keywords = {Motif counting, subgraph counting, graph mining, color coding}
}

@article{10.1145/3182392,
author = {Chen, Xiaowei and Lui, John C. S.},
title = {Mining Graphlet Counts in Online Social Networks},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182392},
doi = {10.1145/3182392},
abstract = {Counting subgraphs is a fundamental analysis task for online social networks (OSNs). Given the sheer size and restricted access of OSN, efficient computation of subgraph counts is highly challenging. Although a number of algorithms have been proposed to estimate the relative counts of subgraphs in OSNs with restricted access, there are only few works which try to solve a more general problem, i.e., counting subgraph frequencies. In this article, we propose an efficient random walk-based framework to estimate the subgraph counts. Our framework generates samples by leveraging consecutive steps of the random walk as well as by observing neighbors of visited nodes. Using the importance sampling technique, we derive unbiased estimators of the subgraph counts. To make better use of the degree information of visited nodes, we also design improved estimators, which increases the accuracy of the estimation with no additional cost. We conduct extensive experimental evaluation on real-world OSNs to confirm our theoretical claims. The experiment results show that our estimators are unbiased, accurate, efficient, and better than the state-of-the-art algorithms. For the Weibo graph with more than 58 million nodes, our method produces estimate of triangle count with an error less than 5% using only 20,000 sampled nodes. Detailed comparison with the state-of-the-art methods demonstrates that our algorithm is 2--10 times more accurate.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {41},
numpages = {38},
keywords = {Graphlet count, random walk, online social networks, Markov chain Monte Carlo}
}

@article{10.1145/3186268,
author = {Nesa, Nashreen and Ghosh, Tania and Banerjee, Indrajit},
title = {IGRM: Improved Grey Relational Model and Its Ensembles for Occupancy Sensing in Internet of Things Applications},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3186268},
doi = {10.1145/3186268},
abstract = {Occupancy detection is one of the many applications of Building Automation Systems (BAS) or Heating, Ventilation, and Air Conditioning (HVAC) control systems, especially, with the rising demand of Internet of Things (IoT) services. This article describes the fusion of data collected from sensors by exploiting their potential to sense occupancy in a room. For this purpose, a sensor test bed is deployed that includes four sensors measuring temperature, relative humidity, distance from the first obstacle, and light along with a Arduino micro-controller to validate our model. In addition, this article proposes three algorithms for efficient fusion of the sensor data that is inspired by the Grey theory. An improved Grey Relational Model (iGRM) is proposed, which acts as the base classifier for the other two algorithms, namely, Grey Relational Model with Bagging (iGRM-BG) and Grey Relational Model with Boosting (iGRM-BT). Furthermore, all three algorithms use a sliding window concept, where only the samples inside the window participate in model training. Also, we have considered varying number of window size for optimal comparison. The algorithms were tested against the experimental data collected through a test bed as well as on a publicly available large dataset, where both the ensemble models, iGRM-BG and iGRM-BT, are seen to enhance the performance of iGRM. The results reveal exceptionally high performances with accuracies above 95% (iGRM) and up to 100% (iGRM-BT) for the experimental dataset and above 98.24% (iGRM) and up to 99.49% (iGRM-BG) using the publicly available dataset. Among the three proposed models, iGRM-BG was observed to outperform both iGRM and iGRM-BT owing to its advantage of being an ensemble model and its robustness against over-fitting.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {47},
numpages = {23},
keywords = {Bagging, boosting, window}
}

@article{10.1145/3183346,
author = {Toth, Edward and Chawla, Sanjay},
title = {GTΔ: Detecting Temporal Changes in Group Stochastic Processes},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3183346},
doi = {10.1145/3183346},
abstract = {Given a portfolio of stocks or a series of frames in a video how do we detect significant changes in a group of values for real-time applications? In this article, we formalize the problem of sequentially detecting temporal changes in a group of stochastic processes. As a solution to this particular problem, we propose the group temporal change (GTΔ) algorithm, a simple yet effective technique for the sequential detection of significant changes in a variety of statistical properties of a group over time. Due to the flexible framework of the GTΔ algorithm, a domain expert is able to select one or more statistical properties that they are interested in monitoring. The usefulness of our proposed algorithm is also demonstrated against state-of-the-art techniques on synthetically generated data as well as on two real-world applications; a portfolio of healthcare stocks over a 20 year period and a video monitoring the activity of our Sun.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {39},
numpages = {24},
keywords = {time series analysis, anomaly detection, Group change detection}
}

@article{10.1145/3178048,
author = {Xie, Wei and Zhu, Feida and Xiao, Jing and Wang, Jianzong},
title = {Social Network Monitoring for Bursty Cascade Detection},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3178048},
doi = {10.1145/3178048},
abstract = {Social network services have become important and efficient platforms for users to share all kinds of information. The capability to monitor user-generated information and detect bursts from information diffusions in these social networks brings value to a wide range of real-life applications, such as viral marketing. However, in reality, as a third party, there is always a cost for gathering information from each user or so-called social network sensor. The question then arises how to select a budgeted set of social network sensors to form the data stream for burst detection without compromising the detection performance. In this article, we present a general sensor selection solution for different burst detection approaches. We formulate this problem as a constraint satisfaction problem that has high computational complexity. To reduce the computational cost, we first reduce most of the constraints by making use of the fact that bursty cascades are rare among the whole population. We then transform the problem into an Linear Programming (LP) problem. Furthermore, we use the sub-gradient method instead of the standard simplex method or interior-point method to solve the LP problem, which makes it possible for our solution to scale up to large social networks. Evaluating our solution on millions of real information cascades, we demonstrate both the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {40},
numpages = {24},
keywords = {sub-gradient method, linear programming, Social network sensors}
}

@article{10.1145/3185059,
author = {Fond, Timothy La and Neville, Jennifer and Gallagher, Brian},
title = {Designing Size Consistent Statistics for Accurate Anomaly Detection in Dynamic Networks},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3185059},
doi = {10.1145/3185059},
abstract = {An important task in network analysis is the detection of anomalous events in a network time series. These events could merely be times of interest in the network timeline or they could be examples of malicious activity or network malfunction. Hypothesis testing using network statistics to summarize the behavior of the network provides a robust framework for the anomaly detection decision process. Unfortunately, choosing network statistics that are dependent on confounding factors like the total number of nodes or edges can lead to incorrect conclusions (e.g., false positives and false negatives). In this article, we describe the challenges that face anomaly detection in dynamic network streams regarding confounding factors. We also provide two solutions to avoiding error due to confounding factors: the first is a randomization testing method that controls for confounding factors, and the second is a set of size-consistent network statistics that avoid confounding due to the most common factors, edge count and node count.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {46},
numpages = {49},
keywords = {Graphs, data mining, statistics}
}

@article{10.1145/3173044,
author = {Peng, Min and Zhu, Jiahui and Wang, Hua and Li, Xuhui and Zhang, Yanchun and Zhang, Xiuzhen and Tian, Gang},
title = {Mining Event-Oriented Topics in Microblog Stream with Unsupervised Multi-View Hierarchical Embedding},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3173044},
doi = {10.1145/3173044},
abstract = {This article presents an unsupervised multi-view hierarchical embedding (UMHE) framework to sufficiently reveal the intrinsic topical knowledge in social events. Event-oriented topics are highly related to such events as it can provide explicit descriptions of what have happened in social community. In many real-world cases, however, it is difficult to include all attributes of microblogs, more often, textual aspects only are available. Traditional topic modelling methods have failed to generate event-oriented topics with the textual aspects, since the inherent relations between topics are often overlooked in these methods. Meanwhile, the metrics in original word vocabulary space might not effectively capture semantic distances. Our UMHE framework overcomes the severe information deficiency and poor feature representation. The UMHE first develops a multi-view Bayesian rose tree to preliminarily generate prior knowledge for latent topics and their relations. With such prior knowledge, we design an unsupervised translation-based hierarchical embedding method to make a better representation of these latent topics. By applying self-adaptive spectral clustering on the embedding space and the original space concomitantly, we eventually extract event-oriented topics in word distributions to express social events. Our framework is purely data-driven and unsupervised, without any external knowledge. Experimental results on TREC Tweets2011 dataset and Sina Weibo dataset demonstrate that the UMHE framework can construct hierarchical structure with high fitness, but also yield topic embeddings with salient semantics; therefore, it can derive event-oriented topics with meaningful descriptions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {38},
numpages = {26},
keywords = {Event-oriented topic, Bayesian rose tree, unsupervised learning, multi-view hierarchical embedding}
}

@article{10.1145/3161885,
author = {Moreno, Sebastian and Neville, Jennifer and Kirshner, Sergey},
title = {Tied Kronecker Product Graph Models to Capture Variance in Network Populations},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3161885},
doi = {10.1145/3161885},
abstract = {Much of the past work on mining and modeling networks has focused on understanding the observed properties of single example graphs. However, in many real-life applications it is important to characterize the structure of populations of graphs. In this work, we analyze the distributional properties of probabilistic generative graph models (PGGMs) for network populations. PGGMs are statistical methods that model the network distribution and match common characteristics of real-world networks. Specifically, we show that most PGGMs cannot reflect the natural variability in graph properties observed across multiple networks because their edge generation process assumes independence among edges. Then, we propose the mixed Kronecker Product Graph Model (mKPGM), a scalable generalization of KPGMs that uses tied parameters to increase the variability of the sampled networks, while preserving the edge probabilities in expectation. We compare mKPGM to several other graph models. The results show that learned mKPGMs accurately represent the characteristics of real-world networks, while also effectively capturing the natural variability in network structure.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {35},
numpages = {40},
keywords = {social network analysis, kronecker product graph models, Graph generation models}
}

@article{10.1145/3168363,
author = {Li, Sheng and Shao, Ming and Fu, Yun},
title = {Multi-View Low-Rank Analysis with Applications to Outlier Detection},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3168363},
doi = {10.1145/3168363},
abstract = {Detecting outliers or anomalies is a fundamental problem in various machine learning and data mining applications. Conventional outlier detection algorithms are mainly designed for single-view data. Nowadays, data can be easily collected from multiple views, and many learning tasks such as clustering and classification have benefited from multi-view data. However, outlier detection from multi-view data is still a very challenging problem, as the data in multiple views usually have more complicated distributions and exhibit inconsistent behaviors. To address this problem, we propose a multi-view low-rank analysis (MLRA) framework for outlier detection in this article. MLRA pursuits outliers from a new perspective, robust data representation. It contains two major components. First, the cross-view low-rank coding is performed to reveal the intrinsic structures of data. In particular, we formulate a regularized rank-minimization problem, which is solved by an efficient optimization algorithm. Second, the outliers are identified through an outlier score estimation procedure. Different from the existing multi-view outlier detection methods, MLRA is able to detect two different types of outliers from multiple views simultaneously. To this end, we design a criterion to estimate the outlier scores by analyzing the obtained representation coefficients. Moreover, we extend MLRA to tackle the multi-view group outlier detection problem. Extensive evaluations on seven UCI datasets, the MovieLens, the USPS-MNIST, and the WebKB datasets demon strate that our approach outperforms several state-of-the-art outlier detection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {32},
numpages = {22},
keywords = {low-rank matrix recovery, Multi-view learning, outlier detection}
}

@article{10.1145/3154410,
author = {Altowim, Yasser and Kalashnikov, Dmitri V. and Mehrotra, Sharad},
title = {ProgressER: Adaptive Progressive Approach to Relational Entity Resolution},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154410},
doi = {10.1145/3154410},
abstract = {Entity resolution (ER) is the process of identifying which entities in a dataset refer to the same real-world object. In relational ER, the dataset consists of multiple entity-sets and relationships among them. Such relationships cause the resolution of some entities to influence the resolution of other entities. For instance, consider a relational dataset that consists of a set of research paper entities and a set of venue entities. In such a dataset, deciding that two research papers are the same may trigger the fact that their venues are also the same. This article proposes a progressive approach to relational ER, named ProgressER, that aims to produce the highest quality result given a constraint on the resolution budget, specified by the user. Such a progressive approach is useful for many emerging analytical applications that require low latency response (and thus cannot tolerate delays caused by cleaning the entire dataset) and/or in situations where the underlying resources are constrained or costly to use. To maximize the quality of the result, ProgressER follows an adaptive strategy that periodically monitors and reassesses the resolution progress to determine which parts of the dataset should be resolved next and how they should be resolved. More specifically, ProgressER divides the input budget into several resolution windows and analyzes the resolution progress at the beginning of each window to generate a resolution plan for the current window. A resolution plan specifies which blocks of entities and which entity pairs within blocks need to be resolved during the plan execution phase of that window. In addition, ProgressER specifies, for each identified pair of entities, the order in which the similarity functions should be applied on the pair. Such an order plays a significant role in reducing the overall cost because applying the first few functions in this order might be sufficient to resolve the pair. The empirical evaluation of ProgressER demonstrates its significant advantage in terms of progressiveness over the traditional ER techniques for the given problem settings.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {33},
numpages = {45},
keywords = {resolution workflow, resolution plan, relational entity resolution, progressive computation, collective entity resolution, Data cleaning, entity resolution}
}

@article{10.1145/3178112,
author = {Bakerman, Jordan and Pazdernik, Karl and Wilson, Alyson and Fairchild, Geoffrey and Bahran, Rian},
title = {Twitter Geolocation: A Hybrid Approach},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3178112},
doi = {10.1145/3178112},
abstract = {Geotagging Twitter messages is an important tool for event detection and enrichment. Despite the availability of both social media content and user network information, these two features are generally utilized separately in the methodology. In this article, we create a hybrid method that uses Twitter content and network information jointly as model features. We use Gaussian mixture models to map the raw spatial distribution of the model features to a predicted field. This approach is scalable to large datasets and provides a natural representation of model confidence. Our method is tested against other approaches and we achieve greater prediction accuracy. The model also improves both precision and coverage.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {34},
numpages = {17},
keywords = {Geotag, Gaussian mixture model, simple accuracy error, prediction region area, Twitter, comprehensive accuracy error}
}

@article{10.1145/3162050,
author = {Khodadadi, Ali and Hosseini, Seyed Abbas and Tavakoli, Erfan and Rabiee, Hamid R.},
title = {Continuous-Time User Modeling in Presence of Badges: A Probabilistic Approach},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3162050},
doi = {10.1145/3162050},
abstract = {User modeling plays an important role in delivering customized web services to the users and improving their engagement. However, most user models in the literature do not explicitly consider the temporal behavior of users. More recently, continuous-time user modeling has gained considerable attention and many user behavior models have been proposed based on temporal point processes. However, typical point process-based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements are among those factors that are neglected, while they have a strong impact on user participation in online services. In this article, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors. We extend the proposed processes to model user actions over the community-based question and answering websites, and propose an inference algorithm based on Variational-Expectation Maximization that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {37},
numpages = {30},
keywords = {badge, temporal point process, variational EM, User modeling, stack overflow, gamification, user profiling}
}

@article{10.1145/3178113,
author = {Yang, Pei and Tan, Qi and He, Jingrui},
title = {Function-on-Function Regression with Mode-Sparsity Regularization},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3178113},
doi = {10.1145/3178113},
abstract = {Functional data is ubiquitous in many domains, such as healthcare, social media, manufacturing process, sensor networks, and so on. The goal of function-on-function regression is to build a mapping from functional predictors to functional response. In this article, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use a mode-sparsity regularization to automatically filter out irrelevant basis functions for both predictors and responses. The proposed approach is further extended to the tensor version to accommodate multiple functional predictors. While allowing the dimensionality of the regression weight matrix or tensor to be relatively large, the mode-sparsity regularized model facilitates the multi-way shrinking of basis functions for each mode. The proposed mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity regularization. We develop an efficient algorithm to solve the problem, which works in an iterative update fashion, and converges to the global optimum. Furthermore, we analyze the generalization performance of the proposed method and derive an upper bound for the consistency between the recovered function and the underlying true function. The effectiveness of the proposed approach is verified on benchmark functional datasets in various domains.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {36},
numpages = {23},
keywords = {Function-on-function regression, mode-sparsity regularization}
}

@article{10.1145/3181707,
author = {van Leeuwen, Matthijs and Chau, Polo and Vreeken, Jilles and Shahaf, Dafna and Faloutsos, Christos},
title = {Editorial: TKDD Special Issue on Interactive Data Exploration and Analytics},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3181707},
doi = {10.1145/3181707},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {1},
numpages = {1}
}

@article{10.1145/3051127,
author = {Yang, Yang and Tang, Jie and Li, Juanzi},
title = {Learning to Infer Competitive Relationships in Heterogeneous Networks},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3051127},
doi = {10.1145/3051127},
abstract = {Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this article, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Overall, we find that similar entities tend to be competitors, with a probability of 4 times higher than chance. On the other hand, in social network, we also find a 10 minutes phenomenon: when two entities are mentioned by the same user within 10 minutes, the likelihood of them being competitors is 25 times higher than chance. Based on the discovered patterns, we propose a novel Topical Factor Graph Model. Generally, our model defines a latent topic layer to bridge the Twitter network and patent network. It then employs a semi-supervised learning algorithm to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46% improvement over alternative methods. Besides, we further demonstrate the competitive relationships inferred by our proposed model can be applied in the job-hopping prediction problem by achieving an average of +10.7% improvement.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {12},
numpages = {23},
keywords = {heterogeneous network, Social network, competitive relationship}
}

@article{10.1145/3070648,
author = {Kamat, Niranjan and Nandi, Arnab},
title = {A Session-Based Approach to Fast-But-Approximate Interactive Data Cube Exploration},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070648},
doi = {10.1145/3070648},
abstract = {With the proliferation of large datasets, sampling has become pervasive in data analysis. Sampling has numerous benefits—from reducing the computation time and cost to increasing the scope of interactive analysis. A popular task in data science, well-suited toward sampling, is the computation of fast-but-approximate aggregations over sampled data. Aggregation is a foundational block of data analysis, with data cube being its primary construct. We observe that such aggregation queries are typically issued in an ad-hoc, interactive setting. In contrast to one-off queries, a typical query session consists of a series of quick queries, interspersed with the user inspecting the results and formulating the next query. The similarity between session queries opens up opportunities for reusing computation of not just query results, but also error estimates. Error estimates need to be provided alongside sampled results for the results to be meaningful. We propose Sesame, a rewrite and caching framework that accelerates the entire interactive <underline>ses</underline>sion of aggregation queries over <underline>sam</underline>pl<underline>e</underline>d data. We focus on two unique and computationally expensive aspects of this use case: query speculation in the presence of sampling, and error computation, and provide novel strategies for result and error reuse. We demonstrate that our approach outperforms conventional sampled aggregation techniques by at least an order of magnitude, without modifying the underlying database.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {9},
numpages = {26},
keywords = {faceted exploration, interactive visualization, Error Reuse, session, aggregation}
}

@article{10.1145/3051126,
author = {Senin, Pavel and Lin, Jessica and Wang, Xing and Oates, Tim and Gandhi, Sunil and Boedihardjo, Arnold P. and Chen, Crystal and Frankenstein, Susan},
title = {GrammarViz 3.0: Interactive Discovery of Variable-Length Time Series Patterns},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3051126},
doi = {10.1145/3051126},
abstract = {The problems of recurrent and anomalous pattern discovery in time series, e.g., motifs and discords, respectively, have received a lot of attention from researchers in the past decade. However, since the pattern search space is usually intractable, most existing detection algorithms require that the patterns have discriminative characteristics and have its length known in advance and provided as input, which is an unreasonable requirement for many real-world problems. In addition, patterns of similar structure, but of different lengths may co-exist in a time series. Addressing these issues, we have developed algorithms for variable-length time series pattern discovery that are based on symbolic discretization and grammar inference—two techniques whose combination enables the structured reduction of the search space and discovery of the candidate patterns in linear time. In this work, we present GrammarViz 3.0—a software package that provides implementations of proposed algorithms and graphical user interface for interactive variable-length time series pattern discovery. The current version of the software provides an alternative grammar inference algorithm that improves the time series motif discovery workflow, and introduces an experimental procedure for automated discretization parameter selection that builds upon the minimum cardinality maximum cover principle and aids the time series recurrent and anomalous pattern discovery.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {10},
numpages = {28},
keywords = {Interactive data mining}
}

@article{10.1145/3023363,
author = {Shi, Lei and Tong, Hanghang and Daianu, Madelaine and Tian, Feng and Thompson, Paul M.},
title = {Visual Analysis of Brain Networks Using Sparse Regression Models},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3023363},
doi = {10.1145/3023363},
abstract = {Studies of the human brain network are becoming increasingly popular in the fields of neuroscience, computer science, and neurology. Despite this rapidly growing line of research, gaps remain on the intersection of data analytics, interactive visual representation, and the human intelligence—all needed to advance our understanding of human brain networks. This article tackles this challenge by exploring the design space of visual analytics. We propose an integrated framework to orchestrate computational models with comprehensive data visualizations on the human brain network. The framework targets two fundamental tasks: the visual exploration of multi-label brain networks and the visual comparison among brain networks across different subject groups. During the first task, we propose a novel interactive user interface to visualize sets of labeled brain networks; in our second task, we introduce sparse regression models to select discriminative features from the brain network to facilitate the comparison. Through user studies and quantitative experiments, both methods are shown to greatly improve the visual comparison performance. Finally, real-world case studies with domain experts demonstrate the utility and effectiveness of our framework to analyze reconstructions of human brain connectivity maps. The perceptually optimized visualization design and the feature selection model calibration are shown to be the key to our significant findings.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {5},
numpages = {30},
keywords = {feature selection, visual analysis, connectome, Brain network}
}

@article{10.1145/3047010,
author = {Makki, Raheleh and Carvalho, Eder and Soto, Axel J. and Brooks, Stephen and Oliveira, Maria Cristina Ferreira De and Milios, Evangelos and Minghim, Rosane},
title = {ATR-Vis: Visual and Interactive Information Retrieval for Parliamentary Discussions in Twitter},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3047010},
doi = {10.1145/3047010},
abstract = {The worldwide adoption of Twitter turned it into one of the most popular platforms for content analysis as it serves as a gauge of the public’s feeling and opinion on a variety of topics. This is particularly true of political discussions and lawmakers’ actions and initiatives. Yet, one common but unrealistic assumption is that the data of interest for analysis is readily available in a comprehensive and accurate form. Data need to be retrieved, but due to the brevity and noisy nature of Twitter content, it is difficult to formulate user queries that match relevant posts that use different terminology without introducing a considerable volume of unwanted content. This problem is aggravated when the analysis must contemplate multiple and related topics of interest, for which comments are being concurrently posted. This article presents Active Tweet Retrieval Visualization (ATR-Vis), a user-driven visual approach for the retrieval of Twitter content applicable to this scenario. The method proposes a set of active retrieval strategies to involve an analyst in such a way that a major improvement in retrieval coverage and precision is attained with minimal user effort. ATR-Vis enables non-technical users to benefit from the aforementioned active learning strategies by providing visual aids to facilitate the requested supervision. This supports the exploration of the space of potentially relevant tweets, and affords a better understanding of the retrieval results. We evaluate our approach in scenarios in which the task is to retrieve tweets related to multiple parliamentary debates within a specific time span. We collected two Twitter datasets, one associated with debates in the Canadian House of Commons during a particular week in May 2014, and another associated with debates in the Brazilian Federal Senate during a selected week in May 2015. The two use cases illustrate the effectiveness of ATR-Vis for the retrieval of relevant tweets, while quantitative results show that our approach achieves high retrieval quality with a modest amount of supervision. Finally, we evaluated our tool with three external users who perform searching in social media as part of their professional work.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {3},
numpages = {33},
keywords = {Information retrieval, active learning, visual analytics}
}

@article{10.1145/3007212,
author = {Galbrun, Esther and Miettinen, Pauli},
title = {Mining Redescriptions with Siren},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3007212},
doi = {10.1145/3007212},
abstract = {In many areas of science, scientists need to find distinct common characterizations of the same objects and, vice versa, to identify sets of objects that admit multiple shared descriptions. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions both in terms of the fauna that inhabits them and of their bioclimatic conditions. In data analysis, the task of automatically generating such alternative characterizations is called redescription mining.If a domain expert wants to use redescription mining in his research, merely being able to find redescriptions is not enough. He must also be able to understand the redescriptions found, adjust them to better match his domain knowledge, test alternative hypotheses with them, and guide the mining process toward results he considers interesting. To facilitate these goals, we introduce Siren, an interactive tool for mining and visualizing redescriptions.Siren allows to obtain redescriptions in an anytime fashion through efficient, distributed mining, to examine the results in various linked visualizations, to interact with the results either directly or via the visualizations, and to guide the mining algorithm toward specific redescriptions. In this article, we explain the features of Siren and why they are useful for redescription mining. We also propose two novel redescription mining algorithms that improve the generalizability of the results compared to the existing ones.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {6},
numpages = {30},
keywords = {Redescription mining, visual data mining, interactive data mining}
}

@article{10.1145/3047017,
author = {Wu, Hao and Sun, Maoyuan and Mi, Peng and Tatti, Nikolaj and North, Chris and Ramakrishnan, Naren},
title = {Interactive Discovery of Coordinated Relationship Chains with Maximum Entropy Models},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3047017},
doi = {10.1145/3047017},
abstract = {Modern visual analytic tools promote human-in-the-loop analysis but are limited in their ability to direct the user toward interesting and promising directions of study. This problem is especially acute when the analysis task is exploratory in nature, e.g., the discovery of potentially coordinated relationships in massive text datasets. Such tasks are very common in domains like intelligence analysis and security forensics where the goal is to uncover surprising coalitions bridging multiple types of relations. We introduce new maximum entropy models to discover surprising chains of relationships leveraging count data about entity occurrences in documents. These models are embedded in a visual analytic system called MERCER (Maximum Entropy Relational Chain ExploRer) that treats relationship bundles as first class objects and directs the user toward promising lines of inquiry. We demonstrate how user input can judiciously direct analysis toward valid conclusions, whereas a purely algorithmic approach could be led astray. Experimental results on both synthetic and real datasets from the intelligence community are presented.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {7},
numpages = {34},
keywords = {Maximum entropy models, multi-relational pattern mining, interactive visual data exploration}
}

@article{10.1145/3047011,
author = {Rayar, Fr\'{e}d\'{e}ric and Barrat, Sabine and Bouali, Fatma and Venturini, Gilles},
title = {A Viewable Indexing Structure for the Interactive Exploration of Dynamic and Large Image Collections},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3047011},
doi = {10.1145/3047011},
abstract = {Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, there is a lack of support for exploratory data mining. One may want to wander around the images and experience serendipity in the exploration process. Thus, effective paradigms not only for organising, but also visualising these image collections become necessary. In this article, we present a study to jointly index and visualise large image collections. The work focuses on satisfying three constraints. First, large image collections, up to million of images, shall be handled. Second, dynamic collections, such as ever-growing collections, shall be processed in an incremental way, without reprocessing the whole collection at each modification. Finally, an intuitive and interactive exploration system shall be provided to the user to allow him to easily mine image collections. To this end, a data partitioning algorithm has been modified and proximity graphs have been used to fit the visualisation purpose. A custom web platform has been implemented to visualise the hierarchical and graph-based hybrid structure. The results of a user evaluation we have conducted show that the exploration of the collections is intuitive and smooth thanks to the proposed structure. Furthermore, the scalability of the proposed indexing method is proved using large public image collections.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {2},
numpages = {26},
keywords = {dynamic collections, large image collections, Interactive visualisation, incremental construction}
}

@article{10.1145/3022186,
author = {Lim, Yongsub and Jung, Minsoo and Kang, U.},
title = {Memory-Efficient and Accurate Sampling for Counting Local Triangles in Graph Streams: From Simple to Multigraphs},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022186},
doi = {10.1145/3022186},
abstract = {How can we estimate local triangle counts accurately in a graph stream without storing the whole graph? How to handle duplicated edges in local triangle counting for graph stream? Local triangle counting, which computes the number of triangles attached to each node in a graph, is a very important problem with wide applications in social network analysis, anomaly detection, web mining, and the like.In this article, we propose algorithms for local triangle counting in a graph stream based on edge sampling: Mascot for a simple graph, and MultiBMascot and MultiWMascot for a multigraph. To develop Mascot, we first present two naive local triangle counting algorithms in a graph stream, called Mascot-C and Mascot-A. Mascot-C is based on constant edge sampling, and Mascot-A improves its accuracy by utilizing more memory spaces. Mascot achieves both accuracy and memory-efficiency of the two algorithms by unconditional triangle counting for a new edge, regardless of whether it is sampled or not. Extending the idea to a multigraph, we develop two algorithms MultiBMascot and MultiWMascot. MultiBMascot enables local triangle counting on the corresponding simple graph of a streamed multigraph without explicit graph conversion; MultiWMascot considers repeated occurrences of an edge as its weight and counts each triangle as the product of its three edge weights. In contrast to the existing algorithm that requires prior knowledge on the target graph and appropriately set parameters, our proposed algorithms require only one parameter of edge sampling probability.Through extensive experiments, we show that for the same number of edges sampled, Mascot provides the best accuracy compared to the existing algorithm as well as Mascot-C and Mascot-A. We also demonstrate that MultiBMascot on a multigraph is comparable to Mascot-C on the counterpart simple graph, and MultiWMascot becomes more accurate for higher degree nodes. Thanks to Mascot, we also discover interesting anomalous patterns in real graphs, including core-peripheries in the web, a bimodal call pattern in a phone call history, and intensive collaboration in DBLP.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {4},
numpages = {28},
keywords = {anomaly detection, Local triangle counting, graph stream mining, edge sampling}
}

@article{10.1145/3070616,
author = {Choo, Jaegul and Kim, Hannah and Clarkson, Edward and Liu, Zhicheng and Lee, Changhyun and Li, Fuxin and Lee, Hanseung and Kannan, Ramakrishnan and Stolper, Charles D. and Stasko, John and Park, Haesun},
title = {VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070616},
doi = {10.1145/3070616},
abstract = {In this article, we present an interactive visual information retrieval and recommendation system, called VisIRR, for large-scale document discovery. VisIRR effectively combines the paradigms of (1) a passive pull through query processes for retrieval and (2) an active push that recommends items of potential interest to users based on their preferences. Equipped with an efficient dynamic query interface against a large-scale corpus, VisIRR organizes the retrieved documents into high-level topics and visualizes them in a 2D space, representing the relationships among the topics along with their keyword summary. In addition, based on interactive personalized preference feedback with regard to documents, VisIRR provides document recommendations from the entire corpus, which are beyond the retrieved sets. Such recommended documents are visualized in the same space as the retrieved documents, so that users can seamlessly analyze both existing and newly recommended ones. This article presents novel computational methods, which make these integrated representations and fast interactions possible for a large-scale document corpus. We illustrate how the system works by providing detailed usage scenarios. Additionally, we present preliminary user study results for evaluating the effectiveness of the system.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {8},
numpages = {20},
keywords = {clustering, topic modeling, dimension reduction, information retrieval, Recommendation}
}

@article{10.1145/3106369,
author = {Trevi\~{n}o, Edgar S. Garc\'{\i}a and Hameed, Muhammad Zaid and Barria, Javier A},
title = {Data Stream Evolution Diagnosis Using Recursive Wavelet Density Estimators},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3106369},
doi = {10.1145/3106369},
abstract = {Data streams are a new class of data that is becoming pervasively important in a wide range of applications, ranging from sensor networks, environmental monitoring to finance. In this article, we propose a novel framework for the online diagnosis of evolution of multidimensional streaming data that incorporates Recursive Wavelet Density Estimators into the context of Velocity Density Estimation. In the proposed framework changes in streaming data are characterized by the use of local and global evolution coefficients. In addition, we propose for the analysis of changes in the correlation structure of the data a recursive implementation of the Pearson correlation coefficient using exponential discounting. Two visualization tools, namely temporal and spatial velocity profiles, are extended in the context of the proposed framework. These are the three main advantages of the proposed method over previous approaches: (1) the memory storage required is minimal and independent of any window size; (2) it has a significantly lower computational complexity; and (3) it makes possible the fast diagnosis of data evolution at all dimensions and at relevant combinations of dimensions with only one pass of the data. With the help of the four examples, we show the framework’s relevance in a change detection context and its potential capability for real world applications.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {14},
numpages = {28},
keywords = {data stream evolution diagnosis, Data streams mining, velocity density estimation, incremental statistics}
}

@article{10.1145/3127876,
author = {Chiasserini, Carla-Fabiana and Garetto, Michel and Leonardi, Emili},
title = {De-Anonymizing Clustered Social Networks by Percolation Graph Matching},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3127876},
doi = {10.1145/3127876},
abstract = {Online social networks offer the opportunity to collect a huge amount of valuable information about billions of users. The analysis of this data by service providers and unintended third parties are posing serious treats to user privacy. In particular, recent work has shown that users participating in more than one online social network can be identified based only on the structure of their links to other users. An effective tool to de-anonymize social network users is represented by graph matching algorithms. Indeed, by exploiting a sufficiently large set of seed nodes, a percolation process can correctly match almost all nodes across the different social networks. In this article, we show the crucial role of clustering, which is a relevant feature of social network graphs (and many other systems). Clustering has both the effect of making matching algorithms more prone to errors, and the potential to greatly reduce the number of seeds needed to trigger percolation. We show these facts by considering a fairly general class of random geometric graphs with variable clustering level. We assume that seeds can be identified in particular sub-regions of the network graph, while no a priori knowledge about the location of the other nodes is required. Under these conditions, we show how clever algorithms can achieve surprisingly good performance while limiting the number of matching errors.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {21},
numpages = {39},
keywords = {Graph matching, de-anonymization, bootstrap percolation}
}

@article{10.1145/3154414,
author = {Shin, Kijung and Hooi, Bryan and Faloutsos, Christos},
title = {Fast, Accurate, and Flexible Algorithms for Dense Subtensor Mining},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154414},
doi = {10.1145/3154414},
abstract = {Given a large-scale and high-order tensor, how can we detect dense subtensors in it? Can we spot them in near-linear time but with quality guarantees? Extensive previous work has shown that dense subtensors, as well as dense subgraphs, indicate anomalous or fraudulent behavior (e.g., lockstep behavior in social networks). However, available algorithms for detecting dense subtensors are not satisfactory in terms of speed, accuracy, and flexibility. In this work, we propose two algorithms, called M-Zoom and M-Biz, for fast and accurate dense-subtensor detection with various density measures. M-Zoom gives a lower bound on the density of detected subtensors, while M-Biz guarantees the local optimality of detected subtensors. M-Zoom and M-Biz can be combined, giving the following advantages: (1) Scalable: scale near-linearly with all aspects of tensors and are up to 114\texttimes{} faster than state-of-the-art methods with similar accuracy, (2) Provably accurate: provide a guarantee on the lowest density and local optimality of the subtensors they find, (3) Flexible: support multi-subtensor detection and size bounds as well as diverse density measures, and (4) Effective: successfully detected edit wars and bot activities in Wikipedia, and spotted network attacks from a TCP dump with near-perfect accuracy (AUC = 0.98).},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {28},
numpages = {30},
keywords = {anomaly detection, fraud detection, Tensor, dense subtensor}
}

@article{10.1145/3127875,
author = {Zhao, Wayne Xin and Fan, Feifan and Wen, Ji-Rong and Chang, Edward Y.},
title = {Joint Representation Learning for Location-Based Social Networks with Multi-Grained Sequential Contexts},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3127875},
doi = {10.1145/3127875},
abstract = {This article studies the problem of learning effective representations for Location-Based Social Networks (LBSN), which is useful in many tasks such as location recommendation and link prediction. Existing network embedding methods mainly focus on capturing topology patterns reflected in social connections, while check-in sequences, the most important data type in LBSNs, are not directly modeled by these models. In this article, we propose a representation learning method for LBSNs called as JRLM++, which models check-in sequences together with social connections. To capture sequential relatedness, JRLM++ characterizes two levels of sequential contexts, namely fine-grained and coarse-grained contexts. We present a learning algorithm tailored to the hierarchical architecture of the proposed model. We conduct extensive experiments on two important applications using real-world datasets. The experimental results demonstrate the superiority of our model. The proposed model can generate effective representations for both users and locations in the same embedding space, which can be further utilized to improve multiple LBSN tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {22},
numpages = {21},
keywords = {contextual information, distributed representation, location recommendation, Check-in sequences, social link prediction}
}

@article{10.1145/3127873,
author = {HU, Guang-Neng and Dai, Xin-Yu and Qiu, Feng-Yu and Xia, Rui and Li, Tao and Huang, Shu-Jian and Chen, Jia-Jun},
title = {Collaborative Filtering with Topic and Social Latent Factors Incorporating Implicit Feedback},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3127873},
doi = {10.1145/3127873},
abstract = {Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized items for different users. Latent factors-based collaborative filtering (CF) has become the popular approaches for RSs due to its accuracy and scalability. Recently, online social networks and user-generated content provide diverse sources for recommendation beyond ratings. Although social matrix factorization (Social MF) and topic matrix factorization (Topic MF) successfully exploit social relations and item reviews, respectively; both of them ignore some useful information. In this article, we investigate the effective data fusion by combining the aforementioned approaches. First, we propose a novel model MR3 to jointly model three sources of information (i.e., ratings, item reviews, and social relations) effectively for rating prediction by aligning the latent factors and hidden topics. Second, we incorporate the implicit feedback from ratings into the proposed model to enhance its capability and to demonstrate its flexibility. We achieve more accurate rating prediction on real-life datasets over various state-of-the-art methods. Furthermore, we measure the contribution from each of the three data sources and the impact of implicit feedback from ratings, followed by the sensitivity analysis of hyperparameters. Empirical studies demonstrate the effectiveness and efficacy of our proposed model and its extension.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {23},
numpages = {30},
keywords = {implicit feedback, collaborative filtering, hidden topics, latent social factors, Recommendation systems}
}

@article{10.1145/3092690,
author = {Wang, Boyue and Hu, Yongli and Gao, Junbin and Sun, Yanfeng and Yin, Baocai},
title = {Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3092690},
doi = {10.1145/3092690},
abstract = {Clustering is one of the fundamental topics in data mining and pattern recognition. As a prospective clustering method, the subspace clustering has made considerable progress in recent researches, e.g., sparse subspace clustering (SSC) and low rank representation (LRR). However, most existing subspace clustering algorithms are designed for vectorial data from linear spaces, thus not suitable for high-dimensional data with intrinsic non-linear manifold structure. For high-dimensional or manifold data, few research pays attention to clustering problems. The purpose of clustering on manifolds tends to cluster manifold-valued data into several groups according to the mainfold-based similarity metric. This article proposes an extended LRR model for manifold-valued Grassmann data that incorporates prior knowledge by minimizing partial sum of singular values instead of the nuclear norm, namely Partial Sum minimization of Singular Values Representation (GPSSVR). The new model not only enforces the global structure of data in low rank, but also retains important information by minimizing only smaller singular values. To further maintain the local structures among Grassmann points, we also integrate the Laplacian penalty with GPSSVR. The proposed model and algorithms are assessed on a public human face dataset, some widely used human action video datasets and a real scenery dataset. The experimental results show that the proposed methods obviously outperform other state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {13},
numpages = {22},
keywords = {Grassmann manifolds, subspace clustering, Low rank representation, Laplacian matrix, partial sum minimization of singular values}
}

@article{10.1145/3154411,
author = {Belcastro, Loris and Marozzo, Fabrizio and Talia, Domenico and Trunfio, Paolo},
title = {G-RoI: Automatic Region-of-Interest Detection Driven by Geotagged Social Media Data},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154411},
doi = {10.1145/3154411},
abstract = {Geotagged data gathered from social media can be used to discover interesting locations visited by users called Places-of-Interest (PoIs). Since a PoI is generally identified by the geographical coordinates of a single point, it is hard to match it with user trajectories. Therefore, it is useful to define an area, called Region-of-Interest (RoI), to represent the boundaries of the PoI’s area. RoI mining techniques are aimed at discovering ROIs from PoIs and other data. Existing RoI mining techniques are based on three main approaches: predefined shapes, density-based clustering, and grid-based aggregation. This article proposes G-RoI, a novel RoI mining technique that exploits the indications contained in geotagged social media items to discover RoIs with a high accuracy. Experiments performed over a set of PoIs in Rome and Paris using social media geotagged data, demonstrate that G-RoI in most cases achieves better results than existing techniques. In particular, the mean F1 score is 0.34 higher than that obtained with the well-known DBSCAN algorithm in Rome RoIs and 0.23 higher in Paris RoIs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {27},
numpages = {22},
keywords = {Social network analysis, Geotagged social media, Regions-of-interest, RoI mining, Places-of-Interest}
}

@article{10.1145/3132088,
author = {Pandove, Divya and Goel, Shivan and Rani, Rinkl},
title = {Systematic Review of Clustering High-Dimensional and Large Datasets},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3132088},
doi = {10.1145/3132088},
abstract = {Technological advancement has enabled us to store and process huge amount of data in relatively short spans of time. The nature of data is rapidly changing, particularly its dimensionality is more commonly multi- and high-dimensional. There is an immediate need to expand our focus to include analysis of high-dimensional and large datasets. Data analysis is becoming a mammoth task, due to incremental increase in data volume and complexity in terms of heterogony of data. It is due to this dynamic computing environment that the existing techniques either need to be modified or discarded to handle new data in multiple high-dimensions. Data clustering is a tool that is used in many disciplines, including data mining, so that meaningful knowledge can be extracted from seemingly unstructured data. The aim of this article is to understand the problem of clustering and various approaches addressing this problem. This article discusses the process of clustering from both microviews (data treating) and macroviews (overall clustering process). Different distance and similarity measures, which form the cornerstone of effective data clustering, are also identified. Further, an in-depth analysis of different clustering approaches focused on data mining, dealing with large-scale datasets is given. These approaches are comprehensively compared to bring out a clear differentiation among them. This article also surveys the problem of high-dimensional data and the existing approaches, that makes it more relevant. It also explores the latest trends in cluster analysis, and the real-life applications of this concept. This survey is exhaustive as it tries to cover all the aspects of clustering in the field of data mining.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {16},
numpages = {68},
keywords = {data clustering applications, data clustering process, large scale data mining, Cluster analysis, clustering tendency, dimensionality reduction}
}

@article{10.1145/3161886,
author = {Chen, Guangyong and Zhu, Fengyuan and Heng, Pheng Ann},
title = {Large-Scale Bayesian Probabilistic Matrix Factorization with Memo-Free Distributed Variational Inference},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3161886},
doi = {10.1145/3161886},
abstract = {Bayesian Probabilistic Matrix Factorization (BPMF) is a powerful model in many dyadic data prediction problems, especially the applications of Recommender system. However, its poor scalability has limited its wide applications on massive data. Based on the conditional independence property of observed entries in BPMF model, we propose a novel distributed memo-free variational inference method for large-scale matrix factorization problems. Compared with the state-of-the-art methods, the proposed method is favored for several attractive properties. Specifically, it does not require tuning of learning rate carefully, shuffling the training set at each iteration, or storing massive redundant variables, and can introduce new agents into the computations on the fly. We conduct extensive experiments on both synthetic and real-world datasets. The experimental results show that our method can converge significantly faster with better prediction performance than alternative algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {31},
numpages = {24},
keywords = {memo-free variational inference, Recommender system, large-scale, BPMF, distributed computation}
}

@article{10.1145/3154401,
author = {Liang, Jiongqian and Ajwani, Deepak and Nicholson, Patrick K. and Sala, Alessandra and Parthasarathy, Srinivasan},
title = {Prioritized Relationship Analysis in Heterogeneous Information Networks},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154401},
doi = {10.1145/3154401},
abstract = {An increasing number of applications are modeled and analyzed in network form, where nodes represent entities of interest and edges represent interactions or relationships between entities. Commonly, such relationship analysis tools assume homogeneity in both node type and edge type. Recent research has sought to redress the assumption of homogeneity and focused on mining heterogeneous information networks (HINs) where both nodes and edges can be of different types. Building on such efforts, in this work, we articulate a novel approach for mining relationships across entities in such networks while accounting for user preference over relationship type and interestingness metric. We formalize the problem as a top-k lightest paths problem, contextualized in a real-world communication network, and seek to find the k most interesting path instances matching the preferred relationship type. Our solution, PROphetic HEuristic Algorithm for Path Searching (PRO-HEAPS), leverages a combination of novel graph preprocessing techniques, well-designed heuristics and the venerable A* search algorithm. We run our algorithm on real-world large-scale graphs and show that our algorithm significantly outperforms a wide variety of baseline approaches with speedups as large as 100X.To widen the range of applications, we also extend PRO-HEAPS to (i) support relationship analysis between two groups of entities and (ii) allow pattern path in the query to contain logical statements with operators AND, OR, NOT, and wild-card “.”. We run experiments using this generalized version of PRO-HEAPS and demonstrate that the advantage of PRO-HEAPS becomes even more pronounced for these general cases. Furthermore, we conduct a comprehensive analysis to study how the performance of PRO-HEAPS varies with respect to various attributes of the input HIN. We finally conduct a case study to demonstrate valuable applications of our algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {29},
numpages = {27},
keywords = {semantic relationship queries, Heterogeneous information networks, graph algorithms}
}

@article{10.1145/3110215,
author = {Ramezani, Maryam and Khodadadi, Ali and Rabiee, Hamid R.},
title = {Community Detection Using Diffusion Information},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3110215},
doi = {10.1145/3110215},
abstract = {Community detection in social networks has become a popular topic of research during the last decade. There exist a variety of algorithms for modularizing the network graph into different communities. However, they mostly assume that partial or complete information of the network graphs are available that is not feasible in many cases. In this article, we focus on detecting communities by exploiting their diffusion information. To this end, we utilize the Conditional Random Fields (CRF) to discover the community structures. The proposed method, community diffusion (CoDi), does not require any prior knowledge about the network structure or specific properties of communities. Furthermore, in contrast to the structure-based community detection methods, this method is able to identify the hidden communities. The experimental results indicate considerable improvements in detecting communities based on accuracy, scalability, and real cascade information measures.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {20},
numpages = {22},
keywords = {Social networks, community detection, conditional random field, information diffusion, social influence}
}

@article{10.1145/3154399,
author = {Huang, Hong and Dong, Yuxiao and Tang, Jie and Yang, Hongxia and Chawla, Nitesh V. and Fu, Xiaoming},
title = {Will Triadic Closure Strengthen Ties in Social Networks?},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154399},
doi = {10.1145/3154399},
abstract = {The social triad—a group of three people—is one of the simplest and most fundamental social groups. Extensive network and social theories have been developed to understand its structure, such as triadic closure and social balance. Over the course of a triadic closure—the transition from two ties to three among three users, the strength dynamics of its social ties, however, are much less well understood. Using two dynamic networks from social media and mobile communication, we examine how the formation of the third tie in a triad affects the strength of the existing two ties. Surprisingly, we find that in about 80% social triads, the strength of the first two ties is weakened although averagely the tie strength in the two networks maintains an increasing or stable trend. We discover that (1) the decrease in tie strength among three males is more sharply than that among females, and (2) the tie strength between celebrities is more likely to be weakened as the closure of a triad than those between ordinary people. Furthermore, we formalize a triadic tie strength dynamics prediction problem to infer whether social ties of a triad will become weakened after its closure. We propose a TRIST method—a kernel density estimation (KDE)-based graphical model—to solve the problem by incorporating user demographics, temporal effects, and structural information. Extensive experiments demonstrate that TRIST offers a greater than 82% potential predictability for inferring triadic tie strength dynamics in both networks. The leveraging of the KDE and structural correlations enables TRIST to outperform baselines by up to 30% in terms of F1-score.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {30},
numpages = {25},
keywords = {dynamics, Social triad, predictive model, tie strength}
}

@article{10.1145/3139240,
author = {Bonab, Hamed R. and Can, Fazli},
title = {GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3139240},
doi = {10.1145/3139240},
abstract = {Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem, which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the Massive Online Analysis (MOA) libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with eight state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {25},
numpages = {33},
keywords = {geometry of voting, concept drift, spatial modeling for online ensembles, Ensemble classifier, evolving data stream, least squares, dynamic weighting}
}

@article{10.1145/3070645,
author = {Kaushal, Vishal and Patwardhan, Manasi},
title = {Emerging Trends in Personality Identification Using Online Social Networks—A Literature Survey},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070645},
doi = {10.1145/3070645},
abstract = {Personality is a combination of all the attributes—behavioral, temperamental, emotional, and mental—that characterizes a unique individual. Ability to identify personalities of people has always been of great interest to the researchers due to its importance. It continues to find highly useful applications in many domains. Owing to the increasing popularity of online social networks, researchers have started looking into the possibility of predicting a user's personality from his online social networking profile, which serves as a rich source of textual as well as non-textual content published by users. In the process of creating social networking profiles, users reveal a lot about themselves both in what they share and how they say it. Studies suggest that the online social networking websites are, in fact, a relevant and valid means of communicating personality. In this article, we review these various studies reported in literature toward identification of personality using online social networks. To the best of our knowledge, this is the first reported survey of its kind at the time of submission. We hope that our contribution, especially in summarizing the previous findings and in identifying the directions for future research in this area, would encourage researchers to do more work in this budding area.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {15},
numpages = {30},
keywords = {personality prediction, mining social network, online social network, Facebook, Personality, Twitter}
}

@article{10.1145/3154417,
author = {Xie, Hong and Ma, Richard T. B. and Lui, John C. S.},
title = {Enhancing Reputation via Price Discounts in E-Commerce Systems: A Data-Driven Approach},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3154417},
doi = {10.1145/3154417},
abstract = {Reputation systems have become an indispensable component of modern E-commerce systems, as they help buyers make informed decisions in choosing trustworthy sellers. To attract buyers and increase the transaction volume, sellers need to earn reasonably high reputation scores. This process usually takes a substantial amount of time. To accelerate this process, sellers can provide price discounts to attract users, but the underlying difficulty is that sellers have no prior knowledge on buyers’ preferences over price discounts. In this article, we develop an online algorithm to infer the optimal discount rate from data. We first formulate an optimization framework to select the optimal discount rate given buyers’ discount preferences, which is a tradeoff between the short-term profit and the ramp-up time (for reputation). We then derive the closed-form optimal discount rate, which gives us key insights in applying a stochastic bandits framework to infer the optimal discount rate from the transaction data with regret upper bounds. We show that the computational complexity of evaluating the performance metrics is infeasibly high, and therefore, we develop efficient randomized algorithms with guaranteed performance to approximate them. Finally, we conduct experiments on a dataset crawled from eBay. Experimental results show that our framework can trade 60% of the short-term profit for reducing the ramp-up time by 40%. This reduction in the ramp-up time can increase the long-term profit of a seller by at least 20%.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {26},
numpages = {29},
keywords = {stochastic bandits, reputation systems, randomized algorithms, price discounts, E-commerce}
}

@article{10.1145/3110216,
author = {Long, Cheng and Wong, Raymond Chi-Wing and Wei, Victor Junqiu},
title = {Profit Maximization with Sufficient Customer Satisfactions},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3110216},
doi = {10.1145/3110216},
abstract = {In many commercial campaigns, we observe that there exists a tradeoff between the number of customers satisfied by the company and the profit gained. Merely satisfying as many customers as possible or maximizing the profit is not desirable. To this end, in this article, we propose a new problem called k-<underline>S</underline>atisfiability <underline>A</underline>ssignment for <underline>M</underline>aximizing the <underline>P</underline>rofit (k-SAMP), where k is a user parameter and a non-negative integer. Given a set P of products and a set O of customers, k-SAMP is to find an assignment between P and O such that at least k customers are satisfied in the assignment and the profit incurred by this assignment is maximized. Although we find that this problem is closely related to two classic computer science problems, namely maximum weight matching and maximum matching, the techniques developed for these classic problems cannot be adapted to our k-SAMP problem. In this work, we design a novel algorithm called Adjust for the k-SAMP problem. Given an assignment A, Adjust iteratively increases the profit of A by adjusting some appropriate matches in A while keeping at least k customers satisfied in A. We prove that Adjust returns a global optimum. Extensive experiments were conducted that verified the efficiency of Adjust.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {19},
numpages = {34},
keywords = {Assignment, users’ satisfactions, profit maximization}
}

@article{10.1145/3106370,
author = {Li, Yixuan and He, Kun and Kloster, Kyle and Bindel, David and Hopcroft, John},
title = {Local Spectral Clustering for Overlapping Community Detection},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3106370},
doi = {10.1145/3106370},
abstract = {Large graphs arise in a number of contexts and understanding their structure and extracting information from them is an important research area. Early algorithms for mining communities have focused on global graph structure, and often run in time proportional to the size of the entire graph. As we explore networks with millions of vertices and find communities of size in the hundreds, it becomes important to shift our attention from macroscopic structure to microscopic structure in large networks. A growing body of work has been adopting local expansion methods in order to identify communities from a few exemplary seed members.In this article, we propose a novel approach for finding overlapping communities called Lemon (Local Expansion via Minimum One Norm). Provided with a few known seeds, the algorithm finds the community by performing a local spectral diffusion. The core idea of Lemon is to use short random walks to approximate an invariant subspace near a seed set, which we refer to as local spectra. Local spectra can be viewed as the low-dimensional embedding that captures the nodes’ closeness in the local network structure. We show that Lemon’s performance in detecting communities is competitive with state-of-the-art methods. Moreover, the running time scales with the size of the community rather than that of the entire graph. The algorithm is easy to implement and is highly parallelizable. We further provide theoretical analysis of the local spectral properties, bounding the measure of tightness of extracted community using the eigenvalues of graph Laplacian.We thoroughly evaluate our approach using both synthetic and real-world datasets across different domains, and analyze the empirical variations when applying our method to inherently different networks in practice. In addition, the heuristics on how the seed set quality and quantity would affect the performance are provided.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {17},
numpages = {27},
keywords = {Community detection, local spectral clustering, graph diffusion, random walk, seed set expansion}
}

@article{10.1145/3139241,
author = {Perozzi, Bryan and Akoglu, Leman},
title = {Discovering Communities and Anomalies in Attributed Graphs: Interactive Visual Exploration and Summarization},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3139241},
doi = {10.1145/3139241},
abstract = {Given a network with node attributes, how can we identify communities and spot anomalies? How can we characterize, describe, or summarize the network in a succinct way? Community extraction requires a measure of quality for connected subgraphs (e.g., social circles). Existing subgraph measures, however, either consider only the connectedness of nodes inside the community and ignore the cross-edges at the boundary (e.g., density) or only quantify the structure of the community and ignore the node attributes (e.g., conductance). In this work, we focus on node-attributed networks and introduce: (1) a new measure of subgraph quality for attributed communities called normality, (2) a community extraction algorithm that uses normality&nbsp;to extract communities and a few characterizing attributes per community, and (3) a summarization and interactive visualization approach for attributed graph exploration. More specifically, (1) we first introduce a new measure to quantify the normality of an attributed subgraph. Our normality&nbsp;measure carefully utilizes structure and attributes together to quantify both the internal consistency and external separability. We then formulate an objective function to automatically infer a few attributes (called the “focus”) and respective attribute weights, so as to maximize the normality&nbsp; score of a given subgraph. Most notably, unlike many other approaches, our measure allows for many cross-edges as long as they can be “exonerated;” i.e., either (i) are expected under a null graph model, and/or (ii) their boundary nodes do not exhibit the focus attributes. Next, (2) we propose AMEN (for Attributed Mining of Entity Networks), an algorithm that simultaneously discovers the communities and their respective focus in a given graph, with a goal to maximize the total normality. Communities for which a focus that yields high normality&nbsp; cannot be found are considered low quality or anomalous. Last, (3) we formulate a summarization task with a multi-criteria objective, which selects a subset of the communities that (i) cover the entire graph well, are (ii) high quality and (iii) diverse in their focus attributes. We further design an interactive visualization interface that presents the communities to a user in an interpretable, user-friendly fashion. The user can explore all the communities, analyze various algorithm-generated summaries, as well as devise their own summaries interactively to characterize the network in a succinct way. As the experiments on real-world attributed graphs show, our proposed approaches effectively find anomalous communities and outperform several existing measures and methods, such as conductance, density, OddBall, and SODA. We also conduct extensive user studies to measure the capability and efficiency that our approach provides to the users toward network summarization, exploration, and sensemaking.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {24},
numpages = {40},
keywords = {summarization, social circles, Attributed graphs, human-in-the-loop, visual analytics, community extraction, network measures, anomaly mining, interaction design, ego networks}
}

@article{10.1145/3106368,
author = {Costa, Gianni and Ortale, Riccardo},
title = {Mining Overlapping Communities and Inner Role Assignments through Bayesian Mixed-Membership Models of Networks with Context-Dependent Interactions},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3106368},
doi = {10.1145/3106368},
abstract = {Community discovery and role assignment have been recently integrated into an unsupervised approach for the exploratory analysis of overlapping communities and inner roles in networks. However, the formation of ties in these prototypical research efforts is not truly realistic, since it does not account for a fundamental aspect of link establishment in real-world networks, i.e., the explicative reasons that cause interactions among nodes. Such reasons can be interpreted as generic requirements of nodes, that are met by other nodes and essentially pertain both to the nodes themselves and to their interaction contexts (i.e., the respective communities and roles).In this article, we present two new model-based machine-learning approaches, wherein community discovery and role assignment are seamlessly integrated and simultaneously performed through approximate posterior inference in Bayesian mixed-membership models of directed networks. The devised models account for the explicative reasons governing link establishment in terms of node-specific and contextual latent interaction factors. The former are inherently characteristic of nodes, while the latter are characterizations of nodes in the context of the individual communities and roles. The generative process of both models assigns nodes to communities with respective roles and connects them through directed links, which are probabilistically governed by their node-specific and contextual interaction factors. The difference between the proposed models lies in the exploitation of the contextual interaction factors. More precisely, in one model, the contextual interaction factors have the same impact on link generation. In the other model, the contextual interaction factors are weighted by the extent of involvement of the linked nodes in the respective communities and roles.We develop MCMC algorithms implementing approximate posterior inference and parameter estimation within our models.Finally, we conduct an intensive comparative experimentation, which demonstrates their superiority in community compactness and link prediction on various real-world and synthetic networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {18},
numpages = {32},
keywords = {Bayesian probabilistic network analysis, Overlapping community detection, role assignment, link prediction}
}

@article{10.1145/3047009,
author = {Datta, Srayan and Adar, Eytan},
title = {C<span class="smallcaps SmallerCapital">ommunity</span>D<span class="smallcaps SmallerCapital">iff</span>: Visualizing Community Clustering Algorithms},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3047009},
doi = {10.1145/3047009},
abstract = {Community detection is an oft-used analytical function of network analysis but can be a black art to apply in practice. Grouping of related nodes is important for identifying patterns in network datasets but also notoriously sensitive to input data and algorithm selection. This is further complicated by the fact that, depending on domain and use case, the ground truth knowledge of the end-user can vary from none to complete. In this work, we present CommunityDiff, an interactive visualization system that combines visualization and active learning (AL) to support the end-user’s analytical process. As the end-user interacts with the system, a continuous refinement process updates both the community labels and visualizations. CommunityDiff features a mechanism for visualizing ensemble spaces, weighted combinations of algorithm output, that can identify patterns, commonalities, and differences among multiple community detection algorithms. Among other features, CommunityDiff introduces an AL mechanism that visually indicates uncertainty about community labels to focus end-user attention and supporting end-user control that ranges from explicitly indicating the number of expected communities to merging and splitting communities. Based on this end-user input, CommunityDiff dynamically recalculates communities. We demonstrate the viability of our through a study of speed of end-user convergence on satisfactory community labels. As part of building CommunityDiff, we describe a design process that can be adapted to other Interactive Machine Learning applications.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {11},
numpages = {34},
keywords = {Community detection, visualization, interactive machine learning}
}

@article{10.1145/3059214,
author = {Zhang, Tianyang and Cui, Peng and Faloutsos, Christos and Lu, Yunfei and Ye, Hao and Zhu, Wenwu and Yang, Shiqiang},
title = {<span class="smallcaps SmallerCapital">come</span>N<span class="smallcaps SmallerCapital">go</span>: A Dynamic Model for Social Group Evolution},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3059214},
doi = {10.1145/3059214},
abstract = {How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups?In this article, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore, we propose a new model comeNgo, a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) Unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) Succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) Accuracy: it can capture various kinds of group evolution patterns preciously, and the goodness of fit increases by 58% over baseline. (d) Usefulness: it can be used in multiple application scenarios, such as forecasting and pattern discovery. Furthermore, our model can provide insights about different evolution patterns of social groups, and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {41},
numpages = {22},
keywords = {dynamic model, temporal patterns, Group evolution}
}

@article{10.1145/3046947,
author = {Anderson, Ashton and Kleinberg, Jon and Mullainathan, Sendhil},
title = {Assessing Human Error Against a Benchmark of Perfection},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046947},
doi = {10.1145/3046947},
abstract = {An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors.To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging for even the best players in the world.We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difficulty of the decision. We identify rich structure in all three of these categories of features, and find strong evidence that in our domain, features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {45},
numpages = {25},
keywords = {Blunder prediction, human decision-making}
}

@article{10.1145/3046945,
author = {Algizawy, Essam and Ogawa, Tetsuji and El-Mahdy, Ahmed},
title = {Real-Time Large-Scale Map Matching Using Mobile Phone Data},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046945},
doi = {10.1145/3046945},
abstract = {With the wide spread use of mobile phones, cellular mobile big data is becoming an important resource that provides a wealth of information with almost no cost. However, the data generally suffers from relatively high spatial granularity, limiting the scope of its application. In this article, we consider, for the first time, the utility of actual mobile big data for map matching allowing for “microscopic” level traffic analysis. The state-of-the-art in map matching generally targets GPS data, which provides far denser sampling and higher location resolution than the mobile data. Our approach extends the typical Hidden-Markov model used in map matching to accommodate for highly sparse location trajectories, exploit the large mobile data volume to learn the model parameters, and exploit the sparsity of the data to provide for real-time Viterbi processing. We study an actual, anonymised mobile trajectories data set of the city of Dakar, Senegal, spanning a year, and generate a corresponding road-level traffic density, at an hourly granularity, for each mobile trajectory. We observed a relatively high correlation between the generated traffic intensities and corresponding values obtained by the gravity and equilibrium models typically used in mobility analysis, indicating the utility of the approach as an alternative means for traffic analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {52},
numpages = {38},
keywords = {low cost, fine-grained spatial tracking, cellular duration records, Mobile big data, adaptive HMM}
}

@article{10.1145/3070644,
author = {Vosoughi, Soroush and Mohsenvand, Mostafa ‘Neo’ and Roy, Deb},
title = {Rumor Gauge: Predicting the Veracity of Rumors on Twitter},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070644},
doi = {10.1145/3070644},
abstract = {The spread of malicious or accidental misinformation in social media, especially in time-sensitive situations, such as real-world emergencies, can have harmful effects on individuals and society. In this work, we developed models for automated verification of rumors (unverified information) that propagate through Twitter. To predict the veracity of rumors, we identified salient features of rumors by examining three aspects of information spread: linguistic style used to express rumors, characteristics of people involved in propagating information, and network propagation dynamics. The predicted veracity of a time series of these features extracted from a rumor (a collection of tweets) is generated using Hidden Markov Models. The verification algorithm was trained and tested on 209 rumors representing 938,806 tweets collected from real-world events, including the 2013 Boston Marathon bombings, the 2014 Ferguson unrest, and the 2014 Ebola epidemic, and many other rumors about various real-world events reported on popular websites that document public rumors. The algorithm was able to correctly predict the veracity of 75% of the rumors faster than any other public source, including journalists and law enforcement officials. The ability to track rumors and predict their outcomes may have practical applications for news consumers, financial markets, journalists, and emergency services, and more generally to help minimize the impact of false information on Twitter.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {50},
numpages = {36},
keywords = {propagation, Twitter, rumor, veracity prediction, fake news}
}

@article{10.1145/3064884,
author = {Costa, Alceu Ferraz and Yamaguchi, Yuto and Traina, Agma Juci Machado and Jr., Caetano Traina and Faloutsos, Christos},
title = {Modeling Temporal Activity to Detect Anomalous Behavior in Social Media},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3064884},
doi = {10.1145/3064884},
abstract = {Social media has become a popular and important tool for human communication. However, due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots, has become a widespread problem. At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article, we focus on the following important question: Can we identify and use patterns of human communication to decide whether a human or a bot controls a user? The first contribution of this article is showing that the distribution of inter-arrival times (IATs) between postings is characterized by following four patterns: (i) heavy-tails, (ii) periodic-spikes, (iii) correlation between consecutive values, and (iv) bimodallity. As our second contribution, we propose a mathematical model named Act-M (Activity Model). We show that Act-M can accurately fit the distribution of IATs from social media users. Finally, we use Act-M to develop a method that detects if users are bots based only on the timing of their postings. We validate Act-M using data from over 55 million postings from four social media services: Reddit, Twitter, Stack-Overflow, and Hacker-News. Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics. Additionally, when detecting bots, Act-M provided a precision higher than 93% and 77% with a sensitivity of 70% for the Twitter and Reddit datasets, respectively.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {49},
numpages = {23},
keywords = {inter-arrival times, communication dynamics, Social media, anomaly detection}
}

@article{10.1145/3059177,
author = {Anagnostopoulos, Christos and Triantafillou, Peter},
title = {Query-Driven Learning for Predictive Analytics of Data Subspace Cardinality},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3059177},
doi = {10.1145/3059177},
abstract = {Fundamental to many predictive analytics tasks is the ability to estimate the cardinality (number of data items) of multi-dimensional data subspaces, defined by query selections over datasets. This is crucial for data analysts dealing with, e.g., interactive data subspace explorations, data subspace visualizations, and in query processing optimization. However, in many modern data systems, predictive analytics may be (i) too costly money-wise, e.g., in clouds, (ii) unreliable, e.g., in modern Big Data query engines, where accurate statistics are difficult to obtain/maintain, or (iii) infeasible, e.g., for privacy issues. We contribute a novel, query-driven, function estimation model of analyst-defined data subspace cardinality. The proposed estimation model is highly accurate in terms of prediction and accommodating the well-known selection queries: multi-dimensional range and distance-nearest neighbors (radius) queries. Our function estimation model: (i) quantizes the vectorial query space, by learning the analysts’ access patterns over a data space, (ii) associates query vectors with their corresponding cardinalities of the analyst-defined data subspaces, (iii) abstracts and employs query vectorial similarity to predict the cardinality of an unseen/unexplored data subspace, and (iv) identifies and adapts to possible changes of the query subspaces based on the theory of optimal stopping. The proposed model is decentralized, facilitating the scaling-out of such predictive analytics queries. The research significance of the model lies in that (i) it is an attractive solution when data-driven statistical techniques are undesirable or infeasible, (ii) it offers a scale-out, decentralized training solution, (iii) it is applicable to different selection query types, and (iv) it offers a performance that is superior to that of data-driven approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {47},
numpages = {46},
keywords = {analytics selection queries, vector regression quantization, optimal stopping theory, Predictive analytics, predictive learning, data subspace exploration}
}

@article{10.1145/3070647,
author = {Wang, Yihan and Song, Shaoxu and Chen, Lei and Yu, Jeffrey Xu and Cheng, Hong},
title = {Discovering Conditional Matching Rules},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070647},
doi = {10.1145/3070647},
abstract = {Matching dependencies (MDs) have recently been proposed to make data dependencies tolerant to various information representations, and found useful in data quality applications such as record matching. Instead of the strict equality function used in traditional dependency syntax (e.g., functional dependencies), MDs specify constraints based on similarity and identification. However, in practice, MDs may still be too strict and applicable only in a subset of tuples in a relation. Thereby, we study the conditional matching dependencies (CMDs), which bind matching dependencies only in a certain part of a table, i.e., MDs conditionally applicable in a subset of tuples. Compared to MDs, CMDs have more expressive power that enables them to satisfy wider application needs. In this article, we study several important theoretical and practical issues of CMDs, including irreducible CMDs with respect to the implication, discovery of CMDs from data, reliable CMDs agreed most by a relation, approximate CMDs almost satisfied in a relation, and finally applications of CMDs in record matching and missing value repairing. Through an extensive experimental evaluation in real data sets, we demonstrate the efficiency of proposed CMDs discovery algorithms and effectiveness of CMDs in real applications.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {46},
numpages = {38},
keywords = {data repair, record matching, Conditional matching dependency}
}

@article{10.1145/3092689,
author = {Aggarwal, Charu C.},
title = {Introduction to Special Issue on the Best Papers from KDD 2016},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3092689},
doi = {10.1145/3092689},
abstract = {This issue contains the best papers from the ACM KDD Conference 2016. As is customary at KDD, special issue papers are invited only from the research track. The top-ranked papers from the KDD 2016 conference are included in this issue. This issue contains a total of six articles, which are from different areas of data mining. A brief description of these articles is also provided in this article.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {39},
numpages = {3},
keywords = {Data mining}
}

@article{10.1145/3059194,
author = {Stefani, Lorenzo De and Epasto, Alessandro and Riondato, Matteo and Upfal, Eli},
title = {TRI\`{E}ST: Counting Local and Global Triangles in Fully Dynamic Streams with Fixed Memory Size},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3059194},
doi = {10.1145/3059194},
abstract = {“Ogni lassada xe persa.”1-- Proverb from Trieste, Italy.We present tri\`{e}st, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully dynamic graph represented as an adversarial stream of edge insertions and deletions.Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities.Our experimental results on very large graphs demonstrate that tri\`{e}st outperforms state-of-the-art approaches in accuracy and exhibits a small update time.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {43},
numpages = {50},
keywords = {reservoir sampling, subgraph counting, Cycle counting}
}

@article{10.1145/3056562,
author = {Chen, Chen and Tong, Hanghang and Xie, Lei and Ying, Lei and He, Qing},
title = {Cross-Dependency Inference in Multi-Layered Networks: A Collaborative Filtering Perspective},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3056562},
doi = {10.1145/3056562},
abstract = {The increasingly connected world has catalyzed the fusion of networks from different domains, which facilitates the emergence of a new network model—multi-layered networks. Examples of such kind of network systems include critical infrastructure networks, biological systems, organization-level collaborations, cross-platform e-commerce, and so forth. One crucial structure that distances multi-layered network from other network models is its cross-layer dependency, which describes the associations between the nodes from different layers. Needless to say, the cross-layer dependency in the network plays an essential role in many data mining applications like system robustness analysis and complex network control. However, it remains a daunting task to know the exact dependency relationships due to noise, limited accessibility, and so forth. In this article, we tackle the cross-layer dependency inference problem by modeling it as a collective collaborative filtering problem. Based on this idea, we propose an effective algorithm F<scp;>ascinate</scp;> that can reveal unobserved dependencies with linear complexity. Moreover, we derive F<scp;>ascinate</scp;>-ZERO, an online variant of F<scp;>ascinate</scp;> that can respond to a newly added node timely by checking its neighborhood dependencies. We perform extensive evaluations on real datasets to substantiate the superiority of our proposed approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {42},
numpages = {26},
keywords = {Multi-layered network, graph mining, cross-layer dependency}
}

@article{10.1145/3046946,
author = {Cheng, Wei and Ni, Jingchao and Zhang, Kai and Chen, Haifeng and Jiang, Guofei and Shi, Yu and Zhang, Xiang and Wang, Wei},
title = {Ranking Causal Anomalies for System Fault Diagnosis via Temporal and Dynamical Analysis on Vanishing Correlations},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046946},
doi = {10.1145/3046946},
abstract = {Detecting system anomalies is an important problem in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be powerful in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: (1) fault propagation in the network is ignored, (2) the root casual anomalies may not always be the nodes with a high percentage of vanishing correlations, (3) temporal patterns of vanishing correlations are not exploited for robust detection, and (4) prior knowledge on anomalous nodes are not exploited for (semi-)supervised detection. To address these limitations, in this article we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network and can perform joint inference on both the structural and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations and can compensate for unstructured measurement noise in the system. Moreover, when the prior knowledge on the anomalous status of some nodes are available at certain time points, our approach is able to leverage them to further enhance the anomaly inference accuracy. When the prior knowledge is noisy, our approach also automatically learns reliable information and reduces impacts from noises. By performing extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets, we demonstrate the effectiveness of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {40},
numpages = {28},
keywords = {nonnegative matrix factorization, label propagation, Causal anomalies ranking}
}

@article{10.1145/3080574,
author = {Boutemine, Oualid and Bouguessa, Mohamed},
title = {Mining Community Structures in Multidimensional Networks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3080574},
doi = {10.1145/3080574},
abstract = {We investigate the problem of community detection in multidimensional networks, that is, networks where entities engage in various interaction types (dimensions) simultaneously. While some approaches have been proposed to identify community structures in multidimensional networks, there are a number of problems still to solve. In fact, the majority of the proposed approaches suffer from one or even more of the following limitations: (1) difficulty detecting communities in networks characterized by the presence of many irrelevant dimensions, (2) lack of systematic procedures to explicitly identify the relevant dimensions of each community, and (3) dependence on a set of user-supplied parameters, including the number of communities, that require a proper tuning. Most of the existing approaches are inadequate for dealing with these three issues in a unified framework. In this paper, we develop a novel approach that is capable of addressing the aforementioned limitations in a single framework. The proposed approach allows automated identification of communities and their sub-dimensional spaces using a novel objective function and a constrained label propagation-based optimization strategy. By leveraging the relevance of dimensions at the node level, the strategy aims to maximize the number of relevant within-community links while keeping track of the most relevant dimensions. A notable feature of the proposed approach is that it is able to automatically identify low dimensional community structures embedded in a high dimensional space. Experiments on synthetic and real multidimensional networks illustrate the suitability of the new method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {51},
numpages = {36},
keywords = {community detection, social networks, Data mining}
}

@article{10.1145/3056563,
author = {Hooi, Bryan and Shin, Kijung and Song, Hyun Ah and Beutel, Alex and Shah, Neil and Faloutsos, Christos},
title = {Graph-Based Fraud Detection in the Face of Camouflage},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3056563},
doi = {10.1145/3056563},
abstract = {Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look “normal.” Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic.Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower--followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4, 000 detected accounts, of which a majority had tweets showing that they used follower-buying services.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {44},
numpages = {26},
keywords = {link analysis, Fraud detection, spam detection}
}

@article{10.1145/3070646,
author = {Wu, Yue and Hoi, Steven C. H. and Mei, Tao and Yu, Nenghai},
title = {Large-Scale Online Feature Selection for Ultra-High Dimensional Sparse Data},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070646},
doi = {10.1145/3070646},
abstract = {Feature selection (FS) is an important technique in machine learning and data mining, especially for large-scale high-dimensional data. Most existing studies have been restricted to batch learning, which is often inefficient and poorly scalable when handling big data in real world. As real data may arrive sequentially and continuously, batch learning has to retrain the model for the new coming data, which is very computationally intensive. Online feature selection (OFS) is a promising new paradigm that is more efficient and scalable than batch learning algorithms. However, existing online algorithms usually fall short in their inferior efficacy. In this article, we present a novel second-order OFS algorithm that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to exploit the second-order information to choose the subset of important features with high confidence weights. Unlike existing OFS methods that often suffer from extra high computational cost, we devise a novel algorithm with a MaxHeap-based approach, which is not only more effective than the existing first-order algorithms, but also significantly more efficient and scalable. Our extensive experiments validated that the proposed technique achieves highly competitive accuracy as compared with state-of-the-art batch FS methods, meanwhile it consumes significantly less computational cost that is orders of magnitude lower. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion non-zero features, and 1-million samples), the proposed algorithm takes less than 3 minutes to run on a single PC.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {48},
numpages = {22},
keywords = {second-order online learning, sparsity, ultra-high dimensionality, Feature selection}
}

@article{10.1145/3014060,
author = {Tang, Xun and Alabduljalil, Maha and Jin, Xin and Yang, Tao},
title = {Partitioned Similarity Search with Cache-Conscious Data Traversal},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3014060},
doi = {10.1145/3014060},
abstract = {All pairs similarity search (APSS) is used in many web search and data mining applications. Previous work has used techniques such as comparison filtering, inverted indexing, and parallel accumulation of partial results. However, shuffling intermediate results can incur significant communication overhead as data scales up. This paper studies a scalable two-phase approach called Partition-based Similarity Search (PSS). The first phase is to partition the data and group vectors that are potentially similar. The second phase is to run a set of tasks where each task compares a partition of vectors with other candidate partitions. Due to data sparsity and the presence of memory hierarchy, accessing feature vectors during the partition comparison phase incurs significant overhead. This paper introduces a cache-conscious design for data layout and traversal to reduce access time through size-controlled data splitting and vector coalescing, and it provides an analysis to guide the choice of optimization parameters. The evaluation results show that for the tested datasets, the proposed approach can lead to an early elimination of unnecessary I/O and data communication while sustaining parallel efficiency with one order of magnitude of performance improvement and it can also be integrated with LSH for approximated APSS.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {34},
numpages = {38},
keywords = {data traversal, All-pairs similarity search, partitioning, memory hierarchy}
}

@article{10.1145/3051128,
author = {Wang, Liang and Yu, Zhiwen and Guo, Bin and Ku, Tao and Yi, Fei},
title = {Moving Destination Prediction Using Sparse Dataset: A Mobility Gradient Descent Approach},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3051128},
doi = {10.1145/3051128},
abstract = {Moving destination prediction offers an important category of location-based applications and provides essential intelligence to business and governments. In existing studies, a common approach to destination prediction is to match the given query trajectory with massive recorded trajectories by similarity calculation. Unfortunately, due to privacy concerns, budget constraints, and many other factors, in most circumstances, we can only obtain a sparse trajectory dataset. In sparse dataset, the available moving trajectories are far from enough to cover all possible query trajectories; thus the predictability of the matching-based approach will decrease remarkably. Toward destination prediction with sparse dataset, instead of searching similar trajectories over the sparse records, we alternatively examine the changes of distances from sampling locations to final destination on query trajectory. The underlying idea is intuitive: It is directly motivated by travel purpose, people always get closer to the final destination during the movement. By borrowing the conception of gradient descent in optimization theory, we propose a novel moving destination prediction approach, namely MGDPre. Building upon the mobility gradient descent, MGDPre only investigates the behavior characteristics of query trajectory itself without matching historical trajectories, and thus is applicable for sparse dataset. We evaluate our approach based on extensive experiments, using GPS trajectories generated by a sample of taxis over a 10-day period in Shenzhen city, China. The results demonstrate that the effectiveness, efficiency, and scalability of our approach outperform state-of-the-art baseline methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {37},
numpages = {33},
keywords = {gradient descent, sparse dataset, Moving destination prediction, space division, Markov transition model}
}

@article{10.1145/3046948,
author = {Fountoulakis, Kimon and Kundu, Abhisek and Kontopoulou, Eugenia-Maria and Drineas, Petros},
title = {A Randomized Rounding Algorithm for Sparse PCA},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046948},
doi = {10.1145/3046948},
abstract = {We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. In the proposed approach, we first solve an ℓ1-penalized version of the NP-hard sparse PCA optimization problem and then we use a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Extensive experimental evaluation indicates that the proposed approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {38},
numpages = {26},
keywords = {rounding randomized algorithm, Sparce pca}
}

@article{10.1145/3022669,
author = {Liu, Xutong and Chen, Feng and Lu, Yen-Cheng and Lu, Chang-Tien},
title = {Spatial Prediction for Multivariate Non-Gaussian Data},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022669},
doi = {10.1145/3022669},
abstract = {With the ever increasing volume of geo-referenced datasets, there is a real need for better statistical estimation and prediction techniques for spatial analysis. Most existing approaches focus on predicting multivariate Gaussian spatial processes, but as the data may consist of non-Gaussian (or mixed type) variables, this creates two challenges: (1) how to accurately capture the dependencies among different data types, both Gaussian and non-Gaussian; and (2) how to efficiently predict multivariate non-Gaussian spatial processes. In this article, we propose a generic approach for predicting multiple response variables of mixed types. The proposed approach accurately captures cross-spatial dependencies among response variables and reduces the computational burden by projecting the spatial process to a lower dimensional space with knot-based techniques. Efficient approximations are provided to estimate posterior marginals of latent variables for the predictive process, and extensive experimental evaluations based on both simulation and real-life datasets are provided to demonstrate the effectiveness and efficiency of this new approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {36},
numpages = {27},
keywords = {Approximate bayesian inference, Gaussian and non-Gaussian processes, Laplace approximation, computational statistics, geostatistics, predictive process model}
}

@article{10.1145/3046941,
author = {Liu, Qi and Xiang, Biao and Yuan, Nicholas Jing and Chen, Enhong and Xiong, Hui and Zheng, Yi and Yang, Yu},
title = {An Influence Propagation View of PageRank},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046941},
doi = {10.1145/3046941},
abstract = {For a long time, PageRank has been widely used for authority computation and has been adopted as a solid baseline for evaluating social influence related applications. However, when measuring the authority of network nodes, the traditional PageRank method does not take the nodes’ prior knowledge into consideration. Also, the connection between PageRank and social influence modeling methods is not clearly established. To that end, this article provides a focused study on understanding PageRank as well as the relationship between PageRank and social influence analysis. Along this line, we first propose a linear social influence model and reveal that this model generalizes the PageRank-based authority computation by introducing some constraints. Then, we show that the authority computation by PageRank can be enhanced if exploiting more reasonable constraints (e.g., from prior knowledge). Next, to deal with the computational challenge of linear model with general constraints, we provide an upper bound for identifying nodes with top authorities. Moreover, we extend the proposed linear model for better measuring the authority of the given node sets, and we also demonstrate the way to quickly identify the top authoritative node sets. Finally, extensive experimental evaluations on four real-world networks validate the effectiveness of the proposed linear model with respect to different constraint settings. The results show that the methods with more reasonable constraints can lead to better ranking and recommendation performance. Meanwhile, the upper bounds formed by PageRank values could be used to quickly locate the nodes and node sets with the highest authorities.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {30},
numpages = {30},
keywords = {priors, PageRank, upper bounds, authority, social influence propagation}
}

@article{10.1145/2996197,
author = {Guo, Ting and Wu, Jia and Zhu, Xingquan and Zhang, Chengqi},
title = {Combining Structured Node Content and Topology Information for Networked Graph Clustering},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2996197},
doi = {10.1145/2996197},
abstract = {Graphs are popularly used to represent objects with shared dependency relationships. To date, all existing graph clustering algorithms consider each node as a single attribute or a set of independent attributes, without realizing that content inside each node may also have complex structures. In this article, we formulate a new networked graph clustering task where a network contains a set of inter-connected (or networked) super-nodes, each of which is a single-attribute graph. The new super-node representation is applicable to many real-world applications, such as a citation network where each node denotes a paper whose content can be described as a graph, and citation relationships between papers form a networked graph (i.e., a super-graph). Networked graph clustering aims to find similar node groups, each of which contains nodes with similar content and structure information. The main challenge is to properly calculate the similarity between super-nodes for clustering. To solve the problem, we propose to characterize node similarity by integrating structure and content information of each super-node. To measure node content similarity, we use cosine distance by considering overlapped attributes between two super-nodes. To measure structure similarity, we propose an Attributed Random Walk Kernel (ARWK) to calculate the similarity between super-nodes. Detailed node content analysis is also included to build relationships between super-nodes with shared internal structure information, so the structure similarity can be calculated in a precise way. By integrating the structure similarity and content similarity as one matrix, the spectral clustering is used to achieve networked graph clustering. Our method enjoys sound theoretical properties, including bounded similarities and better structure similarity assessment than traditional graph clustering methods. Experiments on real-world applications demonstrate that our method significantly outperforms baseline approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {29},
numpages = {29},
keywords = {Networked graphs, topology, super-nodes, clustering, kernel}
}

@article{10.1145/3046791,
author = {Rozenshtein, Polina and Tatti, Nikolaj and Gionis, Aristides},
title = {Finding Dynamic Dense Subgraphs},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3046791},
doi = {10.1145/3046791},
abstract = {Online social networks are often defined by considering interactions of entities at an aggregate level. For example, a call graph is formed among individuals who have called each other at least once; or at least k times. Similarly, in social-media platforms, we consider implicit social networks among users who have interacted in some way, e.g., have made a conversation, have commented to the content of each other, and so on. Such definitions have been used widely in the literature and they have offered significant insights regarding the structure of social networks. However, it is obvious that they suffer from a severe limitation: They neglect the precise time that interactions among the network entities occur.In this article, we consider interaction networks, where the data description contains not only information about the underlying topology of the social network, but also the exact time instances that network entities interact. In an interaction network, an edge is associated with a timestamp, and multiple edges may occur for the same pair of entities. Consequently, interaction networks offer a more fine-grained representation, which can be leveraged to reveal otherwise hidden dynamic phenomena.In the setting of interaction networks, we study the problem of discovering dynamic dense subgraphs whose edges occur in short time intervals. We view such subgraphs as fingerprints of dynamic activity occurring within network communities. Such communities represent groups of individuals who interact with each other in specific time instances, for example, a group of employees who work on a project and whose interaction intensifies before certain project milestones. We prove that the problem we define is NP-hard, and we provide efficient algorithms by adapting techniques for finding dense subgraphs. We also show how to speed-up the proposed methods by exploiting concavity properties of our objective function and by the means of fractional programming. We perform extensive evaluation of the proposed methods on synthetic and real datasets, which demonstrates the validity of our approach and shows that our algorithms can be used to obtain high-quality results.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {27},
numpages = {30},
keywords = {dynamic graphs, community discovery, interaction networks, time-evolving networks, Graph mining, social-network analysis, dense subgraphs}
}

@article{10.1145/3003728,
author = {Feng, Shanshan and Cao, Jian and Wang, Jie and Qian, Shiyou},
title = {Recommendations Based on Comprehensively Exploiting the Latent Factors Hidden in Items’ Ratings and Content},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3003728},
doi = {10.1145/3003728},
abstract = {To improve the performance of recommender systems in a practical manner, several hybrid approaches have been developed by considering item ratings and content information simultaneously. However, most of these hybrid approaches make recommendations based on aggregating different recommendation techniques using various strategies, rather than considering joint modeling of the item’s ratings and content, and thus fail to detect many latent factors that could potentially improve the performance of the recommender systems. For this reason, these approaches continue to suffer from data sparsity and do not work well for recommending items to individual users. A few studies try to describe a user’s preference by detecting items’ latent features from content-description texts as compensation for the sparse ratings. Unfortunately, most of these methods are still generally unable to accomplish recommendation tasks well for two reasons: (1) they learn latent factors from text descriptions or user--item ratings independently, rather than combining them together; and (2) influences of latent factors hidden in texts and ratings are not fully explored. In this study, we propose a probabilistic approach that we denote as latent random walk (LRW) based on the combination of an integrated latent topic model and random walk (RW) with the restart method, which can be used to rank items according to expected user preferences by detecting both their explicit and implicit correlative information, in order to recommend top-ranked items to potentially interested users. As presented in this article, the goal of this work is to comprehensively discover latent factors hidden in items’ ratings and content in order to alleviate the data sparsity problem and to improve the performance of recommender systems. The proposed topic model provides a generative probabilistic framework that discovers users’ implicit preferences and items’ latent features simultaneously by exploiting both ratings and item content information. On the basis of this probabilistic framework, RW can predict a user’s preference for unrated items by discovering global latent relations. In order to show the efficiency of the proposed approach, we test LRW and other state-of-the-art methods on three real-world datasets, namely, CAMRa2011, Yahoo!, and APP. The experiments indicate that our approach outperforms all comparative methods and, in addition, that it is less sensitive to the data sparsity problem, thus demonstrating the robustness of LRW for recommendation tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {35},
numpages = {27},
keywords = {probabilistic topic model, correlative information discovering, random walk with restart, latent factors detection, Recommender system}
}

@article{10.1145/3022668,
author = {Sariy\"{u}ce, Ahmet Erdem and Kaya, Kamer and Saule, Erik and \c{C}ataly\"{u}rek, \"{U}mit V.},
title = {Graph Manipulations for Fast Centrality Computation},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022668},
doi = {10.1145/3022668},
abstract = {The betweenness and closeness metrics are widely used metrics in many network analysis applications. Yet, they are expensive to compute. For that reason, making the betweenness and closeness centrality computations faster is an important and well-studied problem. In this work, we propose the framework BADIOS that manipulates the graph by compressing it and splitting into pieces so that the centrality computation can be handled independently for each piece. Experimental results show that the proposed techniques can be a great arsenal to reduce the centrality computation time for various types and sizes of networks. In particular, it reduces the betweenness centrality computation time of a 4.6 million edges graph from more than 5 days to less than 16 hours. For the same graph, the closeness computation time is decreased from more than 3 days to 6 hours (12.7x speedup).},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {26},
numpages = {25},
keywords = {shortest path, closeness centrality, Betweenness centrality}
}

@article{10.1145/3022185,
author = {Liu, Guannan and Fu, Yanjie and Chen, Guoqing and Xiong, Hui and Chen, Can},
title = {Modeling Buying Motives for Personalized Product Bundle Recommendation},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022185},
doi = {10.1145/3022185},
abstract = {Product bundling is a marketing strategy that offers several products/items for sale as one bundle. While the bundling strategy has been widely used, less efforts have been made to understand how items should be bundled with respect to consumers’ preferences and buying motives for product bundles. This article investigates the relationships between the items that are bought together within a product bundle. To that end, each purchased product bundle is formulated as a bundle graph with items as nodes and the associations between pairs of items in the bundle as edges. The relationships between items can be analyzed by the formation of edges in bundle graphs, which can be attributed to the associations of feature aspects. Then, a probabilistic model BPM (Bundle Purchases with Motives) is proposed to capture the composition of each bundle graph, with two latent factors node-type and edge-type introduced to describe the feature aspects and relationships respectively. Furthermore, based on the preferences inferred from the model, an approach for recommending items to form product bundles is developed by estimating the probability that a consumer would buy an associative item together with the item already bought in the shopping cart. Finally, experimental results on real-world transaction data collected from well-known shopping sites show the effectiveness advantages of the proposed approach over other baseline methods. Moreover, the experiments also show that the proposed model can explain consumers’ buying motives for product bundles in terms of different node-types and edge-types.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {28},
numpages = {26},
keywords = {probabilistic graphical model, Product bundle, recommendation, buying motives}
}

@article{10.1145/3003730,
author = {Peng, Chong and Kang, Zhao and Hu, Yunhong and Cheng, Jie and Cheng, Qiang},
title = {Robust Graph Regularized Nonnegative Matrix Factorization for Clustering},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3003730},
doi = {10.1145/3003730},
abstract = {Matrix factorization is often used for data representation in many data mining and machine-learning problems. In particular, for a dataset without any negative entries, nonnegative matrix factorization (NMF) is often used to find a low-rank approximation by the product of two nonnegative matrices. With reduced dimensions, these matrices can be effectively used for many applications such as clustering. The existing methods of NMF are often afflicted with their sensitivity to outliers and noise in the data. To mitigate this drawback, in this paper, we consider integrating NMF into a robust principal component model, and design a robust formulation that effectively captures noise and outliers in the approximation while incorporating essential nonlinear structures. A set of comprehensive empirical evaluations in clustering applications demonstrates that the proposed method has strong robustness to gross errors and superior performance to current state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {33},
numpages = {30},
keywords = {clustering, manifold, Nonnegative factorization, robust principal component analysis}
}

@article{10.1145/3003729,
author = {Wang, Sen and Li, Xue and Chang*, Xiaojun and Yao, Lina and Sheng, Quan Z. and Long, Guodong},
title = {Learning Multiple Diagnosis Codes for ICU Patients with Local Disease Correlation Mining},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3003729},
doi = {10.1145/3003729},
abstract = {In the era of big data, a mechanism that can automatically annotate disease codes to patients’ records in the medical information system is in demand. The purpose of this work is to propose a framework that automatically annotates the disease labels of multi-source patient data in Intensive Care Units (ICUs). We extract features from two main sources, medical charts and notes. The Bag-of-Words model is used to encode the features. Unlike most of the existing multi-label learning algorithms that globally consider correlations between diseases, our model learns disease correlation locally in the patient data. To achieve this, we derive a local disease correlation representation to enrich the discriminant power of each patient data. This representation is embedded into a unified multi-label learning framework. We develop an alternating algorithm to iteratively optimize the objective function. Extensive experiments have been conducted on a real-world ICU database. We have compared our algorithm with representative multi-label learning algorithms. Evaluation results have shown that our proposed method has state-of-the-art performance in the annotation of multiple diagnostic codes for ICU patients. This study suggests that problems in the automated diagnosis code annotation can be reliably addressed by using a multi-label learning model that exploits disease correlation. The findings of this study will greatly benefit health care and management in ICU considering that the automated diagnosis code annotation can significantly improve the quality and management of health care for both patients and caregivers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {31},
numpages = {21},
keywords = {local correlation exploiting, Diagnosis code annotation, pattern discovery, MIMIC II database, ICU data mining, multi-label learning}
}

@article{10.1145/2992785,
author = {Bae, Seung-Hee and Halperin, Daniel and West, Jevin D. and Rosvall, Martin and Howe, Bill},
title = {Scalable and Efficient Flow-Based Community Detection for Large-Scale Graph Analysis},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2992785},
doi = {10.1145/2992785},
abstract = {Community detection is an increasingly popular approach to uncover important structures in large networks. Flow-based community detection methods rely on communication patterns of the network rather than structural properties to determine communities. The Infomap algorithm in particular optimizes a novel objective function called the map equation and has been shown to outperform other approaches in third-party benchmarks. However, Infomap and its variants are inherently sequential, limiting their use for large-scale graphs.In this article, we propose a novel algorithm to optimize the map equation called RelaxMap. RelaxMap provides two important improvements over Infomap: parallelization, so that the map equation can be optimized over much larger graphs, and prioritization, so that the most important work occurs first, iterations take less time, and the algorithm converges faster. We implement these techniques using OpenMP on shared-memory multicore systems, and evaluate our approach on a variety of graphs from standard graph clustering benchmarks as well as real graph datasets. Our evaluation shows that both techniques are effective: RelaxMap achieves 70% parallel efficiency on eight cores, and prioritization improves algorithm performance by an additional 20--50% on average, depending on the graph properties. Additionally, RelaxMap converges in the similar number of iterations and provides solutions of equivalent quality as the serial Infomap implementation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {32},
numpages = {30},
keywords = {parallelization, graph analysis, prioritization, Community detection}
}

@article{10.1145/2953881,
author = {Wang, Chenguang and Song, Yangqiu and Roth, Dan and Zhang, Ming and Han, Jiawei},
title = {World Knowledge as Indirect Supervision for Document Clustering},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2953881},
doi = {10.1145/2953881},
abstract = {One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then, the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this article, we provide an example of using world knowledge for domain-dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then, we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features.A preliminary version of this work appeared in the proceedings of KDD 2015 [Wang et al. 2015a]. This journal version has made several major improvements. First, we have proposed a new and general learning framework for machine learning with world knowledge as indirect supervision, where document clustering is a special case in the original paper. Second, in order to make our unsupervised semantic parsing method more understandable, we add several real cases from the original sentences to the resulting logic forms with all the necessary information. Third, we add details of the three semantic filtering methods and conduct deep analysis of the three semantic filters, by using case studies to show why the conceptualization-based semantic filter can produce more accurate indirect supervision. Finally, in addition to the experiment on 20 newsgroup data and Freebase, we have extended the experiments on clustering results by using all the combinations of text (20 newsgroup, MCAT, CCAT, ECAT) and world knowledge sources (Freebase, YAGO2).},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {13},
numpages = {36},
keywords = {document clustering, heterogeneous information network, World knowledge, knowledge graph, knowledge base}
}

@article{10.1145/2988235,
author = {Wang, Zhongyuan and Wang, Fang and Wang, Haixun and Hu, Zhirui and Yan, Jun and Li, Fangtao and Wen, Ji-Rong and Li, Zhoujun},
title = {Unsupervised Head--Modifier Detection in Search Queries},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2988235},
doi = {10.1145/2988235},
abstract = {Interpreting the user intent in search queries is a key task in query understanding. Query intent classification has been widely studied. In this article, we go one step further to understand the query from the view of head--modifier analysis. For example, given the query “popular iphone 5 smart cover,” instead of using coarse-grained semantic classes (e.g., find electronic product), we interpret that “smart cover” is the head or the intent of the query and “iphone 5” is its modifier. Query head--modifier detection can help search engines to obtain particularly relevant content, which is also important for applications such as ads matching and query recommendation. We introduce an unsupervised semantic approach for query head--modifier detection. First, we mine a large number of instance level head--modifier pairs from search log. Then, we develop a conceptualization mechanism to generalize the instance level pairs to concept level. Finally, we derive weighted concept patterns that are concise, accurate, and have strong generalization power in head--modifier detection. The developed mechanism has been used in production for search relevance and ads matching. We use extensive experiment results to demonstrate the effectiveness of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {19},
numpages = {28},
keywords = {Query intent, concept pattern, head and modifier, knowledge modeling}
}

@article{10.1145/2994605,
author = {Chang, Yi and Yamada, Makoto and Ortega, Antonio and Liu, Yan},
title = {Lifecycle Modeling for Buzz Temporal Pattern Discovery},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2994605},
doi = {10.1145/2994605},
abstract = {In social media analysis, one critical task is detecting a burst of topics or buzz, which is reflected by extremely frequent mentions of certain keywords in a short-time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task because a buzz time-series often exhibits sudden spikes and heavy tails, wherein most existing time-series models fail. In this article, we propose novel buzz modeling approaches that capture the rise and fade temporal patterns via Product Lifecycle (PLC) model, a classical concept in economics. More specifically, we propose to model multiple peaks in buzz time-series with PLC mixture or PLC group mixture and develop a probabilistic graphical model (K-Mixture of Product Lifecycle (K-MPLC) to automatically discover inherent lifecycle patterns within a collection of buzzes. Furthermore, we effectively utilize the model parameters of PLC mixture or PLC group mixture for burst prediction. Our experimental results show that our proposed methods significantly outperform existing leading approaches on buzz clustering and buzz-type prediction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {20},
numpages = {24},
keywords = {buzz type prediction, buzz clustering, Time-series modeling}
}

@article{10.1145/2996196,
author = {Wei, Qiang and Qiao, Dandan and Zhang, Jin and Chen, Guoqing and Guo, Xunhua},
title = {A Novel Bipartite Graph Based Competitiveness Degree Analysis from Query Logs},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2996196},
doi = {10.1145/2996196},
abstract = {Competitiveness degree analysis is a focal point of business strategy and competitive intelligence, aimed to help managers closely monitor to what extent their rivals are competing with them. This article proposes a novel method, namely BCQ, to measure the competitiveness degree between peers from query logs as an important form of user generated contents, which reflects the “wisdom of crowds” from the search engine users’ perspective. In doing so, a bipartite graph model is developed to capture the competitive relationships through conjoint attributes hidden in query logs, where the notion of competitiveness degree for entity pairs is introduced, and then used to identify the competitive paths mapped in the bipartite graph. Subsequently, extensive experiments are conducted to demonstrate the effectiveness of BCQ to quantify the competitiveness degrees. Experimental results reveal that BCQ can well support competitors ranking, which is helpful for devising competitive strategies and pursuing market performance. In addition, efficiency experiments on synthetic data show a good scalability of BCQ on large scale of query logs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {21},
numpages = {25},
keywords = {competitiveness degree, query logs, Competitive intelligence, bipartite graph}
}

@article{10.1145/2997657,
author = {Lorenzetti, Carlos and Maguitman, Ana and Leake, David and Menczer, Filippo and Reichherzer, Thomas},
title = {Mining for Topics to Suggest Knowledge Model Extensions},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2997657},
doi = {10.1145/2997657},
abstract = {Electronic concept maps, interlinked with other concept maps and multimedia resources, can provide rich knowledge models to capture and share human knowledge. This article presents and evaluates methods to support experts as they extend existing knowledge models, by suggesting new context-relevant topics mined from Web search engines. The task of generating topics to support knowledge model extension raises two research questions: first, how to extract topic descriptors and discriminators from concept maps; and second, how to use these topic descriptors and discriminators to identify candidate topics on the Web with the right balance of novelty and relevance. To address these questions, this article first develops the theoretical framework required for a “topic suggester” to aid information search in the context of a knowledge model under construction. It then presents and evaluates algorithms based on this framework and applied in Extender, an implemented tool for topic suggestion. Extender has been developed and tested within CmapTools, a widely used system for supporting knowledge modeling using concept maps. However, the generality of the algorithms makes them applicable to a broad class of knowledge modeling systems, and to Web search in general.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {23},
numpages = {30},
keywords = {intelligent suggesters, web mining, knowledge discovery, knowledge construction, Concept mapping}
}

@article{10.1145/2987376,
author = {Afrati, Foto and Dolev, Shlomi and Korach, Ephraim and Sharma, Shantanu and Ullman, Jeffrey D.},
title = {Assignment Problems of Different-Sized Inputs in MapReduce},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2987376},
doi = {10.1145/2987376},
abstract = {A MapReduce algorithm can be described by a mapping schema, which assigns inputs to a set of reducers, such that for each required output there exists a reducer that receives all the inputs participating in the computation of this output. Reducers have a capacity that limits the sets of inputs they can be assigned. However, individual inputs may vary in terms of size. We consider, for the first time, mapping schemas where input sizes are part of the considerations and restrictions. One of the significant parameters to optimize in any MapReduce job is communication cost between the map and reduce phases. The communication cost can be optimized by minimizing the number of copies of inputs sent to the reducers. The communication cost is closely related to the number of reducers of constrained capacity that are used to accommodate appropriately the inputs, so that the requirement of how the inputs must meet in a reducer is satisfied. In this work, we consider a family of problems where it is required that each input meets with each other input in at least one reducer. We also consider a slightly different family of problems in which each input of a list, X, is required to meet each input of another list, Y, in at least one reducer. We prove that finding an optimal mapping schema for these families of problems is NP-hard, and present a bin-packing-based approximation algorithm for finding a near optimal mapping schema.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {18},
numpages = {35},
keywords = {Distributed computing, mapping schema, reducer capacity, MapReduce algorithms, and reducer capacity and communication cost trade-off}
}

@article{10.1145/2996467,
author = {Pei, Yuanli and Fern, Xiaoli Z. and Tjahja, Teresa Vania and Rosales, R\'{o}mer},
title = {Comparing Clustering with Pairwise and Relative Constraints: A Unified Framework},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2996467},
doi = {10.1145/2996467},
abstract = {Clustering can be improved with the help of side information about the similarity relationships among instances. Such information has been commonly represented by two types of constraints: pairwise constraints and relative constraints, regarding similarities about instance pairs and triplets, respectively. Prior work has mostly considered these two types of constraints separately and developed individual algorithms to learn from each type. In practice, however, it is critical to understand/compare the usefulness of the two types of constraints as well as the cost of acquiring them, which has not been studied before. This paper provides an extensive comparison of clustering with these two types of constraints. Specifically, we compare their impacts both on human users that provide such constraints and on the learning system that incorporates such constraints into clustering. In addition, to ensure that the comparison of clustering is performed on equal ground (without the potential bias introduced by different learning algorithms), we propose a probabilistic semi-supervised clustering framework that can learn from either type of constraints. Our experiments demonstrate that the proposed semi-supervised clustering framework is highly effective at utilizing both types of constraints to aid clustering. Our user study provides valuable insights regarding the impact of the constraints on human users, and our experiments on clustering with the human-labeled constraints reveal that relative constraint is often more efficient at improving clustering.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {22},
numpages = {26},
keywords = {Semi-supervised clustering, relative constraints, pairwise constraints}
}

@article{10.1145/2976744,
author = {Yu, Kui and Wu, Xindong and Ding, Wei and Pei, Jian},
title = {Scalable and Accurate Online Feature Selection for Big Data},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2976744},
doi = {10.1145/2976744},
abstract = {Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a <underline>S</underline>calable and <underline>A</underline>ccurate <underline>O</underline>n<underline>L</underline>ine <underline>A</underline>pproach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {16},
numpages = {39},
keywords = {Online feature selection, big data, extremely high dimensionality, group features}
}

@article{10.1145/2997656,
author = {Kumar, Dheeraj and Bezdek, James C. and Rajasegarar, Sutharshan and Palaniswami, Marimuthu and Leckie, Christopher and Chan, Jeffrey and Gubbi, Jayavardhana},
title = {Adaptive Cluster Tendency Visualization and Anomaly Detection for Streaming Data},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2997656},
doi = {10.1145/2997656},
abstract = {The growth in pervasive network infrastructure called the Internet of Things (IoT) enables a wide range of physical objects and environments to be monitored in fine spatial and temporal detail. The detailed, dynamic data that are collected in large quantities from sensor devices provide the basis for a variety of applications. Automatic interpretation of these evolving large data is required for timely detection of interesting events. This article develops and exemplifies two new relatives of the visual assessment of tendency (VAT) and improved visual assessment of tendency (iVAT) models, which uses cluster heat maps to visualize structure in static datasets. One new model is initialized with a static VAT/iVAT image, and then incrementally (hence inc-VAT/inc-iVAT) updates the current minimal spanning tree (MST) used by VAT with an efficient edge insertion scheme. Similarly, dec-VAT/dec-iVAT efficiently removes a node from the current VAT MST. A sequence of inc-iVAT/dec-iVAT images can be used for (visual) anomaly detection in evolving data streams and for sliding window based cluster assessment for time series data. The method is illustrated with four real datasets (three of them being smart city IoT data). The evaluation demonstrates the algorithms’ ability to successfully isolate anomalies and visualize changing cluster structure in the streaming data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {24},
numpages = {40},
keywords = {cluster heat maps, sliding window based time series clustering, smart city streaming data analysis, internet of things (IoT), online anomaly detection, Visual assessment of clusters in streaming data}
}

@article{10.1145/2968451,
author = {Smith, Laura M. and Zhu, Linhong and Lerman, Kristina and Percus, Allon G.},
title = {Partitioning Networks with Node Attributes by Compressing Information Flow},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2968451},
doi = {10.1145/2968451},
abstract = {Real-world networks are often organized as modules or communities of similar nodes that serve as functional units. These networks are also rich in content, with nodes having distinguished features or attributes. In order to discover a network’s modular structure, it is necessary to take into account not only its links but also node attributes. We describe an information-theoretic method that identifies modules by compressing descriptions of information flow on a network. Our formulation introduces node content into the description of information flow, which we then minimize to discover groups of nodes with similar attributes that also tend to trap the flow of information. The method is conceptually simple and does not require ad-hoc parameters to specify the number of modules or to control the relative contribution of links and node attributes to network structure. We apply the proposed method to partition real-world networks with known community structure. We demonstrate that adding node attributes helps recover the underlying community structure in content-rich networks more effectively than using links alone. In addition, we show that our method is faster and more accurate than alternative state-of-the-art algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {15},
numpages = {26},
keywords = {community detection, rich networks, Graph partitioning, information theory}
}

@article{10.1145/2935750,
author = {Gao, Zekai J. and Jermaine, Chris},
title = {Distributed Algorithms for Computing Very Large Thresholded Covariance Matrices},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2935750},
doi = {10.1145/2935750},
abstract = {Computation of covariance matrices from observed data is an important problem, as such matrices are used in applications such as principal component analysis (PCA), linear discriminant analysis (LDA), and increasingly in the learning and application of probabilistic graphical models. However, computing an empirical covariance matrix is not always an easy problem. There are two key difficulties associated with computing such a matrix from a very high-dimensional dataset. The first problem is over-fitting. For a p-dimensional covariance matrix, there are p(p − 1)/2 unique, off-diagonal entries in the empirical covariance matrix undefined for large p (say, p &gt; 105), the size n of the dataset is often much smaller than the number of covariances to compute. Over-fitting is a concern in any situation in which the number of parameters learned can greatly exceed the size of the dataset. Thus, there are strong theoretical reasons to expect that for high-dimensional data—even Gaussian data—the empirical covariance matrix is not a good estimate for the true covariance matrix underlying the generative process. The second problem is computational. Computing a covariance matrix takes O(np2) time. For large p (greater than 10,000) and n much greater than p, this is debilitating. In this article, we consider how both of these difficulties can be handled simultaneously. Specifically, a key regularization technique for high-dimensional covariance estimation is thresholding, in which the smallest or least significant entries in the covariance matrix are simply dropped and replaced with the value 0. This suggests an obvious way to address the computational difficulty as well: First, compute the identities of the K entries in the covariance matrix that are actually important in the sense that they will not be removed during thresholding, and then in a second step, compute the values of those entries. This can be done in O(Kn) time. If K ≪ p2 and the identities of the important entries can be computed in reasonable time, then this is a big win. The key technical contribution of this article is the design and implementation of two different distributed algorithms for approximating the identities of the important entries quickly, using sampling. We have implemented these methods and tested them using an 800-core compute cluster. Experiments have been run using real datasets having millions of data points and up to 40, 000 dimensions. These experiments show that the proposed methods are both accurate and efficient.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {12},
numpages = {25},
keywords = {thresholding, text processing, distributed algorithm, Covariance matrices, sampling}
}

@article{10.1145/2983533,
author = {Liu, Bin and Wu, Yao and Gong, Neil Zhenqiang and Wu, Junjie and Xiong, Hui and Ester, Martin},
title = {Structural Analysis of User Choices for Mobile App Recommendation},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2983533},
doi = {10.1145/2983533},
abstract = {Advances in smartphone technology have promoted the rapid development of mobile apps. However, the availability of a huge number of mobile apps in application stores has imposed the challenge of finding the right apps to meet the user needs. Indeed, there is a critical demand for personalized app recommendations. Along this line, there are opportunities and challenges posed by two unique characteristics of mobile apps. First, app markets have organized apps in a hierarchical taxonomy. Second, apps with similar functionalities are competing with each other. Although there are a variety of approaches for mobile app recommendations, these approaches do not have a focus on dealing with these opportunities and challenges. To this end, in this article, we provide a systematic study for addressing these challenges. Specifically, we develop a structural user choice model (SUCM) to learn fine-grained user preferences by exploiting the hierarchical taxonomy of apps as well as the competitive relationships among apps. Moreover, we design an efficient learning algorithm to estimate the parameters for the SUCM model. Finally, we perform extensive experiments on a large app adoption dataset collected from Google Play. The results show that SUCM consistently outperforms state-of-the-art Top-N recommendation methods by a significant margin.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {17},
numpages = {23},
keywords = {structural choices, hierarchy structure, Recommender systems, mobile apps}
}

@article{10.1145/2953883,
author = {Chakraborty, Tanmoy and Srinivasan, Sriram and Ganguly, Niloy and Mukherjee, Animesh and Bhowmick, Sanjukta},
title = {Permanence and Community Structure in Complex Networks},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2953883},
doi = {10.1145/2953883},
abstract = {The goal of community detection algorithms is to identify densely connected units within large networks. An implicit assumption is that all the constituent nodes belong equally to their associated community. However, some nodes are more important in the community than others. To date, efforts have been primarily made to identify communities as a whole, rather than understanding to what extent an individual node belongs to its community. Therefore, most metrics for evaluating communities, for example modularity, are global. These metrics produce a score for each community, not for each individual node. In this article, we argue that the belongingness of nodes in a community is not uniform. We quantify the degree of belongingness of a vertex within a community by a new vertex-based metric called permanence.The central idea of permanence is based on the observation that the strength of membership of a vertex to a community depends upon two factors (i) the extent of connections of the vertex within its community versus outside its community, and (ii) how tightly the vertex is connected internally. We present the formulation of permanence based on these two quantities. We demonstrate that compared to other existing metrics (such as modularity, conductance, and cut-ratio), the change in permanence is more commensurate to the level of perturbation in ground-truth communities. We discuss how permanence can help us understand and utilize the structure and evolution of communities by demonstrating that it can be used to -- (i) measure the persistence of a vertex in a community, (ii) design strategies to strengthen the community structure, (iii) explore the core-periphery structure within a community, and (iv) select suitable initiators for message spreading.We further show that permanence is an excellent metric for identifying communities. We demonstrate that the process of maximizing permanence (abbreviated as MaxPerm) produces meaningful communities that concur with the ground-truth community structure of the networks more accurately than eight other popular community detection algorithms. Finally, we provide mathematical proofs to demonstrate the correctness of finding communities by maximizing permanence. In particular, we show that the communities obtained by this method are (i) less affected by the changes in vertex ordering, and (ii) more resilient to resolution limit, degeneracy of solutions, and asymptotic growth of values.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {14},
numpages = {34},
keywords = {community evaluation metric, Permanence, modularity, community discovery}
}

@article{10.1145/3001938,
author = {Zhu, Wen-Yuan and Peng, Wen-Chih and Chen, Ling-Jyh and Zheng, Kai and Zhou, Xiaofang},
title = {Exploiting Viral Marketing for Location Promotion in Location-Based Social Networks},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3001938},
doi = {10.1145/3001938},
abstract = {With the explosion of smartphones and social network services, location-based social networks (LBSNs) are increasingly seen as tools for businesses (e.g., restaurants and hotels) to promote their products and services. In this article, we investigate the key techniques that can help businesses promote their locations by advertising wisely through the underlying LBSNs. In order to maximize the benefit of location promotion, we formalize it as an influence maximization problem in an LBSN, i.e., given a target location and an LBSN, a set of k users (called seeds) should be advertised initially such that they can successfully propagate and attract many other users to visit the target location. Existing studies have proposed different ways to calculate the information propagation probability, that is, how likely it is that a user may influence another, in the setting of a static social network. However, it is more challenging to derive the propagation probability in an LBSN since it is heavily affected by the target location and the user mobility, both of which are dynamic and query dependent. This article proposes two user mobility models, namely the Gaussian-based and distance-based mobility models, to capture the check-in behavior of individual LBSN users, based on which location-aware propagation probabilities can be derived. Extensive experiments based on two real LBSN datasets have demonstrated the superior effectiveness of our proposals compared with existing static models of propagation probabilities to truly reflect the information propagation in LBSNs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {25},
numpages = {28},
keywords = {Propagation probability, check-in behavior, location-based social network, influence maximization}
}

@article{10.1145/2934692,
author = {Fu, Yanjie and Xiong, Hui and Ge, Yong and Zheng, Yu and Yao, Zijun and Zhou, Zhi-Hua},
title = {Modeling of Geographic Dependencies for Real Estate Ranking},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2934692},
doi = {10.1145/2934692},
abstract = {It is traditionally a challenge for home buyers to understand, compare, and contrast the investment value of real estate. Although a number of appraisal methods have been developed to value real properties, the performances of these methods have been limited by traditional data sources for real estate appraisal. With the development of new ways of collecting estate-related mobile data, there is a potential to leverage geographic dependencies of real estate for enhancing real estate appraisal. Indeed, the geographic dependencies of the investment value of an estate can be from the characteristics of its own neighborhood (individual), the values of its nearby estates (peer), and the prosperity of the affiliated latent business area (zone). To this end, in this paper, we propose a geographic method, named ClusRanking, for real estate appraisal by leveraging the mutual enforcement of ranking and clustering power. ClusRanking is able to exploit geographic individual, peer, and zone dependencies in a probabilistic ranking model. Specifically, we first extract the geographic utility of estates from geography data, estimate the neighborhood popularity of estates by mining taxicab trajectory data, and model the influence of latent business areas. Also, we fuse these three influential factors and predict real estate investment value. Moreover, we simultaneously consider individual, peer and zone dependencies, and derive an estate-specific ranking likelihood as the objective function. Furthermore, we propose an improved method named CR-ClusRanking by incorporating checkin information as a regularization term which reduces the performance volatility of real estate ranking system. Finally, we conduct a comprehensive evaluation with the real estate-related data of Beijing, and the experimental results demonstrate the effectiveness of our proposed methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {11},
numpages = {27},
keywords = {Real estate, ranking, clustering, geographic dependencies}
}

@article{10.1145/2936718,
author = {Wang, Wei and Leskovec, Jure},
title = {Introduction to the Special Issue of Best Papers in ACM SIGKDD 2014},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2936718},
doi = {10.1145/2936718},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {33},
numpages = {2}
}

@article{10.1145/2744204,
author = {Wei, Ying and Song, Yangqiu and Zhen, Yi and Liu, Bo and Yang, Qiang},
title = {Heterogeneous Translated Hashing: A Scalable Solution Towards Multi-Modal Similarity Search},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2744204},
doi = {10.1145/2744204},
abstract = {Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods that map heterogeneous data in a common Hamming space. We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {36},
numpages = {28},
keywords = {heterogeneous translated hashing, similarity search, Hash function learning, scalability}
}

@article{10.1145/2943785,
author = {Qiao, Maoying and Xu, Richard Yi Da and Bian, Wei and Tao, Dacheng},
title = {Fast Sampling for Time-Varying Determinantal Point Processes},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2943785},
doi = {10.1145/2943785},
abstract = {Determinantal Point Processes (DPPs) are stochastic models which assign each subset of a base dataset with a probability proportional to the subset’s degree of diversity. It has been shown that DPPs are particularly appropriate in data subset selection and summarization (e.g., news display, video summarizations). DPPs prefer diverse subsets while other conventional models cannot offer. However, DPPs inference algorithms have a polynomial time complexity which makes it difficult to handle large and time-varying datasets, especially when real-time processing is required. To address this limitation, we developed a fast sampling algorithm for DPPs which takes advantage of the nature of some time-varying data (e.g., news corpora updating, communication network evolving), where the data changes between time stamps are relatively small. The proposed algorithm is built upon the simplification of marginal density functions over successive time stamps and the sequential Monte Carlo (SMC) sampling technique. Evaluations on both a real-world news dataset and the Enron Corpus confirm the efficiency of the proposed algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {8},
numpages = {24},
keywords = {fast sampling, Time-varying determinantal point processes (TV-DPPs), sequential Monte Carlo}
}

@article{10.1145/2953882,
author = {Crescenzi, Pierluigi and D'angelo, Gianlorenzo and Severini, Lorenzo and Velaj, Yllka},
title = {Greedily Improving Our Own Closeness Centrality in a Network},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2953882},
doi = {10.1145/2953882},
abstract = {The closeness centrality is a well-known measure of importance of a vertex within a given complex network. Having high closeness centrality can have positive impact on the vertex itself: hence, in this paper we consider the optimization problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. We will consider both the undirected and the directed graph cases. In both cases, we first prove that the optimization problem does not admit a polynomial-time approximation scheme (unless P = NP), and then propose a greedy approximation algorithm (with an almost tight approximation ratio), whose performance is then tested on synthetic graphs and real-world networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {9},
numpages = {32},
keywords = {Approximation algorithms, graph augmentation, greedy algorithm, large networks}
}

@article{10.1145/2934688,
author = {Shao, Junming and Yang, Qinli and Dang, Hoang-Vu and Schmidt, Bertil and Kramer, Stefan},
title = {Scalable Clustering by Iterative Partitioning and Point Attractor Representation},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2934688},
doi = {10.1145/2934688},
abstract = {Clustering very large datasets while preserving cluster quality remains a challenging data-mining task to date. In this paper, we propose an effective scalable clustering algorithm for large datasets that builds upon the concept of synchronization. Inherited from the powerful concept of synchronization, the proposed algorithm, CIPA (Clustering by Iterative Partitioning and Point Attractor Representations), is capable of handling very large datasets by iteratively partitioning them into thousands of subsets and clustering each subset separately. Using dynamic clustering by synchronization, each subset is then represented by a set of point attractors and outliers. Finally, CIPA identifies the cluster structure of the original dataset by clustering the newly generated dataset consisting of points attractors and outliers from all subsets. We demonstrate that our new scalable clustering approach has several attractive benefits: (a) CIPA faithfully captures the cluster structure of the original data by performing clustering on each separate data iteratively instead of using any sampling or statistical summarization technique. (b) It allows clustering very large datasets efficiently with high cluster quality. (c) CIPA is parallelizable and also suitable for distributed data. Extensive experiments demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {5},
numpages = {23},
keywords = {High-performance algorithm, synchronization, scalable clustering}
}

@article{10.1145/2910586,
author = {Wu, Ou and You, Qiang and Xia, Fen and Ma, Lei and Hu, Weiming},
title = {Listwise Learning to Rank from Crowds},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2910586},
doi = {10.1145/2910586},
abstract = {Learning to rank has received great attention in recent years as it plays a crucial role in many applications such as information retrieval and data mining. The existing concept of learning to rank assumes that each training instance is associated with a reliable label. However, in practice, this assumption does not necessarily hold true as it may be infeasible or remarkably expensive to obtain reliable labels for many learning to rank applications. Therefore, a feasible approach is to collect labels from crowds and then learn a ranking function from crowdsourcing labels. This study explores the listwise learning to rank with crowdsourcing labels obtained from multiple annotators, who may be unreliable. A new probabilistic ranking model is first proposed by combining two existing models. Subsequently, a ranking function is trained by proposing a maximum likelihood learning approach, which estimates ground-truth labels and annotator expertise, and trains the ranking function iteratively. In practical crowdsourcing machine learning, valuable side information (e.g., professional grades) about involved annotators is normally attainable. Therefore, this study also investigates learning to rank from crowd labels when side information on the expertise of involved annotators is available. In particular, three basic types of side information are investigated, and corresponding learning algorithms are consequently introduced. Further, the top-k learning to rank from crowdsourcing labels are explored to deal with long training ranking lists. The proposed algorithms are tested on both synthetic and real-world data. Results reveal that the maximum likelihood estimation approach significantly outperforms the average approach and existing crowdsourcing regression methods. The performances of the proposed algorithms are comparable to those of the learning model in consideration reliable labels. The results of the investigation further indicate that side information is helpful in inferring both ranking functions and expertise degrees of annotators.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {4},
numpages = {39},
keywords = {multiple annotators, crowdsourcing, Listwise learning to rank, side information, probabilistic ranking model}
}

@article{10.1145/2948068,
author = {Li, Xiang and Ling, Charles X. and Wang, Huaimin},
title = {The Convergence Behavior of Naive Bayes on Large Sparse Datasets},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2948068},
doi = {10.1145/2948068},
abstract = {Large and sparse datasets with a lot of missing values are common in the big data era, such as user behaviors over a large number of items. Classification in such datasets is an important topic for machine learning and data mining. Practically, naive Bayes is still a popular classification algorithm for large sparse datasets, as its time and space complexity scales linearly with the size of non-missing values. However, several important questions about the behavior of naive Bayes are yet to be answered. For example, how different mechanisms of data missing, data sparsity, and the number of attributes systematically affect the learning curves and convergence? In this paper, we address several common data missing mechanisms and propose novel data generation methods based on these mechanisms. We generate large and sparse data systematically, and study the entire AUC (Area Under ROC Curve) learning curve and convergence behavior of naive Bayes. We not only have several important experiment observations, but also provide detailed theoretic studies. Finally, we summarize our empirical and theoretic results as an intuitive decision flowchart and a useful guideline for classifying large sparse datasets in practice.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {10},
numpages = {24},
keywords = {large sparse data, Convergence behavior}
}

@article{10.1145/2934693,
author = {Zhang, Xianchao and Zong, Linlin and You, Quanzeng and Yong, Xing},
title = {Sampling for Nystr\"{o}m Extension-Based Spectral Clustering: Incremental Perspective and Novel Analysis},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2934693},
doi = {10.1145/2934693},
abstract = {Sampling is the key aspect for Nystr\"{o}m extension based spectral clustering. Traditional sampling schemes select the set of landmark points on a whole and focus on how to lower the matrix approximation error. However, the matrix approximation error does not have direct impact on the clustering performance. In this article, we propose a sampling framework from an incremental perspective, i.e., the landmark points are selected one by one, and each next point to be sampled is determined by previously selected landmark points. Incremental sampling builds explicit relationships among landmark points; thus, they work together well and provide a theoretical guarantee on the clustering performance. We provide two novel analysis methods and propose two schemes for selecting-the-next-one of the framework. The first scheme is based on clusterability analysis, which provides a better guarantee on clustering performance than schemes based on matrix approximation error analysis. The second scheme is based on loss analysis, which provides maximized predictive ability of the landmark points on the (implicit) labels of the unsampled points. Experimental results on a wide range of benchmark datasets demonstrate the superiorities of our proposed incremental sampling schemes over existing sampling schemes.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {7},
numpages = {25},
keywords = {Nystr\"{o}m extension, clusterability analysis, Spectral clustering, incremental sampling, loss analysis}
}

@article{10.1145/2930671,
author = {Yu, Zhiwen and Tian, Miao and Wang, Zhu and Guo, Bin and Mei, Tao},
title = {Shop-Type Recommendation Leveraging the Data from Social Media and Location-Based Services},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2930671},
doi = {10.1145/2930671},
abstract = {It is an important yet challenging task for investors to determine the most suitable type of shop (e.g., restaurant, fashion) for a newly opened store. Traditional ways are predominantly field surveys and empirical estimation, which are not effective as they lack shop-related data. As social media and location-based services (LBS) are becoming more and more pervasive, user-generated data from these platforms are providing rich information not only about individual consumption experiences, but also about shop attributes. In this paper, we investigate the recommendation of shop types for a given location, by leveraging heterogeneous data that are mainly historical user preferences and location context from social media and LBS. Our goal is to select the most suitable shop type, seeking to maximize the number of customers served from a candidate set of types. We propose a novel bias learning matrix factorization method with feature fusion for shop popularity prediction. Features are defined and extracted from two perspectives: location, where features are closely related to location characteristics, and commercial, where features are about the relationships between shops in the neighborhood. Experimental results show that the proposed method outperforms state-of-the-art solutions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {1},
numpages = {21},
keywords = {Social media, shop-type recommendation, location-based services, matrix factorization}
}

@article{10.1145/2898358,
author = {McDowell, Luke K. and Aha, David W.},
title = {Leveraging Neighbor Attributes for Classification in Sparsely Labeled Networks},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2898358},
doi = {10.1145/2898358},
abstract = {Many analysis tasks involve linked nodes, such as people connected by friendship links. Research on link-based classification (LBC) has studied how to leverage these connections to improve classification accuracy. Most such prior research has assumed the provision of a densely labeled training network. Instead, this article studies the common and challenging case when LBC must use a single sparsely labeled network for both learning and inference, a case where existing methods often yield poor accuracy. To address this challenge, we introduce a novel method that enables prediction via “neighbor attributes,” which were briefly considered by early LBC work but then abandoned due to perceived problems. We then explain, using both extensive experiments and loss decomposition analysis, how using neighbor attributes often significantly improves accuracy. We further show that using appropriate semi-supervised learning (SSL) is essential to obtaining the best accuracy in this domain and that the gains of neighbor attributes remain across a range of SSL choices and data conditions. Finally, given the challenges of label sparsity for LBC and the impact of neighbor attributes, we show that multiple previous studies must be re-considered, including studies regarding the best model features, the impact of noisy attributes, and strategies for active learning.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {2},
numpages = {37},
keywords = {link-based classification, collective inference, statistical relational learning, Collective classification}
}

@article{10.1145/2940329,
author = {Grabocka, Josif and Schilling, Nicolas and Schmidt-Thieme, Lars},
title = {Latent Time-Series Motifs},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2940329},
doi = {10.1145/2940329},
abstract = {Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns.However, this paper proposes an entirely new perspective in finding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a function and time-series motifs as its parameters, therefore we learn the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases where they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {6},
numpages = {20},
keywords = {Time series, motifs, repeated patterns}
}

@article{10.1145/2910585,
author = {Chang, Xiaojun and Nie, Feiping and Yang, Yi and Zhang, Chengqi and Huang, Heng},
title = {Convex Sparse PCA for Unsupervised Feature Learning},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2910585},
doi = {10.1145/2910585},
abstract = {Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology, social science, and the like. Classical PCA and its variants seek for linear projections of the original variables to obtain the low-dimensional feature representations with maximal variance. One limitation is that it is difficult to interpret the results of PCA. Besides, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a Convex Sparse Principal Component Analysis (CSPCA) algorithm and apply it to feature learning. First, we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l2, 1-normminimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. Also, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. Our new objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on seven benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {3},
numpages = {16},
keywords = {Principal component analysis (PCA), feature analysis, convex PCA, sparse PCA}
}

@article{10.1145/2753764,
author = {Xu, Silei and Lui, John C. S.},
title = {Product Selection Problem: Improve Market Share by Learning Consumer Behavior},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2753764},
doi = {10.1145/2753764},
abstract = {It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers’ requirements and how consumers make their purchase decisions so that the new products will be competitive in the market. In this paper, we first present a general distance-based product adoption model to capture consumers’ purchase behavior. Using this model, various distance metrics can be used to describe different real life purchase behavior. We then provide a learning algorithm to decide which set of distance metrics one should use when we are given some accessible historical purchase data. Based on the product adoption model, we formalize the k most marketable products (or k-MMP) selection problem and formally prove that the problem is NP-hard. To tackle this problem, we propose an efficient greedy-based approximation algorithm with a provable solution guarantee. Using submodularity analysis, we prove that our approximation algorithm can achieve at least 63% of the optimal solution. We apply our algorithm on both synthetic datasets and real-world datasets (TripAdvisor.com), and show that our algorithm can easily achieve five or more orders of speedup over the exhaustive search and achieve about 96% of the optimal solution on average. Our experiments also demonstrate the robustness of our distance metric learning method, and illustrate how one can adopt it to improve the accuracy of product selection.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {25},
keywords = {submodular set function, approximation algorithm, Product selection, model learning, consumer behavior}
}

@article{10.1145/2746403,
author = {Jiang, Meng and Cui, Peng and Beutel, Alex and Faloutsos, Christos and Yang, Shiqiang},
title = {Catching Synchronized Behaviors in Large Networks: A Graph Mining Approach},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2746403},
doi = {10.1145/2746403},
abstract = {Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, CatchSync, which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (“synchronicity” and “normality”) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, CatchSync has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free; and (c) it is side-information-oblivious: it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied CatchSync on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; CatchSync consistently outperforms existing competitors, both in detection accuracy by 36% on Twitter and 20% on Tencent Weibo, as well as in speed.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {35},
numpages = {27},
keywords = {Anomaly detection, suspicious behavior, graph mining, connectivity pattern}
}

@article{10.1145/2818714,
author = {Shen, Chih-Ya and Yang, De-Nian and Lee, Wang-Chien and Chen, Ming-Syan},
title = {Spatial-Proximity Optimization for Rapid Task Group Deployment},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2818714},
doi = {10.1145/2818714},
abstract = {Spatial proximity is one of the most important factors for the quick deployment of the task groups in various time-sensitive missions. This article proposes a new spatial query, Spatio-Social Team Query (SSTQ), that forms a strong task group by considering (1) the group’s spatial distance (i.e., transportation time), (2) skills of the candidate group members, and (3) social rapport among the candidates. Efficient processing of SSTQ is very challenging, because the aforementioned spatial, skill, and social factors need to be carefully examined. In this article, therefore, we first formulate two subproblems of SSTQ, namely Hop-Constrained Team Problem (HCTP) and Connection-Oriented Team Query (COTQ). HCTP is a decision problem that considers only social and skill dimensions. We prove that HCTP is NP-Complete. Moreover, based on the hardness of HCTP, we prove that SSTQ is NP-Hard and inapproximable within any factor. On the other hand, COTQ is a special case of SSTQ that relaxes the social constraint. We prove that COTQ is NP-Hard and propose an approximation algorithm for COTQ, namely COTprox. Furthermore, based on the observations on COTprox, we devise an approximation algorithm, SSTprox, with a guaranteed error bound for SSTQ. Finally, to efficiently obtain the optimal solution to SSTQ for small instances, we design two efficient algorithms, SpatialFirst and SkillFirst, with different scenarios in mind. These two algorithms incorporate various effective ordering and pruning techniques to reduce the search space for answering SSTQ. Experimental results on real datasets indicate that the proposed algorithms can efficiently answer SSTQ under various parameter settings.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {47},
numpages = {36},
keywords = {Spatial database, query processing, social network}
}

@article{10.1145/2903148,
author = {Chen, Chen and Tong, Hanghang and Prakash, B. Aditya and Eliassi-Rad, Tina and Faloutsos, Michalis and Faloutsos, Christos},
title = {Eigen-Optimization on Large Graphs by Edge Manipulation},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2903148},
doi = {10.1145/2903148},
abstract = {Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms. },
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {49},
numpages = {30},
keywords = {scalability, Edge manipulation, graph mining, immunization}
}

@article{10.1145/2882968,
author = {Tahani, Maryam and Hemmatyar, Ali M. A. and Rabiee, Hamid R. and Ramezani, Maryam},
title = {Inferring Dynamic Diffusion Networks in Online Media},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2882968},
doi = {10.1145/2882968},
abstract = {Online media play an important role in information societies by providing a convenient infrastructure for different processes. Information diffusion that is a fundamental process taking place on social and information networks has been investigated in many studies. Research on information diffusion in these networks faces two main challenges: (1) In most cases, diffusion takes place on an underlying network, which is latent and its structure is unknown. (2) This latent network is not fixed and changes over time. In this article, we investigate the diffusion network extraction (DNE) problem when the underlying network is dynamic and latent. We model the diffusion behavior (existence probability) of each edge as a stochastic process and utilize the Hidden Markov Model (HMM) to discover the most probable diffusion links according to the current observation of the diffusion process, which is the infection time of nodes and the past diffusion behavior of links. We evaluate the performance of our Dynamic Diffusion Network Extraction (DDNE) method, on both synthetic and real datasets. Experimental results show that the performance of the proposed method is independent of the cascade transmission model and outperforms the state of art method in terms of F-measure.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {44},
numpages = {22},
keywords = {Online media, information diffusion, hidden markov model, dynamic network}
}

@article{10.1145/2912122,
author = {Tong, Hanghang and Wang, Fei and Choudhury, Munmun De and Obradovic, Zoran},
title = {Guest Editorial: Special Issue on Connected Health at Big Data Era (BigChat): A TKDD Special Issue},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2912122},
doi = {10.1145/2912122},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {37},
numpages = {4}
}

@article{10.1145/2898359,
author = {Koh, Yun Sing and Ravana, Sri Devi},
title = {Unsupervised Rare Pattern Mining: A Survey},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2898359},
doi = {10.1145/2898359},
abstract = {Association rule mining was first introduced to examine patterns among frequent items. The original motivation for seeking these rules arose from need to examine customer purchasing behaviour in supermarket transaction data. It seeks to identify combinations of items or itemsets, whose presence in a transaction affects the likelihood of the presence of another specific item or itemsets. In recent years, there has been an increasing demand for rare association rule mining. Detecting rare patterns in data is a vital task, with numerous high-impact applications including medical, finance, and security. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for rare pattern mining. We investigate the problems in finding rare rules using traditional association rule mining. As rare association rule mining has not been well explored, there is still specific groundwork that needs to be established. We will discuss some of the major issues in rare association rule mining and also look at current algorithms. As a contribution, we give a general framework for categorizing algorithms: Apriori and Tree based. We highlight the differences between these methods. Finally, we present several real-world application using rare pattern mining in diverse domains. We conclude our survey with a discussion on open and practical challenges in the field.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {45},
numpages = {29},
keywords = {Association rule mining, rare rules, infrequent patterns}
}

@article{10.1145/2903147,
author = {Cheng, Wei and Guo, Zhishan and Zhang, Xiang and Wang, Wei},
title = {CGC: A Flexible and Robust Approach to Integrating Co-Regularized Multi-Domain Graph for Clustering},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2903147},
doi = {10.1145/2903147},
abstract = {Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus, instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this article, we propose a flexible and robust framework, Co-regularized Graph Clustering (CGC), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. We develop an efficient optimization method that guarantees to find the global optimal solution with a given confidence requirement. The proposed method can automatically identify noisy domains and assign smaller weights to them. This helps to obtain optimal graph partition for the focused domain. Extensive experimental results on UCI benchmark datasets, newsgroup datasets, and biological interaction networks demonstrate the effectiveness of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {46},
numpages = {27},
keywords = {co-regularization, Graph clustering, nonnegative matrix factorization}
}

@article{10.1145/2821513,
author = {Yu, Zhiwen and Wang, Zhitao and Chen, Liming and Guo, Bin and Li, Wenjie},
title = {Featuring, Detecting, and Visualizing Human Sentiment in Chinese Micro-Blog},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2821513},
doi = {10.1145/2821513},
abstract = {Micro-blog has been increasingly used for the public to express their opinions, and for organizations to detect public sentiment about social events or public policies. In this article, we examine and identify the key problems of this field, focusing particularly on the characteristics of innovative words, multi-media elements, and hierarchical structure of Chinese “Weibo.” Based on the analysis, we propose a novel approach and develop associated theoretical and technological methods to address these problems. These include a new sentiment word mining method based on three wording metrics and point-wise information, a rule set model for analyzing sentiment features of different linguistic components, and the corresponding methodology for calculating sentiment on multi-granularity considering emoticon elements as auxiliary affective factors. We evaluate our new word discovery and sentiment detection methods on a real-life Chinese micro-blog dataset. Initial results show that our new diction can improve sentiment detection, and they demonstrate that our multi-level rule set method is more effective, with the average accuracy being 10.2% and 1.5% higher than two existing methods for Chinese micro-blog sentiment analysis. In addition, we exploit visualization techniques to study the relationships between online sentiment and real life. The visualization of detected sentiment can help depict temporal patterns and spatial discrepancy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {48},
numpages = {23},
keywords = {rule set--based model, Sentiment detection, sentiment lexicon expansion, visualization}
}

@article{10.1145/2875427,
author = {Zhu, Yada and He, Jingrui},
title = {Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2875427},
doi = {10.1145/2875427},
abstract = {Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {43},
numpages = {18},
keywords = {temporal, semiconductor, structural, Co-clustering}
}

@article{10.1145/2768830,
author = {Cui, Licong and Tao, Shiqiang and Zhang, Guo-Qiang},
title = {Biomedical Ontology Quality Assurance Using a Big Data Approach},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2768830},
doi = {10.1145/2768830},
abstract = {This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {41},
numpages = {28},
keywords = {lattice, SNOMED CT, terminology quality assurance, Hadoop}
}

@article{10.1145/2890508,
author = {Rayana, Shebuti and Akoglu, Leman},
title = {Less is More: Building Selective Anomaly Ensembles},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2890508},
doi = {10.1145/2890508},
abstract = {Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles—hence “less is more”.In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, RealityMining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {42},
numpages = {33},
keywords = {anomaly mining, rank aggregation, Ensemble methods, anomaly ensembles, event detection, unsupervised learning, dynamic graphs}
}

@article{10.1145/2785970,
author = {WU, Yubao and Zhu, Xiaofeng and Li, Li and Fan, Wei and Jin, Ruoming and Zhang, Xiang},
title = {Mining Dual Networks: Models, Algorithms, and Applications},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2785970},
doi = {10.1145/2785970},
abstract = {Finding the densest subgraph in a single graph is a fundamental problem that has been extensively studied. In many emerging applications, there exist dual networks. For example, in genetics, it is important to use protein interactions to interpret genetic interactions. In this application, one network represents physical interactions among nodes, for example, protein--protein interactions, and another network represents conceptual interactions, for example, genetic interactions. Edges in the conceptual network are usually derived based on certain correlation measure or statistical test measuring the strength of the interaction. Two nodes with strong conceptual interaction may not have direct physical interaction.In this article, we propose the novel dual-network model and investigate the problem of finding the densest connected subgraph (DCS), which has the largest density in the conceptual network and is also connected in the physical network. Density in the conceptual network represents the average strength of the measured interacting signals among the set of nodes. Connectivity in the physical network shows how they interact physically. Such pattern cannot be identified using the existing algorithms for a single network. We show that even though finding the densest subgraph in a single network is polynomial time solvable, the DCS problem is NP-hard. We develop a two-step approach to solve the DCS problem. In the first step, we effectively prune the dual networks, while guarantee that the optimal solution is contained in the remaining networks. For the second step, we develop two efficient greedy methods based on different search strategies to find the DCS. Different variations of the DCS problem are also studied. We perform extensive experiments on a variety of real and synthetic dual networks to evaluate the effectiveness and efficiency of the developed methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {40},
numpages = {37},
keywords = {Dual networks, densest connected subgraphs (DCSs)}
}

@article{10.1145/2768831,
author = {Yang, Pei and Yang, Hongxia and Fu, Haoda and Zhou, Dawei and Ye, Jieping and Lappas, Theodoros and He, Jingrui},
title = {Jointly Modeling Label and Feature Heterogeneity in Medical Informatics},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2768831},
doi = {10.1145/2768831},
abstract = {Multiple types of heterogeneity including label heterogeneity and feature heterogeneity often co-exist in many real-world data mining applications, such as diabetes treatment classification, gene functionality prediction, and brain image analysis. To effectively leverage such heterogeneity, in this article, we propose a novel graph-based model for Learning with both Label and Feature heterogeneity, namely L2F. It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. The objective function for L2F is jointly convex. To solve the optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. One appealing feature of L2F is that it is capable of handling data with missing views and labels. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various biomedical datasets show the effectiveness of the proposed approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {39},
numpages = {25},
keywords = {medical informatics, Heterogeneous learning, multi-label learning, multi-view learning}
}

@article{10.1145/2789212,
author = {Xiong, Feiyu and Kam, Moshe and Hrebien, Leonid and Wang, Beilun and Qi, Yanjun},
title = {Kernelized Information-Theoretic Metric Learning for Cancer Diagnosis Using High-Dimensional Molecular Profiling Data},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2789212},
doi = {10.1145/2789212},
abstract = {With the advancement of genome-wide monitoring technologies, molecular expression data have become widely used for diagnosing cancer through tumor or blood samples. When mining molecular signature data, the process of comparing samples through an adaptive distance function is fundamental but difficult, as such datasets are normally heterogeneous and high dimensional. In this article, we present kernelized information-theoretic metric learning (KITML) algorithms that optimize a distance function to tackle the cancer diagnosis problem and scale to high dimensionality. By learning a nonlinear transformation in the input space implicitly through kernelization, KITML permits efficient optimization, low storage, and improved learning of distance metric. We propose two novel applications of KITML for diagnosing cancer using high-dimensional molecular profiling data: (1) for sample-level cancer diagnosis, the learned metric is used to improve the performance of k-nearest neighbor classification; and (2) for estimating the severity level or stage of a group of samples, we propose a novel set-based ranking approach to extend KITML. For the sample-level cancer classification task, we have evaluated on 14 cancer gene microarray datasets and compared with eight other state-of-the-art approaches. The results show that our approach achieves the best overall performance for the task of molecular-expression-driven cancer sample diagnosis. For the group-level cancer stage estimation, we test the proposed set-KITML approach using three multi-stage cancer microarray datasets, and correctly estimated the stages of sample groups for all three studies.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {38},
numpages = {23},
keywords = {high-dimensional data, cancer diagnosis, Metric learning}
}

@article{10.1145/2824443,
author = {Koutra, Danai and Shah, Neil and Vogelstein, Joshua T. and Gallagher, Brian and Faloutsos, Christos},
title = {D<span class="smallcaps SmallerCapital">elta</span>C<span class="smallcaps SmallerCapital">on</span>: Principled Massive-Graph Similarity Function with Attribution},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2824443},
doi = {10.1145/2824443},
abstract = {How much has a network changed since yesterday? How different is the wiring of Bob’s brain (a left-handed male) and Alice’s brain (a right-handed female), and how is it different? Graph similarity with given node correspondence, i.e., the detection of changes in the connectivity of graphs, arises in numerous settings. In this work, we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs. We propose DeltaCon, a principled, intuitive, and scalable algorithm that assesses the similarity between two graphs on the same nodes (e.g., employees of a company, customers of a mobile carrier). In conjunction, we propose DeltaCon-Attr, a related approach that enables attribution of change or dissimilarity to responsible nodes and edges. Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures. Finally, we employ DeltaCon and DeltaCon-Attr on real applications: (a) we classify people to groups of high and low creativity based on their brain connectivity graphs, (b) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph, and (c) recover pairs of test-retest large brain scans ( ∼17M edges, up to 90M edges) for 21 subjects.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {28},
numpages = {43},
keywords = {network monitoring, anomaly detection, edge attribution, graph comparison, node attribution, Graph similarity, culprit nodes and edges, graph classification}
}

@article{10.1145/2845086,
author = {Lu, Faming and Zeng, Qingtian and Duan, Hua},
title = {Synchronization-Core-Based Discovery of Processes with Decomposable Cyclic Dependencies},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2845086},
doi = {10.1145/2845086},
abstract = {Traditional process discovery techniques mine process models based upon event traces giving little consideration to workflow relevant data recorded in event logs. The neglect of such information usually leads to incorrect discovered models, especially when activities have decomposable cyclic dependencies. To address this problem, the recorded workflow relevant data and decision tree learning technique are utilized to classify cases into case clusters. Each case cluster contains causality and concurrency activity dependencies only. Then, a set of activity ordering relations are derived based on case clusters. And a synchronization-core-based process model is discovered from the ordering relations and composite cases. Finally, the discovered model is transformed to a BPMN model. The proposed approach is validated with a medical treatment process and an open event log. Meanwhile, a prototype system is presented.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {31},
numpages = {29},
keywords = {process discovery, decision tree, Process mining, BPMN}
}

@article{10.1145/2742549,
author = {Guo, Zhen and Zhang, Zhongfei (Mark) and Xing, Eric P. and Faloutsos, Christos},
title = {Multimodal Data Mining in a Multimedia Database Based on Structured Max Margin Learning},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2742549},
doi = {10.1145/2742549},
abstract = {Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article, we exploit the relations among different modalities in a multimedia database and present a framework for general multimodal data mining problem where image annotation and image retrieval are considered as the special cases. Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In addition, in order to reduce the demanding computation, we propose a new max margin structure learning approach called Enhanced Max Margin Learning (EMML) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations. Furthermore, we apply EMML framework to develop an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale. The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {23},
numpages = {30},
keywords = {Multimodal data mining, max margin, image retrieval, image annotation}
}

@article{10.1145/2822897,
author = {Myers, Risa B. and Frenzel MD, John C. and Ruiz Md, Joseph R. and Jermaine, Christopher M.},
title = {Do Anesthesiologists Know What They Are Doing? Mining a Surgical Time-Series Database to Correlate Expert Assessment with Outcomes},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2822897},
doi = {10.1145/2822897},
abstract = {Anesthesiologists are taught to carefully manage patient vital signs during surgery. Unfortunately, there is little empirical evidence that vital sign management, as currently practiced, is correlated with patient outcomes. We seek to validate or repudiate current clinical practice and determine whether or not clinician evaluation of surgical vital signs correlate with outcomes. Using a database of over 90,000 cases, we attempt to determine whether those cases that anesthesiologists would subjectively decide are “low quality” are more likely to result in negative outcomes. The problem reduces to one of multi-dimensional time-series classification. Our approach is to have a set of expert anesthesiologists independently label a small number of training cases, from which we build classifiers and label all 90,000 cases. We then use the labeling to search for correlation with outcomes and compare the prevalence of important 30-day outcomes between providers.To mimic the providers’ quality labels, we consider several standard classification methods, such as dynamic time warping in conjunction with a kNN classifier, as well as complexity invariant distance, and a regression based upon the feature extraction methods outlined by Mao et al. 2012 (using features such as time-series mean, standard deviation, skew, etc.). We also propose a new feature selection mechanism that learns a hidden Markov model to segment the time series; the fraction of time that each series spends in each state is used to label the series using a regression-based classifier. In the end, we obtain strong, empirical evidence that current best practice is correlated with reduced negative patient outcomes. We also learn that all of the experts were able to significantly separate cases by outcome, with higher prevalence of negative 30-day outcomes in the cases labeled as “low quality” for almost all of the outcomes investigated.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {24},
numpages = {27},
keywords = {ordinal regression, hidden Markov model (HMM), Vital signs}
}

@article{10.1145/2842629,
author = {Zhao, Wayne Xin and Wang, Jinpeng and He, Yulan and Wen, Ji-Rong and Chang, Edward Y. and Li, Xiaoming},
title = {Mining Product Adopter Information from Online Reviews for Improving Product Recommendation},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2842629},
doi = {10.1145/2842629},
abstract = {We present in this article an automated framework that extracts product adopter information from online reviews and incorporates the extracted information into feature-based matrix factorization for more effective product recommendation. In specific, we propose a bootstrapping approach for the extraction of product adopters from review text and categorize them into a number of different demographic categories. The aggregated demographic information of many product adopters can be used to characterize both products and users in the form of distributions over different demographic categories. We further propose a graph-based method to iteratively update user- and product-related distributions more reliably in a heterogeneous user--product graph and incorporate them as features into the matrix factorization approach for product recommendation. Our experimental results on a large dataset crawled from JingDong, the largest B2C e-commerce website in China, show that our proposed framework outperforms a number of competitive baselines for product recommendation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {29},
numpages = {23},
keywords = {Online review, product adopter, product recommendation, matrix factorisation}
}

@article{10.1145/2775108,
author = {Prat-P\'{e}rez, Arnau and Dominguez-Sal, David and Brunat, Josep-M. and Larriba-Pey, Josep-Lluis},
title = {Put Three and Three Together: Triangle-Driven Community Detection},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2775108},
doi = {10.1145/2775108},
abstract = {Community detection has arisen as one of the most relevant topics in the field of graph data mining due to its applications in many fields such as biology, social networks, or network traffic analysis. Although the existing metrics used to quantify the quality of a community work well in general, under some circumstances, they fail at correctly capturing such notion. The main reason is that these metrics consider the internal community edges as a set, but ignore how these actually connect the vertices of the community. We propose the Weighted Community Clustering (WCC), which is a new community metric that takes the triangle instead of the edge as the minimal structural motif indicating the presence of a strong relation in a graph. We theoretically analyse WCC in depth and formally prove, by means of a set of properties, that the maximization of WCC guarantees communities with cohesion and structure. In addition, we propose Scalable Community Detection (SCD), a community detection algorithm based on WCC, which is designed to be fast and scalable on SMP machines, showing experimentally that WCC correctly captures the concept of community in social networks using real datasets. Finally, using ground-truth data, we show that SCD provides better quality than the best disjoint community detection algorithms of the state of the art while performing faster.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {22},
numpages = {42},
keywords = {triangles, Community detection, social networks, scalable algorithm, parallel algorithm}
}

@article{10.1145/2829955,
author = {Duarte, Jo\~{a}o and Gama, Jo\~{a}o and Bifet, Albert},
title = {Adaptive Model Rules From High-Speed Data Streams},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2829955},
doi = {10.1145/2829955},
abstract = {Decision rules are one of the most expressive and interpretable models for machine learning. In this article, we present Adaptive Model Rules (AMRules), the first stream rule learning algorithm for regression problems. In AMRules, the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. In order to maintain a regression model compatible with the most recent state of the process generating data, each rule uses a Page-Hinkley test to detect changes in this process and react to changes by pruning the rule set. Online learning might be strongly affected by outliers. AMRules is also equipped with outliers detection mechanisms to avoid model adaption using anomalous examples. In the experimental section, we report the results of AMRules on benchmark regression problems, and compare the performance of our system with other streaming regression algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {30},
numpages = {22},
keywords = {Data streams, rule learning, regression}
}

@article{10.1145/2815625,
author = {Subbian, Karthik and Aggarwal, Charu and Srivastava, Jaideep},
title = {Mining Influencers Using Information Flows in Social Streams},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2815625},
doi = {10.1145/2815625},
abstract = {The problem of discovering information flow trends in social networks has become increasingly relevant due to the increasing amount of content in online social networks, and its relevance as a tool for research into the content trends analysis in the network. An important part of this analysis is to determine the key patterns of flow in the underlying network. Almost all the work in this area has focused on fixed models of the network structure, and edge-based transmission between nodes. In this article, we propose a fully content-centered model of flow analysis in networks, in which the analysis is based on actual content transmissions in the underlying social stream, rather than a static model of transmission on the edges. First, we introduce the problem of influence analysis in the context of information flow in networks. We then propose a novel algorithm InFlowMine to discover the information flow patterns in the network and demonstrate the effectiveness of the discovered information flows using an influence mining application. This application illustrates the flexibility and effectiveness of our information flow model to find topic- or network-specific influencers, or their combinations. We empirically show that our information flow mining approach is effective and efficient than the existing methods on a number of different measures.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {26},
numpages = {28},
keywords = {influencer analysis, information flows, Network analysis}
}

@article{10.1145/2818378,
author = {Namata, Galileo Mark and London, Ben and Getoor, Lise},
title = {Collective Graph Identification},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2818378},
doi = {10.1145/2818378},
abstract = {Data describing networks—such as communication networks, transaction networks, disease transmission networks, collaboration networks, etc.—are becoming increasingly available. While observational data can be useful, it often only hints at the actual underlying process that governs interactions and attributes. For example, an email communication network provides insight into its users and their relationships, but is not the same as the “real” underlying social network. In this article, we introduce the problem of graph identification, i.e., discovering the latent graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence. This entails solving several canonical problems in network analysis: entity resolution (determining when two observations correspond to the same entity), link prediction (inferring the existence of links), and node labeling (inferring hidden attributes). While each of these subproblems has been well studied in isolation, here we consider them as a single, collective task. We present a simple, yet novel, approach to address all three subproblems simultaneously. Our approach, which we refer to as C3, consists of a collection of Coupled Collective Classifiers that are applied iteratively to propagate inferred information among the subproblems. We consider variants of C3 using different learning and inference techniques and empirically demonstrate that C3 is superior, both in terms of predictive accuracy and running time, to state-of-the-art probabilistic approaches on four real problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {25},
numpages = {36},
keywords = {link prediction, collective classification, Entity resolution, semi-supervised learning}
}

@article{10.1145/2829956,
author = {Angiulli, Fabrizio and Fassetti, Fabio},
title = {Toward Generalizing the Unification with Statistical Outliers: The Gradient Outlier Factor Measure},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2829956},
doi = {10.1145/2829956},
abstract = {In this work, we introduce a novel definition of outlier, namely the Gradient Outlier Factor (or GOF), with the aim to provide a definition that unifies with the statistical one on some standard distributions but has a different behavior in the presence of mixture distributions. Intuitively, the GOF score measures the probability to stay in the neighborhood of a certain object. It is directly proportional to the density and inversely proportional to the variation of the density. We derive formal properties under which the GOF definition unifies the statistical outlier definition and show that the unification holds for some standard distributions, while the GOF is able to capture tails in the presence of different distributions even if their densities sensibly differ. Moreover, we provide a probabilistic interpretation of the GOF score, by means of the notion of density of the data density. Experimental results confirm that there are scenarios in which the novel definition can be profitably employed. To the best of our knowledge, except for distance-based outlier, no other data mining outlier definition has a so clearly established relationship with statistical outliers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {27},
numpages = {26},
keywords = {unification between outlier definitions, Outlier detection, statistical outliers}
}

@article{10.1145/2798730,
author = {Rowe, Matthew},
title = {Mining User Development Signals for Online Community Churner Detection},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2798730},
doi = {10.1145/2798730},
abstract = {Churners are users who stop using a given service after previously signing up. In the domain of telecommunications and video games, churners represent a loss of revenue as a user leaving indicates that they will no longer pay for the service. In the context of online community platforms (e.g., community message boards, social networking sites, question--answering systems, etc.), the churning of a user can represent different kinds of loss: of social capital, of expertise, or of a vibrant individual who is a mediator for interaction and communication. Detecting which users are likely to churn from online communities, therefore, enables community managers to offer incentives to entice those users back; as retention is less expensive than re-signing users up. In this article, we tackle the task of detecting churners on four online community platforms by mining user development signals. These signals explain how users have evolved along different dimensions (i.e., social and lexical) relative to their prior behaviour and the community in which they have interacted. We present a linear model, based upon elastic-net regularisation, that uses extracted features from the signals to detect churners. Our evaluation of this model against several state of the art baselines, including our own prior work, empirically demonstrates the superior performance that this approach achieves for several experimental settings. This article presents a novel approach to churn prediction that takes a different route from existing approaches that are based on measuring static social network properties of users (e.g., centrality, in-degree, etc.).},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {21},
numpages = {28},
keywords = {social dynamics, Churn prediction, lifecycle mining, lexical terms, online communities}
}

@article{10.1145/2791295,
author = {Liu, Yashu and Wang, Jie and Ye, Jieping},
title = {An Efficient Algorithm For Weak Hierarchical Lasso},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2791295},
doi = {10.1145/2791295},
abstract = {Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy that has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces a sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution to its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this article, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the General Iterative Shrinkage and Thresholding (GIST) optimization framework, which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed-form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. We have conducted extensive experiments on both synthetic and real datasets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. In addition, we extend the technique to perform the optimization-based hierarchical testing of pairwise interactions for binary classification problems, which is essentially the proximal operator associated with weak hierarchical Lasso. Simulation studies show that the non-convex hierarchical testing framework outperforms the convex relaxation when a hierarchical structure exists between main effects and interactions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {32},
numpages = {24},
keywords = {proximal operator, weak hierarchical Lasso, Sparse learning, non-convex}
}

@article{10.1145/2811268,
author = {Yu, Rose and He, Xinran and Liu, Yan},
title = {GLAD: Group Anomaly Detection in Social Media Analysis},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2811268},
doi = {10.1145/2811268},
abstract = {Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups. Therefore, it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this article, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pairwise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {18},
numpages = {22},
keywords = {topic modeling, Group anomaly, community detection}
}

@article{10.1145/2757282,
author = {Li, Lei and Peng, Wei and Kataria, Saurabh and Sun, Tong and Li, Tao},
title = {Recommending Users and Communities in Social Media},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2757282},
doi = {10.1145/2757282},
abstract = {Social media has become increasingly prevalent in the last few years, not only enabling people to connect with each other by social links, but also providing platforms for people to share information and interact over diverse topics. Rich user-generated information, for example, users’ relationships and daily posts, are often available in most social media service websites. Given such information, a challenging problem is to provide reasonable user and community recommendation for a target user, and consequently, help the target user engage in the daily discussions and activities with his/her friends or like-minded people. In this article, we propose a unified framework of recommending users and communities that utilizes the information in social media. Given a user’s profile or a set of keywords as input, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user or keywords. With the proposed framework, users can find other individuals or communities sharing similar interests, and then have more interaction with these users or within the communities. We present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topical interests of users, as well as the focuses of communities. Extensive experimental evaluation and case studies on a dataset collected from Twitter demonstrate the effectiveness of our proposed framework compared with other probabilistic-topic-model-based recommendation methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {17},
numpages = {27},
keywords = {User recommendation, community recommendation, social media, user-community-topic model}
}

@article{10.1145/2803176,
author = {Zhang, Yao and Prakash, B. Aditya},
title = {Data-Aware Vaccine Allocation Over Large Networks},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2803176},
doi = {10.1145/2803176},
abstract = {Given a graph, like a social/computer network or the blogosphere, in which an infection (or meme or virus) has been spreading for some time, how to select the k best nodes for immunization/quarantining immediately? Most previous works for controlling propagation (say via immunization) have concentrated on developing strategies for vaccination preemptively before the start of the epidemic. While very useful to provide insights in to which baseline policies can best control an infection, they may not be ideal to make real-time decisions as the infection is progressing.In this paper, we study how to immunize healthy nodes, in the presence of already infected nodes. Efficient algorithms for such a problem can help public-health experts make more informed choices, tailoring their decisions to the actual distribution of the epidemic on the ground. First we formulate the Data-Aware Vaccination problem, and prove it is NP-hard and also that it is hard to approximate. Secondly, we propose three effective polynomial-time heuristics DAVA, DAVA-prune and DAVA-fast, of varying degrees of efficiency and performance. Finally, we also demonstrate the scalability and effectiveness of our algorithms through extensive experiments on multiple real networks including large epidemiology datasets (containing millions of interactions). Our algorithms show substantial gains of up to ten times more healthy nodes at the end against many other intuitive and nontrivial competitors.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {20},
numpages = {32},
keywords = {social networks, Graph mining, diffusion, immunization}
}

@article{10.1145/2757281,
author = {Papagelis, Manos},
title = {Refining Social Graph Connectivity via Shortcut Edge Addition},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2757281},
doi = {10.1145/2757281},
abstract = {Small changes on the structure of a graph can have a dramatic effect on its connectivity. While in the traditional graph theory, the focus is on well-defined properties of graph connectivity, such as biconnectivity, in the context of a social graph, connectivity is typically manifested by its ability to carry on social processes. In this paper, we consider the problem of adding a small set of nonexisting edges (shortcuts) in a social graph with the main objective of minimizing its characteristic path length. This property determines the average distance between pairs of vertices and essentially controls how broadly information can propagate through a network. We formally define the problem of interest, characterize its hardness and propose a novel method, path screening, which quickly identifies important shortcuts to guide the augmentation of the graph. We devise a sampling-based variant of our method that can scale up the computation in larger graphs. The claims of our methods are formally validated. Through experiments on real and synthetic data, we demonstrate that our methods are a multitude of times faster than standard approaches, their accuracy outperforms sensible baselines and they can ease the spread of information in a network, for a varying range of conditions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {12},
numpages = {35},
keywords = {network engineering, propagation, mixing time, Graph augmentation, conductance, shortcuts, social networks}
}

@article{10.1145/2778990,
author = {Chakrabarti, Aniket and Satuluri, Venu and Srivathsan, Atreya and Parthasarathy, Srinivasan},
title = {A Bayesian Perspective on Locality Sensitive Hashing with Extensions for Kernel Methods},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2778990},
doi = {10.1145/2778990},
abstract = {Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. In order to reduce the number of candidates to search, locality-sensitive hashing (LSH) based indexing methods are very effective. However, most such methods only use LSH for the first phase of similarity search—that is, efficient indexing for candidate generation. In this article, we present BayesLSH, a principled Bayesian algorithm for the subsequent phase of similarity search—performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH’s output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2 \texttimes{} --20 \texttimes{} for a wide variety of datasets.We also extend the BayesLSH algorithm for kernel methods—in which the similarity between two data objects is defined by a kernel function. Since the embedding of data points in the transformed kernel space is unknown, algorithms such as AllPairs which rely on building inverted index structure for fast similarity search do not work with kernel functions. Exhaustive search across all possible pairs is also not an option since the dataset can be huge and computing the kernel values for each pair can be prohibitive. We propose K-BayesLSH an all-pairs similarity search problem for kernel functions. K-BayesLSH leverages a recently proposed idea—kernelized locality sensitive hashing (KLSH)—for hash bit computation and candidate generation, and uses the aforementioned BayesLSH idea for candidate pruning and similarity estimation. We ran a broad spectrum of experiments on a variety of datasets drawn from different domains and with distinct kernels and find a speedup of 2 \texttimes{} --7 \texttimes{} over vanilla KLSH.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {19},
numpages = {32},
keywords = {all-pairs similarity search, kernel similarity measure, Locality-sensitive hashing, bayesian inference}
}

@article{10.1145/2736698,
author = {Jia, Adele Lu and Shen, Siqi and Bovenkamp, Ruud Van De and Iosup, Alexandru and Kuipers, Fernando and Epema, Dick H. J.},
title = {Socializing by Gaming: Revealing Social Relationships in Multiplayer Online Games},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2736698},
doi = {10.1145/2736698},
abstract = {Multiplayer Online Games (MOGs) like Defense of the Ancients and StarCraft II have attracted hundreds of millions of users who communicate, interact, and socialize with each other through gaming. In MOGs, rich social relationships emerge and can be used to improve gaming services such as match recommendation and game population retention, which are important for the user experience and the commercial value of the companies who run these MOGs. In this work, we focus on understanding social relationships in MOGs. We propose a graph model that is able to capture social relationships of a variety of types and strengths. We apply our model to real-world data collected from three MOGs that contain in total over ten years of behavioral history for millions of players and matches. We compare social relationships in MOGs across different game genres and with regular online social networks like Facebook. Taking match recommendation as an example application of our model, we propose SAMRA, a Socially Aware Match Recommendation Algorithm that takes social relationships into account. We show that our model not only improves the precision of traditional link prediction approaches, but also potentially helps players enjoy games to a higher extent.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {11},
numpages = {29},
keywords = {graph model, user interaction, Multiplayer Online Games (MOGs), social relationship}
}

@article{10.1145/2753765,
author = {Zhang, Lei and Luo, Ping and Tang, Linpeng and Chen, Enhong and Liu, Qi and Wang, Min and Xiong, Hui},
title = {Occupancy-Based Frequent Pattern Mining<sup>*</sup>},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2753765},
doi = {10.1145/2753765},
abstract = {Frequent pattern mining is an important data mining problem with many broad applications. Most studies in this field use support (frequency) to measure the popularity of a pattern, namely the fraction of transactions or sequences that include the pattern in a data set. In this study, we introduce a new interesting measure, namely occupancy, to measure the completeness of a pattern in its supporting transactions or sequences. This is motivated by some real-world pattern recommendation applications in which an interesting pattern should not only be frequent, but also occupies a large portion of its supporting transactions or sequences. With the definition of occupancy we call a pattern dominant if its occupancy value is above a user-specified threshold. Then, our task is to identify the qualified patterns which are both dominant and frequent. Also, we formulate the problem of mining top-k qualified patterns, that is, finding k qualified patterns with maximum values on a user-defined function of support and occupancy, for example, weighted sum of support and occupancy. The challenge to these tasks is that the value of occupancy does not change monotonically when more items are appended to a given pattern. Therefore, we propose a general algorithm called DOFRA (DOminant and FRequent pattern mining Algorithm) for mining these qualified patterns, which explores the upper bound properties on occupancy to drastically reduce the search process. Finally, we show the effectiveness of DOFRA in two real-world applications and also demonstrate the efficiency of DOFRA on several real and large synthetic datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {14},
numpages = {33},
keywords = {dominant pattern, bound estimation, Occupancy}
}

@article{10.1145/2776894,
author = {Chen, Hung-Hsuan and Giles, C. Lee},
title = {ASCOS++: An Asymmetric Similarity Measure for Weighted Networks to Address the Problem of SimRank},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2776894},
doi = {10.1145/2776894},
abstract = {In this article, we explore the relationships among digital objects in terms of their similarity based on vertex similarity measures. We argue that SimRank—a famous similarity measure—and its families, such as P-Rank and SimRank++, fail to capture similar node pairs in certain conditions, especially when two nodes can only reach each other through paths of odd lengths. We present new similarity measures ASCOS and ASCOS++ to address the problem. ASCOS outputs a more complete similarity score than SimRank and SimRank’s families. ASCOS++ enriches ASCOS to include edge weight into the measure, giving all edges and network weights an opportunity to make their contribution. We show that both ASCOS++ and ASCOS can be reformulated and applied on a distributed environment for parallel contribution. Experimental results show that ASCOS++ reports a better score than SimRank and several famous similarity measures. Finally, we re-examine previous use cases of SimRank, and explain appropriate and inappropriate use cases. We suggest future SimRank users following the rules proposed here before na\"{\i}vely applying it. We also discuss the relationship between ASCOS++ and PageRank.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {15},
numpages = {26},
keywords = {Vertex similarity, link prediction, link analysis, SimRank, coauthor network, ASCOS++}
}

@article{10.1145/2751562,
author = {Hong, Liang and Zou, Lei and Zeng, Cheng and Zhang, Luming and Wang, Jian and Tian, Jilei},
title = {Context-Aware Recommendation Using Role-Based Trust Network},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2751562},
doi = {10.1145/2751562},
abstract = {Recommender systems have been studied comprehensively in both academic and industrial fields over the past decade. As user interests can be affected by context at any time and any place in mobile scenarios, rich context information becomes more and more important for personalized context-aware recommendations. Although existing context-aware recommender systems can make context-aware recommendations to some extent, they suffer several inherent weaknesses: (1) Users’ context-aware interests are not modeled realistically, which reduces the recommendation quality; (2) Current context-aware recommender systems ignore trust relations among users. Trust relations are actually context-aware and associated with certain aspects (i.e., categories of items) in mobile scenarios. In this article, we define a term role to model common context-aware interests among a group of users. We propose an efficient role mining algorithm to mine roles from a “user-context-behavior” matrix, and a role-based trust model to calculate context-aware trust value between two users. During online recommendation, given a user u in a context c, an efficient weighted set similarity query (WSSQ) algorithm is designed to build u’s role-based trust network in context c. Finally, we make recommendations to u based on u’s role-based trust network by considering both context-aware roles and trust relations. Extensive experiments demonstrate that our recommendation approach outperforms the state-of-the-art methods in both effectiveness and efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {13},
numpages = {25},
keywords = {context-aware, Recommendation, role, trust network}
}

@article{10.1145/2747880,
author = {Zafarani, Reza and Tang, Lei and Liu, Huan},
title = {User Identification Across Social Media},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2747880},
doi = {10.1145/2747880},
abstract = {People use various social media sites for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built. This profile can help improve online services such as advertising across sites. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We provide evidence on the existence of a mapping among identities of individuals across social media sites, study the feasibility of finding this mapping, and illustrate and develop means for finding this mapping. Our studies show that effective approaches that exploit information redundancies due to users’ unique behavioral patterns can be utilized to find such a mapping. This study paves the way for analysis and mining across social networking sites, and facilitates the creation of novel online services across sites. In particular, recommending friends and advertising across networks, analyzing information diffusion across sites, and studying specific user behavior such as user migration across sites in social media are one of the many areas that can benefit from the results of this study.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {16},
numpages = {30},
keywords = {cross-media analysis, behavioral modeling, Social network analysis, user identification}
}

@article{10.1145/2710021,
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
title = {Large-Scale Cross-Language Web Page Classification via Dual Knowledge Transfer Using Fast Nonnegative Matrix Trifactorization},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2710021},
doi = {10.1145/2710021},
abstract = {With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised learning methods usually require a large amount of training data to obtain accurate and robust classification models. However, labeled Web pages did not increase as fast as the growth of Internet. The lack of sufficient training Web pages in many languages, especially for those in uncommonly used languages, makes it a challenge for traditional classification algorithms to achieve satisfactory performance. To address this, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. In addition, we also observe that the associations between word clusters and Web page classes are another type of reliable carriers to transfer knowledge across languages. With these recognitions, in this article we propose a novel joint nonnegative matrix trifactorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification. Our approach transfers knowledge from the auxiliary language, in which abundant labeled Web pages are available, to the target languages, in which we want to classify Web pages, through two different paths: word cluster approximation and the associations between word clusters and Web page classes. With the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. In order to deal with the large-scale real world data, we further develop the proposed DKT approach by constraining the factor matrices of NMTF to be cluster indicator matrices. Due to the nature of cluster indicator matrices, we can decouple the proposed optimization objective and the resulted subproblems are of much smaller sizes involving much less matrix multiplications, which make our new approach much more computationally efficient. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results have demonstrated the effectiveness of our approach that are consistent with our theoretical analyses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {1},
numpages = {29},
keywords = {nonnegative matrix trifactorization, Cross-language classification, large-scale data, cluster indicator matrix, knowledge transfer}
}

@article{10.1145/2733381,
author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Zimek, Arthur and Sander, J\"{o}rg},
title = {Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2733381},
doi = {10.1145/2733381},
abstract = {An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan’s classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of “outlierness” can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a “flat” (i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {5},
numpages = {51},
keywords = {hierarchical and nonhierarchical clustering, data visualization, outlier detection, Density-based clustering, unsupervised and semisupervised clustering, global/local outliers}
}

@article{10.1145/2717314,
author = {Zhou, Yang and Liu, Ling},
title = {Social Influence Based Clustering and Optimization over Heterogeneous Information Networks},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2717314},
doi = {10.1145/2717314},
abstract = {Social influence analysis has shown great potential for strategic marketing decision. It is well known that people influence one another based on both their social connections and the social activities that they have engaged in the past. In this article, we develop an innovative and high-performance social influence based graph clustering framework with four unique features. First, we explicitly distinguish social connection based influence (self-influence) and social activity based influence (co-influence). We compute the self-influence similarity between two members based on their social connections within a single collaboration network, and compute the co-influence similarity by taking into account not only the set of activities that people participate but also the semantic association between these activities. Second, we define the concept of influence-based similarity by introducing a unified influence-based similarity matrix that employs an iterative weight update method to integrate self-influence and co-influence similarities. Third, we design a dynamic learning algorithm, called SI-Cluster, for social influence based graph clustering. It iteratively partitions a large social collaboration network into K clusters based on both the social network itself and the multiple associated activity information networks, each representing a category of activities that people have engaged. To make the SI-Cluster algorithm converge fast, we transform sophisticated nonlinear fractional programming problem with respect to multiple weights into a straightforward nonlinear parametric programming problem of single variable. Finally, we develop an optimization technique of diagonalizable-matrix approximation to speed up the computation of self-influence similarity and co-influence similarities. Our SI-Cluster-Opt significantly improves the efficiency of SI-Cluster on large graphs while maintaining high quality of clustering results. Extensive experimental evaluation on three real-world graphs shows that, compared to existing representative graph clustering algorithms, our SI-Cluster-Opt approach not only achieves a very good balance between self-influence and co-influence similarities but also scales extremely well for clustering large graphs in terms of time complexity while meeting the guarantee of high density, low entropy and low Davies--Bouldin Index.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {2},
numpages = {53},
keywords = {heterogeneous information network, Graph clustering, social influence}
}

@article{10.1145/2733380,
author = {Ahmed, Rezwan and Karypis, George},
title = {Algorithms for Mining the Coevolving Relational Motifs in Dynamic Networks},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2733380},
doi = {10.1145/2733380},
abstract = {Computational methods and tools that can efficiently and effectively analyze the temporal changes in dynamic complex relational networks enable us to gain significant insights regarding the entity relations and their evolution. This article introduces a new class of dynamic graph patterns, referred to as coevolving relational motifs (CRMs), which are designed to identify recurring sets of entities whose relations change in a consistent way over time. CRMs can provide evidence to the existence of, possibly unknown, coordination mechanisms by identifying the relational motifs that evolve in a similar and highly conserved fashion. We developed an algorithm to efficiently analyze the frequent relational changes between the entities of the dynamic networks and capture all frequent coevolutions as CRMs. Our algorithm follows a depth-first exploration of the frequent CRM lattice and incorporates canonical labeling for redundancy elimination. Experimental results based on multiple real world dynamic networks show that the method is able to efficiently identify CRMs. In addition, a qualitative analysis of the results shows that the discovered patterns can be used as features to characterize the dynamic network.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {4},
numpages = {31},
keywords = {evolving pattern mining, Dynamic networks, network evolution}
}

@article{10.1145/2729980,
author = {Papalexakis, Evangelos E. and Faloutsos, Christos and Sidiropoulos, Nicholas D.},
title = {P<span class="smallcaps SmallerCapital">ar</span>C<span class="smallcaps SmallerCapital">ube</span>: Sparse Parallelizable CANDECOMP-PARAFAC Tensor Decomposition},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2729980},
doi = {10.1145/2729980},
abstract = {How can we efficiently decompose a tensor into sparse factors, when the data do not fit in memory? Tensor decompositions have gained a steadily increasing popularity in data-mining applications; however, the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets. In this work, we propose ParCube, a new and highly parallelizable method for speeding up tensor decompositions that is well suited to produce sparse approximations. Experiments with even moderately large data indicate over 90% sparser outputs and 14 times faster execution, with approximation error close to the current state of the art irrespective of computation and memory requirements. We provide theoretical guarantees for the algorithm’s correctness and we experimentally validate our claims through extensive experiments, including four different real world datasets (Enron, Lbnl, Facebook and Nell), demonstrating its effectiveness for data-mining practitioners. In particular, we are the first to analyze the very large Nell dataset using a sparse tensor decomposition, demonstrating that ParCube enables us to handle effectively and efficiently very large datasets. Finally, we make our highly scalable parallel implementation publicly available, enabling reproducibility of our work.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {3},
numpages = {25},
keywords = {parallel algorithms, sparsity, sampling, Tensors, PARAFAC decomposition, randomized algorithms}
}

@article{10.1145/2747879,
author = {Zhang, Xianchao and Zhang, Xiaotong and Liu, Han},
title = {Smart Multitask Bregman Clustering and Multitask Kernel Clustering},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2747879},
doi = {10.1145/2747879},
abstract = {Traditional clustering algorithms deal with a single clustering task on a single dataset. However, there are many related tasks in the real world, which motivates multitask clustering. Recently some multitask clustering algorithms have been proposed, and among them multitask Bregman clustering (MBC) is a very applicable method. MBC alternatively updates clusters and learns relationships between clusters of different tasks, and the two phases boost each other. However, the boosting does not always have positive effects on improving the clustering performance, it may also cause negative effects. Another issue of MBC is that it cannot deal with nonlinear separable data. In this article, we show that in MBC, the process of using cluster relationship to boost the cluster updating phase may cause negative effects, that is, cluster centroids may be skewed under some conditions. We propose a smart multitask Bregman clustering (S-MBC) algorithm which can identify the negative effects of the boosting and avoid the negative effects if they occur. We then propose a multitask kernel clustering (MKC) framework for nonlinear separable data by using a similar framework like MBC in the kernel space. We also propose a specific optimization method, which is quite different from that of MBC, to implement the MKC framework. Since MKC can also cause negative effects like MBC, we further extend the framework of MKC to a smart multitask kernel clustering (S-MKC) framework in a similar way that S-MBC is extended from MBC. We conduct experiments on 10 real world multitask clustering datasets to evaluate the performance of S-MBC and S-MKC. The results on clustering accuracy show that: (1) compared with the original MBC algorithm MBC, S-MBC and S-MKC perform much better; (2) compared with the convex discriminative multitask relationship clustering (DMTRC) algorithms DMTRC-L and DMTRC-R which also avoid negative transfer, S-MBC and S-MKC perform worse in the (ideal) case in which different tasks have the same cluster number and the empirical label marginal distribution in each task distributes evenly, but better or comparable in other (more general) cases. Moreover, S-MBC and S-MKC can work on the datasets in which different tasks have different number of clusters, violating the assumptions of DMTRC-L and DMTRC-R. The results on efficiency show that S-MBC and S-MKC consume more computational time than MBC and less computational time than DMTRC-L and DMTRC-R. Overall S-MBC and S-MKC are competitive compared with the state-of-the-art multitask clustering algorithms in synthetical terms of accuracy, efficiency and applicability.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {8},
numpages = {29},
keywords = {negative transfer, Multitask clustering, Mercer kernel, Bregman divergence}
}

@article{10.1145/2742801,
author = {Yu, Zhiwen and Wang, Zhu and He, Huilei and Tian, Jilei and Lu, Xinjiang and Guo, Bin},
title = {Discovering Information Propagation Patterns in Microblogging Services},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2742801},
doi = {10.1145/2742801},
abstract = {During the last decade, microblog has become an important social networking service with billions of users all over the world, acting as a novel and efficient platform for the creation and dissemination of real-time information. Modeling and revealing the information propagation patterns in microblogging services cannot only lead to more accurate understanding of user behaviors and provide insights into the underlying sociology, but also enable useful applications such as trending prediction, recommendation and filtering, spam detection and viral marketing. In this article, we aim to reveal the information propagation patterns in Sina Weibo, the biggest microblogging service in China. First, the cascade of each message is represented as a tree based on its retweeting process. Afterwards, we divide the information propagation pattern into two levels, that is, the macro level and the micro level. On one hand, the macro propagation patterns refer to general propagation modes that are extracted by grouping propagation trees based on hierarchical clustering. On the other hand, the micro propagation patterns are frequent information flow patterns that are discovered using tree-based mining techniques. Experimental results show that several interesting patterns are extracted, such as popular message propagation, artificial propagation, and typical information flows between different types of users.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {7},
numpages = {22},
keywords = {propagation tree, microblogging services, message cascade, Information propagation pattern}
}

@article{10.1145/2735634,
author = {Liu, Siyuan and Qu, Qiang and Wang, Shuhui},
title = {Rationality Analytics from Trajectories},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2735634},
doi = {10.1145/2735634},
abstract = {The availability of trajectories tracking the geographical locations of people as a function of time offers an opportunity to study human behaviors. In this article, we study rationality from the perspective of user decision on visiting a point of interest (POI) which is represented as a trajectory. However, the analysis of rationality is challenged by a number of issues, for example, how to model a trajectory in terms of complex user decision processes? and how to detect hidden factors that have significant impact on the rational decision making? In this study, we propose Rationality Analysis Model (RAM) to analyze rationality from trajectories in terms of a set of impact factors. In order to automatically identify hidden factors, we propose a method, Collective Hidden Factor Retrieval (CHFR), which can also be generalized to parse multiple trajectories at the same time or parse individual trajectories of different time periods. Extensive experimental study is conducted on three large-scale real-life datasets (i.e., taxi trajectories, user shopping trajectories, and visiting trajectories in a theme park). The results show that the proposed methods are efficient, effective, and scalable. We also deploy a system in a large theme park to conduct a field study. Interesting findings and user feedback of the field study are provided to support other applications in user behavior mining and analysis, such as business intelligence and user management for marketing purposes.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {10},
numpages = {22},
keywords = {Rationality analytics, decision model, trajectory}
}

@article{10.1145/2749465,
author = {Wei, Wei and Carley, Kathleen M.},
title = {Measuring Temporal Patterns in Dynamic Social Networks},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2749465},
doi = {10.1145/2749465},
abstract = {Given social networks over time, how can we measure network activities across different timesteps with a limited number of metrics? We propose two classes of dynamic metrics for assessing temporal evolution patterns of agents in terms of persistency and emergence. For each class of dynamic metrics, we implement it using three different temporal aggregation models ranging from the most commonly used Average Aggregation Model to more the complex models such as the Exponential Aggregation Model. We argue that the problem of measuring temporal patterns can be formulated using Recency and Primacy effect, which is a concept used to characterize human cognitive processes. Experimental results show that the way metrics model Recency--Primacy effect is closely related to their abilities to measure temporal patterns. Furthermore, our results indicate that future network agent activities can be predicted based on history information using dynamic metrics. By conducting multiple experiments, we are also able to find an optimal length of history information that is most relevant to future activities. This optimal length is highly consistent within a dataset and can be used as an intrinsic metric to evaluate a dynamic social network.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {9},
numpages = {27},
keywords = {aggregating method, temporal analysis, Primacy and Recency effects, Social network analysis}
}

@article{10.1145/2742548,
author = {Berardi, Giacomo and Esuli, Andrea and Sebastiani, Fabrizio},
title = {Utility-Theoretic Ranking for Semiautomated Text Classification},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2742548},
doi = {10.1145/2742548},
abstract = {Semiautomated Text Classification (SATC) may be defined as the task of ranking a set D of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of D with the goal of increasing the overall labelling accuracy of D, the expected increase is maximized. An obvious SATC strategy is to rank D so that the documents that the classifier has labelled with the lowest confidence are top ranked. In this work, we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of validation gain, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method mentioned earlier, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {6},
numpages = {32},
keywords = {Text classification, supervised learning, semiautomated text classification, ranking, cost-sensitive learning}
}

@article{10.1145/2724720,
author = {Mirbakhsh, Nima and Ling, Charles X.},
title = {Improving Top-N Recommendation for Cold-Start Users via Cross-Domain Information},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2724720},
doi = {10.1145/2724720},
abstract = {Making accurate recommendations for cold-start users is a challenging yet important problem in recommendation systems. Including more information from other domains is a natural solution to improve the recommendations. However, most previous work in cross-domain recommendations has focused on improving prediction accuracy with several severe limitations. In this article, we extend our previous work on clustering-based matrix factorization in single domains into cross domains. In addition, we utilize recent results on unobserved ratings. Our new method can more effectively utilize data from auxiliary domains to achieve better recommendations, especially for cold-start users. For example, our method improves the recall to 21% on average for cold-start users, whereas previous methods result in only 15% recall in the cross-domain Amazon dataset. We also observe almost the same improvements in the Epinions dataset. Considering that it is often difficult to make even a small improvement in recommendations, for cold- start users in particular, our result is quite significant.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {33},
numpages = {19},
keywords = {recommendation system, cold start, matrix factorization, Collaborative filtering}
}

@article{10.1145/2728170,
author = {Bonchi, Francesco and Gionis, Aristides and Gullo, Francesco and Tsourakakis, Charalampos E. and Ukkonen, Antti},
title = {Chromatic Correlation Clustering},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2728170},
doi = {10.1145/2728170},
abstract = {We study a novel clustering problem in which the pairwise relations between objects are categorical. This problem can be viewed as clustering the vertices of a graph whose edges are of different types (colors). We introduce an objective function that ensures the edges within each cluster have, as much as possible, the same color. We show that the problem is NP-hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as a pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy to implement, and parameter-free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function based on the alternating-minimization paradigm.We also extend our objective function to handle cases where object’s relations are described by multiple labels. We modify our randomized approximation algorithm to optimize such an extended objective function and show that its approximation guarantee remains proportional to the maximum degree of the graph.We test our algorithms on synthetic and real data from the domains of social media, protein-interaction networks, and bibliometrics. Results reveal that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective-function value.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {24},
keywords = {edge-labeled graphs, Clustering, correlation clustering}
}

@article{10.1145/2700407,
author = {Lin, Bing-Rong and Kifer, Daniel},
title = {Information Measures in Statistical Privacy and Data Processing Applications},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700407},
doi = {10.1145/2700407},
abstract = {In statistical privacy, utility refers to two concepts: information preservation, how much statistical information is retained by a sanitizing algorithm, and usability, how (and with how much difficulty) one extracts this information to build statistical models, answer queries, and so forth. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation, and, afterward, the data consumers process the sanitized output according to their various individual needs [Ghosh et al. 2009; Williams and McSherry 2010].We analyze the information-preserving properties of utility measures with a combination of two new and three existing utility axioms and study how violations of an axiom can be fixed. We show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy all of the axioms. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability—if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn’t Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision-theoretic postprocessing algorithm empirically outperforms previously proposed approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {28},
numpages = {29},
keywords = {differential privacy, decision theory, information measures, utility, Privacy, minimax}
}

@article{10.1145/2700386,
author = {Xie, Hong and Lui, John C. S.},
title = {Mathematical Modeling and Analysis of Product Rating with Partial Information},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700386},
doi = {10.1145/2700386},
abstract = {Many Web services like Amazon, Epinions, and TripAdvisor provide historical product ratings so that users can evaluate the quality of products. Product ratings are important because they affect how well a product will be adopted by the market. The challenge is that we only have partial information on these ratings: each user assigns ratings to only a small subset of products. Under this partial information setting, we explore a number of fundamental questions. What is the minimum number of ratings a product needs so that one can make a reliable evaluation of its quality? How may users’ misbehavior, such as cheating in product rating, affect the evaluation result? To answer these questions, we present a probabilistic model to capture various important factors (e.g., rating aggregation rules, rating behavior) that may influence the product quality assessment under the partial information setting. We derive the minimum number of ratings needed to produce a reliable indicator on the quality of a product. We extend our model to accommodate users’ misbehavior in product rating. We derive the maximum fraction of misbehaving users that a rating aggregation rule can tolerate and the minimum number of ratings needed to compensate. We carry out experiments using both synthetic and real-world data (from Amazon and TripAdvisor). We not only validate our model but also show that the “average rating rule” produces more reliable and robust product quality assessments than the “majority rating rule” and the “median rating rule” in aggregating product ratings. Last, we perform experiments on two movie rating datasets (from Flixster and Netflix) to demonstrate how to apply our framework to improve the applications of recommender systems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {26},
numpages = {33},
keywords = {bias, rating aggregation rule, misbehavior, true quality, minimum number of ratings, Product rating}
}

@article{10.1145/2700406,
author = {Esuli, Andrea and Sebastiani, Fabrizio},
title = {Optimizing Text Quantifiers for Multivariate Loss Functions},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700406},
doi = {10.1145/2700406},
abstract = {We address the problem of quantification, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or prevalence) of the class in a dataset of unlabeled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabeled items that have been assigned the class, and tuning the obtained counts according to some heuristics. In this article, we depart from the tradition of using general-purpose classifiers and use instead a supervised learning model for structured prediction, capable of generating classifiers directly optimized for the (multivariate and nonlinear) function used for evaluating quantification accuracy. The experiments that we have run on 5,500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing state-of-the-art quantification methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {27},
numpages = {27},
keywords = {supervised learning, prior estimation, Kullback-Leibler divergence, prevalence estimation, text classification, loss functions, Quantification}
}

@article{10.1145/2700385,
author = {Huang, Hao and Yoo, Shinjae and Yu, Dantong and Qin, Hong},
title = {Density-Aware Clustering Based on Aggregated Heat Kernel and Its Transformation},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700385},
doi = {10.1145/2700385},
abstract = {Current spectral clustering algorithms suffer from the sensitivity to existing noise and parameter scaling and may not be aware of different density distributions across clusters. If these problems are left untreated, the consequent clustering results cannot accurately represent true data patterns, in particular, for complex real-world datasets with heterogeneous densities. This article aims to solve these problems by proposing a diffusion-based Aggregated Heat Kernel (AHK) to improve the clustering stability, and a Local Density Affinity Transformation (LDAT) to correct the bias originating from different cluster densities. AHK statistically models the heat diffusion traces along the entire time scale, so it ensures robustness during the clustering process, while LDAT probabilistically reveals the local density of each instance and suppresses the local density bias in the affinity matrix. Our proposed framework integrates these two techniques systematically. As a result, it not only provides an advanced noise-resisting and density-aware spectral mapping to the original dataset but also demonstrates the stability during the processing of tuning the scaling parameter (which usually controls the range of neighborhood). Furthermore, our framework works well with the majority of similarity kernels, which ensures its applicability to many types of data and problem domains. The systematic experiments on different applications show that our proposed algorithm outperforms state-of-the-art clustering algorithms for the data with heterogeneous density distributions and achieves robust clustering performance with respect to tuning the scaling parameter and handling various levels and types of noise.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {29},
numpages = {35},
keywords = {Aggregated Heat Kernel, Local Density Affinity Transformation}
}

@article{10.1145/2701430,
author = {Liu, Guimei and Zhang, Haojun and Feng, Mengling and Wong, Limsoon and Ng, See-Kiong},
title = {Supporting Exploratory Hypothesis Testing and Analysis},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2701430},
doi = {10.1145/2701430},
abstract = {Conventional hypothesis testing is carried out in a hypothesis-driven manner. A scientist must first formulate a hypothesis based on what he or she sees and then devise a variety of experiments to test it. Given the rapid growth of data, it has become virtually impossible for a person to manually inspect all data to find all of the interesting hypotheses for testing. In this article, we propose and develop a data-driven framework for automatic hypothesis testing and analysis. We define a hypothesis as a comparison between two or more subpopulations. We find subpopulations for comparison using frequent pattern mining techniques and then pair them up for statistical hypothesis testing. We also generate additional information for further analysis of the hypotheses that are deemed significant. The number of hypotheses generated can be very large, and many of them are very similar. We develop algorithms to remove redundant hypotheses and present a succinct set of significant hypotheses to users. We conducted a set of experiments to show the efficiency and effectiveness of the proposed algorithms. The results show that our system can help users (1) identify significant hypotheses efficiently, (2) isolate the reasons behind significant hypotheses efficiently, and (3) find confounding factors that form Simpson’s paradoxes with discovered significant hypotheses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {31},
numpages = {24},
keywords = {comparative data analysis, exploratory data mining, Exploratory hypothesis testing, actionable knowledge}
}

@article{10.1145/2710020,
author = {Greco, Gianluigi and Guzzo, Antonella and Lupia, Francesco and Pontieri, Luigi},
title = {Process Discovery under Precedence Constraints},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2710020},
doi = {10.1145/2710020},
abstract = {Process discovery has emerged as a powerful approach to support the analysis and the design of complex processes. It consists of analyzing a set of traces registering the sequence of tasks performed along several enactments of a transactional system, in order to build a process model that can explain all the episodes recorded over them. An approach to accomplish this task is presented that can benefit from the background knowledge that, in many cases, is available to the analysts taking care of the process (re-)design. The approach is based on encoding the information gathered from the log and the (possibly) given background knowledge in terms of precedence constraints, that is, of constraints over the topology of the resulting process models. Mining algorithms are eventually formulated in terms of reasoning problems over precedence constraints, and the computational complexity of such problems is thoroughly analyzed by tracing their tractability frontier. Solution algorithms are proposed and their properties analyzed. These algorithms have been implemented in a prototype system, and results of a thorough experimental activity are discussed.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {32},
numpages = {39},
keywords = {computational complexity, Process mining, graph analysis}
}

@article{10.1145/2700409,
author = {Yu, Kui and Ding, Wei and Simovici, Dan A. and Wang, Hao and Pei, Jian and Wu, Xindong},
title = {Classification with Streaming Features: An Emerging-Pattern Mining Approach},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700409},
doi = {10.1145/2700409},
abstract = {Many datasets from real-world applications have very high-dimensional or increasing feature space. It is a new research problem to learn and maintain a classifier to deal with very high dimensionality or streaming features. In this article, we adapt the well-known emerging-pattern--based classification models and propose a semi-streaming approach. For streaming features, it is computationally expensive or even prohibitive to mine long-emerging patterns, and it is nontrivial to integrate emerging-pattern mining with feature selection. We present an online feature selection step, which is capable of selecting and maintaining a pool of effective features from a feature stream. Then, in our offline step, separated from the online step, we periodically compute and update emerging patterns from the pool of selected features from the online step. We evaluate the effectiveness and efficiency of the proposed method using a series of benchmark datasets and a real-world case study on Mars crater detection. Our proposed method yields classification performance comparable to the state-of-art static classification methods. Most important, the proposed method is significantly faster and can efficiently handle datasets with streaming features.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {30},
numpages = {31},
keywords = {streaming features, Emerging patterns, classification, feature selection}
}

@article{10.1145/2629585,
author = {Gopal, Siddharth and Yang, Yiming},
title = {Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629585},
doi = {10.1145/2629585},
abstract = {In this article, we address open challenges in large-scale classification, focusing on how to effectively leverage the dependency structures (hierarchical or graphical) among class labels, and how to make the inference scalable in jointly optimizing all model parameters. We propose two main approaches, namely the hierarchical Bayesian inference framework and the recursive regularization scheme. The key idea in both approaches is to reinforce the similarity among parameter across the nodes in a hierarchy or network based on the proximity and connectivity of the nodes. For scalability, we develop hierarchical variational inference algorithms and fast dual coordinate descent training procedures with parallelization. In our experiments for classification problems with hundreds of thousands of classes and millions of training instances with terabytes of parameters, the proposed methods show consistent and statistically significant improvements over other competing approaches, and the best results on multiple benchmark datasets for large-scale classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {18},
numpages = {23},
keywords = {Bayesian methods, hierarchical classification, Large-scale optimization}
}

@article{10.1145/2700403,
author = {Vlachos, Michail and Schneider, Johannes and Vassiliadis, Vassilios G.},
title = {On Data Publishing with Clustering Preservation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700403},
doi = {10.1145/2700403},
abstract = {The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms to establish ownership in the event of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on dataset utility after the protection process. This work presents techniques that explicitly address this topic and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. Our approach considers all prevalent hierarchical clustering variants: single-, complete-, and average-linkage. We imprint the ownership in a dataset using watermarking principles, and we derive tight bounds on the expansion/contraction of distances incurred by the process. We leverage our analysis to design fast algorithms for right protection without exhaustively searching the vast design space. Finally, because the right-protection process introduces a user-tunable distortion on the dataset, we explore the possibility of using this mechanism for data obfuscation. We quantify the tradeoff between obfuscation and utility for spatiotemporal datasets and discover very favorable characteristics of the process. An additional advantage is that when one is interested in both right-protecting and obfuscating the original data values, the proposed mechanism can accomplish both tasks simultaneously.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {23},
numpages = {30},
keywords = {Distance-Based Mining, Restricted Isometry Property, Distortion Estimation, Watermarking}
}

@article{10.1145/2700405,
author = {Hu, Juhua and Zhan, De-Chuan and Wu, Xintao and Jiang, Yuan and Zhou, Zhi-Hua},
title = {Pairwised Specific Distance Learning from Physical Linkages},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700405},
doi = {10.1145/2700405},
abstract = {In real tasks, usually a good classification performance can only be obtained when a good distance metric is obtained; therefore, distance metric learning has attracted significant attention in the past few years. Typical studies of distance metric learning evaluate how to construct an appropriate distance metric that is able to separate training data points from different classes or satisfy a set of constraints (e.g., must-links and/or cannot-links). It is noteworthy that this task becomes challenging when there are only limited labeled training data points and no constraints are given explicitly. Moreover, most existing approaches aim to construct a global distance metric that is applicable to all data points. However, different data points may have different properties and may require different distance metrics. We notice that data points in real tasks are often connected by physical links (e.g., people are linked with each other in social networks; personal webpages are often connected to other webpages, including nonpersonal webpages), but the linkage information has not been exploited in distance metric learning. In this article, we develop a pairwised specific distance (PSD) approach that exploits the structures of physical linkages and in particular captures the key observations that nonmetric and clique linkages imply the appearance of different or unique semantics, respectively. It is noteworthy that, rather than generating a global distance, PSD generates different distances for different pairs of data points; this property is desired in applications involving complicated data semantics. We mainly present PSD for multi-class learning and further extend it to multi-label learning. Experimental results validate the effectiveness of PSD, especially in the scenarios in which there are very limited labeled training data points and no explicit constraints are given.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {20},
numpages = {27},
keywords = {unlabeled data, multi-label learning, nonmetric linkage, Distance metric learning, multi-class learning, physical linkages}
}

@article{10.1145/2700404,
author = {Soundarajan, Sucheta and Hopcroft, John E.},
title = {Use of Local Group Information to Identify Communities in Networks},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700404},
doi = {10.1145/2700404},
abstract = {The recent interest in networks has inspired a broad range of work on algorithms and techniques to characterize, identify, and extract communities from networks. Such efforts are complicated by a lack of consensus on what a “community” truly is, and these disagreements have led to a wide variety of mathematical formulations for describing communities. Often, these mathematical formulations, such as modularity and conductance, have been founded in the general principle that communities, like a G(n, p) graph, are “round,” with connections throughout the entire community, and so algorithms were developed to optimize such mathematical measures. More recently, a variety of algorithms have been developed that, rather than expecting connectivity through the entire community, seek out very small groups of well-connected nodes and then connect these groups into larger communities. In this article, we examine seven real networks, each containing external annotation that allows us to identify “annotated communities.” A study of these annotated communities gives insight into why the second category of community detection algorithms may be more successful than the first category. We then present a flexible algorithm template that is based on the idea of joining together small sets of nodes. In this template, we first identify very small, tightly connected “subcommunities” of nodes, each corresponding to a single node’s “perception” of the network around it. We then create a new network in which each node represents such a subcommunity, and then identify communities in this new network. Because each node can appear in multiple subcommunities, this method allows us to detect overlapping communities. When evaluated on real data, we show that our template outperforms many other state-of-the-art algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {21},
numpages = {27},
keywords = {communities, Social networks}
}

@article{10.1145/2700398,
author = {Zhang, Jing and Tang, Jie and Li, Juanzi and Liu, Yang and Xing, Chunxiao},
title = {Who Influenced You? Predicting Retweet via Social Influence Locality},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700398},
doi = {10.1145/2700398},
abstract = {Social influence occurs when one’s opinions, emotions, or behaviors are affected by others in a social network. However, social influence takes many forms, and its underlying mechanism is still unclear. For example, how is one’s behavior influenced by a group of friends who know each other and by the friends from different ego friend circles? In this article, we study the social influence problem in a large microblogging network. Particularly, we consider users’ (re)tweet behaviors and focus on investigating how friends in one’s ego network influence retweet behaviors. We propose a novel notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain an F1-score of 71.65% for predicting users’ retweet behaviors by training a logistic regression classifier based on the defined influence locality functions. We incorporate social influence locality into a factor graph model, which can further leverage the network-based correlation. Our experiments on the large microblogging network show that the model significantly improves the precision of retweet prediction. Our analysis also reveals several intriguing discoveries. For example, if you have six friends retweeting a microblog, the average likelihood that you will also retweet it strongly depends on the structure among the six friends: The likelihood will significantly drop (only ⅙) when the six friends do not know each other, compared with the case when the six friends know each other.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {25},
numpages = {26},
keywords = {retweet prediction, Social network, microblog network, social influence}
}

@article{10.1145/2700393,
author = {Wang, Pinghui and Zhao, Junzhou and Lui, John C. S. and Towsley, Don and Guan, Xiaohong},
title = {Unbiased Characterization of Node Pairs over Large Graphs},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700393},
doi = {10.1145/2700393},
abstract = {Characterizing user pair relationships is important for applications such as friend recommendation and interest targeting in online social networks (OSNs). Due to the large-scale nature of such networks, it is infeasible to enumerate all user pairs and thus sampling is used. In this article, we show that it is a great challenge for OSN service providers to characterize user pair relationships, even when they possess the complete graph topology. The reason is that when sampling techniques (i.e., uniform vertex sampling (UVS) and random walk (RW)) are naively applied, they can introduce large biases, particularly for estimating similarity distribution of user pairs with constraints like existence of mutual neighbors, which is important for applications such as identifying network homophily. Estimating statistics of user pairs is more challenging in the absence of the complete topology information, as an unbiased sampling technique like UVS is usually not allowed and exploring the OSN graph topology is expensive. To address these challenges, we present unbiased sampling methods to characterize user pair properties based on UVS and RW techniques. We carry out an evaluation of our methods to show their accuracy and efficiency. Finally, we apply our methods to three OSNs—Foursquare, Douban, and Xiami—and discover that significant homophily is present in these networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {22},
numpages = {27},
keywords = {random walks, graph sampling, homophily, Social network}
}

@article{10.1145/2700399,
author = {Melo, Pedro O. S. Vaz De and Faloutsos, Christos and Assun\c{c}\~{a}o, Renato and Alves, Rodrigo and Loureiro, Antonio A. F.},
title = {Universal and Distinct Properties of Communication Dynamics: How to Generate Realistic Inter-Event Times},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700399},
doi = {10.1145/2700399},
abstract = {With the advancement of information systems, means of communications are becoming cheaper, faster, and more available. Today, millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want. They can access their e-mails, comment on weblogs, watch and post videos and photos (as well as comment on them), and make phone calls or text messages almost ubiquitously. Given this scenario, in this article, we tackle a fundamental aspect of this new era of communication: How the time intervals between communication events behave for different technologies and means of communications. Are there universal patterns for the Inter-Event Time Distribution (IED)? How do inter-event times behave differently among particular technologies? To answer these questions, we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets. Moreover, we propose the use of the Self-Feeding Process (SFP) to generate inter-event times between communications. The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We also show three potential applications of the SFP: as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {24},
numpages = {31},
keywords = {inter-event times, generative model, Communication dynamics}
}

@article{10.1145/2663356,
author = {Yin, Hongzhi and Cui, Bin and Chen, Ling and Hu, Zhiting and Zhang, Chengqi},
title = {Modeling Location-Based User Rating Profiles for Personalized Recommendation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2663356},
doi = {10.1145/2663356},
abstract = {This article proposes LA-LDA, a location-aware probabilistic generative model that exploits location-based ratings to model user profiles and produce recommendations. Most of the existing recommendation models do not consider the spatial information of users or items; however, LA-LDA supports three classes of location-based ratings, namely spatial user ratings for nonspatial items, nonspatial user ratings for spatial items, and spatial user ratings for spatial items. LA-LDA consists of two components, ULA-LDA and ILA-LDA, which are designed to take into account user and item location information, respectively. The component ULA-LDA explicitly incorporates and quantifies the influence from local public preferences to produce recommendations by considering user home locations, whereas the component ILA-LDA recommends items that are closer in both taste and travel distance to the querying users by capturing item co-occurrence patterns, as well as item location co-occurrence patterns. The two components of LA-LDA can be applied either separately or collectively, depending on the available types of location-based ratings. To demonstrate the applicability and flexibility of the LA-LDA model, we deploy it to both top-k recommendation and cold start recommendation scenarios. Experimental evidence on large-scale real-world data, including the data from Gowalla (a location-based social network), DoubanEvent (an event-based social network), and MovieLens (a movie recommendation system), reveal that LA-LDA models user profiles more accurately by outperforming existing recommendation models for top-k recommendation and the cold start problem.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {19},
numpages = {41},
keywords = {recommender system, probabilistic generative model, User profile, cold start, location-based services}
}

@article{10.1145/2700993,
author = {Gionis, Aristides and Li, Hang},
title = {Introduction to the Special Issue ACM SIGKDD 2013},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700993},
doi = {10.1145/2700993},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {15e},
numpages = {2}
}

@article{10.1145/2700395,
author = {Jha, Madhav and Seshadhri, C. and Pinar, Ali},
title = {A Space-Efficient Streaming Algorithm for Estimating Transitivity and Triangle Counts Using the Birthday Paradox},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700395},
doi = {10.1145/2700395},
abstract = {We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O(√n) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 40,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph by storing a minuscule fraction of edges.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {15},
numpages = {21},
keywords = {Triangle counting, transitivity, streaming graphs, streaming algorithms, birthday paradox, clustering coefficient}
}

@article{10.1145/2700408,
author = {Wang, Zheng and Ye, Jieping},
title = {Querying Discriminative and Representative Samples for Batch Mode Active Learning},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700408},
doi = {10.1145/2700408},
abstract = {Empirical risk minimization (ERM) provides a principled guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label, and these data may not follow the source distribution. In this article, we generalize the ERM principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound, we develop a practical batch mode active learning method. The proposed formulation involves a nonconvex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. We further extend our method to multiclass active learning by introducing novel pseudolabels in the multiclass case and developing an efficient algorithm. Experiments on benchmark datasets and real-world applications demonstrate the superior performance of our proposed method compared to state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {17},
numpages = {23},
keywords = {empirical risk minimization, representative and discriminative, Active learning, maximum mean discrepancy}
}

@article{10.1145/2700394,
author = {Tang, Lu-An and Yu, Xiao and Gu, Quanquan and Han, Jiawei and Jiang, Guofei and Leung, Alice and Porta, Thomas La},
title = {A Framework of Mining Trajectories from Untrustworthy Data in Cyber-Physical System},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700394},
doi = {10.1145/2700394},
abstract = {A cyber-physical system (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context-sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance, and traffic control. One key research problem of CPS is called mining lines in the sand. With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy, and (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. This study proposes a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone model from the historical trajectories to track multiple intruders. Finally, the system validates the mining results and updates sensors’ reliability scores in a feedback process. In addition, LoRM (Line-on-the-Road Miner) is proposed for trajectory discovery on road networks—mining lines on the roads. LoRM employs a filtering-and-refinement framework to reduce the distance computational overhead on road networks and uses a shortest-path-measure to track intruders. The proposed methods are evaluated with extensive experiments on big datasets. The experimental results show that the proposed methods achieve higher accuracy and efficiency in trajectory mining tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {16},
numpages = {35},
keywords = {trajectory, sensor network, Cyber-physical system}
}

@article{10.1145/2629474,
author = {Wang, Guangtao and Song, Qinbao and Zhang, Xueying and Zhang, Kaiyuan},
title = {A Generic Multilabel Learning-Based Classification Algorithm Recommendation Method},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629474},
doi = {10.1145/2629474},
abstract = {As more and more classification algorithms continue to be developed, recommending appropriate algorithms to a given classification problem is increasingly important. This article first distinguishes the algorithm recommendation methods by two dimensions: (1) meta-features, which are a set of measures used to characterize the learning problems, and (2) meta-target, which represents the relative performance of the classification algorithms on the learning problem. In contrast to the existing algorithm recommendation methods whose meta-target is usually in the form of either the ranking of candidate algorithms or a single algorithm, this article proposes a new and natural multilabel form to describe the meta-target. This is due to the fact that there would be multiple algorithms being appropriate for a given problem in practice. Furthermore, a novel multilabel learning-based generic algorithm recommendation method is proposed, which views the algorithm recommendation as a multilabel learning problem and solves the problem by the mature multilabel learning algorithms. To evaluate the proposed multilabel learning-based recommendation method, extensive experiments with 13 well-known classification algorithms, two kinds of meta-targets such as algorithm ranking and single algorithm, and five different kinds of meta-features are conducted on 1,090 benchmark learning problems. The results show the effectiveness of our proposed multilabel learning-based recommendation method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {7},
numpages = {30},
keywords = {multilabel k nearest neighbors, multiple comparison procedure, Algorithm automatic recommendation, multilabel classification}
}

@article{10.1145/2629587,
author = {Tang, Jiliang and Liu, Huan},
title = {Feature Selection for Social Media Data},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629587},
doi = {10.1145/2629587},
abstract = {Feature selection is widely used in preparing high-dimensional data for effective data mining. The explosive popularity of social media produces massive and high-dimensional data at an unprecedented rate, presenting new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attribute-value data such as posts, tweets, comments, and images, and (2) linked data that provides social context for posts and describes the relationships between social media users as well as who generates the posts, and so on. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this article, we study a novel feature selection problem of selecting features for social media data with its social context. In detail, we illustrate the differences between attribute-value data and social media data, investigate if linked data can be exploited in a new feature selection framework by taking advantage of social science theories. We design and conduct experiments on datasets from real-world social media Web sites, and the empirical results demonstrate that the proposed framework can significantly improve the performance of feature selection. Further experiments are conducted to evaluate the effects of user--user and user--post relationships manifested in linked data on feature selection, and research issues for future work will be discussed.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {19},
numpages = {27},
keywords = {social context, social media data, Feature selection}
}

@article{10.1145/2601437,
author = {Miettinen, Pauli and Vreeken, Jilles},
title = {MDL4BMF: Minimum Description Length for Boolean Matrix Factorization},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601437},
doi = {10.1145/2601437},
abstract = {Matrix factorizations—where a given data matrix is approximated by a product of two or more factor matrices—are powerful data mining tools. Among other tasks, matrix factorizations are often used to separate global structure from noise. This, however, requires solving the “model order selection problem” of determining the proper rank of the factorization, that is, to answer where fine-grained structure stops, and where noise starts.Boolean Matrix Factorization (BMF)—where data, factors, and matrix product are Boolean—has in recent years received increased attention from the data mining community. The technique has desirable properties, such as high interpretability and natural sparsity. Yet, so far no method for selecting the correct model order for BMF has been available. In this article, we propose the use of the Minimum Description Length (MDL) principle for this task. Besides solving the problem, this well-founded approach has numerous benefits; for example, it is automatic, does not require a likelihood function, is fast, and, as experiments show, is highly accurate.We formulate the description length function for BMF in general—making it applicable for any BMF algorithm. We discuss how to construct an appropriate encoding: starting from a simple and intuitive approach, we arrive at a highly efficient data-to-model--based encoding for BMF. We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization, analyze the complexity of the problem, and perform an extensive experimental evaluation to study its behavior.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {18},
numpages = {31},
keywords = {model selection, Boolean rank, minimum description length principle, parameter free, model order selection, Boolean matrix factorization, pattern sets, MDL, summarization}
}

@article{10.1145/2629504,
author = {Huai, Baoxing and Chen, Enhong and Zhu, Hengshu and Xiong, Hui and Bao, Tengfei and Liu, Qi and Tian, Jilei},
title = {Toward Personalized Context Recognition for Mobile Users: A Semisupervised Bayesian HMM Approach},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629504},
doi = {10.1145/2629504},
abstract = {The problem of mobile context recognition targets the identification of semantic meaning of context in a mobile environment. This plays an important role in understanding mobile user behaviors and thus provides the opportunity for the development of better intelligent context-aware services. A key step of context recognition is to model the personalized contextual information of mobile users. Although many studies have been devoted to mobile context modeling, limited efforts have been made on the exploitation of the sequential and dependency characteristics of mobile contextual information. Also, the latent semantics behind mobile context are often ambiguous and poorly understood. Indeed, a promising direction is to incorporate some domain knowledge of common contexts, such as “waiting for a bus” or “having dinner,” by modeling both labeled and unlabeled context data from mobile users because there are often few labeled contexts available in practice. To this end, in this article, we propose a sequence-based semisupervised approach to modeling personalized context for mobile users. Specifically, we first exploit the Bayesian Hidden Markov Model (B-HMM) for modeling context in the form of probabilistic distributions and transitions of raw context data. Also, we propose a sequential model by extending B-HMM with the prior knowledge of contextual features to model context more accurately. Then, to efficiently learn the parameters and initial values of the proposed models, we develop a novel approach for parameter estimation by integrating the Dirichlet Process Mixture (DPM) model and the Mixture Unigram (MU) model. Furthermore, by incorporating both user-labeled and unlabeled data, we propose a semisupervised learning-based algorithm to identify and model the latent semantics of context. Finally, experimental results on real-world data clearly validate both the efficiency and effectiveness of the proposed approaches for recognizing personalized context of mobile users.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {10},
numpages = {29},
keywords = {Context recognition, hidden Markov model}
}

@article{10.1145/2630421,
author = {Gundecha, Pritam and Barbier, Geoffrey and Tang, Jiliang and Liu, Huan},
title = {User Vulnerability and Its Reduction on a Social Networking Site},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2630421},
doi = {10.1145/2630421},
abstract = {Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user’s social network, privacy settings alone are often inadequate to protect a user’s profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual users’ vulnerability? (2) With the diversity of one’s social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual user’s perspective. User vulnerability is dependent on whether or not the user’s friends’ privacy settings protect the friend and the individual’s network of friends (which includes the user). We show that it is feasible to measure and assess user vulnerability and reduce one’s vulnerability without changing the structure of a social networking site. The approach is to unfriend one’s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him or her would significantly reduce one’s own social status. We formulate this novel problem as vulnerability minimization with social utility constraints. We formally define the optimization problem and provide an approximation algorithm with a proven bound. Finally, we conduct a large-scale evaluation of a new framework using a Facebook dataset. We resort to experiments and observe how much vulnerability an individual user can be decreased by unfriending a vulnerable friend. We compare performance of different unfriending strategies and discuss the security risk of new friend requests. Additionally, by employing different forms of social utility, we confirm that the balance between user vulnerability and social utility can be practically achieved.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {12},
numpages = {25},
keywords = {social network, Vulnerability, privacy}
}

@article{10.1145/2629564,
author = {Wang, Pinghui and Lui, John C. S. and Ribeiro, Bruno and Towsley, Don and Zhao, Junzhou and Guan, Xiaohong},
title = {Efficiently Estimating Motif Statistics of Large Networks},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629564},
doi = {10.1145/2629564},
abstract = {Exploring statistics of locally connected subgraph patterns (also known as network motifs) has helped researchers better understand the structure and function of biological and Online Social Networks (OSNs). Nowadays, the massive size of some critical networks—often stored in already overloaded relational databases—effectively limits the rate at which nodes and edges can be explored, making it a challenge to accurately discover subgraph statistics. In this work, we propose sampling methods to accurately estimate subgraph statistics from as few queried nodes as possible. We present sampling algorithms that efficiently and accurately estimate subgraph properties of massive networks. Our algorithms require no precomputation or complete network topology information. At the same time, we provide theoretical guarantees of convergence. We perform experiments using widely known datasets and show that, for the same accuracy, our algorithms require an order of magnitude less queries (samples) than the current state-of-the-art algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {8},
numpages = {27},
keywords = {subgraph patterns, network motifs, Social network, random walks, graph sampling}
}

@article{10.1145/2641574,
author = {Huang, Hao and Qin, Hong and Yoo, Shinjae and Yu, Dantong},
title = {Physics-Based Anomaly Detection Defined on Manifold Space},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2641574},
doi = {10.1145/2641574},
abstract = {Current popular anomaly detection algorithms are capable of detecting global anomalies but often fail to distinguish local anomalies from normal instances. Inspired by contemporary physics theory (i.e., heat diffusion and quantum mechanics), we propose two unsupervised anomaly detection algorithms. Building on the embedding manifold derived from heat diffusion, we devise Local Anomaly Descriptor (LAD), which faithfully reveals the intrinsic neighborhood density. It uses a scale-dependent umbrella operator to bridge global and local properties, which makes LAD more informative within an adaptive scope of neighborhood. To offer more stability of local density measurement on scaling parameter tuning, we formulate Fermi Density Descriptor (FDD), which measures the probability of a fermion particle being at a specific location. By choosing the stable energy distribution function, FDD steadily distinguishes anomalies from normal instances with any scaling parameter setting. To further enhance the efficacy of our proposed algorithms, we explore the utility of anisotropic Gaussian kernel (AGK), which offers better manifold-aware affinity information. We also quantify and examine the effect of different Laplacian normalizations for anomaly detection. Comprehensive experiments on both synthetic and benchmark datasets verify that our proposed algorithms outperform the existing anomaly detection algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {14},
numpages = {39},
keywords = {heat diffusion, Laplace operator, quantum mechanics, Anomaly detection}
}

@article{10.1145/2637484,
author = {Duan, Lian and Street, W. Nick and Liu, Yanchi and Xu, Songhua and Wu, Brook},
title = {Selecting the Right Correlation Measure for Binary Data},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2637484},
doi = {10.1145/2637484},
abstract = {Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. Although there are numerous measures available for evaluating correlations, different correlation measures provide drastically different results. Piatetsky-Shapiro provided three mandatory properties for any reasonable correlation measure, and Tan et al. proposed several properties to categorize correlation measures; however, it is still hard for users to choose the desirable correlation measures according to their needs. In order to solve this problem, we explore the effectiveness problem in three ways. First, we propose two desirable properties and two optional properties for correlation measure selection and study the property satisfaction for different correlation measures. Second, we study different techniques to adjust correlation measures and propose two new correlation measures: the Simplified χ2 with Continuity Correction and the Simplified χ2 with Support. Third, we analyze the upper and lower bounds of different measures and categorize them by the bound differences. Combining these three directions, we provide guidelines for users to choose the proper measure according to their needs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {13},
numpages = {28},
keywords = {association rules, correlation, Knowledge discovery}
}

@article{10.1145/2629668,
author = {Liu, Siyuan and Chen, Lei and Ni, Lionel M.},
title = {Anomaly Detection from Incomplete Data},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629668},
doi = {10.1145/2629668},
abstract = {Anomaly detection (a.k.a., outlier or burst detection) is a well-motivated problem and a major data mining and knowledge discovery task. In this article, we study the problem of population anomaly detection, one of the key issues related to event monitoring and population management within a city. Through studying detected population anomalies, we can trace and analyze these anomalies, which could help to model city traffic design and event impact analysis and prediction. Although a significant and interesting issue, it is very hard to detect population anomalies and retrieve anomaly trajectories, especially given that it is difficult to get actual and sufficient population data. To address the difficulties of a lack of real population data, we take advantage of mobile phone networks, which offer enormous spatial and temporal communication data on persons. More importantly, we claim that we can utilize these mobile phone data to infer and approximate population data. Thus, we can study the population anomaly detection problem by taking advantages of unique features hidden in mobile phone data. In this article, we present a system to conduct Population Anomaly Detection (PAD). First, we propose an effective clustering method, correlation-based clustering, to cluster the incomplete location information from mobile phone data (i.e., from mobile call volume distribution to population density distribution). Then, we design an adaptive parameter-free detection method, R-scan, to capture the distributed dynamic anomalies. Finally, we devise an efficient algorithm, BT-miner, to retrieve anomaly trajectories. The experimental results from real-life mobile phone data confirm the effectiveness and efficiency of the proposed algorithms. Finally, the proposed methods are realized as a pilot system in a city in China.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {11},
numpages = {22},
keywords = {anomaly trajectory, correlation-based clustering, mobile phone, Anomaly detection}
}

@article{10.1145/2611380,
author = {Zheng, Li and Li, Tao and Ding, Chris},
title = {A Framework for Hierarchical Ensemble Clustering},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2611380},
doi = {10.1145/2611380},
abstract = {Ensemble clustering, as an important extension of the clustering problem, refers to the problem of combining different (input) clusterings of a given dataset to generate a final (consensus) clustering that is a better fit in some sense than existing clusterings. Over the past few years, many ensemble clustering approaches have been developed. However, most of them are designed for partitional clustering methods, and few research efforts have been reported for ensemble hierarchical clustering methods. In this article, a hierarchical ensemble clustering framework that can naturally combine both partitional clustering and hierarchical clustering results is proposed. In addition, a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation is developed based on the ultra-metric distance for hierarchical clustering. We study three important problems: dendrogram description, dendrogram combination, and dendrogram selection. We develop two approaches for dendrogram selection based on tree distances, and we investigate various dendrogram distances for representing dendrograms. We provide a systematic empirical study of the ensemble hierarchical clustering problem. Experimental results demonstrate the effectiveness of our proposed approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {9},
numpages = {23},
keywords = {ensemble selection, Hierarchical ensemble clustering, ultra-metric}
}

@article{10.1145/2641760,
author = {Paul, Saurabh and Boutsidis, Christos and Magdon-Ismail, Malik and Drineas, Petros},
title = {Random Projections for Linear Support Vector Machines},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2641760},
doi = {10.1145/2641760},
abstract = {Let X be a data matrix of rank ρ, whose rows represent n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique that is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within ϵ-relative error, ensuring comparable generalization as in the original space in the case of classification. For regression, we show that the margin is preserved to ϵ-relative error with high probability. We present extensive experiments with real and synthetic data to support our theory.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {22},
numpages = {25},
keywords = {Classification, support vector machines, dimensionality reduction}
}

@article{10.1145/2629586,
author = {Riondato, Matteo and Upfal, Eli},
title = {Efficient Discovery of Association Rules and Frequent Itemsets through Sampling with Tight Performance Guarantees},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629586},
doi = {10.1145/2629586},
abstract = {The tasks of extracting (top-K) Frequent Itemsets (FIs) and Association Rules (ARs) are fundamental primitives in data mining and database applications. Exact algorithms for these problems exist and are widely used, but their running time is hindered by the need of scanning the entire dataset, possibly multiple times. High-quality approximations of FIs and ARs are sufficient for most practical uses. Sampling techniques can be used for fast discovery of approximate solutions, but works exploring this technique did not provide satisfactory performance guarantees on the quality of the approximation due to the difficulty of bounding the probability of under- or oversampling any one of an unknown number of frequent itemsets. We circumvent this issue by applying the statistical concept of Vapnik-Chervonenkis (VC) dimension to develop a novel technique for providing tight bounds on the sample size that guarantees approximation of the (top-K) FIs and ARs within user-specified parameters. The resulting sample size is linearly dependent on the VC-dimension of a range space associated with the dataset. We analyze the VC-dimension of this range space and show that it is upper bounded by an easy-to-compute characteristic quantity of the dataset, the d-index, namely, the maximum integer d such that the dataset contains at least d transactions of length at least d such that no one of them is a superset of or equal to another. We show that this bound is tight for a large class of datasets. The resulting sample size is a significant improvement over previous known results. We present an extensive experimental evaluation of our technique on real and artificial datasets, demonstrating the practicality of our methods, and showing that they achieve even higher quality approximations than what is guaranteed by the analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {20},
numpages = {32},
keywords = {frequent itemsets, sampling, data mining, VC-dimension, Association rules}
}

@article{10.1145/2629617,
author = {Ordonez, Carlos and Garcia-Alvarado, Carlos and Baladandayuthapani, Veerabhadaran},
title = {Bayesian Variable Selection in Linear Regression in One Pass for Large Datasets},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629617},
doi = {10.1145/2629617},
abstract = {Bayesian models are generally computed with Markov Chain Monte Carlo (MCMC) methods. The main disadvantage of MCMC methods is the large number of iterations they need to sample the posterior distributions of model parameters, especially for large datasets. On the other hand, variable selection remains a challenging problem due to its combinatorial search space, where Bayesian models are a promising solution. In this work, we study how to accelerate Bayesian model computation for variable selection in linear regression. We propose a fast Gibbs sampler algorithm, a widely used MCMC method that incorporates several optimizations. We use a Zellner prior for the regression coefficients, an improper prior on variance, and a conjugate prior Gaussian distribution, which enable dataset summarization in one pass, thus exploiting an augmented set of sufficient statistics. Thereafter, the algorithm iterates in main memory. Sufficient statistics are indexed with a sparse binary vector to efficiently compute matrix projections based on selected variables. Discovered variable subsets probabilities, selecting and discarding each variable, are stored on a hash table for fast retrieval in future iterations. We study how to integrate our algorithm into a Database Management System (DBMS), exploiting aggregate User-Defined Functions for parallel data summarization and stored procedures to manipulate matrices with arrays. An experimental evaluation with real datasets evaluates accuracy and time performance, comparing our DBMS-based algorithm with the R package. Our algorithm is shown to produce accurate results, scale linearly on dataset size, and run orders of magnitude faster than the R package.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {3},
numpages = {14},
keywords = {MCMC, Gibbs sampler, on-line algorithm, Sufficient statistics, variable selection}
}

@article{10.1145/2641758,
author = {Hern´ndez-Orallo, Jos\'{e}},
title = {Probabilistic Reframing for Cost-Sensitive Regression},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2641758},
doi = {10.1145/2641758},
abstract = {Common-day applications of predictive models usually involve the full use of the available contextual information. When the operating context changes, one may fine-tune the by-default (incontextual) prediction or may even abstain from predicting a value (a reject). Global reframing solutions, where the same function is applied to adapt the estimated outputs to a new cost context, are possible solutions here. An alternative approach, which has not been studied in a comprehensive way for regression in the knowledge discovery and data mining literature, is the use of a local (e.g., probabilistic) reframing approach, where decisions are made according to the estimated output and a reliability, confidence, or probability estimation. In this article, we advocate for a simple two-parameter (mean and variance) approach, working with a normal conditional probability density. Given the conditional mean produced by any regression technique, we develop lightweight “enrichment” methods that produce good estimates of the conditional variance, which are used by the probabilistic (local) reframing methods. We apply these methods to some very common families of cost-sensitive problems, such as optimal predictions in (auction) bids, asymmetric loss scenarios, and rejection rules.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {17},
numpages = {55},
keywords = {reliability estimation in regression, asymmetric loss, calibration, conditional density estimation, reframing, Cost-sensitive regression}
}

@article{10.1145/2641761,
author = {Erd\H{o}s, D\'{o}ra and Gemulla, Rainer and Terzi, Evimaria},
title = {Reconstructing Graphs from Neighborhood Data},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2641761},
doi = {10.1145/2641761},
abstract = {Consider a social network and suppose that we are only given the number of common friends between each pair of users. Can we reconstruct the underlying network? Similarly, consider a set of documents and the words that appear in them. If we only know the number of common words for every pair of documents, as well as the number of common documents for every pair of words, can we infer which words appear in which documents? In this article, we develop a general methodology for answering questions like these.We formalize these questions in what we call the Reconstruct problem: given information about the common neighbors of nodes in a network, our goal is to reconstruct the hidden binary matrix that indicates the presence or absence of relationships between individual nodes. In fact, we propose two different variants of this problem: one where the number of connections of every node (i.e., the degree of every node) is known and a second one where it is unknown. We call these variants the degree-aware and the degree-oblivious versions of the Reconstruct problem, respectively.Our algorithms for both variants exploit the properties of the singular value decomposition of the hidden binary matrix. More specifically, we show that using the available neighborhood information, we can reconstruct the hidden matrix by finding the components of its singular value decomposition and then combining them appropriately. Our extensive experimental study suggests that our methods are able to reconstruct binary matrices of different characteristics with up to 100% accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {23},
numpages = {22},
keywords = {singular value decomposition, Bipartite graph reconstruction, adjacency matrix}
}

@article{10.1145/2629618,
author = {Boedihardjo, Arnold P. and Lu, Chang-Tien and Wang, Bingsheng},
title = {A Framework for Exploiting Local Information to Enhance Density Estimation of Data Streams},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629618},
doi = {10.1145/2629618},
abstract = {The Probability Density Function (PDF) is the fundamental data model for a variety of stream mining algorithms. Existing works apply the standard nonparametric Kernel Density Estimator (KDE) to approximate the PDF of data streams. As a result, the stream-based KDEs cannot accurately capture complex local density features. In this article, we propose the use of Local Region (LRs) to model local density information in univariate data streams. In-depth theoretical analyses are presented to justify the effectiveness of the LR-based KDE. Based on the analyses, we develop the General Local rEgion AlgorithM (GLEAM) to enhance the estimation quality of structurally complex univariate distributions for existing stream-based KDEs. A set of algorithmic optimizations is designed to improve the query throughput of GLEAM and to achieve its linear order computation. Additionally, a comprehensive suite of experiments was conducted to test the effectiveness and efficiency of GLEAM.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {2},
numpages = {38},
keywords = {General Local rEgion AlgorithM (GLEAM), Local region information}
}

@article{10.1145/2601435,
author = {Acharya, Ayan and Hruschka, Eduardo R. and Ghosh, Joydeep and Acharyya, Sreangsu},
title = {An Optimization Framework for Combining Ensembles of Classifiers and Clusterers with Applications to Nontransductive Semisupervised Learning and Transfer Learning},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601435},
doi = {10.1145/2601435},
abstract = {Unsupervised models can provide supplementary soft constraints to help classify new “target” data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This article describes a general optimization framework that takes as input class membership estimates from existing classifiers learned on previously encountered “source” (or training) data, as well as a similarity matrix from a cluster ensemble operating solely on the target (or test) data to be classified, and yields a consensus labeling of the target data. More precisely, the application settings considered are nontransductive semisupervised and transfer learning scenarios where the training data are used only to build an ensemble of classifiers and are subsequently discarded before classifying the target data. The framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by na\"{\i}vely applying classifiers learned on the original task to the target data. In addition, we show that the proposed approach, even not being conceptually transductive, can provide better results compared to some popular transductive learning techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {1},
numpages = {35},
keywords = {Classification, semisupervised learning, transductive learning, transfer learning, clustering, ensembles}
}

@article{10.1145/2629616,
author = {Xu, Zhiqiang and Ke, Yiping and Wang, Yi and Cheng, Hong and Cheng, James},
title = {GBAGC: A General Bayesian Framework for Attributed Graph Clustering},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629616},
doi = {10.1145/2629616},
abstract = {Graph clustering, also known as community detection, is a long-standing problem in data mining. In recent years, with the proliferation of rich attribute information available for objects in real-world graphs, how to leverage not only structural but also attribute information for clustering attributed graphs becomes a new challenge. Most existing works took a distance-based approach. They proposed various distance measures to fuse structural and attribute information and then applied standard techniques for graph clustering based on these distance measures. In this article, we take an alternative view and propose a novel Bayesian framework for attributed graph clustering. Our framework provides a general and principled solution to modeling both the structural and the attribute aspects of a graph. It avoids the artificial design of a distance measure in existing methods and, furthermore, can seamlessly handle graphs with different types of edges and vertex attributes. We develop an efficient variational method for graph clustering under this framework and derive two concrete algorithms for clustering unweighted and weighted attributed graphs. Experimental results on large real-world datasets show that our algorithms significantly outperform the state-of-the-art distance-based method, in terms of both effectiveness and efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {5},
numpages = {43},
keywords = {Bayesian method, Attributed graph clustering, model-based clustering}
}

@article{10.1145/2629511,
author = {Coscia, Michele and Rossetti, Giulio and Giannotti, Fosca and Pedreschi, Dino},
title = {Uncovering Hierarchical and Overlapping Communities with a Local-First Approach},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629511},
doi = {10.1145/2629511},
abstract = {Community discovery in complex networks is the task of organizing a network’s structure by grouping together nodes related to each other. Traditional approaches are based on the assumption that there is a global-level organization in the network. However, in many scenarios, each node is the bearer of complex information and cannot be classified in disjoint clusters. The top-down global view of the partition approach is not designed for this. Here, we represent this complex information as multiple latent labels, and we postulate that edges in the networks are created among nodes carrying similar labels. The latent labels are the communities a node belongs to and we discover them with a simple local-first approach to community discovery. This is achieved by democratically letting each node vote for the communities it sees surrounding it in its limited view of the global system, its ego neighborhood, using a label propagation algorithm, assuming that each node is aware of the label it shares with each of its connections. The local communities are merged hierarchically, unveiling the modular organization of the network at the global level and identifying overlapping groups and groups of groups. We tested this intuition against the state-of-the-art overlapping community discovery and found that our new method advances in the chosen scenarios in the quality of the obtained communities. We perform a test on benchmark and on real-world networks, evaluating the quality of the community coverage by using the extracted communities to predict the metadata attached to the nodes, which we consider external information about the latent labels. We also provide an explanation about why real-world networks contain overlapping communities and how our logic is able to capture them. Finally, we show how our method is deterministic, is incremental, and has a limited time complexity, so that it can be used on real-world scale networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {6},
numpages = {27},
keywords = {community discovery, Complex networks, data mining}
}

@article{10.1145/2629328,
author = {Fei, Hongliang and Huan, Jun},
title = {Structured Sparse Boosting for Graph Classification},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2629328},
doi = {10.1145/2629328},
abstract = {Boosting is a highly effective algorithm that produces a linear combination of weak classifiers (a.k.a. base learners) to obtain high-quality classification models. In this article, we propose a generalized logit boost algorithm in which base learners have structural relationships in the functional space. Although such relationships are generic, our work is particularly motivated by the emerging topic of pattern-based classification for semistructured data including graphs. Toward an efficient incorporation of the structure information, we have designed a general model in which we use an undirected graph to capture the relationship of subgraph-based base learners. In our method, we employ both L1 and Laplacian-based L2 regularization to logit boosting to achieve model sparsity and smoothness in the functional space spanned by the base learners. We have derived efficient optimization algorithms based on coordinate descent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping base learners and that the resulting estimator is consistent. Additionally, motivated by the connection between logit boosting and logistic regression, we extend our structured sparse regularization framework to logistic regression for vectorial data in which features are structured. Using comprehensive experimental study and comparing our work with the state-of-the-art, we have demonstrated the effectiveness of the proposed learning method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {4},
numpages = {22},
keywords = {semistructured data, graph classification, boosting, structural sparsity, Regularization, feature selection, logistic regression}
}

@article{10.1145/2641759,
author = {Burton, Scott H. and Giraud-Carrier, Christophe G.},
title = {Discovering Social Circles in Directed Graphs},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2641759},
doi = {10.1145/2641759},
abstract = {We examine the problem of identifying social circles, or sets of cohesive and mutually aware nodes surrounding an initial query set, in directed graphs where the complete graph is not known beforehand. This problem differs from local community mining, in that the query set defines the circle of interest. We explicitly handle edge direction, as in many cases relationships are not symmetric, and focus on the local context because many real-world graphs cannot be feasibly known. We outline several issues that are unique to this context, introduce a quality function to measure the value of including a particular node in an emerging social circle, and describe a greedy social circle discovery algorithm. We demonstrate the effectiveness of this approach on artificial benchmarks, large networks with topical community labels, and several real-world case studies.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {21},
numpages = {27},
keywords = {Social circles, local community search, directed graphs}
}

@article{10.1145/2601436,
author = {Ge, Yong and Jiang, Guofei and Ding, Min and Xiong, Hui},
title = {Ranking Metric Anomaly in Invariant Networks},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601436},
doi = {10.1145/2601436},
abstract = {The management of large-scale distributed information systems relies on the effective use and modeling of monitoring data collected at various points in the distributed information systems. A traditional approach to model monitoring data is to discover invariant relationships among the monitoring data. Indeed, we can discover all invariant relationships among all pairs of monitoring data and generate invariant networks, where a node is a monitoring data source (metric) and a link indicates an invariant relationship between two monitoring data. Such an invariant network representation can help system experts to localize and diagnose the system faults by examining those broken invariant relationships and their related metrics, since system faults usually propagate among the monitoring data and eventually lead to some broken invariant relationships. However, at one time, there are usually a lot of broken links (invariant relationships) within an invariant network. Without proper guidance, it is difficult for system experts to manually inspect this large number of broken links. To this end, in this article, we propose the problem of ranking metrics according to the anomaly levels for a given invariant network, while this is a nontrivial task due to the uncertainties and the complex nature of invariant networks. Specifically, we propose two types of algorithms for ranking metric anomaly by link analysis in invariant networks. Along this line, we first define two measurements to quantify the anomaly level of each metric, and introduce the mRank algorithm. Also, we provide a weighted score mechanism and develop the gRank algorithm, which involves an iterative process to obtain a score to measure the anomaly levels. In addition, some extended algorithms based on mRank and gRank algorithms are developed by taking into account the probability of being broken as well as noisy links. Finally, we validate all the proposed algorithms on a large number of real-world and synthetic data sets to illustrate the effectiveness and efficiency of different algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {8},
numpages = {30},
keywords = {link analysis, invariant networks, Metric anomaly ranking}
}

@article{10.1145/2611520,
author = {Shabtai, Asaf and Bercovitch, Maya and Rokach, Lior and Elovici, Yuval},
title = {Optimizing Data Misuse Detection},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2611520},
doi = {10.1145/2611520},
abstract = {Data misuse may be performed by entities such as an organization's employees and business partners who are granted access to sensitive information and misuse their privileges. We assume that users can be either trusted or untrusted. The access of untrusted parties to data objects (e.g., client and patient records) should be monitored in an attempt to detect misuse. However, monitoring data objects is resource intensive and time-consuming and may also cause disturbance or inconvenience to the involved employees. Therefore, the monitored data objects should be carefully selected. In this article, we present two optimization problems carefully designed for selecting specific data objects for monitoring, such that the detection rate is maximized and the monitoring effort is minimized. In the first optimization problem, the goal is to select data objects for monitoring that are accessed by at most c trusted agents while ensuring access to at least k monitored objects by each untrusted agent (both c and k are integer variable). As opposed to the first optimization problem, the goal of the second optimization problem is to select monitored data objects that maximize the number of monitored data objects accessed by untrusted agents while ensuring that each trusted agent does not access more than d monitored data objects (d is an integer variable as well). Two efficient heuristic algorithms for solving these optimization problems are proposed, and experiments were conducted simulating different scenarios to evaluate the algorithms’ performance. Moreover, we compared the heuristic algorithms’ performance to the optimal solution and conducted sensitivity analysis on the three parameters (c, k, and d) and on the ratio between the trusted and untrusted agents.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {16},
numpages = {23},
keywords = {Data misuse, honeytokens, data monitoring, information security}
}

@article{10.1145/2611378,
author = {Lin, Ming and Weng, Shifeng and Zhang, Changshui},
title = {On the Sample Complexity of Random Fourier Features for Online Learning: How Many Random Fourier Features Do We Need?},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2611378},
doi = {10.1145/2611378},
abstract = {We study the sample complexity of random Fourier features for online kernel learning—that is, the number of random Fourier features required to achieve good generalization performance. We show that when the loss function is strongly convex and smooth, online kernel learning with random Fourier features can achieve an O(log T /T) bound for the excess risk with only O(1/λ2) random Fourier features, where T is the number of training examples and λ is the modulus of strong convexity. This is a significant improvement compared to the existing result for batch kernel learning that requires O(T) random Fourier features to achieve a generalization bound O(1/√T). Our empirical study verifies that online kernel learning with a limited number of random Fourier features can achieve similar generalization performance as online learning using full kernel matrix. We also present an enhanced online learning algorithm with random Fourier features that improves the classification performance by multiple passes of training examples and a partial average.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {13},
numpages = {19},
keywords = {Nystr\"{o}m, kernel learning, sampling complexity}
}

@article{10.1145/2538028,
author = {Zhang, Yu and Yeung, Dit-Yan},
title = {A Regularization Approach to Learning Task Relationships in Multitask Learning},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2538028},
doi = {10.1145/2538028},
abstract = {Multitask learning is a learning paradigm that seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this article, we propose a regularization approach to learning the relationships between tasks in multitask learning. This approach can be viewed as a novel generalization of the regularized formulation for single-task learning. Besides modeling positive task correlation, our approach—multitask relationship learning (MTRL)—can also describe negative task correlation and identify outlier tasks based on the same underlying principle. By utilizing a matrix-variate normal distribution as a prior on the model parameters of all tasks, our MTRL method has a jointly convex objective function. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multitask learning setting and then generalize it to the asymmetric setting as well. We also discuss some variants of the regularization approach to demonstrate the use of other matrix-variate priors for learning task relationships. Moreover, to gain more insight into our model, we also study the relationships between MTRL and some existing multitask learning methods. Experiments conducted on a toy problem as well as several benchmark datasets demonstrate the effectiveness of MTRL as well as its high interpretability revealed by the task covariance matrix.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {12},
numpages = {31},
keywords = {Multitask learning, task relationship, regularization framework}
}

@article{10.1145/2601434,
author = {Huang, Jin and Nie, Feiping and Huang, Heng and Ding, Chris},
title = {Robust Manifold Nonnegative Matrix Factorization},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601434},
doi = {10.1145/2601434},
abstract = {Nonnegative Matrix Factorization (NMF) has been one of the most widely used clustering techniques for exploratory data analysis. However, since each data point enters the objective function with squared residue error, a few outliers with large errors easily dominate the objective function. In this article, we propose a Robust Manifold Nonnegative Matrix Factorization (RMNMF) method using ℓ2,1-norm and integrating NMF and spectral clustering under the same clustering framework. We also point out the solution uniqueness issue for the existing NMF methods and propose an additional orthonormal constraint to address this problem. With the new constraint, the conventional auxiliary function approach no longer works. We tackle this difficult optimization problem via a novel Augmented Lagrangian Method (ALM)--based algorithm and convert the original constrained optimization problem on one variable into a multivariate constrained problem. The new objective function then can be decomposed into several subproblems that each has a closed-form solution. More importantly, we reveal the connection of our method with robust K-means and spectral clustering, and we demonstrate its theoretical significance. Extensive experiments have been conducted on nine benchmark datasets, and all empirical results show the effectiveness of our method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {11},
numpages = {21},
keywords = {robust NMF, nonnegative matrix factorization (NMF), manifold, Clustering}
}

@article{10.1145/2532169,
author = {Schifanella, Claudio and Candan, K. Sel\c{c}uk and Sapino, Maria Luisa},
title = {Multiresolution Tensor Decompositions with Mode Hierarchies},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2532169},
doi = {10.1145/2532169},
abstract = {Tensors (multidimensional arrays) are widely used for representing high-order dimensional data, in applications ranging from social networks, sensor data, and Internet traffic. Multiway data analysis techniques, in particular tensor decompositions, allow extraction of hidden correlations among multiway data and thus are key components of many data analysis frameworks. Intuitively, these algorithms can be thought of as multiway clustering schemes, which consider multiple facets of the data in identifying clusters, their weights, and contributions of each data element. Unfortunately, algorithms for fitting multiway models are, in general, iterative and very time consuming. In this article, we observe that, in many applications, there is a priori background knowledge (or metadata) about one or more domain dimensions. This metadata is often in the form of a hierarchy that clusters the elements of a given data facet (or mode). We investigate whether such single-mode data hierarchies can be used to boost the efficiency of tensor decomposition process, without significant impact on the final decomposition quality. We consider each domain hierarchy as a guide to help provide higher- or lower-resolution views of the data in the tensor on demand and we rely on these metadata-induced multiresolution tensor representations to develop a multiresolution approach to tensor decomposition. In this article, we focus on an alternating least squares (ALS)--based implementation of the two most important decomposition models such as the PARAllel FACtors (PARAFAC, which decomposes a tensor into a diagonal tensor and a set of factor matrices) and the Tucker (which produces as result a core tensor and a set of dimension-subspaces matrices). Experiment results show that, when the available metadata is used as a rough guide, the proposed multiresolution method helps fit both PARAFAC and Tucker models with consistent (under different parameters settings) savings in execution time and memory consumption, while preserving the quality of the decomposition.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {10},
numpages = {38},
keywords = {Tucker, multiresolution, PARAFAC, Tensor decomposition}
}

@article{10.1145/2601439,
author = {Zhang, Gensheng and Jiang, Xiao and Luo, Ping and Wang, Min and Li, Chengkai},
title = {Discovering General Prominent Streaks in Sequence Data},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601439},
doi = {10.1145/2601439},
abstract = {This article studies the problem of prominent streak discovery in sequence data. Given a sequence of values, a prominent streak is a long consecutive subsequence consisting of only large (small) values, such as consecutive games of outstanding performance in sports, consecutive hours of heavy network traffic, and consecutive days of frequent mentioning of a person in social media. Prominent streak discovery provides insightful data patterns for data analysis in many real-world applications and is an enabling technique for computational journalism. Given its real-world usefulness and complexity, the research on prominent streaks in sequence data opens a spectrum of challenging problems.A baseline approach to finding prominent streaks is a quadratic algorithm that exhaustively enumerates all possible streaks and performs pairwise streak dominance comparison. For more efficient methods, we make the observation that prominent streaks are in fact skyline points in two dimensions—streak interval length and minimum value in the interval. Our solution thus hinges on the idea to separate the two steps in prominent streak discovery: candidate streak generation and skyline operation over candidate streaks. For candidate generation, we propose the concept of local prominent streak (LPS). We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence, in comparison with the quadratic number of candidates produced by the brute-force baseline method. We develop efficient algorithms based on the concept of LPS. The nonlinear local prominent streak (NLPS)-based method considers a superset of LPSs as candidates, and the linear local prominent streak (LLPS)-based method further guarantees to consider only LPSs. The proposed properties and algorithms are also extended for discovering general top-k, multisequence, and multidimensional prominent streaks. The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {9},
numpages = {37},
keywords = {time series database, Computational journalism, skyline query, sequence database}
}

@article{10.1145/2556608,
author = {Chen, Wei and Tang, Jie},
title = {Introduction to Special Issue on Computational Aspects of Social and Information Networks: Theory, Methodologies, and Applications (TKDD-CASIN)},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2556608},
doi = {10.1145/2556608},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {1},
numpages = {2}
}

@article{10.1145/2556612,
author = {Mcauley, Julian and Leskovec, Jure},
title = {Discovering Social Circles in Ego Networks},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2556612},
doi = {10.1145/2556612},
abstract = {People's personal social networks are big and cluttered, and currently there is no good way to automatically organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g., “circles” on Google+, and “lists” on Facebook and Twitter). However, circles are laborious to construct and must be manually updated whenever a user's network grows. In this article, we study the novel task of automatically identifying users' social circles. We pose this task as a multimembership node clustering problem on a user's ego network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle, we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter, for all of which we obtain hand-labeled ground truth.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {4},
numpages = {28},
keywords = {machine learning, Community detection, social circles, ego networks}
}

@article{10.1145/2518176,
author = {Jin, Ruoming and Lee, Victor E. and Li, Longjie},
title = {Scalable and Axiomatic Ranking of Network Role Similarity},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2518176},
doi = {10.1145/2518176},
abstract = {A key task in analyzing social networks and other complex networks is role analysis: describing and categorizing nodes according to how they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithms known for graph automorphism are nonpolynomial. Moreover, since exact equivalence is rare, a more meaningful task is measuring the role similarity between any two nodes. This task is closely related to the structural or link-based similarity problem that SimRank addresses. However, SimRank and other existing similarity measures are not sufficient because they do not guarantee to recognize automorphically or structurally equivalent nodes. This article makes two contributions. First, we present and justify several axiomatic properties necessary for a role similarity measure or metric. Second, we present RoleSim, a new similarity metric that satisfies these axioms and can be computed with a simple iterative algorithm. We rigorously prove that RoleSim satisfies all of these axiomatic properties. We also introduce Iceberg RoleSim, a scalable algorithm that discovers all pairs with RoleSim scores above a user-defined threshold θ. We demonstrate the interpretative power of RoleSim on both both synthetic and real datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {3},
numpages = {37},
keywords = {vertex similarity, Complex network, ranking, social network, automorphic equivalence, role similarity}
}

@article{10.1145/2556613,
author = {Zhong, Erheng and Fan, Wei and Yang, Qiang},
title = {User Behavior Learning and Transfer in Composite Social Networks},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2556613},
doi = {10.1145/2556613},
abstract = {Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization, and recommendation. A major challenge lies in that although many previous works model user behavior from only historical behavior logs, the available user behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., ⩾ 99.9% empty), which makes models overfit the rare observations and fail to provide accurate predictions. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter, and Tencent’s QQ. Importantly, users’ behaviors and interests in different networks influence one another. This provides an opportunity to leverage the knowledge of user behaviors in different networks by considering the overlapping users in different networks as bridges, in order to alleviate the data sparsity problem, and enhance the predictive performance of user behavior modeling. Combining different networks “simply and naively” does not work well. In this article, we formulate the problem to model multiple networks as “adaptive composite transfer” and propose a framework called ComSoc. ComSoc first selects the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users. It then builds topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. With different relational regularization, we introduce different implementations, corresponding to different ways to transfer knowledge from composite social relations. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior models significantly improve the predictive accuracy over a number of existing approaches on several real-world applications, including a very large social networking dataset from Tencent Inc.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {6},
numpages = {32},
keywords = {social network analysis, transfer learning, Composite social network}
}

@article{10.1145/2556609,
author = {Yang, Zhi and Wilson, Christo and Wang, Xiao and Gao, Tingting and Zhao, Ben Y. and Dai, Yafei},
title = {Uncovering Social Network Sybils in the Wild},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2556609},
doi = {10.1145/2556609},
abstract = {Sybil accounts are fake identities created to unfairly increase the power or resources of a single malicious user. Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but they have not been able to perform large-scale measurements to detect them or measure their activities. In this article, we describe our efforts to detect, characterize, and understand Sybil account activity in the Renren Online Social Network (OSN). We use ground truth provided by Renren Inc. to build measurement-based Sybil detectors and deploy them on Renren to detect more than 100,000 Sybil accounts. Using our full dataset of 650,000 Sybils, we examine several aspects of Sybil behavior. First, we study their link creation behavior and find that contrary to prior conjecture, Sybils in OSNs do not form tight-knit communities. Next, we examine the fine-grained behaviors of Sybils on Renren using clickstream data. Third, we investigate behind-the-scenes collusion between large groups of Sybils. Our results reveal that Sybils with no explicit social ties still act in concert to launch attacks. Finally, we investigate enhanced techniques to identify stealthy Sybils. In summary, our study advances the understanding of Sybil behavior on OSNs and shows that Sybils can effectively avoid existing community-based Sybil detectors. We hope that our results will foster new research on Sybil detection that is based on novel types of Sybil features.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {2},
numpages = {29},
keywords = {spam, Online social networks, measurement, Sybil attacks, user behavior}
}

@article{10.1145/2527231,
author = {Abrahao, Bruno and Soundarajan, Sucheta and Hopcroft, John and Kleinberg, Robert},
title = {A Separability Framework for Analyzing Community Structure},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2527231},
doi = {10.1145/2527231},
abstract = {Four major factors govern the intricacies of community extraction in networks: (1) the literature offers a multitude of disparate community detection algorithms whose output exhibits high structural variability across the collection, (2) communities identified by algorithms may differ structurally from real communities that arise in practice, (3) there is no consensus characterizing how to discriminate communities from noncommunities, and (4) the application domain includes a wide variety of networks of fundamentally different natures. In this article, we present a class separability framework to tackle these challenges through a comprehensive analysis of community properties. Our approach enables the assessment of the structural dissimilarity among the output of multiple community detection algorithms and between the output of algorithms and communities that arise in practice. In addition, our method provides us with a way to organize the vast collection of community detection algorithms by grouping those that behave similarly. Finally, we identify the most discriminative graph-theoretical properties of community signature and the small subset of properties that account for most of the biases of the different community detection algorithms. We illustrate our approach with an experimental analysis, which reveals nuances of the structure of real and extracted communities. In our experiments, we furnish our framework with the output of 10 different community detection procedures, representative of categories of popular algorithms available in the literature, applied to a diverse collection of large-scale real network datasets whose domains span biology, online shopping, and social systems. We also analyze communities identified by annotations that accompany the data, which reflect exemplar communities in various domain. We characterize these communities using a broad spectrum of community properties to produce the different structural classes. As our experiments show that community structure is not a universal concept, our framework enables an informed choice of the most suitable community detection method for identifying communities of a specific type in a given network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {5},
numpages = {29},
keywords = {detection algorithms, Class separability, community structure, networks}
}

@article{10.1145/2541268.2541270,
author = {Huang, Jin and Nie, Feiping and Huang, Heng and Tu, Yi-Cheng and Lei, Yu},
title = {Social Trust Prediction Using Heterogeneous Networks},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2541268.2541270},
doi = {10.1145/2541268.2541270},
abstract = {Along with increasing popularity of social websites, online users rely more on the trustworthiness information to make decisions, extract and filter information, and tag and build connections with other users. However, such social network data often suffer from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches are primarily based on exploring trust graph topology itself. However, research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behaviors and tastes. To take advantage of the ancillary information for trust prediction, the challenge then becomes what to transfer and how to transfer. In this article, we address this problem by aggregating heterogeneous social networks and propose a novel joint social networks mining (JSNM) method. Our new joint learning model explores the user-group-level similarity between correlated graphs and simultaneously learns the individual graph structure; therefore, the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the proposed objective function, we use the alternative technique to break down the objective function into several manageable subproblems. We further introduce the auxiliary function to solve the optimization problems with rigorously proved convergence. The extensive experiments have been conducted on both synthetic and real- world data. All empirical results demonstrate the effectiveness of our method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {17},
numpages = {21},
keywords = {nonnegative matrix factorization, social network, Trust prediction, transfer learning}
}

@article{10.1145/2541268.2541273,
author = {Ang, Hock Hee and Gopalkrishnan, Vivekanand and Hoi, Steven C. H. and Ng, Wee Keong},
title = {Classification in P2P Networks with Cascade Support Vector Machines},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2541268.2541273},
doi = {10.1145/2541268.2541273},
abstract = {Classification in Peer-to-Peer (P2P) networks is important to many real applications, such as distributed intrusion detection, distributed recommendation systems, and distributed antispam detection. However, it is very challenging to perform classification in P2P networks due to many practical issues, such as scalability, peer dynamism, and asynchronism. This article investigates the practical techniques of constructing Support Vector Machine (SVM) classifiers in the P2P networks. In particular, we demonstrate how to efficiently cascade SVM in a P2P network with the use of reduced SVM. In addition, we propose to fuse the concept of cascade SVM with bootstrap aggregation to effectively balance the trade-off between classification accuracy, model construction, and prediction cost. We provide theoretical insights for the proposed solutions and conduct an extensive set of empirical studies on a number of large-scale datasets. Encouraging results validate the efficacy of the proposed approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {20},
numpages = {29},
keywords = {distributed classification, cascade SVM, bootstrap aggregation, P2P networks}
}

@article{10.1145/2541268.2541272,
author = {Balc\'{a}zar, Jos\'{e} L.},
title = {Formal and Computational Properties of the Confidence Boost of Association Rules},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2541268.2541272},
doi = {10.1145/2541268.2541272},
abstract = {Some existing notions of redundancy among association rules allow for a logical-style characterization and lead to irredundant bases of absolutely minimum size. We push the intuition of redundancy further to find an intuitive notion of novelty of an association rule, with respect to other rules. Namely, an irredundant rule is so because its confidence is higher than what the rest of the rules would suggest; then, one can ask: how much higher?We propose to measure such a sort of novelty through the confidence boost of a rule. Acting as a complement to confidence and support, the confidence boost helps to obtain small and crisp sets of mined association rules and solves the well-known problem that, in certain cases, rules of negative correlation may pass the confidence bound. We analyze the properties of two versions of the notion of confidence boost, one of them a natural generalization of the other. We develop algorithms to filter rules according to their confidence boost, compare the concept to some similar notions in the literature, and describe the results of some experimentation employing the new notions on standard benchmark datasets. We describe an open source association mining tool that embodies one of our variants of confidence boost in such a way that the data mining process does not require the user to select any value for any parameter.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {19},
numpages = {41},
keywords = {Association rule mining, confidence, association rule quality}
}

@article{10.1145/2541268.2541271,
author = {Guzzo, Antonella and Moccia, Luigi and Sacc\`{a}, Domenico and Serra, Edoardo},
title = {Solving Inverse Frequent Itemset Mining with Infrequency Constraints via Large-Scale Linear Programs},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2541268.2541271},
doi = {10.1145/2541268.2541271},
abstract = {Inverse frequent set mining (IFM) is the problem of computing a transaction database D satisfying given support constraints for some itemsets, which are typically the frequent ones. This article proposes a new formulation of IFM, called IFMI (IFM with infrequency constraints), where the itemsets that are not listed as frequent are constrained to be infrequent; that is, they must have a support less than or equal to a specified unique threshold. An instance of IFMI can be seen as an instance of the original IFM by making explicit the infrequency constraints for the minimal infrequent itemsets, corresponding to the so-called negative generator border defined in the literature. The complexity increase from PSPACE (complexity of IFM) to NEXP (complexity of IFMI) is caused by the cardinality of the negative generator border, which can be exponential in the original input size. Therefore, the article introduces a specific problem parameter κ that computes an upper bound to this cardinality using a hypergraph interpretation for which minimal infrequent itemsets correspond to minimal transversals. By fixing a constant k, the article formulates a k-bounded definition of the problem, called k-IFMI, that collects all instances for which the value of the parameter κ is less than or equal to k—its complexity is in PSPACE as for IFM. The bounded problem is encoded as an integer linear program with a large number of variables (actually exponential w.r.t. the number of constraints), which is thereafter approximated by relaxing integer constraints—the decision problem of solving the linear program is proven to be in NP. In order to solve the linear program, a column generation technique is used that is a variation of the simplex method designed to solve large-scale linear programs, in particular with a huge number of variables. The method at each step requires the solution of an auxiliary integer linear program, which is proven to be NP hard in this case and for which a greedy heuristic is presented. The resulting overall column generation solution algorithm enjoys very good scaling as evidenced by the intensive experimentation, thereby paving the way for its application in real-life scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {18},
numpages = {39},
keywords = {inverse problem, column generation simplex, minimal hypergraph transversals, Frequent itemset mining}
}

@article{10.1145/2541268.2541269,
author = {Adali, Sibel and Magdon-Ismail, Malik and Lu, Xiaohui},
title = {IHypR: Prominence Ranking in Networks of Collaborations with Hyperedges<sup>1</sup>},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2541268.2541269},
doi = {10.1145/2541268.2541269},
abstract = {We present a new algorithm called iHypR for computing prominence of actors in social networks of collaborations. Our algorithm builds on the assumption that prominent actors collaborate on prominent objects, and prominent objects are naturally grouped into prominent clusters or groups (hyperedges in a graph). iHypR makes use of the relationships between actors, objects, and hyperedges to compute a global prominence score for the actors in the network. We do not assume the hyperedges are given in advance. Hyperedges computed by our method can perform as well or even better than “true” hyperedges. Our algorithm is customized for networks of collaborations, but it is generally applicable without further tuning. We show, through extensive experimentation with three real-life data sets and multiple external measures of prominence, that our algorithm outperforms existing well-known algorithms. Our work is the first to offer such an extensive evaluation. We show that unlike most existing algorithms, the performance is robust across multiple measures of performance. Further, we give a detailed study of the sensitivity of our algorithm to different data sets and the design choices within the algorithm that a user may wish to change. Our article illustrates the various trade-offs that must be considered in computing prominence in collaborative social networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {16},
numpages = {32},
keywords = {social networks, Prominence, collaboration}
}

@article{10.1145/2513092.2513093,
author = {Agarwal, Deepak and Caruana, Rich and Pei, Jian and Wang, Ke},
title = {Introduction to the Special Issue ACM SIGKDD 2012},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2513092.2513093},
doi = {10.1145/2513092.2513093},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {9},
numpages = {2}
}

@article{10.1145/2500492,
author = {Sun, Yizhou and Norick, Brandon and Han, Jiawei and Yan, Xifeng and Yu, Philip S. and Yu, Xiao},
title = {PathSelClus: Integrating Meta-Path Selection with User-Guided Object Clustering in Heterogeneous Information Networks},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2500492},
doi = {10.1145/2500492},
abstract = {Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is their potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (seeds) than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weight for each meta-path that is consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights mutually enhance each other. Our experiments with several clustering tasks in two real networks and one synthetic network demonstrate the power of the algorithm in comparison with the baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {11},
numpages = {23},
keywords = {Heterogeneous information networks, user-guided clustering, meta-path selection}
}

@article{10.1145/2513092.2513096,
author = {Ji, Ming and Lin, Binbin and He, Xiaofei and Cai, Deng and Han, Jiawei},
title = {Parallel Field Ranking},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2513092.2513096},
doi = {10.1145/2513092.2513096},
abstract = {Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this article, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {15},
numpages = {21},
keywords = {Manifold, vector field, ranking}
}

@article{10.1145/2500491,
author = {Briggs, Forrest and Fern, Xiaoli Z. and Raich, Raviv and Lou, Qi},
title = {Instance Annotation for Multi-Instance Multi-Label Learning},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2500491},
doi = {10.1145/2500491},
abstract = {Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose a regularized rank-loss objective designed for instance annotation, which can be instantiated with different aggregation models connecting instance-level labels with bag-level label sets. The aggregation models that we consider can be factored as a linear function of a “support instance” for each class, which is a single feature vector representing a whole bag. Hence we name our proposed methods rank-loss Support Instance Machines (SIM). We propose two optimization methods for the rank-loss objective, which is nonconvex. One is a heuristic method that alternates between updating support instances, and solving a convex problem in which the support instances are treated as constant. The other is to apply the constrained concave-convex procedure (CCCP), which can also be interpreted as iteratively updating support instances and solving a convex problem. To solve the convex problem, we employ the Pegasos framework of primal subgradient descent, and prove that it finds an ϵ-suboptimal solution in runtime that is linear in the number of bags, instances, and 1/ϵ. Additionally, we suggest a method of extending the linear learning algorithm to nonlinear classification, without increasing the runtime asymptotically. Experiments on artificial and real-world datasets including images and audio show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {14},
numpages = {30},
keywords = {subgradient, image annotation, Instance annotation, multi-label, support vector machine, multi-instance, bioacoustics}
}

@article{10.1145/2513092.2513094,
author = {Chattopadhyay, Rita and Wang, Zheng and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},
title = {Batch Mode Active Sampling Based on Marginal Probability Distribution Matching},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2513092.2513094},
doi = {10.1145/2513092.2513094},
abstract = {Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on certain criteria. In this article we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimize the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and two biomedical image databases demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. In addition, we present a joint optimization framework for performing both transfer and active learning simultaneously unlike the existing approaches of learning in two separate stages, that is, typically, transfer learning followed by active learning. We specifically minimize a common objective of reducing distribution difference between the domain adapted source, the queried and labeled samples and the rest of the unlabeled target domain data. Our empirical studies on two biomedical image databases and on a publicly available 20 Newsgroups dataset show that incorporation of uncertainty information and transfer learning further improves the performance of the proposed active learning based classifier. Our empirical studies also show that the proposed transfer-active method based on the joint optimization framework performs significantly better than a framework which implements transfer and active learning in two separate stages.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {13},
numpages = {25},
keywords = {marginal probability distribution, maximum mean discrepancy, Active learning, transfer learning}
}

@article{10.1145/2500490,
author = {Bellare, Kedar and Iyengar, Suresh and Parameswaran, Aditya and Rastogi, Vibhor},
title = {Active Sampling for Entity Matching with Guarantees},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2500490},
doi = {10.1145/2500490},
abstract = {In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or nonduplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0--1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more nonduplicate pairs than duplicate pairs). To address this, a recent paper [Arasu et al. 2010] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst case.Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sublinear label complexity (under certain distributional assumptions). Our algorithm uses as a black box any active learning module that minimizes 0--1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {12},
numpages = {24},
keywords = {deduplication, Entity matching, active learning, imbalanced data}
}

@article{10.1145/2500489,
author = {Rakthanmanon, Thanawin and Campana, Bilson and Mueen, Abdullah and Batista, Gustavo and Westover, Brandon and Zhu, Qiang and Zakaria, Jesin and Keogh, Eamonn},
title = {Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2500489},
doi = {10.1145/2500489},
abstract = {Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {10},
numpages = {31},
keywords = {similarity search, lower bounds, Time series}
}

@article{10.1145/2499907.2499911,
author = {Peng, Jing and Seetharaman, Guna and Fan, Wei and Varde, Aparna},
title = {Exploiting Fisher and Fukunaga-Koontz Transforms in Chernoff Dimensionality Reduction},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2499907.2499911},
doi = {10.1145/2499907.2499911},
abstract = {Knowledge discovery from big data demands effective representation of data. However, big data are often characterized by high dimensionality, which makes knowledge discovery more difficult. Many techniques for dimensionality reudction have been proposed, including well-known Fisher's Linear Discriminant Analysis (LDA). However, the Fisher criterion is incapable of dealing with heteroscedasticity in the data. A technique based on the Chernoff criterion for linear dimensionality reduction has been proposed that is capable of exploiting heteroscedastic information in the data. While the Chernoff criterion has been shown to outperform the Fisher's, a clear understanding of its exact behavior is lacking. In this article, we show precisely what can be expected from the Chernoff criterion. In particular, we show that the Chernoff criterion exploits the Fisher and Fukunaga-Koontz transforms in computing its linear discriminants. Furthermore, we show that a recently proposed decomposition of the data space into four subspaces is incomplete. We provide arguments on how to best enrich the decomposition of the data space in order to account for heteroscedasticity in the data. Finally, we provide experimental results validating our theoretical analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {8},
numpages = {25},
keywords = {dimensionality reduction, Chernoff distance, LDA, FKT, Feature evaluation and selection}
}

@article{10.1145/2499907.2499909,
author = {Yang, Haiqin and Lyu, Michael R. and King, Irwin},
title = {Efficient Online Learning for Multitask Feature Selection},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2499907.2499909},
doi = {10.1145/2499907.2499909},
abstract = {Learning explanatory features across multiple related tasks, or MultiTask Feature Selection (MTFS), is an important problem in the applications of data mining, machine learning, and bioinformatics. Previous MTFS methods fulfill this task by batch-mode training. This makes them inefficient when data come sequentially or when the number of training data is so large that they cannot be loaded into the memory simultaneously. In order to tackle these problems, we propose a novel online learning framework to solve the MTFS problem. A main advantage of the online algorithm is its efficiency in both time complexity and memory cost. The weights of the MTFS models at each iteration can be updated by closed-form solutions based on the average of previous subgradients. This yields the worst-case bounds of the time complexity and memory cost at each iteration, both in the order of O(d \texttimes{} Q), where d is the number of feature dimensions and Q is the number of tasks. Moreover, we provide theoretical analysis for the average regret of the online learning algorithms, which also guarantees the convergence rate of the algorithms. Finally, we conduct detailed experiments to show the characteristics and merits of the online learning algorithms in solving several MTFS problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {6},
numpages = {27},
keywords = {Supervised learning, online learning, dual averaging method, multitask learning, feature selection}
}

@article{10.1145/2499907.2499910,
author = {Zhang, Yu and Yeung, Dit-Yan},
title = {Multilabel Relationship Learning},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2499907.2499910},
doi = {10.1145/2499907.2499910},
abstract = {Multilabel learning problems are commonly found in many applications. A characteristic shared by many multilabel learning problems is that some labels have significant correlations between them. In this article, we propose a novel multilabel learning method, called MultiLabel Relationship Learning (MLRL), which extends the conventional support vector machine by explicitly learning and utilizing the relationships between labels. Specifically, we model the label relationships using a label covariance matrix and use it to define a new regularization term for the optimization problem. MLRL learns the model parameters and the label covariance matrix simultaneously based on a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem can be solved efficiently. The relationship between MLRL and two widely used maximum margin methods for multilabel learning is investigated. Moreover, we also propose a semisupervised extension of MLRL, called SSMLRL, to demonstrate how to make use of unlabeled data to help learn the label covariance matrix. Through experiments conducted on some multilabel applications, we find that MLRL not only gives higher classification accuracy but also has better interpretability as revealed by the label covariance matrix.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {7},
numpages = {30},
keywords = {Multilabel learning, label relationship}
}

@article{10.1145/2499907.2499908,
author = {Lou, Tiancheng and Tang, Jie and Hopcroft, John and Fang, Zhanpeng and Ding, Xiaowen},
title = {Learning to Predict Reciprocity and Triadic Closure in Social Networks},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2499907.2499908},
doi = {10.1145/2499907.2499908},
abstract = {We study how links are formed in social networks. In particular, we focus on investigating how a reciprocal (two-way) link, the basic relationship in social networks, is developed from a parasocial (one-way) relationship and how the relationships further develop into triadic closure, one of the fundamental processes of link formation.We first investigate how geographic distance and interactions between users influence the formation of link structure among users. Then we study how social theories including homophily, social balance, and social status are satisfied over networks with parasocial and reciprocal relationships. The study unveils several interesting phenomena. For example, “friend's friend is a friend” indeed exists in the reciprocal relationship network, but does not hold in the parasocial relationship network.We propose a learning framework to formulate the problems of predicting reciprocity and triadic closure into a graphical model. We demonstrate that it is possible to accurately infer 90% of reciprocal relationships in a Twitter network. The proposed model also achieves better performance (+20--30% in terms of F1-measure) than several alternative methods for predicting the triadic closure formation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {5},
numpages = {25},
keywords = {social influence, link prediction, Social network, predictive model, Twitter, reciprocal relationship}
}

@article{10.1145/2601433,
author = {Webb, Geoffrey I. and Vreeken, Jilles},
title = {Efficient Discovery of the Most Interesting Associations},
year = {2013},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601433},
doi = {10.1145/2601433},
abstract = {Self-sufficient itemsets have been proposed as an effective approach to summarizing the key associations in data. However, their computation appears highly demanding, as assessing whether an itemset is self-sufficient requires consideration of all pairwise partitions of the itemset into pairs of subsets as well as consideration of all supersets. This article presents the first published algorithm for efficiently discovering self-sufficient itemsets. This branch-and-bound algorithm deploys two powerful pruning mechanisms based on upper bounds on itemset value and statistical significance level. It demonstrates that finding top-k productive and nonredundant itemsets, with postprocessing to identify those that are not independently productive, can efficiently identify small sets of key associations. We present extensive evaluation of the strengths and limitations of the technique, including comparisons with alternative approaches to finding the most interesting associations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {15},
numpages = {31},
keywords = {statistical association mining, itemset mining, interestingness, Association mining}
}

@article{10.1145/2601438,
author = {Ahmed, Nesreen K. and Neville, Jennifer and Kompella, Ramana},
title = {Network Sampling: From Static to Streaming Graphs},
year = {2013},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2601438},
doi = {10.1145/2601438},
abstract = {Network sampling is integral to the analysis of social, information, and biological networks. Since many real-world networks are massive in size, continuously evolving, and/or distributed in nature, the network structure is often sampled in order to facilitate study. For these reasons, a more thorough and complete understanding of network sampling is critical to support the field of network science. In this paper, we outline a framework for the general problem of network sampling by highlighting the different objectives, population and units of interest, and classes of network sampling methods. In addition, we propose a spectrum of computational models for network sampling methods, ranging from the traditionally studied model based on the assumption of a static domain to a more challenging model that is appropriate for streaming domains. We design a family of sampling methods based on the concept of graph induction that generalize across the full spectrum of computational models (from static to streaming) while efficiently preserving many of the topological properties of the input graphs. Furthermore, we demonstrate how traditional static sampling algorithms can be modified for graph streams for each of the three main classes of sampling methods: node, edge, and topology-based sampling. Experimental results indicate that our proposed family of sampling methods more accurately preserve the underlying properties of the graph in both static and streaming domains. Finally, we study the impact of network sampling algorithms on the parameter estimation and performance evaluation of relational classification algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {7},
numpages = {56},
keywords = {graph streams, relational classification, social network analysis, Network sampling}
}

@article{10.1145/2536775,
author = {Eyal, Ron and Rosenfeld, Avi and Sina, Sigal and Kraus, Sarit},
title = {Predicting and Identifying Missing Node Information in Social Networks},
year = {2013},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2536775},
doi = {10.1145/2536775},
abstract = {In recent years, social networks have surged in popularity. One key aspect of social network research is identifying important missing information that is not explicitly represented in the network, or is not visible to all. To date, this line of research typically focused on finding the connections that are missing between nodes, a challenge typically termed as the link prediction problem.This article introduces the missing node identification problem, where missing members in the social network structure must be identified. In this problem, indications of missing nodes are assumed to exist. Given these indications and a partial network, we must assess which indications originate from the same missing node and determine the full network structure.Toward solving this problem, we present the missing node identification by spectral clustering algorithm (MISC), an approach based on a spectral clustering algorithm, combined with nodes’ pairwise affinity measures that were adopted from link prediction research. We evaluate the performance of our approach in different problem settings and scenarios, using real-life data from Facebook. The results show that our approach has beneficial results and can be effective in solving the missing node identification problem. In addition, this article also presents R-MISC, which uses a sparse matrix representation, efficient algorithms for calculating the nodes’ pairwise affinity, and a proprietary dimension reduction technique to enable scaling the MISC algorithm to large networks of more than 100,000 nodes. Last, we consider problem settings where some of the indications are unknown. Two algorithms are suggested for this problem: speculative MISC, based on MISC, and missing link completion, based on classical link prediction literature. We show that speculative MISC outperforms missing link completion.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {14},
numpages = {35},
keywords = {missing nodes, spectral clustering, Social networks}
}

@article{10.1145/2435209.2435210,
author = {Angiulli, Fabrizio and Fassetti, Fabio},
title = {Nearest Neighbor-Based Classification of Uncertain Data},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2435209.2435210},
doi = {10.1145/2435209.2435210},
abstract = {This work deals with the problem of classifying uncertain data. With this aim we introduce the Uncertain Nearest Neighbor (UNN) rule, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available. The UNN rule relies on the concept of nearest neighbor class, rather than on that of nearest neighbor object. The nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor. The evidence is that the former concept is much more powerful than the latter in the presence of uncertainty, in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario. An effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic (un)certain test object is designed, based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation. Experimental results are presented, showing that the UNN rule is effective and efficient in classifying uncertain data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {1},
numpages = {35},
keywords = {probability density functions, nearest neighbor, nearest neighbor rule, Classification, uncertain data}
}

@article{10.1145/2435209.2435212,
author = {Bayati, Mohsen and Gleich, David F. and Saberi, Amin and Wang, Ying},
title = {Message-Passing Algorithms for Sparse Network Alignment},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2435209.2435212},
doi = {10.1145/2435209.2435212},
abstract = {Network alignment generalizes and unifies several approaches for forming a matching or alignment between the vertices of two graphs. We study a mathematical programming framework for network alignment problem and a sparse variation of it where only a small number of matches between the vertices of the two graphs are possible. We propose a new message passing algorithm that allows us to compute, very efficiently, approximate solutions to the sparse network alignment problems with graph sizes as large as hundreds of thousands of vertices. We also provide extensive simulations comparing our algorithms with two of the best solvers for network alignment problems on two synthetic matching problems, two bioinformatics problems, and three large ontology alignment problems including a multilingual problem with a known labeled alignment.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {3},
numpages = {31},
keywords = {Network alignment, message-passing, belief propagation, graph matching}
}

@article{10.1145/2435209.2435213,
author = {Li, Bin and Hoi, Steven C. H. and Zhao, Peilin and Gopalkrishnan, Vivekanand},
title = {Confidence Weighted Mean Reversion Strategy for Online Portfolio Selection},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2435209.2435213},
doi = {10.1145/2435209.2435213},
abstract = {Online portfolio selection has been attracting increasing attention from the data mining and machine learning communities. All existing online portfolio selection strategies focus on the first order information of a portfolio vector, though the second order information may also be beneficial to a strategy. Moreover, empirical evidence shows that relative stock prices may follow the mean reversion property, which has not been fully exploited by existing strategies. This article proposes a novel online portfolio selection strategy named Confidence Weighted Mean Reversion (CWMR). Inspired by the mean reversion principle in finance and confidence weighted online learning technique in machine learning, CWMR models the portfolio vector as a Gaussian distribution, and sequentially updates the distribution by following the mean reversion trading principle. CWMR’s closed-form updates clearly reflect the mean reversion trading idea. We also present several variants of CWMR algorithms, including a CWMR mixture algorithm that is theoretical universal. Empirically, CWMR strategy is able to effectively exploit the power of mean reversion for online portfolio selection. Extensive experiments on various real markets show that the proposed strategy is superior to the state-of-the-art techniques. The experimental testbed including source codes and data sets is available online.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {4},
numpages = {38},
keywords = {mean reversion, online learning, Portfolio selection, confidence weighted learning}
}

@article{10.1145/2435209.2435211,
author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao and Gong, Yihong},
title = {Comparative Document Summarization via Discriminative Sentence Selection},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2435209.2435211},
doi = {10.1145/2435209.2435211},
abstract = {Given a collection of document groups, a natural question is to identify the differences among these groups. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences that represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {2},
numpages = {18},
keywords = {discriminative sentence selection, Comparative document summarization}
}

@article{10.1145/2382577.2382578,
author = {Ghosh, Joydeep and Smyth, Padhraic and Tomkins, Andrew and Caruana, Rich},
title = {Special Issue on Best of SIGKDD 2011},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382578},
doi = {10.1145/2382577.2382578},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {14},
numpages = {2}
}

@article{10.1145/2382577.2382583,
author = {Wilkinson, Leland and Anand, Anushka and Dang, Tuan Nhon},
title = {Substantial Improvements in the Set-Covering Projection Classifier CHIRP (Composite Hypercubes on Iterated Random Projections)},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382583},
doi = {10.1145/2382577.2382583},
abstract = {In Wilkinson et al. [2011] we introduced a new set-covering random projection classifier that achieved average error lower than that of other classifiers in the Weka platform. This classifier was based on an L∞ norm distance function and exploited an iterative sequence of three stages (projecting, binning, and covering) to deal with the curse of dimensionality, computational complexity, and nonlinear separability. We now present substantial changes that improve robustness and reduce training and testing time by almost an order of magnitude without jeopardizing CHIRP's outstanding error performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {19},
numpages = {18},
keywords = {random projections, Supervised classification}
}

@article{10.1145/2382577.2382582,
author = {Chattopadhyay, Rita and Sun, Qian and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},
title = {Multisource Domain Adaptation and Its Application to Early Detection of Fatigue},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382582},
doi = {10.1145/2382577.2382582},
abstract = {We consider the characterization of muscle fatigue through a noninvasive sensing mechanism such as Surface ElectroMyoGraphy (SEMG). While changes in the properties of SEMG signals with respect to muscle fatigue have been reported in the literature, the large variation in these signals across different individuals makes the task of modeling and classification of SEMG signals challenging. Indeed, the variation in SEMG parameters from subject to subject creates differences in the data distribution. In this article, we propose two transfer learning frameworks based on the multisource domain adaptation methodology for detecting different stages of fatigue using SEMG signals, that addresses the distribution differences. In the proposed frameworks, the SEMG data of a subject represent a domain; data from multiple subjects in the training set form the multiple source domains and the test subject data form the target domain. SEMG signals are predominantly different in conditional probability distribution across subjects. The key feature of the first framework is a novel weighting scheme that addresses the conditional probability distribution differences across multiple domains (subjects) and the key feature of the second framework is a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multisource domain adaptation formulation using the weighted Rademacher complexity measure. We have validated the proposed frameworks on Surface ElectroMyoGram signals collected from 8 people during a fatigue-causing repetitive gripping activity. Comprehensive experiments on the SEMG dataset demonstrate that the proposed method improves the classification accuracy by 20% to 30% over the cases without any domain adaptation method and by 13% to 30% over existing state-of-the-art domain adaptation methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {18},
numpages = {26},
keywords = {transfer learning, surface electromyogram, subject-based variability, Multisource domain adaption}
}

@article{10.1145/2382577.2382580,
author = {Mampaey, Michael and Vreeken, Jilles and Tatti, Nikolaj},
title = {Summarizing Data Succinctly with the Most Informative Itemsets},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382580},
doi = {10.1145/2382577.2382580},
abstract = {Knowledge discovery from data is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and therefore, what results we would find interesting and/or surprising. Given new knowledge about the data, our expectations will change. Hence, in order to avoid redundant results, knowledge discovery algorithms ideally should follow such an iterative updating procedure.With this in mind, we introduce a well-founded approach for succinctly summarizing data with the most informative itemsets; using a probabilistic maximum entropy model, we iteratively find the itemset that provides us the most novel information—that is, for which the frequency in the data surprises us the most—and in turn we update our model accordingly. As we use the maximum entropy principle to obtain unbiased probabilistic models, and only include those itemsets that are most informative with regard to the current model, the summaries we construct are guaranteed to be both descriptive and nonredundant.The algorithm that we present, called mtv, can either discover the top-k most informative itemsets, or we can employ either the Bayesian Information Criterion (bic) or the Minimum Description Length (mdl) principle to automatically identify the set of itemsets that together summarize the data well. In other words, our method will “tell you what you need to know” about the data. Importantly, it is a one-phase algorithm: rather than picking itemsets from a user-provided candidate set, itemsets and their supports are mined on-the-fly. To further its applicability, we provide an efficient method to compute the maximum entropy distribution using Quick Inclusion-Exclusion.Experiments on our method, using synthetic, benchmark, and real data, show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet nonredundant itemsets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {16},
numpages = {42},
keywords = {summarization, minimum description length principle, maximum entropy, pattern sets, MDL, inclusion-exclusion, BIC, Frequent itemsets}
}

@article{10.1145/2382577.2382579,
author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia and Stitelman, Ori},
title = {Leakage in Data Mining: Formulation, Detection, and Avoidance},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382579},
doi = {10.1145/2382577.2382579},
abstract = {Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {15},
numpages = {21},
keywords = {predictive modeling, leakage, Data mining}
}

@article{10.1145/2382577.2382581,
author = {Chu, Shumo and Cheng, James},
title = {Triangle Listing in Massive Networks},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382581},
doi = {10.1145/2382577.2382581},
abstract = {Triangle listing is one of the fundamental algorithmic problems whose solution has numerous applications especially in the analysis of complex networks, such as the computation of clustering coefficients, transitivity, triangular connectivity, trusses, etc. Existing algorithms for triangle listing are mainly in-memory algorithms, whose performance cannot scale with the massive volume of today's fast growing networks. When the input graph cannot fit in main memory, triangle listing requires random disk accesses that can incur prohibitively huge I/O cost. Some streaming, semistreaming, and sampling algorithms have been proposed but these are approximation algorithms. We propose an I/O-efficient algorithm for triangle listing. Our algorithm is exact and avoids random disk access. Our results show that our algorithm is scalable and outperforms the state-of-the-art in-memory and local triangle estimation algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {17},
numpages = {32},
keywords = {massive networks, clustering coefficients, triangle counting, large graphs, Triangle listing}
}

@article{10.1145/2362383.2362387,
author = {Vaz de Melo, Pedro O. S. and Almeida, Virgilio A. F. and Loureiro, Antonio A. F. and Faloutsos, Christos},
title = {Forecasting in the NBA and Other Team Sports: Network Effects in Action},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2362383.2362387},
doi = {10.1145/2362383.2362387},
abstract = {The multi-million sports-betting market is based on the fact that the task of predicting the outcome of a sports event is very hard. Even with the aid of an uncountable number of descriptive statistics and background information, only a few can correctly guess the outcome of a game or a league. In this work, our approach is to move away from the traditional way of predicting sports events, and instead to model sports leagues as networks of players and teams where the only information available is the work relationships among them. We propose two network-based models to predict the behavior of teams in sports leagues. These models are parameter-free, that is, they do not have a single parameter, and moreover are sport-agnostic: they can be applied directly to any team sports league. First, we view a sports league as a network in evolution, and we infer the implicit feedback behind network changes and properties over the years. Then, we use this knowledge to construct the network-based prediction models, which can, with a significantly high probability, indicate how well a team will perform over a season. We compare our proposed models with other prediction models in two of the most popular sports leagues: the National Basketball Association (NBA) and the Major League Baseball (MLB). Our model shows consistently good results in comparison with the other models and, relying upon the network properties of the teams, we achieved a ≈ 14% rank prediction accuracy improvement over our best competitor.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {13},
numpages = {27},
keywords = {Complex networks, sports analytics, social networks}
}

@article{10.1145/2362383.2362386,
author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao and Gong, Yihong},
title = {Comparative Document Summarization via Discriminative Sentence Selection},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2362383.2362386},
doi = {10.1145/2362383.2362386},
abstract = {Given a collection of document groups, a natural question is to identify the differences among them. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem, that of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences which represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {12},
numpages = {18},
keywords = {Comparative document summarization, discriminative sentence selection}
}

@article{10.1145/2362383.2362385,
author = {Chan, Jeffrey and Bailey, James and Leckie, Christopher and Houle, Michael},
title = {CiForager: Incrementally Discovering Regions of Correlated Change in Evolving Graphs},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2362383.2362385},
doi = {10.1145/2362383.2362385},
abstract = {Data mining techniques for understanding how graphs evolve over time have become increasingly important. Evolving graphs arise naturally in diverse applications such as computer network topologies, multiplayer games and medical imaging. A natural and interesting problem in evolving graph analysis is the discovery of compact subgraphs that change in a similar manner. Such subgraphs are known as regions of correlated change and they can both summarise change patterns in graphs and help identify the underlying events causing these changes. However, previous techniques for discovering regions of correlated change suffer from limited scalability, making them unsuitable for analysing the evolution of very large graphs. In this paper, we introduce a new algorithm called ciForager, that addresses this scalability challenge and offers considerable improvements. The efficiency of ciForager is based on the use of new incremental techniques for detecting change, as well as the use of Voronoi representations for efficiently determining distance. We experimentally show that ciForager can achieve speedups of up to 1000 times over previous approaches. As a result, it becomes feasible for the first time to discover regions of correlated change in extremely large graphs, such as the entire BGP routing topology of the Internet.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {11},
numpages = {50},
keywords = {shortest path distance, connected components, Dynamic graph mining, fault detection, correlated change}
}

@article{10.1145/2362383.2362384,
author = {Wang, Zhenxing and Chan, Laiwan},
title = {Learning Bayesian Networks from Markov Random Fields: An Efficient Algorithm for Linear Models},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2362383.2362384},
doi = {10.1145/2362383.2362384},
abstract = {Dependency analysis is a typical approach for Bayesian network learning, which infers the structures of Bayesian networks by the results of a series of conditional independence (CI) tests. In practice, testing independence conditioning on large sets hampers the performance of dependency analysis algorithms in terms of accuracy and running time for the following reasons. First, testing independence on large sets of variables with limited samples is not stable. Second, for most dependency analysis algorithms, the number of CI tests grows at an exponential rate with the sizes of conditioning sets, and the running time grows of the same rate. Therefore, determining how to reduce the number of CI tests and the sizes of conditioning sets becomes a critical step in dependency analysis algorithms. In this article, we address a two-phase algorithm based on the observation that the structures of Markov random fields are similar to those of Bayesian networks. The first phase of the algorithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; the second phase of the algorithm removes redundant edges according to CI tests to get the true Bayesian network. Both phases use Markov blanket information to reduce the sizes of conditioning sets and the number of CI tests without sacrificing accuracy. An empirical study shows that the two-phase algorithm performs well in terms of accuracy and efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {10},
numpages = {31},
keywords = {Bayesian networks, causal modeling, graphical models}
}

@article{10.1145/2297456.2297458,
author = {Das, Sanmay and Magdon-Ismail, Malik},
title = {A Model for Information Growth in Collective Wisdom Processes},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2297456.2297458},
doi = {10.1145/2297456.2297458},
abstract = {Collaborative media such as wikis have become enormously successful venues for information creation. Articles accrue information through the asynchronous editing of users who arrive both seeking information and possibly able to contribute information. Most articles stabilize to high-quality, trusted sources of information representing the collective wisdom of all the users who edited the article. We propose a model for information growth which relies on two main observations: (i) as an article’s quality improves, it attracts visitors at a faster rate (a rich-get-richer phenomenon); and, simultaneously, (ii) the chances that a new visitor will improve the article drops (there is only so much that can be said about a particular topic). Our model is able to reproduce many features of the edit dynamics observed on Wikipedia; in particular, it captures the observed rise in the edit rate, followed by 1/t decay. Despite differences in the media, we also document similar features in the comment rates for a segment of the LiveJournal blogosphere.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {6},
numpages = {10},
keywords = {dynamical systems, social networks, Collective intelligence}
}

@article{10.1145/2297456.2297461,
author = {Bhattacharya, Indrajit and Godbole, Shantanu and Joshi, Sachindra and Verma, Ashish},
title = {Cross-Guided Clustering: Transfer of Relevant Supervision across Tasks},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2297456.2297461},
doi = {10.1145/2297456.2297461},
abstract = {Lack of supervision in clustering algorithms often leads to clusters that are not useful or interesting to human reviewers. We investigate if supervision can be automatically transferred for clustering a target task, by providing a relevant supervised partitioning of a dataset from a different source task. The target clustering is made more meaningful for the human user by trading-off intrinsic clustering goodness on the target task for alignment with relevant supervised partitions in the source task, wherever possible. We propose a cross-guided clustering algorithm that builds on traditional k-means by aligning the target clusters with source partitions. The alignment process makes use of a cross-task similarity measure that discovers hidden relationships across tasks. When the source and target tasks correspond to different domains with potentially different vocabularies, we propose a projection approach using pivot vocabularies for the cross-domain similarity measure. Using multiple real-world and synthetic datasets, we show that our approach improves clustering accuracy significantly over traditional k-means and state-of-the-art semi-supervised clustering baselines, over a wide range of data characteristics and parameter settings.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {9},
numpages = {28},
keywords = {cluster alignment, transfer, Multitask}
}

@article{10.1145/2297456.2297457,
author = {Mavroeidis, Dimitrios and Magdalinos, Panagis},
title = {A Sequential Sampling Framework for Spectral K-Means Based on Efficient Bootstrap Accuracy Estimations: Application to Distributed Clustering},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2297456.2297457},
doi = {10.1145/2297456.2297457},
abstract = {The scalability of learning algorithms has always been a central concern for data mining researchers, and nowadays, with the rapid increase in data storage capacities and availability, its importance has increased. To this end, sampling has been studied by several researchers in an effort to derive sufficiently accurate models using only small data fractions. In this article we focus on spectral k-means, that is, the k-means approximation as derived by the spectral relaxation, and propose a sequential sampling framework that iteratively enlarges the sample size until the k-means results (objective function and cluster structure) become indistinguishable from the asymptotic (infinite-data) output. In the proposed framework we adopt a commonly applied principle in data mining research that considers the use of minimal assumptions concerning the data generating distribution. This restriction imposes several challenges, mainly related to the efficiency of the sequential sampling procedure. These challenges are addressed using elements of matrix perturbation theory and statistics. Moreover, although the main focus is on spectral k-means, we also demonstrate that the proposed framework can be generalized to handle spectral clustering.The proposed sequential sampling framework is consecutively employed for addressing the distributed clustering problem, where the task is to construct a global model for data that resides in distributed network nodes. The main challenge in this context is related to the bandwidth constraints that are commonly imposed, thus requiring that the distributed clustering algorithm consumes a minimal amount of network load. This illustrates the applicability of the proposed approach, as it enables the determination of a minimal sample size that can be used for constructing an accurate clustering model that entails the distributional characteristics of the data. As opposed to the relevant distributed k-means approaches, our framework takes into account the fact that the choice of the number of clusters has a crucial effect on the required amount of communication. More precisely, the proposed algorithm is able to derive a statistical estimation of the required relative sizes for all possible values of k. This unique feature of our distributed clustering framework enables a network administrator to choose an economic solution that identifies the crude cluster structure of a dataset and not devote excessive network resources for identifying all the “correct” detailed clusters.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {5},
numpages = {37},
keywords = {distributed clustering, bootstrapping, matrix perturbation theory, clustering, Asymptotic convergence, sampling, spectral}
}

@article{10.1145/2297456.2297459,
author = {Xu, Tianbing and Zhang, Zhongfei and Yu, Philip S. and Long, Bo},
title = {Generative Models for Evolutionary Clustering},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2297456.2297459},
doi = {10.1145/2297456.2297459},
abstract = {This article studies evolutionary clustering, a recently emerged hot topic with many important applications, noticeably in dynamic social network analysis. In this article, based on the recent literature on nonparametric Bayesian models, we have developed two generative models: DPChain and HDP-HTM. DPChain is derived from the Dirichlet process mixture (DPM) model, with an exponential decaying component along with the time. HDP-HTM combines the hierarchical dirichlet process (HDP) with a hierarchical transition matrix (HTM) based on the proposed Infinite hierarchical Markov state model (iHMS). Both models substantially advance the literature on evolutionary clustering, in the sense that not only do they both perform better than those in the existing literature, but more importantly, they are capable of automatically learning the cluster numbers and explicitly addressing the corresponding issues. Extensive evaluations have demonstrated the effectiveness and the promise of these two solutions compared to the state-of-the-art literature.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {7},
numpages = {27},
keywords = {Evolutionary clustering, hierarchical transition matrix, DPChain, HDP-HTM, iHMS}
}

@article{10.1145/2297456.2297460,
author = {Wang, Shaojun and Schuurmans, Dale and Zhao, Yunxin},
title = {The Latent Maximum Entropy Principle},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2297456.2297460},
doi = {10.1145/2297456.2297460},
abstract = {We present an extension to Jaynes’ maximum entropy principle that incorporates latent variables. The principle of latent maximum entropy we propose is different from both Jaynes’ maximum entropy principle and maximum likelihood estimation, but can yield better estimates in the presence of hidden variables and limited training data. We first show that solving for a latent maximum entropy model poses a hard nonlinear constrained optimization problem in general. However, we then show that feasible solutions to this problem can be obtained efficiently for the special case of log-linear models---which forms the basis for an efficient approximation to the latent maximum entropy principle. We derive an algorithm that combines expectation-maximization with iterative scaling to produce feasible log-linear solutions. This algorithm can be interpreted as an alternating minimization algorithm in the information divergence, and reveals an intimate connection between the latent maximum entropy and maximum likelihood principles. To select a final model, we generate a series of feasible candidates, calculate the entropy of each, and choose the model that attains the highest entropy. Our experimental results show that estimation based on the latent maximum entropy principle generally gives better results than maximum likelihood when estimating latent variable models on small observed data samples.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {8},
numpages = {42},
keywords = {expectation maximization, Maximum entropy, latent variable models, iterative scaling, information geometry}
}

@article{10.1145/2133360.2133362,
author = {Li, Chun and Yang, Qingyan and Wang, Jianyong and Li, Ming},
title = {Efficient Mining of Gap-Constrained Subsequences and Its Various Applications},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133362},
doi = {10.1145/2133360.2133362},
abstract = {Mining frequent subsequence patterns is a typical data-mining problem and various efficient sequential pattern mining algorithms have been proposed. In many application domains (e.g., biology), the frequent subsequences confined by the predefined gap requirements are more meaningful than the general sequential patterns. In this article, we propose two algorithms, Gap-BIDE for mining closed gap-constrained subsequences from a set of input sequences, and Gap-Connect for mining repetitive gap-constrained subsequences from a single input sequence. Inspired by some state-of-the-art closed or constrained sequential pattern mining algorithms, the Gap-BIDE algorithm adopts an efficient approach to finding the complete set of closed sequential patterns with gap constraints, while the Gap-Connect algorithm efficiently mines an approximate set of long patterns by connecting short patterns. We also present several methods for feature selection from the set of gap-constrained patterns for the purpose of classification and clustering. Our extensive performance study shows that our approaches are very efficient in mining frequent subsequences with gap constraints, and the gap-constrained pattern based classification/clustering approaches can achieve high-quality results.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {2},
numpages = {39},
keywords = {Sequential pattern, repetitive subsequence, closed subsequence, gap-constraint}
}

@article{10.1145/2133360.2133363,
author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
title = {Isolation-Based Anomaly Detection},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133363},
doi = {10.1145/2133360.2133363},
abstract = {Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods.As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {3},
numpages = {39},
keywords = {isolation forest, ensemble methods, outlier detection, isolation, Anomaly detection, random tree ensemble, binary tree}
}

@article{10.1145/2133360.2133364,
author = {Jin, Yu and Duffield, Nick and Erman, Jeffrey and Haffner, Patrick and Sen, Subhabrata and Zhang, Zhi-Li},
title = {A Modular Machine Learning System for Flow-Level Traffic Classification in Large Networks},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133364},
doi = {10.1145/2133360.2133364},
abstract = {The ability to accurately and scalably classify network traffic is of critical importance to a wide range of management tasks of large networks, such as tier-1 ISP networks and global enterprise networks. Guided by the practical constraints and requirements of traffic classification in large networks, in this article, we explore the design of an accurate and scalable machine learning based flow-level traffic classification system, which is trained on a dataset of flow-level data that has been annotated with application protocol labels by a packet-level classifier. Our system employs a lightweight modular architecture, which combines a series of simple linear binary classifiers, each of which can be efficiently implemented and trained on vast amounts of flow data in parallel, and embraces three key innovative mechanisms, weighted threshold sampling, logistic calibration, and intelligent data partitioning, to achieve scalability while attaining high accuracy. Evaluations using real traffic data from multiple locations in a large ISP show that our system accurately reproduces the labels of the packet level classifier when runs on (unlabeled) flow records, while meeting the scalability and stability requirements of large ISP networks. Using training and test datasets that are two months apart and collected from two different locations, the flow error rates are only 3% for TCP flows and 0.4% for UDP flows. We further show that such error rates can be reduced by combining the information of spatial distributions of flows, or collective traffic statistics, during classification. We propose a novel two-step model, which seamlessly integrates these collective traffic statistics into the existing traffic classification system. Experimental results display performance improvement on all traffic classes and an overall error rate reduction by 15%. In addition to a high accuracy, at runtime, our implementation easily scales to classify traffic on 10Gbps links.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {4},
numpages = {34},
keywords = {Communications network, traffic classification, machine learning}
}

@article{10.1145/2133360.2133361,
author = {Ienco, Dino and Pensa, Ruggero G. and Meo, Rosa},
title = {From Context to Distance: Learning Dissimilarity for Categorical Data Clustering},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133361},
doi = {10.1145/2133360.2133361},
abstract = {Clustering data described by categorical attributes is a challenging task in data mining applications. Unlike numerical attributes, it is difficult to define a distance between pairs of values of a categorical attribute, since the values are not ordered. In this article, we propose a framework to learn a context-based distance for categorical attributes. The key intuition of this work is that the distance between two values of a categorical attribute Ai can be determined by the way in which the values of the other attributes Aj are distributed in the dataset objects: if they are similarly distributed in the groups of objects in correspondence of the distinct values of Ai a low value of distance is obtained. We propose also a solution to the critical point of the choice of the attributes Aj. We validate our approach by embedding our distance learning framework in a hierarchical clustering algorithm. We applied it on various real world and synthetic datasets, both low and high-dimensional. Experimental results show that our method is competitive with respect to the state of the art of categorical data clustering approaches. We also show that our approach is scalable and has a low impact on the overall computational time of a clustering task.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {1},
numpages = {25},
keywords = {distance learning, clustering, Categorical data}
}

@article{10.1145/2086737.2086738,
author = {Elkan, Charles and Koren, Yehuda},
title = {Guest Editorial for Special Issue KDD’10},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086738},
doi = {10.1145/2086737.2086738},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {18},
numpages = {2}
}

@article{10.1145/2086737.2086744,
author = {Shahaf, Dafna and Guestrin, Carlos},
title = {Connecting Two (or Less) Dots: Discovering Structure in News Articles},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086744},
doi = {10.1145/2086737.2086744},
abstract = {Finding information is becoming a major part of our daily life. Entire sectors, from Web users to scientists and intelligence analysts, are increasingly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture.In this article, we investigate methods for automatically connecting the dots---providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the health care debate (2009).We formalize the characteristics of a good chain and provide a fast search-driven algorithm to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. We also provide a method to handle partially-specified endpoints, for users who do not know both ends of a story. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate that the objective we propose captures the users’ intuitive notion of coherence, and that our algorithm effectively helps users understand the news.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {24},
numpages = {31},
keywords = {Coherence}
}

@article{10.1145/2086737.2086741,
author = {Gomez-Rodriguez, Manuel and Leskovec, Jure and Krause, Andreas},
title = {Inferring Networks of Diffusion and Influence},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086741},
doi = {10.1145/2086737.2086741},
abstract = {Information diffusion and virus propagation are fundamental processes taking place in networks. While it is often possible to directly observe when nodes become infected with a virus or publish the information, observing individual transmissions (who infects whom, or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and finds provably near-optimal networks.We demonstrate the effectiveness of our approach by tracing information diffusion in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news for the top 1,000 media sites and blogs tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {21},
numpages = {37},
keywords = {social networks, blogs, information cascades, news media, meme-tracking, Networks of diffusion}
}

@article{10.1145/2086737.2086739,
author = {Iwata, Tomoharu and Yamada, Takeshi and Sakurai, Yasushi and Ueda, Naonori},
title = {Sequential Modeling of Topic Dynamics with Multiple Timescales},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086739},
doi = {10.1145/2086737.2086739},
abstract = {We propose an online topic model for sequentially analyzing the time evolution of topics in document collections. Topics naturally evolve with multiple timescales. For example, some words may be used consistently over one hundred years, while other words emerge and disappear over periods of a few days. Thus, in the proposed model, current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch. Considering both the long- and short-timescale dependency yields a more robust model. We derive efficient online inference procedures based on a stochastic EM algorithm, in which the model is sequentially updated using newly obtained data; this means that past data are not required to make the inference. We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {19},
numpages = {27},
keywords = {multiscale, online learning, Topic model, time-series analysis}
}

@article{10.1145/2086737.2086743,
author = {Yu, Hsiang-Fu and Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen},
title = {Large Linear Classification When Data Cannot Fit in Memory},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086743},
doi = {10.1145/2086737.2086743},
abstract = {Recent advances in linear classification have shown that for applications such as document classification, the training process can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. Because data cannot fit in memory, many design considerations are very different from those for traditional algorithms. We discuss and compare with existing approaches that are able to handle data larger than memory. Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {23},
numpages = {23},
keywords = {linear classification, large-scale learning, support vector machines, Block minimization methods}
}

@article{10.1145/2086737.2086742,
author = {Chen, Jianhui and Liu, Ji and Ye, Jieping},
title = {Learning Incoherent Sparse and Low-Rank Patterns from Multiple Tasks},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086742},
doi = {10.1145/2086737.2086742},
abstract = {We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multitask learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is nonconvex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose employing the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is nondifferentiable and the feasible domain is nontrivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of the projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and a Euclidean projection subproblem. We also present two projected gradient algorithms and analyze their rates of convergence in detail. In addition, we illustrate the use of the presented projected gradient algorithms for the proposed multitask learning formulation using the least squares loss. Experimental results on a collection of real-world data sets demonstrate the effectiveness of the proposed multitask learning formulation and the efficiency of the proposed projected gradient algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {22},
numpages = {31},
keywords = {trace norm, low-rank and sparse patterns, Multitask learning}
}

@article{10.1145/2086737.2086740,
author = {Huh, Seungil and Fienberg, Stephen E.},
title = {Discriminative Topic Modeling Based on Manifold Learning},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2086737.2086740},
doi = {10.1145/2086737.2086740},
abstract = {Topic modeling has become a popular method used for data analysis in various domains including text documents. Previous topic model approaches, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These approaches, however do not take into account the manifold structure of the data, which is generally informative for nonlinear dimensionality reduction mapping. More recent topic model approaches, Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown resulting benefits. But they fall short of achieving full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this article, we propose a new approach, Discriminative Topic Model (DTM), which separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving local consistency. We also present a novel model-fitting algorithm based on the generalized EM algorithm and the concept of Pareto improvement. We empirically demonstrate the success of DTM in terms of unsupervised clustering and semisupervised classification accuracies on text corpora and robustness to parameters compared to state-of-the-art techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {20},
numpages = {25},
keywords = {dimensionality reduction, document clustering and classification, Topic modeling, semisupervised learning}
}

@article{10.1145/1993077.1993079,
author = {Maier, Marc and Rattigan, Matthew and Jensen, David},
title = {Indexing Network Structure with Shortest-Path Trees},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1993077.1993079},
doi = {10.1145/1993077.1993079},
abstract = {The ability to discover low-cost paths in networks has practical consequences for knowledge discovery and social network analysis tasks. Many analytic techniques for networks require finding low-cost paths, but exact methods for search become prohibitive for large networks, and data sets are steadily increasing in size. Short paths can be found efficiently by utilizing an index of network structure, which estimates network distances and enables rapid discovery of short paths. Through experiments on synthetic networks, we demonstrate that one such novel network structure index based on the shortest-path tree outperforms other previously proposed indices. We also show that it generalizes across arbitrarily weighted networks of various structures and densities, provides accurate estimates of distance, and has efficient time and space complexity. We present results on real data sets for several applications, including navigation, diameter estimation, centrality computation, and clustering---all made efficient by virtue of the network structure index.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {15},
numpages = {25},
keywords = {social network analysis, weighted networks, network structure index, Knowledge discovery in graphs}
}

@article{10.1145/1993077.1993078,
author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao and Chi, Yun and Gong, Yihong},
title = {Integrating Document Clustering and Multidocument Summarization},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1993077.1993078},
doi = {10.1145/1993077.1993078},
abstract = {Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix and then conduct the clustering process. Although many of these clustering methods can group the documents effectively, it is still hard for people to capture the meaning of the documents since there is no satisfactory interpretation for each document cluster. A straightforward solution is to first cluster the documents and then summarize each document cluster using summarization methods. However, most of the current summarization methods are solely based on the sentence-term matrix and ignore the context dependence of the sentences. As a result, the generated summaries lack guidance from the document clusters. In this article, we propose a new language model to simultaneously cluster and summarize documents by making use of both the document-term and sentence-term matrices. By utilizing the mutual influence of document clustering and summarization, our method makes; (1) a better document clustering method with more meaningful interpretation; and (2) an effective document summarization method with guidance from document clustering. Experimental results on various document datasets show the effectiveness of our proposed method and the high interpretability of the generated summaries.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {14},
numpages = {26},
keywords = {nonnegative matrix factorization with given bases, multidocument summarization, Document clustering}
}

@article{10.1145/1993077.1993080,
author = {Wong, Raymond Chi-Wing and Fu, Ada Wai-Chee and Wang, Ke and Yu, Philip S. and Pei, Jian},
title = {Can the Utility of Anonymized Data Be Used for Privacy Breaches?},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1993077.1993080},
doi = {10.1145/1993077.1993080},
abstract = {Group based anonymization is the most widely studied approach for privacy-preserving data publishing. Privacy models/definitions using group based anonymization includes k-anonymity, l-diversity, and t-closeness, to name a few. The goal of this article is to raise a fundamental issue regarding the privacy exposure of the approaches using group based anonymization. This has been overlooked in the past. The group based anonymization approach by bucketization basically hides each individual record behind a group to preserve data privacy. If not properly anonymized, patterns can actually be derived from the published data and be used by an adversary to breach individual privacy. For example, from the medical records released, if patterns such as that people from certain countries rarely suffer from some disease can be derived, then the information can be used to imply linkage of other people in an anonymized group with this disease with higher likelihood. We call the derived patterns from the published data the foreground knowledge. This is in contrast to the background knowledge that the adversary may obtain from other channels, as studied in some previous work. Finally, our experimental results show such an attack is realistic in the privacy benchmark dataset under the traditional group based anonymization approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {16},
numpages = {24},
keywords = {data publishing, k-anonymity, Privacy preservation, l-diversity}
}

@article{10.1145/1993077.1993081,
author = {Lin, Yu-Ru and Sun, Jimeng and Sundaram, Hari and Kelliher, Aisling and Castro, Paul and Konuru, Ravi},
title = {Community Discovery via Metagraph Factorization},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1993077.1993081},
doi = {10.1145/1993077.1993081},
abstract = {This work aims at discovering community structure in rich media social networks through analysis of time-varying, multirelational data. Community structure represents the latent social context of user actions. It has important applications such as search and recommendation. The problem is particularly useful in the enterprise domain, where extracting emergent community structure on enterprise social media can help in forming new collaborative teams, in expertise discovery, and in the long term reorganization of enterprises based on collaboration patterns. There are several unique challenges: (a) In social media, the context of user actions is constantly changing and coevolving; hence the social context contains time-evolving multidimensional relations. (b) The social context is determined by the available system features and is unique in each social media platform; hence the analysis of such data needs to flexibly incorporate various system features. In this article we propose MetaFac (MetaGraph Factorization), a framework that extracts community structures from dynamic, multidimensional social contexts and interactions. Our work has three key contributions: (1) metagraph, a novel relational hypergraph representation for modeling multirelational and multidimensional social data; (2) an efficient multirelational factorization method for community extraction on a given metagraph; (3) an online method to handle time-varying relations through incremental metagraph factorization. Extensive experiments on real-world social data collected from an enterprise and the public Digg social media Web site suggest that our technique is scalable and is able to extract meaningful communities from social media contexts. We illustrate the usefulness of our framework through two prediction tasks: (1) in the enterprise dataset, the task is to predict users’ future interests on tag usage, and (2) in the Digg dataset, the task is to predict users’ future interests in voting and commenting on Digg stories. Our prediction significantly outperforms baseline methods (including aspect model and tensor analysis), indicating the promising direction of using metagraphs for handling time-varying social relational contexts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
articleno = {17},
numpages = {44},
keywords = {dynamic social network analysis, community discovery, MetaFac, relational hypergraph, nonnegative tensor factorization, metagraph factorization}
}

@article{10.1145/1921632.1921636,
author = {Dunlavy, Daniel M. and Kolda, Tamara G. and Acar, Evrim},
title = {Temporal Link Prediction Using Matrix and Tensor Factorizations},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921636},
doi = {10.1145/1921632.1921636},
abstract = {The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T + 1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {10},
numpages = {27},
keywords = {tensor factorization, link prediction, CANDECOMP, PARAFAC, evolution, Link mining}
}

@article{10.1145/1921632.1921635,
author = {de Vries, Timothy and Ke, Hui and Chawla, Sanjay and Christen, Peter},
title = {Robust Record Linkage Blocking Using Suffix Arrays and Bloom Filters},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921635},
doi = {10.1145/1921632.1921635},
abstract = {Record linkage is an important data integration task that has many practical uses for matching, merging and duplicate removal in large and diverse databases. However, quadratic scalability for the brute force approach of comparing all possible pairs of records necessitates the design of appropriate indexing or blocking techniques. The aim of these techniques is to cheaply remove candidate record pairs that are unlikely to match. We design and evaluate an efficient and highly scalable blocking approach based on suffix arrays. Our suffix grouping technique exploits the ordering used by the index to merge similar blocks at marginal extra cost, resulting in a much higher accuracy while retaining the high scalability of the base suffix array method. Efficiently grouping similar suffixes is carried out with the use of a sliding window technique. We carry out an in-depth analysis of our method and show results from experiments using real and synthetic data, which highlight the importance of using efficient indexing and blocking in real-world applications where datasets contain millions of records. We extend our disk-based methods with the capability to utilise main memory based storage to construct Bloom filters, which we have found to cause significant speedup by reducing the number of costly database queries by up to 70% in real data. We give practical implementation details and show how Bloom filters can be easily applied to Suffix Array based indexing.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {9},
numpages = {27},
keywords = {Record linkage, suffix arrays, blocking}
}

@article{10.1145/1921632.1921633,
author = {Sun, Jimeng and Liu, Yan and Tang, Jie and Apte, Chid},
title = {Introduction to Special Issue on Large-Scale Data Mining},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921633},
doi = {10.1145/1921632.1921633},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {7},
numpages = {1}
}

@article{10.1145/1921632.1921639,
author = {Menon, Aditya Krishna and Elkan, Charles},
title = {Fast Algorithms for Approximating the Singular Value Decomposition},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921639},
doi = {10.1145/1921632.1921639},
abstract = {A low-rank approximation to a matrix A is a matrix with significantly smaller rank than A, and which is close to A according to some norm. Many practical applications involving the use of large matrices focus on low-rank approximations. By reducing the rank or dimensionality of the data, we reduce the complexity of analyzing the data. The singular value decomposition is the most popular low-rank matrix approximation. However, due to its expensive computational requirements, it has often been considered intractable for practical applications involving massive data. Recent developments have tried to address this problem, with several methods proposed to approximate the decomposition with better asymptotic runtime. We present an empirical study of these techniques on a variety of dense and sparse datasets. We find that a sampling approach of Drineas, Kannan and Mahoney is often, but not always, the best performing method. This method gives solutions with high accuracy much faster than classical SVD algorithms, on large sparse datasets in particular. Other modern methods, such as a recent algorithm by Rokhlin and Tygert, also offer savings compared to classical SVD algorithms. The older sampling methods of Achlioptas and McSherry are shown to sometimes take longer than classical SVD.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {13},
numpages = {36},
keywords = {experimental evaluation, low rank approximation, Singular value decomposition}
}

@article{10.1145/1921632.1921637,
author = {Magdalinos, Panagis and Doulkeridis, Christos and Vazirgiannis, Michalis},
title = {Enhancing Clustering Quality through Landmark-Based Dimensionality Reduction},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921637},
doi = {10.1145/1921632.1921637},
abstract = {Scaling up data mining algorithms for data of both high dimensionality and cardinality has been lately recognized as one of the most challenging problems in data mining research. The reason is that typical data mining tasks, such as clustering, cannot produce high quality results when applied on high-dimensional and/or large (in terms of cardinality) datasets. Data preprocessing and in particular dimensionality reduction constitute promising tools to deal with this problem. However, most of the existing dimensionality reduction algorithms share also the same disadvantages with data mining algorithms, when applied on large datasets of high dimensionality. In this article, we propose a fast and efficient dimensionality reduction algorithm (FEDRA), which is particularly scalable and therefore suitable for challenging datasets. FEDRA follows the landmark-based paradigm for embedding data objects in a low-dimensional projection space. By means of a theoretical analysis, we prove that FEDRA is efficient, while we demonstrate the achieved quality of results through experiments on datasets of higher cardinality and dimensionality than those employed in the evaluation of competitive algorithms. The obtained results prove that FEDRA manages to retain or ameliorate clustering quality while projecting in less than 10% of the initial dimensionality. Moreover, our algorithm produces embeddings that enable the faster convergence of clustering algorithms. Therefore, FEDRA emerges as a powerful and generic tool for data pre-processing, which can be integrated in other data mining algorithms, thus enhancing their performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {11},
numpages = {44},
keywords = {Landmarks, dimensionality reduction, clustering quality}
}

@article{10.1145/1921632.1921634,
author = {Kang, U. and Tsourakakis, Charalampos E. and Appel, Ana Paula and Faloutsos, Christos and Leskovec, Jure},
title = {HADI: Mining Radii of Large Graphs},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921634},
doi = {10.1145/1921632.1921634},
abstract = {Given large, multimillion-node graphs (e.g., Facebook, Web-crawls, etc.), how do they evolve over time? How are they connected? What are the central nodes and the outliers? In this article we define the Radius plot of a graph and show how it can answer these questions. However, computing the Radius plot is prohibitively expensive for graphs reaching the planetary scale.There are two major contributions in this article: (a) We propose HADI (HAdoop DIameter and radii estimator), a carefully designed and fine-tuned algorithm to compute the radii and the diameter of massive graphs, that runs on the top of the Hadoop/MapReduce system, with excellent scale-up on the number of available machines (b) We run HADI on several real world datasets including YahooWeb (6B edges, 1/8 of a Terabyte), one of the largest public graphs ever analyzed.Thanks to HADI, we report fascinating patterns on large networks, like the surprisingly small effective diameter, the multimodal/bimodal shape of the Radius plot, and its palindrome motion over time.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {8},
numpages = {24},
keywords = {small web, hadoop, graph mining, HADI, Radius plot}
}

@article{10.1145/1921632.1921638,
author = {Cheng, Hong and Zhou, Yang and Yu, Jeffrey Xu},
title = {Clustering Large Attributed Graphs: A Balance between Structural and Attribute Similarities},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1921632.1921638},
doi = {10.1145/1921632.1921638},
abstract = {Social networks, sensor networks, biological networks, and many other information networks can be modeled as a large graph. Graph vertices represent entities, and graph edges represent their relationships or interactions. In many large graphs, there is usually one or more attributes associated with every graph vertex to describe its properties. In many application domains, graph clustering techniques are very useful for detecting densely connected groups in a large graph as well as for understanding and visualizing a large graph. The goal of graph clustering is to partition vertices in a large graph into different clusters based on various criteria such as vertex connectivity or neighborhood similarity. Many existing graph clustering methods mainly focus on the topological structure for clustering, but largely ignore the vertex properties, which are often heterogenous. In this article, we propose a novel graph clustering algorithm, SA-Cluster, which achieves a good balance between structural and attribute similarities through a unified distance measure. Our method partitions a large graph associated with attributes into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values. An effective method is proposed to automatically learn the degree of contributions of structural similarity and attribute similarity. Theoretical analysis is provided to show that SA-Cluster is converging quickly through iterative cluster refinement. Some optimization techniques on matrix computation are proposed to further improve the efficiency of SA-Cluster on large graphs. Extensive experimental results demonstrate the effectiveness of SA-Cluster through comparisons with the state-of-the-art graph clustering and summarization methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {12},
numpages = {33},
keywords = {structural proximity, attribute similarity, Graph clustering}
}

@article{10.1145/1870096.1870097,
author = {Zhong, Ning and Piatetsky-Shapiro, Gregory and Yao, Yiyu and Yu, Philip S.},
title = {ACM TKDD Special Issue on Knowledge Discovery for Web Intelligence},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870097},
doi = {10.1145/1870096.1870097},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {1},
numpages = {1}
}

@article{10.1145/1870096.1870102,
author = {Liu, Kun and Terzi, Evimaria},
title = {A Framework for Computing the Privacy Scores of Users in Online Social Networks},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870102},
doi = {10.1145/1870096.1870102},
abstract = {A large body of work has been devoted to address corporate-scale privacy concerns related to social networks. Most of this work focuses on how to share social networks owned by organizations without revealing the identities or the sensitive relationships of the users involved. Not much attention has been given to the privacy risk of users posed by their daily information-sharing activities.In this article, we approach the privacy issues raised in online social networks from the individual users’ viewpoint: we propose a framework to compute the privacy score of a user. This score indicates the user’s potential risk caused by his or her participation in the network. Our definition of privacy score satisfies the following intuitive properties: the more sensitive information a user discloses, the higher his or her privacy risk. Also, the more visible the disclosed information becomes in the network, the higher the privacy risk. We develop mathematical models to estimate both sensitivity and visibility of the information. We apply our methods to synthetic and real-world data and demonstrate their efficacy and practical utility.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {6},
numpages = {30},
keywords = {information propagation, Social networks, expectation maximization, item-response theory, maximum-likelihood estimation}
}

@article{10.1145/1870096.1870101,
author = {Sakurai, Yasushi and Faloutsos, Christos and Papadimitriou, Spiros},
title = {Fast Discovery of Group Lag Correlations in Streams},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870101},
doi = {10.1145/1870096.1870101},
abstract = {The study of data streams has received considerable attention in various communities (theory, databases, data mining, networking), due to several important applications, such as network analysis, sensor monitoring, financial data analysis, and moving object tracking. Our goal in this article is to monitor multiple numerical streams and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations and anticorrelations are frequent and very interesting in practice. For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in drinking water may lead to fewer dental cavities some years later. Other lag settings include network analysis, sensor monitoring, financial data analysis, and tracking of moving objects. Such data streams are often correlated or anticorrelated, but with unknown lag.We propose BRAID, a method of detecting lag correlations among data streams. BRAID can handle data streams of semi-infinite length incrementally, quickly, and with small resource consumption. However, BRAID requires space and time quadratic on a number of streams k. We also propose ThinBRAID, which is even faster than BRAID, requiring O(k) space and time per time tick. Our theoretical analysis shows that BRAID/ThinBRAID can estimate lag correlations with little or, often, with no error. Our experiments on real and realistic data show that BRAID and ThinBRAID detect the correct lag perfectly most of the time (the largest relative error was about 1%), while they are significantly faster (up to 40,000 times) than the na\"{\i}ve implementation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {5},
numpages = {43},
keywords = {Time-series, cross-correlation, data streams}
}

@article{10.1145/1870096.1870100,
author = {Plangprasopchok, Anon and Lerman, Kristina},
title = {Modeling Social Annotation: A Bayesian Approach},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870100},
doi = {10.1145/1870096.1870100},
abstract = {Collaborative tagging systems, such as Delicious, CiteULike, and others, allow users to annotate resources, for example, Web pages or scientific papers, with descriptive labels called tags. The social annotations contributed by thousands of users can potentially be used to infer categorical knowledge, classify documents, or recommend new relevant information. Traditional text inference methods do not make the best use of social annotation, since they do not take into account variations in individual users’ perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes the interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which offers a way to automatically estimate these numbers. In particular, the model allows the number of interests and topics to change as suggested by the structure of the data. We evaluate the proposed model in detail on the synthetic and real-world data by comparing its performance to Latent Dirichlet Allocation on the topic extraction task. For the latter evaluation, we apply the model to infer topics of Web resources from social annotations obtained from Delicious in order to discover new resources similar to a specified one. Our empirical results demonstrate that the proposed model is a promising method for exploiting social knowledge contained in user-generated annotations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {4},
numpages = {32},
keywords = {Collaborative tagging, resource discovery, probabilistic model, social annotation, social information processing}
}

@article{10.1145/1870096.1870099,
author = {Bouguessa, Mohamed and Wang, Shengrui and Dumoulin, Benoit},
title = {Discovering Knowledge-Sharing Communities in Question-Answering Forums},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870099},
doi = {10.1145/1870096.1870099},
abstract = {In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users’ activities over one full year.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {3},
numpages = {49},
keywords = {Clustering, mixture models, transaction data}
}

@article{10.1145/1870096.1870098,
author = {Tang, Jie and Yao, Limin and Zhang, Duo and Zhang, Jing},
title = {A Combination Approach to Web User Profiling},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870098},
doi = {10.1145/1870096.1870098},
abstract = {In this article, we study the problem of Web user profiling, which is aimed at finding, extracting, and fusing the “semantic”-based user profile from the Web. Previously, Web user profiling was often undertaken by creating a list of keywords for the user, which is (sometimes even highly) insufficient for main applications. This article formalizes the profiling problem as several subtasks: profile extraction, profile integration, and user interest discovery. We propose a combination approach to deal with the profiling tasks. Specifically, we employ a classification model to identify relevant documents for a user from the Web and propose a Tree-Structured Conditional Random Fields (TCRF) to extract the profile information from the identified documents; we propose a unified probabilistic model to deal with the name ambiguity problem (several users with the same name) when integrating the profile information extracted from different sources; finally, we use a probabilistic topic model to model the extracted user profiles, and construct the user interest model. Experimental results on an online system show that the combination approach to different profiling tasks clearly outperforms several baseline methods. The extracted profiles have been applied to expert finding, an important application on the Web. Experiments show that the accuracy of expert finding can be improved (ranging from +6% to +26% in terms of MAP) by taking advantage of the profiles.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {2},
numpages = {44},
keywords = {information extraction, topic modeling, text mining, name disambiguation, social network, User profiling}
}

@article{10.1145/1839490.1839494,
author = {Becchetti, Luca and Boldi, Paolo and Castillo, Carlos and Gionis, Aristides},
title = {Efficient Algorithms for Large-Scale Local Triangle Counting},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839494},
doi = {10.1145/1839490.1839494},
abstract = {In this article, we study the problem of approximate local triangle counting in large graphs. Namely, given a large graph G=(V,E) we want to estimate as accurately as possible the number of triangles incident to every node v∈ V in the graph. We consider the question both for undirected and directed graphs. The problem of computing the global number of triangles in a graph has been considered before, but to our knowledge this is the first contribution that addresses the problem of approximate local triangle counting with a focus on the efficiency issues arising in massive graphs and that also considers the directed case. The distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications. For example, we show that the measures we compute can help detect the presence of spamming activity in large-scale Web graphs, as well as to provide useful features for content quality assessment in social networks.For computing the local number of triangles (undirected and directed), we propose two approximation algorithms, which are based on the idea of min-wise independent permutations [Broder et al. 1998]. Our algorithms operate in a semi-streaming fashion, using O(|V|) space in main memory and performing O(log |V|) sequential scans over the edges of the graph. The first algorithm we describe in this article also uses O(|E|) space of external memory during computation, while the second algorithm uses only main memory. We present the theoretical analysis as well as experimental results on large graphs, demonstrating the practical efficiency of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {13},
numpages = {28},
keywords = {massive-graph computing, Web computing, Clustering coefficient, social networks}
}

@article{10.1145/1839490.1839491,
author = {Thomas, Lini T. and Valluri, Satyanarayana R. and Karlapalem, Kamalakar},
title = {MARGIN: Maximal Frequent Subgraph Mining},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839491},
doi = {10.1145/1839490.1839491},
abstract = {The exponential number of possible subgraphs makes the problem of frequent subgraph mining a challenge. The set of maximal frequent subgraphs is much smaller to that of the set of frequent subgraphs providing ample scope for pruning. MARGIN is a maximal subgraph mining algorithm that moves among promising nodes of the search space along the “border” of the infrequent and frequent subgraphs. This drastically reduces the number of candidate patterns in the search space. The proof of correctness of the algorithm is presented. Experimental results validate the efficiency and utility of the technique proposed.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {10},
numpages = {42},
keywords = {Graph mining, maximal frequent subgraph mining}
}

@article{10.1145/1839490.1839493,
author = {Chen, Jinlin and Xiao, Keli},
title = {BISC: A Bitmap Itemset Support Counting Approach for Efficient Frequent Itemset Mining},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839493},
doi = {10.1145/1839490.1839493},
abstract = {The performance of a depth-first frequent itemset (FI) miming algorithm is closely related to the total number of recursions. In previous approaches this is mainly decided by the total number of FIs, which results in poor performance when a large number of FIs are involved. To solve this problem, a three-strategy adaptive algorithm, bitmap itemset support counting (BISC), is presented. The core strategy, BISC1, is used in the innermost steps of the recursion. For a database D with only s frequent items, a depth-first approach need up to s levels of recursions to detect all the FIs (up to 2s). BISC1 completely replaces these recursions with a special summation that directly calculates the supports of all the possible 2s candidate itemsets. With BISC1 the run-time is entirely independent of the database after one database scan, and the per-candidate cost is only s. To offset the exponential growth of cost (both time and space) with BISC1 as s increases, a second strategy, BISC2, is introduced to effectively double the acceptable range of s. BISC2 divides an itemset into prefix and suffix and improves the performance by pruning all the itemsets with infrequent prefixes. If the total number of frequent items in D is high, the classic database projection strategy is used. In this case for the first s items a single run of BISC (1 or 2) is applied. For each of the remaining items, a projected database is created and the mining process proceeds recursively. To achieve optimal performance, BISC adaptively decides which strategy to use based on the dataset and minimum support. Experiments show that BISC outperforms previous approaches in all the datasets tested. Even though this does not guarantee that BISC will always perform the best, the result is impressive given the fact that most existing algorithms are only efficient in some types of datasets. The memory usage of BISC is also comparable to those of other algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {12},
numpages = {37},
keywords = {Data mining algorithms, frequent itemset mining, association rule mining}
}

@article{10.1145/1839490.1839492,
author = {Deodhar, Meghana and Ghosh, Joydeep},
title = {SCOAL: A Framework for Simultaneous Co-Clustering and Learning from Complex Data},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839492},
doi = {10.1145/1839490.1839492},
abstract = {For difficult classification or regression problems, practitioners often segment the data into relatively homogeneous groups and then build a predictive model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. In this work, we consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two sets, that is, the data is dyadic in nature. A pivoting operation now results in the dependent variable showing up as entries in a “customer by product” data matrix. We present the Simultaneous CO-clustering And Learning (SCOAL) framework, based on the key idea of interleaving co-clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based co-clustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is typically better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on a variety of datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {11},
numpages = {31},
keywords = {multimodal data, dyadic data, regression, co-clustering, classification, Predictive modeling}
}

@article{10.1145/1839490.1839495,
author = {Zhang, Yin and Zhou, Zhi-Hua},
title = {Multilabel Dimensionality Reduction via Dependence Maximization},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839495},
doi = {10.1145/1839490.1839495},
abstract = {Multilabel learning deals with data associated with multiple labels simultaneously. Like other data mining and machine learning tasks, multilabel learning also suffers from the curse of dimensionality. Dimensionality reduction has been studied for many years, however, multilabel dimensionality reduction remains almost untouched. In this article, we propose a multilabel dimensionality reduction method, MDDM, with two kinds of projection strategies, attempting to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a eigen-decomposition problem which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {14},
numpages = {21},
keywords = {Dimensionality reduction, multilabel learning}
}

@article{10.1145/1839490.1839496,
author = {Cui, Ying and Fern, Xiaoli Z. and Dy, Jennifer G.},
title = {Learning Multiple Nonredundant Clusterings},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1839490.1839496},
doi = {10.1145/1839490.1839496},
abstract = {Real-world applications often involve complex data that can be interpreted in many different ways. When clustering such data, there may exist multiple groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. However, traditional clustering is restricted to finding only one single clustering of the data. In this article, we propose a new clustering paradigm for exploratory data analysis: find all non-redundant clustering solutions of the data, where data points in the same cluster in one solution can belong to different clusters in other partitioning solutions. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to the current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We study the relationship between the two approaches. We also combine our framework with techniques for automatically finding the number of clusters in the different solutions, and study stopping criteria for determining when all meaningful solutions are discovered. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied clustering solutions that are interesting and meaningful.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {15},
numpages = {32},
keywords = {diverse clustering, orthogonalization, Nonredundant clustering, disparate clustering}
}

@article{10.1145/1857947.1857948,
author = {Wang, Wei},
title = {TKDD Special Issue SIGKDD 2009},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1857947.1857948},
doi = {10.1145/1857947.1857948},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {16},
numpages = {1}
}

@article{10.1145/1857947.1857952,
author = {Wu, Mingxi and Jermaine, Chris and Ranka, Sanjay and Song, Xiuyao and Gums, John},
title = {A Model-Agnostic Framework for Fast Spatial Anomaly Detection},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1857947.1857952},
doi = {10.1145/1857947.1857952},
abstract = {Given a spatial dataset placed on an n \texttimes{}n grid, our goal is to find the rectangular regions within which subsets of the dataset exhibit anomalous behavior. We develop algorithms that, given any user-supplied arbitrary likelihood function, conduct a likelihood ratio hypothesis test (LRT) over each rectangular region in the grid, rank all of the rectangles based on the computed LRT statistics, and return the top few most interesting rectangles. To speed this process, we develop methods to prune rectangles without computing their associated LRT statistics.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {20},
numpages = {30},
keywords = {antibiotic resistance, spatial anomaly, Anomaly detection, likelihood ratio test}
}

@article{10.1145/1857947.1857951,
author = {Liu, Chao and Guo, Fan and Faloutsos, Christos},
title = {Bayesian Browsing Model: Exact Inference of Document Relevance from Petabyte-Scale Data},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1857947.1857951},
doi = {10.1145/1857947.1857951},
abstract = {A fundamental challenge in utilizing Web search click data is to infer user-perceived relevance from the search log. Not only is the inference a difficult problem involving statistical reasonings but the bulky size, together with the ever-increasing nature, of the log data imposes extra requirements on scalability. In this paper, we propose the Bayesian Browsing Model (BBM), which performs exact inference of the document relevance, only requires a single pass of the data (i.e., the optimal scalability), and is shown effective.We present two sets of experiments to evaluate the model effectiveness and scalability. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM outperforms the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click log set, spanning a quarter of petabyte, we showcase the scalability of BBM: we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {19},
numpages = {26},
keywords = {click log analysis, Web search, Bayesian models}
}

@article{10.1145/1857947.1857949,
author = {Chen, Ye and Pavlov, Dmitry and Canny, John F.},
title = {Behavioral Targeting: The Art of Scaling Up Simple Algorithms},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1857947.1857949},
doi = {10.1145/1857947.1857949},
abstract = {Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework. With our parallel algorithm and the resulting system, we can build above 450 BT-category models from the entire Yahoo’s user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset.Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in-place feature vector generation algorithm with strict linear-time complexity O(n) regardless of the granularity of sliding target window. (3) An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {17},
numpages = {31},
keywords = {high-performance and terascale computing, behavioral targeting, Distributed data mining, statistical methods, large-scale, generalized linear model, Hadoop, user modeling, nonnegative matrix factorization, grid computing, MapReduce, parallel data mining}
}

@article{10.1145/1857947.1857950,
author = {Mohammed, Noman and Fung, Benjamin C. M. and Hung, Patrick C. K. and Lee, Cheuk-Kwong},
title = {Centralized and Distributed Anonymization for High-Dimensional Healthcare Data},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1857947.1857950},
doi = {10.1145/1857947.1857950},
abstract = {Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients’ privacy. In this article, we study the privacy concerns of sharing patient information between the Hong Kong Red Cross Blood Transfusion Service (BTS) and the public hospitals. We generalize their information and privacy requirements to the problems of centralized anonymization and distributed anonymization, and identify the major challenges that make traditional data anonymization methods not applicable. Furthermore, we propose a new privacy model called LKC-privacy to overcome the challenges and present two anonymization algorithms to achieve LKC-privacy in both the centralized and the distributed scenarios. Experiments on real-life data demonstrate that our anonymization algorithms can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {18},
numpages = {33},
keywords = {Privacy, healthcare, anonymity, classification}
}

@article{10.1145/1754428.1754431,
author = {Ji, Shuiwang and Tang, Lei and Yu, Shipeng and Ye, Jieping},
title = {A Shared-Subspace Learning Framework for Multi-Label Classification},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1754428.1754431},
doi = {10.1145/1754428.1754431},
abstract = {Multi-label problems arise in various domains such as multi-topic document categorization, protein function prediction, and automatic image annotation. One natural way to deal with such problems is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. Since multiple labels share the same input space, and the semantics conveyed by different labels are usually correlated, it is essential to exploit the correlation information contained in different labels. In this paper, we consider a general framework for extracting shared structures in multi-label classification. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is nonconvex. For high-dimensional problems, direct computation of the solution is expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases, thus elucidating their intrinsic relationships. We further show that the proposed framework can be extended to the kernel-induced feature space. We have conducted extensive experiments on multi-topic web page categorization and automatic gene expression pattern image annotation tasks, and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {8},
numpages = {29},
keywords = {shared subspace, least squares loss, kernel methods, gene expression pattern image annotation, web page categorization, Multi-label classification, singular value decomposition}
}

@article{10.1145/1754428.1754429,
author = {Vadera, Sunil},
title = {CSNL: A Cost-Sensitive Non-Linear Decision Tree Algorithm},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1754428.1754429},
doi = {10.1145/1754428.1754429},
abstract = {This article presents a new decision tree learning algorithm called CSNL that induces Cost-Sensitive Non-Linear decision trees. The algorithm is based on the hypothesis that nonlinear decision nodes provide a better basis than axis-parallel decision nodes and utilizes discriminant analysis to construct nonlinear decision trees that take account of costs of misclassification.The performance of the algorithm is evaluated by applying it to seventeen datasets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the datasets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using nonlinear decision nodes.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {6},
numpages = {25},
keywords = {Decision tree learning, cost-sensitive learning}
}

@article{10.1145/1754428.1754430,
author = {Kandylas, Vasileios and Upham, S. Phineas and Ungar, Lyle H.},
title = {Analyzing Knowledge Communities Using Foreground and Background Clusters},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1754428.1754430},
doi = {10.1145/1754428.1754430},
abstract = {Insight into the growth (or shrinkage) of “knowledge communities” of authors that build on each other's work can be gained by studying the evolution over time of clusters of documents. We cluster documents based on the documents they cite in common using the Streemer clustering method, which finds cohesive foreground clusters (the knowledge communities) embedded in a diffuse background. We build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors and use these models to study the drivers of community growth and the predictors of how widely a paper will be cited. We find that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and use narrow vocabulary and that papers that lie on the periphery of a community have the highest impact, while those not in any community have the lowest impact.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {7},
numpages = {35},
keywords = {Text mining, clustering, community evolution, knowledge communities, citation analysis}
}

@article{10.1145/1754428.1754432,
author = {Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco},
title = {Data Mining for Discrimination Discovery},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1754428.1754432},
doi = {10.1145/1754428.1754432},
abstract = {In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Discrimination in credit, mortgage, insurance, labor market, and education has been investigated by researchers in economics and human sciences. With the advent of automatic decision support systems, such as credit scoring systems, the ease of data collection opens several challenges to data analysts for the fight against discrimination. In this article, we introduce the problem of discovering discrimination through data mining in a dataset of historical decision records, taken by humans or by automatic systems. We formalize the processes of direct and indirect discrimination discovery by modelling protected-by-law groups and contexts where discrimination occurs in a classification rule based syntax. Basically, classification rules extracted from the dataset allow for unveiling contexts of unlawful discrimination, where the degree of burden over protected-by-law groups is formalized by an extension of the lift measure of a classification rule. In direct discrimination, the extracted rules can be directly mined in search of discriminatory contexts. In indirect discrimination, the mining process needs some background knowledge as a further input, for example, census data, that combined with the extracted rules might allow for unveiling contexts of discriminatory decisions. A strategy adopted for combining extracted classification rules with background knowledge is called an inference model. In this article, we propose two inference models and provide automatic procedures for their implementation. An empirical assessment of our results is provided on the German credit dataset and on the PKDD Discovery Challenge 1999 financial dataset.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {9},
numpages = {40},
keywords = {classification rules, Discrimination}
}

@article{10.1145/1644873.1644878,
author = {Zaki, Mohammed J. and Carothers, Christopher D. and Szymanski, Boleslaw K.},
title = {VOGUE: A Variable Order Hidden Markov Model with Duration Based on Frequent Sequence Mining},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644878},
doi = {10.1145/1644873.1644878},
abstract = {We present VOGUE, a novel, variable order hidden Markov model with state durations, that combines two separate techniques for modeling complex patterns in sequential data: pattern mining and data modeling. VOGUE relies on a variable gap sequence mining method to extract frequent patterns with different lengths and gaps between elements. It then uses these mined sequences to build a variable order hidden Markov model (HMM), that explicitly models the gaps. The gaps implicitly model the order of the HMM, and they explicitly model the duration of each state. We apply VOGUE to a variety of real sequence data taken from domains such as protein sequence classification, Web usage logs, intrusion detection, and spelling correction. We show that VOGUE has superior classification accuracy compared to regular HMMs, higher-order HMMs, and even special purpose HMMs like HMMER, which is a state-of-the-art method for protein classification. The VOGUE implementation and the datasets used in this article are available as open-source.1},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {5},
numpages = {31},
keywords = {higher-order HMM, sequence mining and modeling, Hidden Markov models, HMM with duration, variable-order HMM}
}

@article{10.1145/1644873.1644875,
author = {Syed, Zeeshan and Stultz, Collin and Kellis, Manolis and Indyk, Piotr and Guttag, John},
title = {Motif Discovery in Physiological Datasets: A Methodology for Inferring Predictive Elements},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644875},
doi = {10.1145/1644873.1644875},
abstract = {In this article, we propose a methodology for identifying predictive physiological patterns in the absence of prior knowledge. We use the principle of conservation to identify activity that consistently precedes an outcome in patients, and describe a two-stage process that allows us to efficiently search for such patterns in large datasets. This involves first transforming continuous physiological signals from patients into symbolic sequences, and then searching for patterns in these reduced representations that are strongly associated with an outcome.Our strategy of identifying conserved activity that is unlikely to have occurred purely by chance in symbolic data is analogous to the discovery of regulatory motifs in genomic datasets. We build upon existing work in this area, generalizing the notion of a regulatory motif and enhancing current techniques to operate robustly on non-genomic data. We also address two significant considerations associated with motif discovery in general: computational efficiency and robustness in the presence of degeneracy and noise. To deal with these issues, we introduce the concept of active regions and new subset-based techniques such as a two-layer Gibbs sampling algorithm. These extensions allow for a framework for information inference, where precursors are identified as approximately conserved activity of arbitrary complexity preceding multiple occurrences of an event.We evaluated our solution on a population of patients who experienced sudden cardiac death and attempted to discover electrocardiographic activity that may be associated with the endpoint of death. To assess the predictive patterns discovered, we compared likelihood scores for motifs in the sudden death population against control populations of normal individuals and those with non-fatal supraventricular arrhythmias. Our results suggest that predictive motif discovery may be able to identify clinically relevant information even in the absence of significant prior knowledge.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {2},
numpages = {23},
keywords = {motifs, knowledge discovery, Gibbs sampling, physiological signals, inference, data mining}
}

@article{10.1145/1644873.1644874,
author = {Koren, Yehuda},
title = {Factor in the Neighbors: Scalable and Accurate Collaborative Filtering},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644874},
doi = {10.1145/1644873.1644874},
abstract = {Recommender systems provide users with personalized suggestions for products or services. These systems often rely on collaborating filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The most common approach to CF is based on neighborhood models, which originate from similarities between products or users. In this work we introduce a new neighborhood model with an improved prediction accuracy. Unlike previous approaches that are based on heuristic similarities, we model neighborhood relations by minimizing a global cost function. Further accuracy improvements are achieved by extending the model to exploit both explicit and implicit feedback by the users. Past models were limited by the need to compute all pairwise similarities between items or users, which grow quadratically with input size. In particular, this limitation vastly complicates adopting user similarity models, due to the typical large number of users. Our new model solves these limitations by factoring the neighborhood model, thus making both item-item and user-user implementations scale linearly with the size of the data. The methods are tested on the Netflix data, with encouraging results.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {1},
numpages = {24},
keywords = {collaborative filtering, Recommender systems, Netflix Prize}
}

@article{10.1145/1644873.1644877,
author = {Plantevit, Marc and Laurent, Anne and Laurent, Dominique and Teisseire, Maguelonne and Choong, Yeow WEI},
title = {Mining Multidimensional and Multilevel Sequential Patterns},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644877},
doi = {10.1145/1644873.1644877},
abstract = {Multidimensional databases have been designed to provide decision makers with the necessary tools to help them understand their data. This framework is different from transactional data as the datasets contain huge volumes of historicized and aggregated data defined over a set of dimensions that can be arranged through multiple levels of granularities. Many tools have been proposed to query the data and navigate through the levels of granularity. However, automatic tools are still missing to mine this type of data in order to discover regular specific patterns. In this article, we present a method for mining sequential patterns from multidimensional databases, at the same time taking advantage of the different dimensions and levels of granularity, which is original compared to existing work. The necessary definitions and algorithms are extended from regular sequential patterns to this particular case. Experiments are reported, showing the significance of this approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {4},
numpages = {37},
keywords = {Sequential patterns, multidimensional databases, hierarchy, multilevel patterns, frequent patterns}
}

@article{10.1145/1644873.1644876,
author = {Webb, Geoffrey I.},
title = {Self-Sufficient Itemsets: An Approach to Screening Potentially Interesting Associations between Items},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644876},
doi = {10.1145/1644873.1644876},
abstract = {Self-sufficient itemsets are those whose frequency cannot be explained solely by the frequency of either their subsets or of their supersets. We argue that itemsets that are not self-sufficient will often be of little interest to the data analyst, as their frequency should be expected once that of the itemsets on which their frequency depends is known. We present tests for statistically sound discovery of self-sufficient itemsets, and computational techniques that allow those tests to be applied as a post-processing step for any itemset discovery algorithm. We also present a measure for assessing the degree of potential interest in an itemset that complements these statistical measures.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {3},
numpages = {20},
keywords = {statistical evaluation, association rules, itemset screening, Association discovery, itemset discovery}
}

@article{10.1145/1631162.1631163,
author = {Mannila, Heikki and Gunopulos, Dimitrios},
title = {ACM TKDD Special Issue ACM SIGKDD 2007 and ACM SIGKDD 2008},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631163},
doi = {10.1145/1631162.1631163},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {15},
numpages = {2}
}

@article{10.1145/1631162.1631165,
author = {Chi, Yun and Song, Xiaodan and Zhou, Dengyong and Hino, Koji and Tseng, Belle L.},
title = {On Evolutionary Spectral Clustering},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631165},
doi = {10.1145/1631162.1631165},
abstract = {Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this article, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k-means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {17},
numpages = {30},
keywords = {preserving cluster quality, temporal smoothness, Evolutionary spectral clustering, preserving cluster membership}
}

@article{10.1145/1631162.1631169,
author = {Kiernan, Jerry and Terzi, Evimaria},
title = {Constructing Comprehensive Summaries of Large Event Sequences},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631169},
doi = {10.1145/1631162.1631169},
abstract = {Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns appearing in a sequence. While interesting, these patterns do not give a comprehensive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this article, we take an alternative approach and build short summaries that describe an entire sequence, and discover local dependencies between event types.We formally define the summarization problem as an optimization problem that balances shortness of the summary with accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {21},
numpages = {31},
keywords = {Event sequences, log mining, summarization}
}

@article{10.1145/1631162.1631164,
author = {Asur, Sitaram and Parthasarathy, Srinivasan and Ucar, Duygu},
title = {An Event-Based Framework for Characterizing the Evolutionary Behavior of Interaction Graphs},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631164},
doi = {10.1145/1631162.1631164},
abstract = {Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use nonoverlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We show how semantic information can be incorporated to reason about community-behavior events. We also demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {16},
numpages = {36},
keywords = {diffusion of innovations, evolutionary analysis, Dynamic interaction networks}
}

@article{10.1145/1631162.1631168,
author = {Bilgic, Mustafa and Getoor, Lise},
title = {Reflect and Correct: A Misclassification Prediction Approach to Active Inference},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631168},
doi = {10.1145/1631162.1631168},
abstract = {Information diffusion, viral marketing, graph-based semi-supervised learning, and collective classification all attempt to model and exploit the relationships among nodes in a network to improve the performance of node labeling algorithms. However, sometimes the advantage of exploiting the relationships can become a disadvantage. Simple models like label propagation and iterative classification can aggravate a misclassification by propagating mistakes in the network, while more complex models that define and optimize a global objective function, such as Markov random fields and graph mincuts, can misclassify a set of nodes jointly. This problem can be mitigated if the classification system is allowed to ask for the correct labels for a few of the nodes during inference. However, determining the optimal set of labels to acquire is intractable under relatively general assumptions, which forces us to resort to approximate and heuristic techniques. We describe three such techniques in this article. The first one is based on directly approximating the value of the objective function of label acquisition and greedily acquiring the label that provides the most improvement. The second technique is a simple technique based on the analogy we draw between viral marketing and label acquisition. Finally, we propose a method, which we refer to as reflect and correct, that can learn and predict when the classification system is likely to make mistakes and suggests acquisitions to correct those mistakes. We empirically show on a variety of synthetic and real-world datasets that the reflect and correct method significantly outperforms the other two techniques, as well as other approaches based on network structural measures such as node degree and network clustering.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {20},
numpages = {32},
keywords = {viral marketing, label acquisition, information diffusion, collective classification, Active inference}
}

@article{10.1145/1631162.1631166,
author = {Fujiwara, Yasuhiro and Sakurai, Yasushi and Kitsuregawa, Masaru},
title = {Fast Likelihood Search for Hidden Markov Models},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631166},
doi = {10.1145/1631162.1631166},
abstract = {Hidden Markov models (HMMs) are receiving considerable attention in various communities and many applications that use HMMs have emerged such as mental task classification, biological analysis, traffic monitoring, and anomaly detection. This article has two goals; The first goal is exact and efficient identification of the model whose state sequence has the highest likelihood for the given query sequence (more precisely, no HMM that actually has a high-probability path for the given sequence is missed by the algorithm), and the second goal is exact and efficient monitoring of streaming data sequences to find the best model. We propose SPIRAL, a fast search method for HMM datasets. SPIRAL is based on three ideas; (1) it clusters states of models to compute approximate likelihood, (2) it uses several granularities and approximates likelihood values in search processing, and (3) it focuses on just the promising likelihood computations by pruning out low-likelihood state sequences. Experiments verify the effectiveness of SPIRAL and show that it is more than 490 times faster than the naive method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {18},
numpages = {37},
keywords = {upper bound, likelihood, Hidden Markov model}
}

@article{10.1145/1631162.1631167,
author = {Zhang, Xiang and Zou, Fei and Wang, Wei},
title = {Efficient Algorithms for Genome-Wide Association Study},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1631162.1631167},
doi = {10.1145/1631162.1631167},
abstract = {Studying the association between quantitative phenotype (such as height or weight) and single nucleotide polymorphisms (SNPs) is an important problem in biology. To understand underlying mechanisms of complex phenotypes, it is often necessary to consider joint genetic effects across multiple SNPs. ANOVA (analysis of variance) test is routinely used in association study. Important findings from studying gene-gene (SNP-pair) interactions are appearing in the literature. However, the number of SNPs can be up to millions. Evaluating joint effects of SNPs is a challenging task even for SNP-pairs. Moreover, with large number of SNPs correlated, permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power, which dramatically increases the computational cost of association study.In this article, we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype. We propose an efficient algorithm, FastANOVA, for performing ANOVA tests on SNP-pairs in a batch mode, which also supports large permutation test. We derive an upper bound of SNP-pair ANOVA test, which can be expressed as the sum of two terms. The first term is based on single-SNP ANOVA test. The second term is based on the SNPs and independent of any phenotype permutation. Furthermore, SNP-pairs can be organized into groups, each of which shares a common upper bound. This allows for maximum reuse of intermediate computation, efficient upper bound estimation, and effective SNP-pair pruning. Consequently, FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones. Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs. The principles used in FastANOVA can be applied to categorical phenotypes and other statistics such as Chi-square test.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {19},
numpages = {28},
keywords = {permutation test, ANOVA test, Association study}
}

@article{10.1145/1552303.1552304,
author = {Torvik, Vetle I. and Smalheiser, Neil R.},
title = {Author Name Disambiguation in MEDLINE},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1552303.1552304},
doi = {10.1145/1552303.1552304},
abstract = {Background: We recently described “Author-ity,” a model for estimating the probability that two articles in MEDLINE, sharing the same author name, were written by the same individual. Features include shared title words, journal name, coauthors, medical subject headings, language, affiliations, and author name features (middle initial, suffix, and prevalence in MEDLINE). Here we test the hypothesis that the Author-ity model will suffice to disambiguate author names for the vast majority of articles in MEDLINE. Methods: Enhancements include: (a) incorporating first names and their variants, email addresses, and correlations between specific last names and affiliation words; (b) new methods of generating large unbiased training sets; (c) new methods for estimating the prior probability; (d) a weighted least squares algorithm for correcting transitivity violations; and (e) a maximum likelihood based agglomerative algorithm for computing clusters of articles that represent inferred author-individuals. Results: Pairwise comparisons were computed for all author names on all 15.3 million articles in MEDLINE (2006 baseline), that share last name and first initial, to create Author-ity 2006, a database that has each name on each article assigned to one of 6.7 million inferred author-individual clusters. Recall is estimated at ∼98.8%. Lumping (putting two different individuals into the same cluster) affects ∼0.5% of clusters, whereas splitting (assigning articles written by the same individual to &gt;1 cluster) affects ∼2% of articles. Impact: The Author-ity model can be applied generally to other bibliographic databases. Author name disambiguation allows information retrieval and data integration to become person-centered, not just document-centered, setting the stage for new data mining and social network tools that will facilitate the analysis of scholarly publishing and collaboration behavior. Availability: The Author-ity 2006 database is available for nonprofit academic research, and can be freely queried via http://arrowsmith.psych.uic.edu.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {11},
numpages = {29},
keywords = {bibliographic databases, Name disambiguation}
}

@article{10.1145/1552303.1552307,
author = {Wan, Li and Ng, Wee Keong and Dang, Xuan Hong and Yu, Philip S. and Zhang, Kuan},
title = {Density-Based Clustering of Data Streams at Multiple Resolutions},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1552303.1552307},
doi = {10.1145/1552303.1552307},
abstract = {In data stream clustering, it is desirable to have algorithms that are able to detect clusters of arbitrary shape, clusters that evolve over time, and clusters with noise. Existing stream data clustering algorithms are generally based on an online-offline approach: The online component captures synopsis information from the data stream (thus, overcoming real-time and memory constraints) and the offline component generates clusters using the stored synopsis. The online-offline approach affects the overall performance of stream data clustering in various ways: the ease of deriving synopsis from streaming data; the complexity of data structure for storing and managing synopsis; and the frequency at which the offline component is used to generate clusters. In this article, we propose an algorithm that (1) computes and updates synopsis information in constant time; (2) allows users to discover clusters at multiple resolutions; (3) determines the right time for users to generate clusters from the synopsis information; (4) generates clusters of higher purity than existing algorithms; and (5) determines the right threshold function for density-based clustering based on the fading model of stream data. To the best of our knowledge, no existing data stream algorithms has all of these features. Experimental results show that our algorithm is able to detect arbitrarily shaped, evolving clusters with high quality.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {14},
numpages = {28},
keywords = {evolving data streams, density based clustering, Data mining algorithms}
}

@article{10.1145/1552303.1552306,
author = {Zhou, Bin and Pei, Jian},
title = {Link Spam Target Detection Using Page Farms},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1552303.1552306},
doi = {10.1145/1552303.1552306},
abstract = {Currently, most popular Web search engines adopt some link-based ranking methods such as PageRank. Driven by the huge potential benefit of improving rankings of Web pages, many tricks have been attempted to boost page rankings. The most common way, which is known as link spam, is to make up some artificially designed link structures. Detecting link spam effectively is a big challenge. In this article, we develop novel and effective detection methods for link spam target pages using page farms. The essential idea is intuitive: whether a page is the beneficiary of link spam is reflected by how it collects its PageRank score. Technically, how a target page collects its PageRank score is modeled by a page farm, which consists of pages contributing a major portion of the PageRank score of the target page. We propose two spamicity measures based on page farms. They can be used as an effective measure to check whether the pages are link spam target pages. An empirical study using a newly available real dataset strongly suggests that our method is effective. It outperforms the state-of-the-art methods like SpamRank and SpamMass in both precision and recall.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {13},
numpages = {38},
keywords = {PageRank, Page Farm, Link Spam}
}

@article{10.1145/1552303.1552305,
author = {Tu, Li and Chen, Yixin},
title = {Stream Data Clustering Based on Grid Density and Attraction},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1552303.1552305},
doi = {10.1145/1552303.1552305},
abstract = {Clustering real-time stream data is an important and challenging problem. Existing algorithms such as CluStream are based on the k-means algorithm. These clustering algorithms have difficulties finding clusters of arbitrary shapes and handling outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this article proposes D-Stream, a framework for clustering stream data using a density-based approach.Our algorithm uses an online component that maps each input data record into a grid and an offline component that computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream and a attraction-based mechanism to accurately generate cluster boundaries.Exploiting the intricate relationships among the decay factor, attraction, data density, and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {12},
numpages = {27},
keywords = {clustering, density-based algorithms, data mining, Stream data}
}

@article{10.1145/1514888.1514890,
author = {Mehler, Andrew and Skiena, Steven},
title = {Expanding Network Communities from Representative Examples},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514890},
doi = {10.1145/1514888.1514890},
abstract = {We present an approach to leverage a small subset of a coherent community within a social network into a much larger, more representative sample. Our problem becomes identifying a small conductance subgraph containing many (but not necessarily all) members of the given seed set. Starting with an initial seed set representing a sample of a community, we seek to discover as much of the full community as possible.We present a general method for network community expansion, demonstrating that our methods work well in expanding communities in real world networks starting from small given seed groups (20 to 400 members). Our approach is marked by incremental expansion from the seeds with retrospective analysis to determine the ultimate boundaries of our community. We demonstrate how to increase the robustness of the general approach through bootstrapping multiple random partitions of the input set into seed and evaluation groups.We go beyond statistical comparisons against gold standards to careful subjective evaluations of our expanded communities. This process explains the causes of most disagreement between our expanded communities and our gold-standards—arguing that our expansion methods provide more reliable communities than can be extracted from reference sources/gazetteers such as Wikipedia.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {7},
numpages = {27},
keywords = {Discrete mathematics, news analysis, social networks, community discovery, graph theory, artificial intelligence}
}

@article{10.1145/1514888.1514892,
author = {Kimura, Masahiro and Saito, Kazumi and Motoda, Hiroshi},
title = {Blocking Links to Minimize Contamination Spread in a Social Network},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514892},
doi = {10.1145/1514888.1514892},
abstract = {We address the problem of minimizing the propagation of undesirable things, such as computer viruses or malicious rumors, by blocking a limited number of links in a network, which is converse to the influence maximization problem in which the most influential nodes for information diffusion is searched in a social network. This minimization problem is more fundamental than the problem of preventing the spread of contamination by removing nodes in a network. We introduce two definitions for the contamination degree of a network, accordingly define two contamination minimization problems, and propose methods for efficiently finding good approximate solutions to these problems on the basis of a naturally greedy strategy. Using large social networks, we experimentally demonstrate that the proposed methods outperform conventional link-removal methods. We also show that unlike the case of blocking a limited number of nodes, the strategy of removing nodes with high out-degrees is not necessarily effective for these problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {9},
numpages = {23},
keywords = {Contamination diffusion, link analysis, social networks}
}

@article{10.1145/1514888.1514891,
author = {Lin, Yu-Ru and Chi, Yun and Zhu, Shenghuo and Sundaram, Hari and Tseng, Belle L.},
title = {Analyzing Communities and Their Evolutions in Dynamic Social Networks},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514891},
doi = {10.1145/1514888.1514891},
abstract = {We discover communities from social network data and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. This novel framework will discover communities and capture their evolution with temporal smoothness given by historic community structures. Our approach relies on formulating the problem in terms of maximum a posteriori (MAP) estimation, where the community structure is estimated both by the observed networked data and by the prior distribution given by historic community structures. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {8},
numpages = {31},
keywords = {soft membership, nonnegative matrix factorization, evolution net, evolution, community net, Community}
}

@article{10.1145/1514888.1514893,
author = {Agichtein, Eugene and Liu, Yandong and Bian, Jiang},
title = {Modeling Information-Seeker Satisfaction in Community Question Answering},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514893},
doi = {10.1145/1514888.1514893},
abstract = {Question Answering Communities such as Naver, Baidu Knows, and Yahoo! Answers have emerged as popular, and often effective, means of information seeking on the web. By posting questions for other participants to answer, information seekers can obtain specific answers to their questions. Users of CQA portals have already contributed millions of questions, and received hundreds of millions of answers from other participants. However, CQA is not always effective: in some cases, a user may obtain a perfect answer within minutes, and in others it may require hours—and sometimes days—until a satisfactory answer is contributed. We investigate the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants. We present a general prediction model, and develop a variety of content, structure, and community-focused features for this task. Our experimental results, obtained from a large-scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satisfaction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction. We also explore personalized models of asker satisfaction, and show that when sufficient interaction history exists, personalization can significantly improve prediction accuracy over a “one-size-fits-all” model. Our models and predictions could be useful for a variety of applications, such as user intent inference, answer ranking, interface design, and query suggestion and routing.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {10},
numpages = {27},
keywords = {Community question answering, information seeker satisfaction}
}

@article{10.1145/1514888.1514889,
author = {Liu, Huan and Salerno, John and Young, Michael and Agrawal, Rakesh and Yu, Philip S.},
title = {Introduction to Special Issue on Social Computing, Behavioral Modeling, and Prediction},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514889},
doi = {10.1145/1514888.1514889},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {6},
numpages = {3}
}

@article{10.1145/1497577.1497582,
author = {Chen, Bee-Chung and Ramakrishnan, Raghu and Shavlik, Jude W. and Tamma, Pradeep},
title = {Bellwether Analysis: Searching for Cost-Effective Query-Defined Predictors in Large Databases},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1497577.1497582},
doi = {10.1145/1497577.1497582},
abstract = {How to mine massive datasets is a challenging problem with great potential value. Motivated by this challenge, much effort has concentrated on developing scalable versions of machine learning algorithms. However, the cost of mining large datasets is not just computational; preparing the datasets into the “right form” so that learning algorithms can be applied is usually costly, due to the human labor that is typically required and a large number of choices in data preparation, which include selecting different subsets of data and aggregating data at different granularities. We make the key observation that, for a number of practically motivated problems, these choices can be defined using database queries and analyzed in an automatic and systematic manner. Specifically, we propose a new class of data-mining problem, called bellwether analysis, in which the goal is to find a few query-defined predictors (e.g., first week sales of Peoria, IL of an item) that can be used to accurately predict the result of a target query (e.g., first year worldwide sales of the item) from a large number of queries that define candidate predictors. To make a prediction for a new item, the data needed to generate such predictors has to be collected (e.g., selling the new item in Peoria, IL for a week and collecting the sales data). A useful predictor is one that has high prediction accuracy and a low data-collection cost. We call such a cost-effective predictor a bellwether.This article introduces bellwether analysis, which integrates database query processing and predictive modeling into a single framework, and provides scalable algorithms for large datasets that cannot fit in main memory. Through a series of extensive experiments, we show that bellwethers do exist in real-world databases, and that our computation techniques achieve good efficiency on large datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {5},
numpages = {49},
keywords = {data cube, predictive models, bellwether, scalable algorithms, Cost-effective prediction, OLAP queries}
}

@article{10.1145/1497577.1497580,
author = {Cerf, Lo\"{\i}c and Besson, J\'{e}r\'{e}my and Robardet, C\'{e}line and Boulicaut, Jean-Fran\c{c}ois},
title = {Closed Patterns Meet <i>n</i>-Ary Relations},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1497577.1497580},
doi = {10.1145/1497577.1497580},
abstract = {Set pattern discovery from binary relations has been extensively studied during the last decade. In particular, many complete and efficient algorithms for frequent closed set mining are now available. Generalizing such a task to n-ary relations (n ≥ 2) appears as a timely challenge. It may be important for many applications, for example, when adding the time dimension to the popular objects \texttimes{} features binary case. The generality of the task (no assumption being made on the relation arity or on the size of its attribute domains) makes it computationally challenging. We introduce an algorithm called Data-Peeler. From an n-ary relation, it extracts all closed n-sets satisfying given piecewise (anti) monotonic constraints. This new class of constraints generalizes both monotonic and antimonotonic constraints. Considering the special case of ternary relations, Data-Peeler outperforms the state-of-the-art algorithms CubeMiner and Trias by orders of magnitude. These good performances must be granted to a new clever enumeration strategy allowing to efficiently enforce the closeness property. The relevance of the extracted closed n-sets is assessed on real-life 3-and 4-ary relations. Beyond natural 3-or 4-ary relations, expanding a relation with an additional attribute can help in enforcing rather abstract constraints such as the robustness with respect to binarization. Furthermore, a collection of closed n-sets is shown to be an excellent starting point to compute a tiling of the dataset.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {3},
numpages = {36},
keywords = {n-ary relations, Closed patterns, constraint properties, constraint-based mining, tiling}
}

@article{10.1145/1497577.1497579,
author = {Dhurandhar, Amit and Dobra, Alin},
title = {Semi-Analytical Method for Analyzing Models and Model Selection Measures Based on Moment Analysis},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1497577.1497579},
doi = {10.1145/1497577.1497579},
abstract = {In this article we propose a moment-based method for studying models and model selection measures. By focusing on the probabilistic space of classifiers induced by the classification algorithm rather than on that of datasets, we obtain efficient characterizations for computing the moments, which is followed by visualization of the resulting formulae that are too complicated for direct interpretation. By assuming the data to be drawn independently and identically distributed from the underlying probability distribution, and by going over the space of all possible datasets, we establish general relationships between the generalization error, hold-out-set error, cross-validation error, and leave-one-out error. We later exemplify the method and the results by studying the behavior of the errors for the naive Bayes classifier.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {2},
numpages = {51},
keywords = {classification, generalization error, Model selection}
}

@article{10.1145/1497577.1497581,
author = {Angiulli, Fabrizio and Fassetti, Fabio},
title = {DOLPHIN: An Efficient Algorithm for Mining Distance-Based Outliers in Very Large Datasets},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1497577.1497581},
doi = {10.1145/1497577.1497581},
abstract = {In this work a novel distance-based outlier detection algorithm, named DOLPHIN, working on disk-resident datasets and whose I/O cost corresponds to the cost of sequentially reading the input dataset file twice, is presented.It is both theoretically and empirically shown that the main memory usage of DOLPHIN amounts to a small fraction of the dataset and that DOLPHIN has linear time performance with respect to the dataset size. DOLPHIN gains efficiency by naturally merging together in a unified schema three strategies, namely the selection policy of objects to be maintained in main memory, usage of pruning rules, and similarity search techniques. Importantly, similarity search is accomplished by the algorithm without the need of preliminarily indexing the whole dataset, as other methods do.The algorithm is simple to implement and it can be used with any type of data, belonging to either metric or nonmetric spaces. Moreover, a modification to the basic method allows DOLPHIN to deal with the scenario in which the available buffer of main memory is smaller than its standard requirements. DOLPHIN has been compared with state-of-the-art distance-based outlier detection algorithms, showing that it is much more efficient.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {4},
numpages = {57},
keywords = {distance-based outliers, outlier detection, Data mining}
}

@article{10.1145/1497577.1497578,
author = {Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Zimek, Arthur},
title = {Clustering High-Dimensional Data: A Survey on Subspace Clustering, Pattern-Based Clustering, and Correlation Clustering},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1497577.1497578},
doi = {10.1145/1497577.1497578},
abstract = {As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition—if at all—with one or two competitors, or even with a so-called “na\"{\i}ve” ad hoc solution, but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this survey, we try to clarify: (i) the different problem definitions related to subspace clustering in general; (ii) the specific difficulties encountered in this field of research; (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches; and (iv) how several prominent solutions tackle different problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {1},
numpages = {58},
keywords = {high-dimensional data, clustering, Survey}
}

@article{10.1145/1460797.1460801,
author = {Zhang, Zhenjie and Lakshmanan, Laks V. S. and Tung, Anthony K. H.},
title = {On Domination Game Analysis for Microeconomic Data Mining},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1460797.1460801},
doi = {10.1145/1460797.1460801},
abstract = {Game theory is a powerful tool for analyzing the competitions among manufacturers in a market. In this article, we present a study on combining game theory and data mining by introducing the concept of domination game analysis. We present a multidimensional market model, where every dimension represents one attribute of a commodity. Every product or customer is represented by a point in the multidimensional space, and a product is said to “dominate” a customer if all of its attributes can satisfy the requirements of the customer. The expected market share of a product is measured by the expected number of the buyers in the customers, all of which are equally likely to buy any product dominating him. A Nash equilibrium is a configuration of the products achieving stable expected market shares for all products. We prove that Nash equilibrium in such a model can be computed in polynomial time if every manufacturer tries to modify its product in a round robin manner. To further improve the efficiency of the computation, we also design two algorithms for the manufacturers to efficiently find their best response to other products in the market.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {18},
numpages = {27},
keywords = {game theory, data mining, Domination game}
}

@article{10.1145/1460797.1460799,
author = {Jiang, Daxin and Pei, Jian},
title = {Mining Frequent Cross-Graph Quasi-Cliques},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1460797.1460799},
doi = {10.1145/1460797.1460799},
abstract = {Joint mining of multiple datasets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in bioinformatics, jointly mining multiple gene expression datasets obtained by different labs or during various biological processes may overcome the heavy noise in the data. Moreover, by joint mining of gene expression data and protein-protein interaction data, we may discover clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways.In this article, we investigate a novel data mining problem, mining frequent cross-graph quasi-cliques, which is generalized from several interesting applications in bioinformatics, cross-market customer segmentation, social network analysis, and Web mining. In a graph, a set of vertices S is a γ-quasi-clique (0 &lt; γ ≤ 1) if each vertex v in S directly connects to at least γ ⋅ (|S| − 1) other vertices in S. Given a set of graphs G1, …, Gn and parameter min_sup (0 &lt; min_sup ≤ 1), a set of vertices S is a frequent cross-graph quasi-clique if S is a γ-quasi-clique in at least min_sup ⋅ n graphs, and there does not exist a proper superset of S having the property.We build a general model, show why the complete set of frequent cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop practical algorithms which exploit several interesting and effective techniques and heuristics to efficaciously mine frequent cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful frequent cross-graph quasi-cliques in bioinformatics. The experimental results also show that our algorithms are efficient and scalable.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {16},
numpages = {42},
keywords = {clique, joint mining, Graph mining, bioinformatics}
}

@article{10.1145/1460797.1460798,
author = {Chuang, Kun-Ta and Chen, Hung-Leng and Chen, Ming-Syan},
title = {Feature-Preserved Sampling over Streaming Data},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1460797.1460798},
doi = {10.1145/1460797.1460798},
abstract = {In this article, we explore a novel sampling model, called feature preserved sampling (FPS) that sequentially generates a high-quality sample over sliding windows. The sampling quality we consider refers to the degree of consistency between the sample proportion and the population proportion of each attribute value in a window. Due to the time-variant nature of real-world datasets, users are more likely to be interested in the most recent data. However, previous works have not been able to generate a high-quality sample over sliding windows that precisely preserves up-to-date population characteristics. Motivated by this shortcoming, we have developed the FPS algorithm, which has several advantages: (1) it sequentially generates a sample from a time-variant data source over sliding windows; (2) the execution time of FPS is linear with respect to the database size; (3) the relative proportional differences between the sample proportions and population proportions of most distinct attribute values are guaranteed to be below a specified error threshold, ε, while the relative proportion differences of the remaining attribute values are as close to ε as possible, which ensures that the generated sample is of high quality; (4) the sample rate is close to the user specified rate so that a high quality sampling result can be obtained without increasing the sample size; (5) by a thorough analytical and empirical study, we prove that FPS has acceptable space overheads, especially when the attribute values have Zipfian distributions, and FPS can also excellently preserve the population proportion of multivariate features in the sample; and (6) FPS can be applied to infinite streams and finite datasets equally, and the generated samples can be used for various applications. Our experiments on both real and synthetic data validate that FPS can effectively obtain a high quality sample of the desired size. In addition, while using the sample generated by FPS in various mining applications, a significant improvement in efficiency can be achieved without compromising the model's precision.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {15},
numpages = {45},
keywords = {sampling, Streaming mining}
}

@article{10.1145/1460797.1460800,
author = {Domeniconi, Carlotta and Al-Razgan, Muna},
title = {Weighted Cluster Ensembles: Methods and Analysis},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1460797.1460800},
doi = {10.1145/1460797.1460800},
abstract = {Cluster ensembles offer a solution to challenges inherent to clustering arising from its ill-posed nature. Cluster ensembles can provide robust and stable solutions by leveraging the consensus across multiple clustering results, while averaging out emergent spurious structures that arise due to the various biases to which each participating algorithm is tuned. In this article, we address the problem of combining multiple weighted clusters that belong to different subspaces of the input space. We leverage the diversity of the input clusterings in order to generate a consensus partition that is superior to the participating ones. Since we are dealing with weighted clusters, our consensus functions make use of the weight vectors associated with the clusters. We demonstrate the effectiveness of our techniques by running experiments with several real datasets, including high-dimensional text data. Furthermore, we investigate in depth the issue of diversity and accuracy for our ensemble methods. Our analysis and experimental results show that the proposed techniques are capable of producing a partition that is as good as or better than the best individual clustering.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {17},
numpages = {40},
keywords = {subspace clustering, Cluster ensembles, data mining, consensus functions, accuracy and diversity measures, text data}
}

@article{10.1145/1409620.1409624,
author = {Vaidya, Jaideep and Clifton, Chris and Kantarcioglu, Murat and Patterson, A. Scott},
title = {Privacy-Preserving Decision Trees over Vertically Partitioned Data},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1409620.1409624},
doi = {10.1145/1409620.1409624},
abstract = {Privacy and security concerns can prevent sharing of data, derailing data-mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. We introduce a generalized privacy-preserving variant of the ID3 algorithm for vertically partitioned data distributed over two or more parties. Along with a proof of security, we discuss what would be necessary to make the protocols completely secure. We also provide experimental results, giving a first demonstration of the practical complexity of secure multiparty computation-based data mining.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {14},
numpages = {27},
keywords = {Decision tree classification, privacy}
}

@article{10.1145/1409620.1409622,
author = {Mangasarian, Olvi L. and Wild, Edward W. and Fung, Glenn M.},
title = {Privacy-Preserving Classification of Vertically Partitioned Data via Random Kernels},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1409620.1409622},
doi = {10.1145/1409620.1409622},
abstract = {We propose a novel privacy-preserving support vector machine (SVM) classifier for a data matrix A whose input feature columns are divided into groups belonging to different entities. Each entity is unwilling to share its group of columns or make it public. Our classifier is based on the concept of a reduced kernel K(A, B′), where B′ is the transpose of a random matrix B. The column blocks of B corresponding to the different entities are privately generated by each entity and never made public. The proposed linear or nonlinear SVM classifier, which is public but does not reveal any of the privately held data, has accuracy comparable to that of an ordinary SVM classifier that uses the entire set of input features directly.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {12},
numpages = {16},
keywords = {vertically partitioned data, Privacy preserving classification, support vector machines}
}

@article{10.1145/1409620.1409621,
author = {Sun, Jimeng and Tao, Dacheng and Papadimitriou, Spiros and Yu, Philip S. and Faloutsos, Christos},
title = {Incremental Tensor Analysis: Theory and Applications},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1409620.1409621},
doi = {10.1145/1409620.1409621},
abstract = {How do we find patterns in author-keyword associations, evolving over time? Or in data cubes (tensors), with product-branchcustomer sales information? And more generally, how to summarize high-order data cubes (tensors)? How to incrementally update these patterns over time? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks, and many more settings. However, they have only two orders (i.e., matrices, like author and keyword in the previous example).We propose to envision such higher-order data as tensors, and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce a general framework, incremental tensor analysis (ITA), which efficiently computes a compact summary for high-order and high-dimensional data, and also reveals the hidden correlations. Three variants of ITA are presented: (1) dynamic tensor analysis (DTA); (2) streaming tensor analysis (STA); and (3) window-based tensor analysis (WTA). In paricular, we explore several fundamental design trade-offs such as space efficiency, computational cost, approximation accuracy, time dependency, and model complexity.We implement all our methods and apply them in several real settings, such as network anomaly detection, multiway latent semantic indexing on citation networks, and correlation study on sensor measurements. Our empirical studies show that the proposed methods are fast and accurate and that they find interesting patterns and outliers on the real datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {11},
numpages = {37},
keywords = {stream mining, Tensor, multilinear algebra}
}

@article{10.1145/1409620.1409623,
author = {Lakshmanan, LAKS V. S. and Ng, Raymond T. and Ramesh, Ganesh},
title = {On Disclosure Risk Analysis of Anonymized Itemsets in the Presence of Prior Knowledge},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1409620.1409623},
doi = {10.1145/1409620.1409623},
abstract = {Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis-a-vis the risk of disclosing proprietary or sensitive information. Among the various methods employed for “sanitizing” the data prior to disclosure, we focus in this article on anonymization, given its widespread use in practice. We do due diligence to the question “just how safe is the anonymized data?” We consider both those scenarios when the hacker has no information and, more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining and address the safety question at two different levels: (i) how likely of being cracked (i.e., re-identified by a hacker), are the identities of individual items and (ii) how likely are sets of items cracked? For capturing the prior knowledge of the hacker, we propose a belief function, which amounts to an educated guess of the frequency of each item. For various classes of belief functions which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of cracks of single items and for itemsets, the probability of cracking the itemsets. While obtaining, exact values for more general situations is computationally hard, we propose a series of heuristics called the O-estimates. They are easy to compute and are shown fairly accurate, justified by empirical results on real benchmark datasets. Based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma. Our recipe operates at two different levels, depending on whether the data owner wants to reason in terms of single items or sets of items (or both). Finally, we present techniques for ascertaining a hacker's knowledge of correlation in terms of co-occurrence of items likely. This information regarding the hacker's knowledge can be incorporated into our framework of disclosure risk analysis and we present experimental results demonstrating how this knowledge affects the heuristic estimates we have developed.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {13},
numpages = {44},
keywords = {frequent itemsets, matching, Disclosure risk, belief function, sampling, anonymization, correlation, hacker, bipartite graphs, prior knowledge}
}

@article{10.1145/1376815.1376818,
author = {Tan, Henry and Hadzic, Fedja and Dillon, Tharam S. and Chang, Elizabeth and Feng, Ling},
title = {Tree Model Guided Candidate Generation for Mining Frequent Subtrees from XML Documents},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1376815.1376818},
doi = {10.1145/1376815.1376818},
abstract = {Due to the inherent flexibilities in both structure and semantics, XML association rules mining faces few challenges, such as: a more complicated hierarchical data structure and ordered data context. Mining frequent patterns from XML documents can be recast as mining frequent tree structures from a database of XML documents. In this study, we model a database of XML documents as a database of rooted labeled ordered subtrees. In particular, we are mainly concerned with mining frequent induced and embedded ordered subtrees. Our main contributions are as follows. We describe our unique embedding list representation of the tree structure, which enables efficient implementation of our Tree Model Guided (TMG) candidate generation. TMG is an optimal, nonredundant enumeration strategy that enumerates all the valid candidates that conform to the structural aspects of the data. We show through a mathematical model and experiments that TMG has better complexity compared to the commonly used join approach. In this article, we propose two algorithms, MB3-Miner and iMB3-Miner. MB3-Miner mines embedded subtrees. iMB3-Miner mines induced and/or embedded subtrees by using the maximum level of embedding constraint. Our experiments with both synthetic and real datasets against two well-known algorithms for mining induced and embedded subtrees, demonstrate the effectiveness and the efficiency of the proposed techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {9},
numpages = {43},
keywords = {Tree mining, TMG, TreeMiner, tree model guided, FREQT}
}

@article{10.1145/1376815.1376816,
author = {Ge, Rong and Ester, Martin and Gao, Byron J. and Hu, Zengjian and Bhattacharya, Binay and Ben-Moshe, Boaz},
title = {Joint Cluster Analysis of Attribute Data and Relationship Data: The Connected <i>k</i>-Center Problem, Algorithms and Applications},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1376815.1376816},
doi = {10.1145/1376815.1376816},
abstract = {Attribute data and relationship data are two principal types of data, representing the intrinsic and extrinsic properties of entities. While attribute data have been the main source of data for cluster analysis, relationship data such as social networks or metabolic networks are becoming increasingly available. It is also common to observe both data types carry complementary information such as in market segmentation and community identification, which calls for a joint cluster analysis of both data types so as to achieve better results. In this article, we introduce the novel Connected k-Center (CkC) problem, a clustering model taking into account attribute data as well as relationship data. We analyze the complexity of the problem and prove its NP-hardness. Therefore, we analyze the approximability of the problem and also present a constant factor approximation algorithm. For the special case of the CkC problem where the relationship data form a tree structure, we propose a dynamic programming method giving an optimal solution in polynomial time. We further present NetScan, a heuristic algorithm that is efficient and effective for large real databases. Our extensive experimental evaluation on real datasets demonstrates the meaningfulness and accuracy of the NetScan results.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {7},
numpages = {35},
keywords = {market segmentation, document clustering, NP-hardness, approximation algorithms, joint cluster analysis, relationship data, community identification, Attribute data}
}

@article{10.1145/1376815.1376817,
author = {Gupta, Gunjan and Ghosh, Joydeep},
title = {Bregman Bubble Clustering: A Robust Framework for Mining Dense Clusters},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1376815.1376817},
doi = {10.1145/1376815.1376817},
abstract = {In classical clustering, each data point is assigned to at least one cluster. However, in many applications only a small subset of the available data is relevant for the problem and the rest needs to be ignored in order to obtain good clusters. Certain nonparametric density-based clustering methods find the most relevant data as multiple dense regions, but such methods are generally limited to low-dimensional data and do not scale well to large, high-dimensional datasets. Also, they use a specific notion of “distance”, typically Euclidean or Mahalanobis distance, which further limits their applicability. On the other hand, the recent One Class Information Bottleneck (OC-IB) method is fast and works on a large class of distortion measures known as Bregman Divergences, but can only find a single dense region. This article presents a broad framework for finding k dense clusters while ignoring the rest of the data. It includes a seeding algorithm that can automatically determine a suitable value for k. When k is forced to 1, our method gives rise to an improved version of OC-IB with optimality guarantees. We provide a generative model that yields the proposed iterative algorithm for finding k dense regions as a special case. Our analysis reveals an interesting and novel connection between the problem of finding dense regions and exponential mixture models; a hard model corresponding to k exponential mixtures with a uniform background results in a set of k dense clusters. The proposed method describes a highly scalable algorithm for finding multiple dense regions that works with any Bregman Divergence, thus extending density based clustering to a variety of non-Euclidean problems not addressable by earlier methods. We present empirical results on three artificial, two microarray and one text dataset to show the relevance and effectiveness of our methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {8},
numpages = {49},
keywords = {exponential family, Density-based clustering, expectation maximization, Bregman divergences, One Class classification}
}

@article{10.1145/1376815.1376819,
author = {Islam, Aminul and Inkpen, Diana},
title = {Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1376815.1376819},
doi = {10.1145/1376815.1376819},
abstract = {We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {10},
numpages = {25},
keywords = {Semantic similarity of words, corpus-based measures, similarity of short texts}
}

@article{10.1145/1342320.1342326,
author = {Hashimoto, Kosuke and Aoki-Kinoshita, Kiyoko Flora and Ueda, Nobuhisa and Kanehisa, Minoru and Mamitsuka, Hiroshi},
title = {A New Efficient Probabilistic Model for Mining Labeled Ordered Trees Applied to Glycobiology},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342326},
doi = {10.1145/1342320.1342326},
abstract = {Mining frequent patterns from large datasets is an important issue in data mining. Recently, complex and unstructured (or semi-structured) datasets have appeared as targets for major data mining applications, including text mining, web mining and bioinformatics. Our work focuses on labeled ordered trees, which are typically semi-structured datasets. In bioinformatics, carbohydrate sugar chains, or glycans, can be modeled as labeled ordered trees. Glycans are the third major class of biomolecules, having important roles in signaling and recognition. For mining labeled ordered trees, we propose a new probabilistic model and its efficient learning scheme which significantly improves the time and space complexity of an existing probabilistic model for labeled ordered trees. We evaluated the performance of the proposed model, comparing it with those of other probabilistic models, using synthetic as well as real datasets from glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results on real data from a variety of biological viewpoints, verifying known facts in glycobiology.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {6},
numpages = {30},
keywords = {maximum likelihood, Expectation-maximization, probabilistic models, labeled ordered trees}
}

@article{10.1145/1342320.1342323,
author = {Sahay, Saurav and Mukherjea, Sougata and Agichtein, Eugene and Garcia, Ernest V. and Navathe, Shamkant B. and Ram, Ashwin},
title = {Discovering Semantic Biomedical Relations Utilizing the Web},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342323},
doi = {10.1145/1342320.1342323},
abstract = {To realize the vision of a Semantic Web for Life Sciences, discovering relations between resources is essential. It is very difficult to automatically extract relations from Web pages expressed in natural language formats. On the other hand, because of the explosive growth of information, it is difficult to manually extract the relations. In this paper we present techniques to automatically discover relations between biomedical resources from the Web. For this purpose we retrieve relevant information from Web Search engines and Pubmed database using various lexico-syntactic patterns as queries over SOAP web services. The patterns are initially handcrafted but can be progressively learnt. The extracted relations can be used to construct and augment ontologies and knowledge bases. Experiments are presented for general biomedical relation discovery and domain specific search to show the usefulness of our technique.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {3},
numpages = {15},
keywords = {Ontology construction, relation identification}
}

@article{10.1145/1342320.1342322,
author = {Jin, Ying and Murali, T. M. and Ramakrishnan, Naren},
title = {Compositional Mining of Multirelational Biological Datasets},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342322},
doi = {10.1145/1342320.1342322},
abstract = {High-throughput biological screens are yielding ever-growing streams of information about multiple aspects of cellular activity. As more and more categories of datasets come online, there is a corresponding multitude of ways in which inferences can be chained across them, motivating the need for compositional data mining algorithms. In this article, we argue that such compositional data mining can be effectively realized by functionally cascading redescription mining and biclustering algorithms as primitives. Both these primitives mirror shifts of vocabulary that can be composed in arbitrary ways to create rich chains of inferences. Given a relational database and its schema, we show how the schema can be automatically compiled into a compositional data mining program, and how different domains in the schema can be related through logical sequences of biclustering and redescription invocations. This feature allows us to rapidly prototype new data mining applications, yielding greater understanding of scientific datasets. We describe two applications of compositional data mining: (i) matching terms across categories of the Gene Ontology and (ii) understanding the molecular mechanisms underlying stress response in human cells.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {2},
numpages = {35},
keywords = {redescription mining, bioinformatics, Biclustering, inductive logic programming, compositional data mining}
}

@article{10.1145/1342320.1342321,
author = {Zaki, Mohammed J. and Karypis, George and Yang, Jiong and Wang, Wei},
title = {Introduction to Special Issue on Bioinformatics},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342321},
doi = {10.1145/1342320.1342321},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {1},
numpages = {1}
}

@article{10.1145/1342320.1342325,
author = {Lu, Yijuan and Tian, Qi and Neary, Jennifer and Liu, Feng and Wang, Yufeng},
title = {Adaptive Discriminant Analysis for Microarray-Based Classification},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342325},
doi = {10.1145/1342320.1342325},
abstract = {Microarray technology has generated enormous amounts of high-dimensional gene expression data, providing a unique platform for exploring gene regulatory networks. However, the curse of dimensionality plagues effort to analyze these high throughput data. Linear Discriminant Analysis (LDA) and Biased Discriminant Analysis (BDA) are two popular techniques for dimension reduction, which pay attention to different roles of the positive and negative samples in finding discriminating subspace. However, the drawbacks of these two methods are obvious: LDA has limited efficiency in classifying sample data from subclasses with different distributions, and BDA does not account for the underlying distribution of negative samples.In this paper, we propose a novel dimension reduction technique for microarray analysis: Adaptive Discriminant Analysis (ADA), which effectively exploits favorable attributes of both BDA and LDA and avoids their unfavorable ones. ADA can find a good discriminative subspace with adaptation to different sample distributions. It not only alleviates the problem of high dimensionality, but also enhances the classification performance in the subspace with na\"{\i}ve Bayes classifier. To learn the best model fitting the real scenario, boosted Adaptive Discriminant Analysis is further proposed. Extensive experiments on the yeast cell cycle regulation data set, and the expression data of the red blood cell cycle in malaria parasite Plasmodium falciparum demonstrate the superior performance of ADA and boosted ADA. We also present some putative genes of specific functional classes predicted by boosted ADA. Their potential functionality is confirmed by independent predictions based on Gene Ontology, demonstrating that ADA and boosted ADA are effective dimension reduction methods for microarray-based classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {5},
numpages = {20},
keywords = {BDA, boosted ADA, microarray, ADA, LDA, dimension reduction}
}

@article{10.1145/1342320.1342324,
author = {Ye, Jieping and Chen, Jianhui and Janardan, Ravi and Kumar, Sudhir},
title = {Developmental Stage Annotation of Drosophila Gene Expression Pattern Images via an Entire Solution Path for LDA},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1342320.1342324},
doi = {10.1145/1342320.1342324},
abstract = {Gene expression in a developing embryo occurs in particular cells (spatial patterns) in a time-specific manner (temporal patterns), which leads to the differentiation of cell fates. Images of a Drosophila melanogaster embryo at a given developmental stage, showing a particular gene expression pattern revealed by a gene-specific probe, can be compared for spatial overlaps. The comparison is fundamentally important to formulating and testing gene interaction hypotheses. Expression pattern comparison is most biologically meaningful when images from a similar time point (developmental stage) are compared. In this paper, we present LdaPath, a novel formulation of Linear Discriminant Analysis (LDA) for automatic developmental stage range classification. It employs multivariate linear regression with the L1-norm penalty controlled by a regularization parameter for feature extraction and visualization. LdaPath computes an entire solution path for all values of regularization parameter with essentially the same computational cost as fitting one LDA model. Thus, it facilitates efficient model selection. It is based on the equivalence relationship between LDA and the least squares method for multiclass classifications. This equivalence relationship is established under a mild condition, which we show empirically to hold for many high-dimensional datasets, such as expression pattern images. Our experiments on a collection of 2705 expression pattern images show the effectiveness of the proposed algorithm. Results also show that the LDA model resulting from LdaPath is sparse, and irrelevant features may be removed. Thus, LdaPath provides a general framework for simultaneous feature selection and feature extraction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {4},
numpages = {21},
keywords = {linear regression, Gene expression pattern image, linear discriminant analysis, dimensionality reduction}
}

@article{10.1145/1324172.1324174,
author = {Cormode, Graham and Korn, Flip and Muthukrishnan, S. and Srivastava, Divesh},
title = {Finding Hierarchical Heavy Hitters in Streaming Data},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1324172.1324174},
doi = {10.1145/1324172.1324174},
abstract = {Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (“heavy hitters”) corresponding to hierarchically discounted frequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input.In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a “child” node can have multiple “parent” nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions.We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {2},
numpages = {48},
keywords = {approximation algorithms, Data mining, network data analysis}
}

@article{10.1145/1324172.1324176,
author = {Halkidi, M. and Gunopulos, D. and Vazirgiannis, M. and Kumar, N. and Domeniconi, C.},
title = {A Clustering Framework Based on Subjective and Objective Validity Criteria},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1324172.1324176},
doi = {10.1145/1324172.1324176},
abstract = {Clustering, as an unsupervised learning process is a challenging problem, especially in cases of high-dimensional datasets. Clustering result quality can benefit from user constraints and objective validity assessment. In this article, we propose a semisupervised framework for learning the weighted Euclidean subspace, where the best clustering can be achieved. Our approach capitalizes on: (i) user constraints; and (ii) the quality of intermediate clustering results in terms of their structural properties. The proposed framework uses the clustering algorithm and the validity measure as its parameters. We develop and discuss algorithms for learning and tuning the weights of contributing dimensions and defining the “best” clustering obtained by satisfying user constraints. Experimental results on benchmark datasets demonstrate the superiority of the proposed approach in terms of improved clustering accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {4},
numpages = {25},
keywords = {similarity measure learning, space learning, Semisupervised learning, cluster validity, data mining}
}

@article{10.1145/1324172.1324175,
author = {Somaiya, Manas and Jermaine, Christopher and Ranka, Sanjay},
title = {Learning Correlations Using the Mixture-of-Subsets Model},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1324172.1324175},
doi = {10.1145/1324172.1324175},
abstract = {Using a mixture of random variables to model data is a tried-and-tested method common in data mining, machine learning, and statistics. By using mixture modeling it is often possible to accurately model even complex, multimodal data via very simple components. However, the classical mixture model assumes that a data point is generated by a single component in the model. A lot of datasets can be modeled closer to the underlying reality if we drop this restriction. We propose a probabilistic framework, the mixture-of-subsets (MOS) model, by making two fundamental changes to the classical mixture model. First, we allow a data point to be generated by a set of components, rather than just a single component. Next, we limit the number of data attributes that each component can influence. We also propose an EM framework to learn the MOS model from a dataset, and experimentally evaluate it on real, high-dimensional datasets. Our results show that the MOS model learned from the data represents the underlying nature of the data accurately.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {3},
numpages = {42},
keywords = {EM algorithm, high-dimensional data, Mixture modeling}
}

@article{10.1145/1324172.1324173,
author = {Tang, Lei and Liu, Huan and Zhang, Jianping and Agarwal, Nitin and Salerno, John J.},
title = {Topic Taxonomy Adaptation for Group Profiling},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/1324172.1324173},
doi = {10.1145/1324172.1324173},
abstract = {A topic taxonomy is an effective representation that describes salient features of virtual groups or online communities. A topic taxonomy consists of topic nodes. Each internal node is defined by its vertical path (i.e., ancestor and child nodes) and its horizonal list of attributes (or terms). In a text-dominant environment, a topic taxonomy can be used to flexibly describe a group's interests with varying granularity. However, the stagnant nature of a taxonomy may fail to timely capture the dynamic change of a group's interest. This article addresses the problem of how to adapt a topic taxonomy to the accumulated data that reflects the change of a group's interest to achieve dynamic group profiling. We first discuss the issues related to topic taxonomy. We next formulate taxonomy adaptation as an optimization problem to find the taxonomy that best fits the data. We then present a viable algorithm that can efficiently accomplish taxonomy adaptation. We conduct extensive experiments to evaluate our approach's efficacy for group profiling, compare the approach with some alternatives, and study its performance for dynamic group profiling. While pointing out various applications of taxonomy adaption, we suggest some future work that can take advantage of burgeoning Web 2.0 services for online targeted marketing, counterterrorism in connecting dots, and community tracking.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {1},
numpages = {28},
keywords = {Topic taxonomy, text hierarchical classification, taxonomy adjustment, group interest, dynamic profiling}
}

@article{10.1145/1297332.1297333,
author = {Bayardop, Roberto and Bennett, Kristin P. and Das, Gautam and Gunopulos, Dimitrios and Gehrke, Johannes},
title = {Introduction to Special Issue ACM SIGKDD 2006},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297333},
doi = {10.1145/1297332.1297333},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {9–es},
numpages = {1}
}

@article{10.1145/1297332.1297336,
author = {Koren, Yehuda and North, Stephen C. and Volinsky, Chris},
title = {Measuring and Extracting Proximity Graphs in Networks},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297336},
doi = {10.1145/1297332.1297336},
abstract = {Measuring distance or some other form of proximity between objects is a standard data mining tool. Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks. We propose a new way of measuring and extracting proximity in networks called “cycle-free effective conductance” (CFEC). Importantly, the measured proximity is accompanied with a proximity subgraph which allows assessing and understanding measured values. Our proximity calculation can handle more than two endpoints, directed edges, is statistically well behaved, and produces an effectiveness score for the computed subgraphs. We provide an efficient algorithm to measure and extract proximity. Also, we report experimental results and show examples for four large network datasets: a telecommunications calling graph, the IMDB actors graph, an academic coauthorship network, and a movie recommendation system.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {12–es},
numpages = {30},
keywords = {proximity, escape probability, Connection subgraph, graph mining, proximity subgraph, cycle-free escape probability, random walk}
}

@article{10.1145/1297332.1297338,
author = {Gionis, Aristides and Mannila, Heikki and Mielik\"{a}inen, Taneli and Tsaparas, Panayiotis},
title = {Assessing Data Mining Results via Swap Randomization},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297338},
doi = {10.1145/1297332.1297338},
abstract = {The problem of assessing the significance of data mining results on high-dimensional 0--1 datasets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by standard statistical tests such as chi-square, or other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are difficult to apply to sets of patterns or other complex results of data mining algorithms. In this article, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins as the given dataset, computing the results of interest on the randomized instances and comparing them to the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and spectral analysis. To generate random datasets with given margins, we use variations of a Markov chain approach which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is expected, given the row and column margins of the datasets, while for other datasets the discovered structure conveys information that is not captured by the margin counts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {14–es},
numpages = {32},
keywords = {swaps, 0--1 data, Significance testing, randomization tests}
}

@article{10.1145/1297332.1297337,
author = {Ihler, Alexander and Hutchins, Jon and Smyth, Padhraic},
title = {Learning to Detect Events with Markov-Modulated Poisson Processes},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297337},
doi = {10.1145/1297332.1297337},
abstract = {Time-series of count data occur in many different contexts, including Internet navigation logs, freeway traffic monitoring, and security logs associated with buildings. In this article we describe a framework for detecting anomalous events in such data using an unsupervised learning approach. Normal periodic behavior is modeled via a time-varying Poisson process model, which in turn is modulated by a hidden Markov process that accounts for bursty events. We outline a Bayesian framework for learning the parameters of this model from count time-series. Two large real-world datasets of time-series counts are used as testbeds to validate the approach, consisting of freeway traffic data and logs of people entering and exiting a building. We show that the proposed model is significantly more accurate at detecting known events than a more traditional threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and to make inferences about different aspects of events such as number of vehicles or people involved. The results indicate that the Markov-modulated Poisson framework provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {13–es},
numpages = {23},
keywords = {Event detection, Markov modulated, Poisson}
}

@article{10.1145/1297332.1297335,
author = {Mei, Qiaozhu and Xin, Dong and Cheng, Hong and Han, Jiawei and Zhai, Chengxiang},
title = {Semantic Annotation of Frequent Patterns},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297335},
doi = {10.1145/1297332.1297335},
abstract = {Using frequent patterns to analyze data has been one of the fundamental approaches in many data mining applications. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step—interpreting the discovered frequent patterns. Although the compression and summarization of frequent patterns has been studied in some recent work, the proposed techniques there can only annotate a frequent pattern with nonsemantical information (e.g., support), which provides only limited help for a user to understand the patterns.In this article, we study the novel problem of generating semantic annotations for frequent patterns. The goal is to discover the hidden meanings of a frequent pattern by annotating it with in-depth, concise, and structured information. We propose a general approach to generate such an annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach can well incorporate the user's prior knowledge, and has potentially many applications, such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {11–es},
numpages = {30},
keywords = {pattern context, pattern semantic analysis, pattern annotation, Frequent pattern}
}

@article{10.1145/1297332.1297334,
author = {B\"{o}hm, Christian and Faloutsos, Christos and Pan, Jia-Yu and Plant, Claudia},
title = {RIC: Parameter-Free Noise-Robust Clustering},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/1297332.1297334},
doi = {10.1145/1297332.1297334},
abstract = {How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? As most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and are sensitive to noise. In this article, we propose a robust framework for determining a natural clustering of a given dataset, based on the minimum description length (MDL) principle. The proposed framework, robust information-theoretic clustering (RIC), is orthogonal to any known clustering algorithm: Given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods. In an extension, we propose a fully automatic stand-alone clustering method and efficiency improvements. RIC scales well with the dataset size. Extensive experiments on synthetic and real-world datasets validate the proposed RIC framework.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
pages = {10–es},
numpages = {29},
keywords = {noise robustness, Clustering, data summarization, parameter-free data mining}
}

@article{10.1145/1267066.1267069,
author = {Huang, Jen-Wei and Dai, Bi-Ru and Chen, Ming-Syan},
title = {Twain: Two-End Association Miner with Precise Frequent Exhibition Periods},
year = {2007},
issue_date = {August 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1267066.1267069},
doi = {10.1145/1267066.1267069},
abstract = {We investigate the general model of mining associations in a temporal database, where the exhibition periods of items are allowed to be different from one to another. The database is divided into partitions according to the time granularity imposed. Such temporal association rules allow us to observe short-term but interesting patterns that are absent when the whole range of the database is evaluated altogether. Prior work may omit some temporal association rules and thus have limited practicability. To remedy this and to give more precise frequent exhibition periods of frequent temporal itemsets, we devise an efficient algorithm Twain (standing for TWo end AssocIation miNer.) Twain not only generates frequent patterns with more precise frequent exhibition periods, but also discovers more interesting frequent patterns. Twain employs Start time and End time of each item to provide precise frequent exhibition period while progressively handling itemsets from one partition to another. Along with one scan of the database, Twain can generate frequent 2-itemsets directly according to the cumulative filtering threshold. Then, Twain adopts the scan reduction technique to generate all frequent k-itemsets (k &gt; 2) from the generated frequent 2-itemsets. Theoretical properties of Twain are derived as well in this article. The experimental results show that Twain outperforms the prior works in the quality of frequent patterns, execution time, I/O cost, CPU overhead and scalability.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
pages = {8–es},
numpages = {33},
keywords = {temporal, Association}
}

@article{10.1145/1267066.1267068,
author = {Zhang, Minghua and Kao, Ben and Cheung, David W. and Yip, Kevin Y.},
title = {Mining Periodic Patterns with Gap Requirement from Sequences},
year = {2007},
issue_date = {August 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1267066.1267068},
doi = {10.1145/1267066.1267068},
abstract = {We study a problem of mining frequently occurring periodic patterns with a gap requirement from sequences. Given a character sequence S of length L and a pattern P of length l, we consider P a frequently occurring pattern in S if the probability of observing P given a randomly picked length-l subsequence of S exceeds a certain threshold. In many applications, particularly those related to bioinformatics, interesting patterns are periodic with a gap requirement. That is to say, the characters in P should match subsequences of S in such a way that the matching characters in S are separated by gaps of more or less the same size. We show the complexity of the mining problem and discuss why traditional mining algorithms are computationally infeasible. We propose practical algorithms for solving the problem and study their characteristics. We also present a case study in which we apply our algorithms on some DNA sequences. We discuss some interesting patterns obtained from the case study.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
pages = {7–es},
numpages = {39},
keywords = {periodic pattern, gap requirement, Sequence mining}
}

@article{10.1145/1267066.1267067,
author = {Loh, Wei-Yin and Chen, Chien-Wei and Zheng, Wei},
title = {Extrapolation Errors in Linear Model Trees},
year = {2007},
issue_date = {August 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1267066.1267067},
doi = {10.1145/1267066.1267067},
abstract = {Prediction errors from a linear model tend to be larger when extrapolation is involved, particularly when the model is wrong. This article considers the problem of extrapolation and interpolation errors when a linear model tree is used for prediction. It proposes several ways to curtail the size of the errors, and uses a large collection of real datasets to demonstrate that the solutions are effective in reducing the average mean squared prediction error. The article also provides a proof that, if a linear model is correct, the proposed solutions have no undesirable effects as the training sample size tends to infinity.},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
pages = {6–es},
numpages = {17},
keywords = {Decision tree, regression, statistics, prediction}
}

@article{10.1145/1217299.1217300,
author = {Han, Jiawei},
title = {Introduction},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1217299.1217300},
doi = {10.1145/1217299.1217300},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
pages = {1–es},
numpages = {2}
}

@article{10.1145/1217299.1217301,
author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
title = {Graph Evolution: Densification and Shrinking Diameters},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1217299.1217301},
doi = {10.1145/1217299.1217301},
abstract = {How do real graphs evolve over time? What are normal growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).Existing graph generation models do not exhibit these types of behavior even at a qualitative level. We provide a new graph generator, based on a forest fire spreading process that has a simple, intuitive justification, requires very few parameters (like the flammability of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.We also notice that the forest fire model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point.Last, we analyze the connection between the temporal evolution of the degree distribution and densification of a graph. We find that the two are fundamentally related. We also observe that real networks exhibit this type of relation between densification and the degree distribution.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
pages = {2–es},
numpages = {41},
keywords = {small-world phenomena, Densification power laws, heavy-tailed distributions, graph mining, graph generators}
}

@article{10.1145/1217299.1217302,
author = {Machanavajjhala, Ashwin and Kifer, Daniel and Gehrke, Johannes and Venkitasubramaniam, Muthuramakrishnan},
title = {<i>L</i>-Diversity: Privacy beyond <i>k</i>-Anonymity},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1217299.1217302},
doi = {10.1145/1217299.1217302},
abstract = {Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called k-anonymity has gained popularity. In a k-anonymized dataset, each record is indistinguishable from at least k − 1 other records with respect to certain identifying attributes.In this article, we show using two simple attacks that a k-anonymized dataset has some subtle but severe privacy problems. First, an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. This is a known problem. Second, attackers often have background knowledge, and we show that k-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks, and we propose a novel and powerful privacy criterion called ℓ-diversity that can defend against such attacks. In addition to building a formal foundation for ℓ-diversity, we show in an experimental evaluation that ℓ-diversity is practical and can be implemented efficiently.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
pages = {3–es},
numpages = {52},
keywords = {Data privacy, ℓ-diversity, privacy-preserving data publishing, k-anonymity}
}

@article{10.1145/1217299.1217304,
author = {Bhattacharya, Indrajit and Getoor, Lise},
title = {Collective Entity Resolution in Relational Data},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1217299.1217304},
doi = {10.1145/1217299.1217304},
abstract = {Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
pages = {5–es},
numpages = {36},
keywords = {data cleaning, graph clustering, record linkage, Entity resolution}
}

@article{10.1145/1217299.1217303,
author = {Gionis, Aristides and Mannila, Heikki and Tsaparas, Panayiotis},
title = {Clustering Aggregation},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1217299.1217303},
doi = {10.1145/1217299.1217303},
abstract = {We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function.In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering. Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
pages = {4–es},
numpages = {30},
keywords = {clustering categorical data, Data clustering, correlation clustering, clustering aggregation}
}

