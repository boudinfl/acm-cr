@article{10.1145/3408889,
author = {Eiras-Franco, Carlos and Mart\'{\i}nez-Rego, David and Kanthan, Leslie and Pi\~{n}eiro, C\'{e}sar and Bahamonde, Antonio and Guijarro-Berdi\~{n}as, Bertha and Alonso-Betanzos, Amparo},
title = {Fast Distributed <i>k</i>NN Graph Construction Using Auto-Tuned Locality-Sensitive Hashing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3408889},
doi = {10.1145/3408889},
abstract = {The k-nearest-neighbors (kNN) graph is a popular and powerful data structure that is used in various areas of Data Science, but the high computational cost of obtaining it hinders its use on large datasets. Approximate solutions have been described in the literature using diverse techniques, among which Locality-sensitive Hashing (LSH) is a promising alternative that still has unsolved problems. We present Variable Resolution Locality-sensitive Hashing, an algorithm that addresses these problems to obtain an approximate kNN graph at a significantly reduced computational cost. Its usability is greatly enhanced by its capacity to automatically find adequate hyperparameter values, a common hindrance to LSH-based methods. Moreover, we provide an implementation in the distributed computing framework Apache Spark that takes advantage of the structure of the algorithm to efficiently distribute the computational load across multiple machines, enabling practitioners to apply this solution to very large datasets. Experimental results show that our method offers significant improvements over the state-of-the-art in the field and shows very good scalability as more machines are added to the computation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {18},
keywords = {automl, locality-sensitive hashing, k nearest neighbors, scalability, Big data}
}

@article{10.1145/3405790,
author = {Jan, Zohaib Md. and Verma, Brijesh},
title = {Multiple Elimination of Base Classifiers in Ensemble Learning Using Accuracy and Diversity Comparisons},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3405790},
doi = {10.1145/3405790},
abstract = {When generating ensemble classifiers, selecting the best set of classifiers from the base classifier pool is considered a combinatorial problem and an efficient classifier selection methodology must be utilized. Different researchers have used different strategies such as evolutionary algorithms, genetic algorithms, rule-based algorithms, simulated annealing, and so forth to select the best set of classifiers that can maximize overall ensemble classifier accuracy. In this article, we present a novel classifier selection approach to generate an ensemble classifier. The proposed approach selects classifiers in multiple rounds of elimination. In each round, a classifier is given a chance to be selected to become a part of the ensemble, if it can contribute to the overall ensemble accuracy or diversity; otherwise, it is put back into the pool. Each classifier is given multiple opportunities to participate in rounds of selection and they are discarded only if they have no remaining chances. The process is repeated until no classifier in the pool has any chance left to participate in the round of selection. To test the efficacy of the proposed approach, 13 benchmark datasets from the UCI repository are used and results are compared with single classifier models and existing state-of-the-art ensemble classifier approaches. Statistical significance testing is conducted to further validate the results, and an analysis is provided.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {67},
numpages = {17},
keywords = {clustering, classification, Ensemble classifiers, multiple classifier systems}
}

@article{10.1145/3402818,
author = {Levy, Sharon and Xiong, Wenhan and Belding, Elizabeth and Wang, William Yang},
title = {SafeRoute: Learning to Navigate Streets Safely in an Urban Environment},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3402818},
doi = {10.1145/3402818},
abstract = {Recent studies show that 85% of women have changed their traveled routes to avoid harassment and assault. Despite this, current mapping tools do not empower users with information to take charge of their personal safety. We propose SafeRoute, a novel solution to the problem of navigating cities and avoiding street harassment and crime. Unlike other street navigation applications, SafeRoute introduces a new type of path generation via deep reinforcement learning. This enables us to successfully optimize for multi-criteria path-finding and incorporate representation learning within our framework. Our agent learns to pick favorable streets to create a safe and short path with a reward function that incorporates safety and efficiency. Given access to recent crime reports in many urban cities, we train our model for experiments in Boston, New York, and San Francisco. We test our model on areas of these cities, specifically the populated downtown regions with high foot traffic. We evaluate SafeRoute and successfully improve over state-of-the-art methods by up to 17% in local average distance from crimes while decreasing path length by up to 7%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {66},
numpages = {17},
keywords = {Safe routing, multi-preference routing, deep reinforcement learning}
}

@article{10.1145/3408890,
author = {Banerjee, Debopriyo and Rao, Krothapalli Sreenivasa and Sural, Shamik and Ganguly, Niloy},
title = {BOXREC: Recommending a Box of Preferred Outfits in Online Shopping},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3408890},
doi = {10.1145/3408890},
abstract = {Fashionable outfits are generally created by expert fashionistas, who use their creativity and in-depth understanding of fashion to make attractive outfits. Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user’s fashion taste. However, none of these consider a user’s preference of price range for individual clothing types or an overall shopping budget for a set of items. In this article, we propose a box recommendation framework—BOXREC—which at first collects user preferences across different item types (namely, top-wear, bottom-wear, and foot-wear) including price range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price ranges), creates all possible combinations of three preferred items (belonging to distinct item types), and verifies each combination using an outfit scoring framework—BOXREC-OSF. Finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget. We create an extensively annotated dataset of male fashion items across various types and categories (each having associated price) and a manually annotated positive and negative formal as well as casual outfit dataset. We consider a set of recently published pairwise compatibility prediction methods as competitors of BOXREC-OSF. Empirical results show superior performance of BOXREC-OSF over the baseline methods. We found encouraging results by performing both quantitative and qualitative analysis of the recommendations produced by BOXREC. Finally, based on user feedback corresponding to the recommendations given by BOXREC, we show that disliked or unpopular items can be a part of attractive outfits.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {69},
numpages = {28},
keywords = {pagination, E-commerce, outfit compatibility, fashion, neural networks, optimization, composite recommendation}
}

@article{10.1145/3406540,
author = {Li, Junwei and Wu, Le and Hong, Richang and Zhang, Kun and Ge, Yong and Li, Yan},
title = {A Joint Neural Model for User Behavior Prediction on Social Networking Platforms},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3406540},
doi = {10.1145/3406540},
abstract = {Social networking services provide platforms for users to perform two kinds of behaviors: consumption behavior (e.g., recommending items of interest) and social link behavior (e.g., recommending potential social links). Accurately modeling and predicting users’ two kinds of behaviors are two core tasks in these platforms with various applications. Recently, with the advance of neural networks, many neural-based models have been designed to predict a single users’ behavior, i.e., social link behavior or consumption behavior. Compared to the classical shallow models, these neural-based models show better performance to drive a user’s behavior by modeling the complex patterns. However, there are few works exploiting whether it is possible to design a neural-based model to jointly predict users’ two kinds of behaviors to further enhance the prediction performance. In fact, social scientists have already shown that users’ two kinds of behaviors are not isolated; people trend to the consumption recommendation of friends on social platforms and would like to make new friends with like-minded users. While some previous works jointly model users’ two kinds of behaviors with shallow models, we argue that the correlation between users’ two kinds of behaviors are complex, which could not be well-designed with shallow linear models. To this end, in this article, we propose a neural joint behavior prediction model named Neural Joint Behavior Prediction Model (NJBP) to mutually enhance the prediction performance of these two tasks on social networking platforms. Specifically, there are two key characteristics of our proposed model: First, to model the correlation of users’ two kinds of behaviors, we design a fusion layer in the neural network to model the positive correlation of users’ two kinds of behaviors. Second, as the observed links in the social network are often very sparse, we design a new link-based loss function that could preserve the social network topology. After that, we design a joint optimization function to allow the two behaviors modeling tasks to be trained to mutually enhance each other. Finally, extensive experimental results on two real-world datasets show that our proposed method is on average 7.14% better than the best baseline on social link behavior while 6.21% on consumption behavior prediction. Compared with the pair-wise loss function on two datasets, our proposed link-based loss function improves at least 4.69% on the social link behavior prediction and 4.72% on the consumption behavior prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {72},
numpages = {25},
keywords = {consumption behavior, topology information, social link behavior, behavior prediction, Joint neural networks}
}

@article{10.1145/3383261,
author = {Zheng, Zimu and Pu, Jie and Liu, Linghui and Wang, Dan and Mei, Xiangming and Zhang, Sen and Dai, Quanyu},
title = {Contextual Anomaly Detection in Solder Paste Inspection with Multi-Task Learning},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3383261},
doi = {10.1145/3383261},
abstract = {In this article, we study solder paste inspection (SPI), an important stage that is used in the semiconductor manufacturing industry, where abnormal boards should be detected. A highly accurate SPI can substantially reduce human expert involvement, as well as reduce the waste in disposing of the boards in good condition. A key difference today is that because of increasing demand in board customization, the number of board types increases substantially and quantity of the boards produced in each type decreases. Thus, the previous approaches where a fine-tuned model is developed for each board type are no longer viable.Intrinsically, our problem is an anomaly detection problem. A major specialty in today’s SPI is that the target tasks for prediction cannot be fully pre-determined due to context changes during the solder paste printing stage. Our experiences show that a conventional approach to first define a set of tasks and train these tasks offline will lead to low accuracy. Here, we propose a novel multi-task approach, where the performance of all target tasks is ensured simultaneously. We note that the SPI process is streamlined and automatic, allowing the SPI time for only a few seconds. We propose a fast clustering algorithm that reuses existing models to avoid retraining and fine tune in the inference phase. We evaluate our approach using 3-month data collected from production lines. We show that we can reduce 81.28% of false alarms. This can translate to annual savings of $11.3 million.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {65},
numpages = {17},
keywords = {multi-task learning, Contextual anomaly detection}
}

@article{10.1145/3404855,
author = {Li, Pan and Tuzhilin, Alexander},
title = {Latent Unexpected Recommendations},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3404855},
doi = {10.1145/3404855},
abstract = {Unexpected recommender system constitutes an important tool to tackle the problem of filter bubbles and user boredom, which aims at providing unexpected and satisfying recommendations to target users at the same time. Previous unexpected recommendation methods only focus on the straightforward relations between current recommendations and user expectations by modeling unexpectedness in the feature space, thus resulting in the loss of accuracy measures to improve unexpectedness performance. In contrast to these prior models, we propose to model unexpectedness in the latent space of user and item embeddings, which allows us to capture hidden and complex relations between new recommendations and historic purchases. In addition, we develop a novel Latent Closure (LC) method to construct a hybrid utility function and provide unexpected recommendations based on the proposed model. Extensive experiments on three real-world datasets illustrate superiority of our proposed approach over the state-of-the-art unexpected recommendation models, which leads to significant increase in unexpectedness measure without sacrificing any accuracy metric under all experimental settings in this article.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {70},
numpages = {25},
keywords = {latent embeddings, beyond-accuracy objectives, latent closure, latent space, Unexpected recommendation}
}

@article{10.1145/3411749,
author = {Fang, Xiu Susie and Sheng, Quan Z. and Wang, Xianzhi and Zhang, Wei Emma and Ngu, Anne H. H. and Yang, Jian},
title = {From Appearance to Essence: Comparing Truth Discovery Methods without Using Ground Truth},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3411749},
doi = {10.1145/3411749},
abstract = {Truth discovery has been widely studied in recent years as a fundamental means for resolving the conflicts in multi-source data. Although many truth discovery methods have been proposed based on different considerations and intuitions, investigations show that no single method consistently outperforms the others. To select the right truth discovery method for a specific application scenario, it becomes essential to evaluate and compare the performance of different methods. A drawback of current research efforts is that they commonly assume the availability of certain ground truth for the evaluation of methods. However, the ground truth may be very limited or even impossible to obtain, rendering the evaluation biased. In this article, we present CompTruthHyp, a generic approach for comparing the performance of truth discovery methods without using ground truth. In particular, our approach calculates the probability of observations in a dataset based on the output of different methods. The probability is then ranked to reflect the performance of these methods. We review and compare 12 representative truth discovery methods and consider both single-valued and multi-valued objects. The empirical studies on both real-world and synthetic datasets demonstrate the effectiveness of our approach for comparing truth discovery methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {74},
numpages = {24},
keywords = {performance evaluation, Web search, truth discovery methods, multi-valued objects, single-valued objects, sparse ground truth}
}

@article{10.1145/3406541,
author = {Tama, Bayu Adhi and Comuzzi, Marco and Ko, Jonghyeon},
title = {An Empirical Investigation of Different Classifiers, Encoding, and Ensemble Schemes for Next Event Prediction Using Business Process Event Logs},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3406541},
doi = {10.1145/3406541},
abstract = {There is a growing need for empirical benchmarks that support researchers and practitioners in selecting the best machine learning technique for given prediction tasks. In this article, we consider the next event prediction task in business process predictive monitoring, and we extend our previously published benchmark by studying the impact on the performance of different encoding windows and of using ensemble schemes. The choice of whether to use ensembles and which scheme to use often depends on the type of data and classification task. While there is a general understanding that ensembles perform well in predictive monitoring of business processes, next event prediction is a task for which no other benchmarks involving ensembles are available. The proposed benchmark helps researchers to select a high-performing individual classifier or ensemble scheme given the variability at the case level of the event log under consideration. Experimental results show that choosing an optimal number of events for feature encoding is challenging, resulting in the need to consider each event log individually when selecting an optimal value. Ensemble schemes improve the performance of low-performing classifiers in this task, such as SVM, whereas high-performing classifiers, such as tree-based classifiers, are not better off when ensemble schemes are considered.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {68},
numpages = {34},
keywords = {predictive monitoring, Classifier ensembles, individual classifier, business process, homogeneous ensembles, next event prediction, empirical benchmark}
}

@article{10.1145/3396949,
author = {Albaqsami, Ahmad and Hosseini, Maryam S. and Jasemi, Masoomeh and Bagherzadeh, Nader},
title = {Adaptive HTF-MPR: An Adaptive Heterogeneous TensorFlow Mapper Utilizing Bayesian Optimization and Genetic Algorithms},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3396949},
doi = {10.1145/3396949},
abstract = {Deep neural networks are widely used in many artificial intelligence applications. They have demonstrated state-of-the-art accuracy on many artificial intelligence tasks. For this high accuracy to occur, deep neural networks require the right parameter values. This is achieved by a process known as training. The training of large amounts of data via many iterations comes at a high cost in regard to computation time and energy. Optimal resource allocation would therefore reduce the training time. TensorFlow, a computational graph library developed by Google, alleviates the development of neural network models and provides the means to train these networks. In this article, we propose Adaptive HTF-MPR to carry out the resource allocation, or mapping, on TensorFlow. Adaptive HTF-MPR searches for the best mapping in a hybrid approach. We applied the proposed methodology on two well-known image classifiers: VGG-16 and AlexNet. We also performed a full analysis of the solution space of MNIST Softmax. Our results demonstrate that Adaptive HTF-MPR outperforms the default homogeneous TensorFlow mapping. In addition to the speed up, Adaptive HTF-MPR can react to changes in the state of the system and adjust to an improved mapping.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {55},
numpages = {25},
keywords = {computational graphs, Bayesian optimization, gradient boosting regressor, adaptivity, neural networks, genetic algorithms, Task mapping}
}

@article{10.1145/3403931,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Simperl, Elena},
title = {Mapping Points of Interest Through Street View Imagery and Paid Crowdsourcing},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3403931},
doi = {10.1145/3403931},
abstract = {We present the Virtual City Explorer (VCE), an online crowdsourcing platform for the collection of rich geotagged information in urban environments. Compared to other volunteered geographic information approaches, which are constrained by the number and availability of mapping enthusiasts on the ground, the VCE uses digital street imagery to allow people to virtually explore a city from anywhere in the world, using a browser or a mobile phone. In addition, contributions in VCE are designed as paid microtasks—small jobs that can be carried out without any specific knowledge of the local area or previous mapping expertise in exchange for a fee. We tested the VCE in two cities to map points of interest (PoIs) in transport and mobility, using FigureEight to recruit participants. We were able to show that our platform enables crowdworkers to submit PoI location seamlessly, cover almost all of the tested areas, and discover several PoIs not reported by other approaches. This allows the VCE to complement existing approaches that leverage experts or grassroot communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {63},
numpages = {28},
keywords = {Crowdsourcing, urban auditing, microtasks, geospatial information, mapping, geographic information}
}

@article{10.1145/3397850,
author = {Dai, Chenglong and Pi, Dechang and Becker, Stefanie I.},
title = {Shapelet-Transformed Multi-Channel EEG Channel Selection},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3397850},
doi = {10.1145/3397850},
abstract = {This article proposes an approach to select EEG channels based on EEG shapelet transformation, aiming to reduce the setup time and inconvenience for subjects and to improve the applicable performance of Brain-Computer Interfaces (BCIs). In detail, the method selects top-k EEG channels by solving a logistic loss-embedded minimization problem with respect to EEG shapelet learning, hyperplane learning, and EEG channel weight learning simultaneously. Especially, to learn distinguished EEG shapelets for weighting contributions of each EEG channel to the logistic loss, EEG shapelet similarity is also minimized during the procedure. Furthermore, the gradient descent strategy is adopted in the article to solve the non-convex optimization problem, which finally leads to the algorithm termed StEEGCS. In a result, classification accuracy, with those EEG channels selected by StEEGCS, is improved compared to that with all EEG channels, and classification time consumption is reduced as well. Additionally, the comparisons with several state-of-the-art EEG channel selection methods on several real-world EEG datasets also demonstrate the efficacy and superiority of StEEGCS.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {58},
numpages = {27},
keywords = {shapelet similarity minimization, channel contribution, EEG shapelets, EEG channel selection}
}

@article{10.1145/3400730,
author = {Yadamjav, Munkh-Erdene and Bao, Zhifeng and Zheng, Baihua and Choudhury, Farhana M. and Samet, Hanan},
title = {Querying Recurrent Convoys over Trajectory Data},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3400730},
doi = {10.1145/3400730},
abstract = {Moving objects equipped with location-positioning devices continuously generate a large amount of spatio-temporal trajectory data. An interesting finding over a trajectory stream is a group of objects that are travelling together for a certain period of time. We observe that existing studies on mining co-moving objects do not consider an important correlation between co-moving objects, which is the reoccurrence of the co-moving pattern. In this study, we propose the problem of finding recurrent co-moving patterns from streaming trajectories, enabling us to discover recent co-moving patterns that are repeated within a given time period. Experimental results on real-life trajectory data verify the efficiency and effectiveness of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {59},
numpages = {24},
keywords = {spatio-temporal index, Recurrent convoy query, co-moving pattern}
}

@article{10.1145/3394118,
author = {Muralidhar, Nikhil and Tabassum, Anika and Chen, Liangzhe and Chinthavali, Supriya and Ramakrishnan, Naren and Prakash, B. Aditya},
title = {Cut-n-Reveal: Time Series Segmentations with Explanations},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3394118},
doi = {10.1145/3394118},
abstract = {Recent hurricane events have caused unprecedented amounts of damage on critical infrastructure systems and have severely threatened our public safety and economic health. The most observable (and severe) impact of these hurricanes is the loss of electric power in many regions, which causes breakdowns in essential public services. Understanding power outages and how they evolve during a hurricane provides insights on how to reduce outages in the future, and how to improve the robustness of the underlying critical infrastructure systems. In this article, we propose a novel scalable segmentation with explanations framework to help experts understand such datasets. Our method, CnR (Cut-n-Reveal), first finds a segmentation of the outage sequences based on the temporal variations of the power outage failure process so as to capture major pattern changes. This temporal segmentation procedure is capable of accounting for both the spatial and temporal correlations of the underlying power outage process. We then propose a novel explanation optimization formulation to find an intuitive explanation of the segmentation such that the explanation highlights the culprit time series of the change in each segment. Through extensive experiments, we show that our method consistently outperforms competitors in multiple real datasets with ground truth. We further study real county-level power outage data from several recent hurricanes (Matthew, Harvey, Irma) and show that CnR recovers important, non-trivial, and actionable patterns for domain experts, whereas baselines typically do not give meaningful results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {53},
numpages = {26},
keywords = {Multivariate time series, spatio-temporal segmentation}
}

@article{10.1145/3402446,
author = {Tu, Xiaoguang and Ma, Zheng and Zhao, Jian and Du, Guodong and Xie, Mei and Feng, Jiashi},
title = {Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3402446},
doi = {10.1145/3402446},
abstract = {Face anti-spoofing aims to detect presentation attack to face recognition--based authentication systems. It has drawn growing attention due to the high security demand. The widely adopted CNN-based methods usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of novel patterns or unseen scenes, leading to poor generalization performance. Furthermore, almost all current methods treat face anti-spoofing as a prior step to face recognition, which prolongs the response time and makes face authentication inefficient. In this article, we try to boost the generalizability and applicability of face anti-spoofing methods by designing a new generalizable face authentication CNN (GFA-CNN) model with three novelties. First, GFA-CNN introduces a simple yet effective total pairwise confusion loss for CNN training that properly balances contributions of all spoofing patterns for recognizing the spoofing faces. Second, it incorporate a fast domain adaptation component to alleviate negative effects brought by domain variation. Third, it deploys filter diversification learning to make the learned representations more adaptable to new scenes. In addition, the proposed GFA-CNN works in a multi-task manner—it performs face anti-spoofing and face recognition simultaneously. Experimental results on five popular face anti-spoofing and face recognition benchmarks show that GFA-CNN outperforms previous face anti-spoofing methods on cross-test protocols significantly and also well preserves the identity information of input face images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {60},
numpages = {19},
keywords = {domain adaptation, face recognition, face anti-spoofing, Deep learning, computer vision}
}

@article{10.1145/3397464,
author = {Zhou, Yuxiang and Liao, Lejian and Gao, Yang and Huang, Heyan and Wei, Xiaochi},
title = {A Discriminative Convolutional Neural Network with Context-Aware Attention},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3397464},
doi = {10.1145/3397464},
abstract = {Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting n-gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual n-gram features adaptively. In this article, we propose a discriminative CNN with context-aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {57},
numpages = {21},
keywords = {convolution neural networks, Text mining, attention method}
}

@article{10.1145/3396501,
author = {Huang, Jizhou and Wang, Haifeng and Zhang, Wei and Liu, Ting},
title = {Multi-Task Learning for Entity Recommendation and Document Ranking in Web Search},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3396501},
doi = {10.1145/3396501},
abstract = {Entity recommendation, providing users with an improved search experience by proactively recommending related entities to a given query, has become an indispensable feature of today’s Web search engine. Existing studies typically only consider the query issued at the current timestep while ignoring the in-session user search behavior (short-term search history) or historical user search behavior across all sessions (long-term search history) when generating entity recommendations. As a consequence, they may fail to recommend entities of interest relevant to a user’s actual information need. In this work, we believe that both short-term and long-term search history convey valuable evidence that could help understand the user’s search intent behind a query, and take both of them into consideration for entity recommendation. Furthermore, there has been little work on exploring whether the use of other companion tasks in Web search such as document ranking as auxiliary tasks could improve the performance of entity recommendation. To this end, we propose a multi-task learning framework with deep neural networks (DNNs) to jointly learn and optimize two companion tasks in Web search engines: entity recommendation and document ranking, which can be easily trained in an end-to-end manner. Specifically, we regard document ranking as an auxiliary task to improve the main task of entity recommendation, where the representations of queries, sessions, and users are shared across all tasks and optimized by the multi-task objective during training. We evaluate our approach using large-scale, real-world search logs of a widely-used commercial Web search engine. We also performed extensive ablation experiments over a number of facets of the proposed multi-task DNN model to figure out their relative importance. The experimental results show that both short-term and long-term search history can bring significant improvements in recommendation effectiveness, and the combination of both outperforms using either of them individually. In addition, the experiments show that the performance of both entity recommendation and document ranking can be significantly improved, which demonstrates the effectiveness of using multi-task learning to jointly optimize the two companion tasks in Web search.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {54},
numpages = {24},
keywords = {neural networks, context-aware, multi-task learning, Web search, personalized, document ranking, Entity recommendation}
}

@article{10.1145/3397463,
author = {Liu, Rui and Liu, Runze and Pugliese, Andrea and Subrahmanian, V. S.},
title = {STARS: Defending against Sockpuppet-Based Targeted Attacks on Reviewing Systems},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3397463},
doi = {10.1145/3397463},
abstract = {Customers of virtually all online marketplaces rely upon reviews in order to select the product or service they wish to buy. These marketplaces in turn deploy review fraud detection systems so that the integrity of reviews is preserved. A well-known problem with review fraud detection systems is their underlying assumption that the majority of reviews are honest-this assumption leads to a vulnerability where an attacker can try to generate many fake reviews of a product. In this article, we consider the case where a company wishes to fraudulently promote its product through fake reviews and propose the Sockpuppet-based Targeted Attack on Reviewing Systems (STARS for short). STARS enables an attacker to enter fake reviews for a product from multiple, apparently independent, sockpuppet accounts. We show that the STARS attack enables companies to successfully promote their product against seven recent, well-known review fraud detectors on four datasets (Amazon, Epinions, and the BitcoinAlpha and OTC exchanges) by significant margins. To protect against the STARS attack, we propose a new fraud detection algorithm called RTV. RTV introduces a new class of users (called trusted users) and also considers reviews left by verified users which were not considered in existing review fraud detectors. We show that RTV significantly mitigates the impact of the STARS attack across the four datasets listed above.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {56},
numpages = {25},
keywords = {Data mining and knowledge discovery, online commerce and recommendation systems, social and information networks}
}

@article{10.1145/3403578,
author = {Yuan, Kun and Liu, Guannan and Wu, Junjie and Xiong, Hui},
title = {Dancing with Trump in the Stock Market: A Deep Information Echoing Model},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3403578},
doi = {10.1145/3403578},
abstract = {It is always deemed crucial to identify the key factors that could have significant impact on the stock market trend. Recently, an interesting phenomenon has emerged that some of President Trump’s posts in Twitter can surge into a dominant role on the stock market for a certain time period, although studies along this line are still in their infancy. Therefore, in this article, we study whether and how this new-rising information can help boost the performance of stock market prediction. Specifically, we have found that the echoing reinforced effect of financial news with Trump’s market-related tweets can influence the market movement—that is, some of Trump’s tweets directly impact the stock market in a short time, and the impact can be further intensified when it echoes with other financial news reports. Along this line, we propose a deep information echoing model to predict the hourly stock market trend, such as the rise and fall of the Dow Jones Industrial Average. In particular, to model the discovered echoing reinforced impact, we design a novel information echoing module with a gating mechanism in a sequential deep learning framework to capture the fused knowledge from both Trump’s tweets and financial news. Extensive experiments have been conducted on the real-world U.S. stock market data to validate the effectiveness of our model and its interpretability in understanding the usability of Trump’s posts. Our proposed deep echoing model outperforms other baselines by achieving the best accuracy of 60.42% and obtains remarkable accumulated profits in a trading simulation, which confirms our assumption that Trump’s tweets contain indicative information for short-term market trends. Furthermore, we find that Trump’s tweets about trade and political events are more likely to be associated with short-term market movement, and it seems interesting that the impact would not degrade as time passes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {62},
numpages = {22},
keywords = {information echoing, deep learning, Twitter, Trump, Stock market prediction}
}

@article{10.1145/3402445,
author = {Tian, Qing and Zhang, Wenqiang and Cao, Meng and Wang, Liping and Chen, Songcan and Yin, Hujun},
title = {Moment-Guided Discriminative Manifold Correlation Learning on Ordinal Data},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3402445},
doi = {10.1145/3402445},
abstract = {Canonical correlation analysis (CCA) is a typical and useful learning paradigm in big data analysis for capturing correlation across multiple views of the same objects. When dealing with data with additional ordinal information, traditional CCA suffers from poor performance due to ignoring the ordinal relationships within the data. Such data is becoming increasingly common, as either temporal or sequential information is often associated with the data collection process. To incorporate the ordinal information into the objective function of CCA, the so-called ordinal discriminative CCA has been presented in the literature. Although ordinal discriminative CCA can yield better ordinal regression results, its performance deteriorates when data is corrupted with noise and outliers, as it tends to smear the order information contained in class centers. To address this issue, in this article we construct a robust manifold-preserved ordinal discriminative correlation regression (rmODCR). The robustness is achieved by replacing the traditional (l2-norm) class centers with lp-norm centers, where p is efficiently estimated according to the moments of the data distributions, as well as by incorporating the manifold distribution information of the data in the objective optimization. In addition, we further extend the robust manifold-preserved ordinal discriminative correlation regression to deep convolutional architectures. Extensive experimental evaluations have demonstrated the superiority of the proposed methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {61},
numpages = {18},
keywords = {ordinal regression, Canonical correlation analysis, lp-norm centers, moment, manifold learning}
}

@article{10.1145/3400066,
author = {Wilson, Garrett and Cook, Diane J.},
title = {A Survey of Unsupervised Deep Domain Adaptation},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3400066},
doi = {10.1145/3400066},
abstract = {Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {51},
numpages = {46},
keywords = {Domain adaptation, generative adversarial networks, deep learning}
}

@article{10.1145/3394138,
author = {Chen, Chaochao and Zhou, Jun and Wu, Bingzhe and Fang, Wenjing and Wang, Li and Qi, Yuan and Zheng, Xiaolin},
title = {Practical Privacy Preserving POI Recommendation},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3394138},
doi = {10.1145/3394138},
abstract = {Point-of-Interest (POI) recommendation has been extensively studied and successfully applied in industry recently. However, most existing approaches build centralized models on the basis of collecting users’ data. Both private data and models are held by the recommender, which causes serious privacy concerns. In this article, we propose a novel Privacy preserving POI Recommendation (PriRec) framework. First, to protect data privacy, users’ private data (features and actions) are kept on their own side, e.g., Cellphone or Pad. Meanwhile, the public data that need to be accessed by all the users are kept by the recommender to reduce the storage costs of users’ devices. Those public data include: (1) static data only related to the status of POI, such as POI categories, and (2) dynamic data dependent on user-POI actions such as visited counts. The dynamic data could be sensitive, and we develop local differential privacy techniques to release such data to the public with privacy guarantees. Second, PriRec follows the representations of Factorization Machine (FM) that consists of a linear model and the feature interaction model. To protect the model privacy, the linear models are saved on the users’ side, and we propose a secure decentralized gradient descent protocol for users to learn it collaboratively. The feature interaction model is kept by the recommender since there is no privacy risk, and we adopt a secure aggregation strategy in a federated learning paradigm to learn it. To this end, PriRec keeps users’ private raw data and models in users’ own hands, and protects user privacy to a large extent. We apply PriRec in real-world datasets, and comprehensive experiments demonstrate that, compared with FM, PriRec achieves comparable or even better recommendation accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {20},
keywords = {POI recommendation, decentralization, Privacy preserving, local differential privacy, secret sharing}
}

@article{10.1145/3384344,
author = {Feygin, Sidney A. and Lazarus, Jessica R. and Forscher, Edward H. and Golfier-Vetterli, Valentine and Lee, Jonathan W. and Gupta, Abhishek and Waraich, Rashid A. and Sheppard, Colin J. R. and Bayen, Alexandre M.},
title = {BISTRO: Berkeley Integrated System for Transportation Optimization},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3384344},
doi = {10.1145/3384344},
abstract = {The current trend toward urbanization and adoption of flexible and innovative mobility technologies will have complex and difficult-to-predict effects on urban transportation systems. Comprehensive methodological frameworks that account for the increasingly uncertain future state of the urban mobility landscape do not yet exist. Furthermore, few approaches have enabled the massive ingestion of urban data in planning tools capable of offering the flexibility of scenario-based design.This article introduces Berkeley Integrated System for Transportation Optimization (BISTRO), a new open source transportation planning decision support system that uses an agent-based simulation and optimization approach to anticipate and develop adaptive plans for possible technological disruptions and growth scenarios. The new framework was evaluated in the context of a machine learning competition hosted within Uber Technologies, Inc., in which over 400 engineers and data scientists participated. For the purposes of this competition, a benchmark model, based on the city of Sioux Falls, South Dakota, was adapted to the BISTRO framework. An important finding of this study was that in spite of rigorous analysis and testing done prior to the competition, the two top-scoring teams discovered an unbounded region of the search space, rendering the solutions largely uninterpretable for the purposes of decision-support. On the other hand, a follow-on study aimed to fix the objective function. It served to demonstrate BISTRO’s utility as a human-in-the-loop cyberphysical system: one that uses scenario-based optimization algorithms as a feedback mechanism to assist urban planners with iteratively refining objective function and constraints specification on intervention strategies. The portfolio of transportation intervention strategy alternatives eventually chosen achieves high-level regional planning goals developed through participatory stakeholder engagement practices.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {38},
numpages = {27},
keywords = {system dynamics, big data, digital decision support systems, smart cities, Agent-based models, urban informatics, computing with heterogeneous data, intelligent transportation systems, human mobility}
}

@article{10.1145/3391708,
author = {Kim, Jungeun and Lee, Jae-Gil and Lee, Byung Suk and Liu, Jiajun},
title = {Geosocial Co-Clustering: A Novel Framework for Geosocial Community Detection},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391708},
doi = {10.1145/3391708},
abstract = {As location-based services using mobile devices have become globally popular these days, social network analysis (especially, community detection) increasingly benefits from combining social relationships with geographic preferences. In this regard, this article addresses the emerging problem of geosocial community detection. We first formalize the problem of geosocial co-clustering, which co-clusters the users in social networks and the locations they visited. Geosocial co-clustering detects higher-quality communities than existing approaches by improving the mapping clusterability, whereby users in the same community tend to visit locations in the same region. While geosocial co-clustering is soundly formalized as non-negative matrix tri-factorization, conventional matrix tri-factorization algorithms suffer from a significant computational overhead when handling large-scale datasets. Thus, we also develop an efficient framework for geosocial co-clustering, called GEOsocial COarsening and DEcomposition&nbsp;(GEOCODE). To achieve efficient matrix tri-factorization, GEOCODE reduces the numbers of users and locations through coarsening and then decomposes the single whole matrix tri-factorization into a set of multiple smaller sub-matrix tri-factorizations. Thorough experiments conducted using real-world geosocial networks show that GEOCODE reduces the elapsed time by 19–69 times while achieving the accuracy of up to 94.8% compared with the state-of-the-art co-clustering algorithm. Furthermore, the benefit of the mapping clusterability is clearly demonstrated through a local expert recommendation application.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {45},
numpages = {26},
keywords = {spatial similarity, co-clustering, mapping clusterability, social similarity, non-negative matrix factorization, Geosocial networks}
}

@article{10.1145/3391250,
author = {Ma, Jing and Gao, Wei and Joty, Shafiq and Wong, Kam-Fai},
title = {An Attention-Based Rumor Detection Model with Tree-Structured Recursive Neural Networks},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391250},
doi = {10.1145/3391250},
abstract = {Rumor spread in social media severely jeopardizes the credibility of online content. Thus, automatic debunking of rumors is of great importance to keep social media a healthy environment. While facing a dubious claim, people often dispute its truthfulness sporadically in their posts containing various cues, which can form useful evidence with long-distance dependencies. In this work, we propose to learn discriminative features from microblog posts by following their non-sequential propagation structure and generate more powerful representations for identifying rumors. For modeling non-sequential structure, we first represent the diffusion of microblog posts with propagation trees, which provide valuable clues on how a claim in the original post is transmitted and developed over time. We then present a bottom-up and a top-down tree-structured models based on Recursive Neural Networks (RvNN) for rumor representation learning and classification, which naturally conform to the message propagation process in microblogs. To enhance the rumor representation learning, we reveal that effective rumor detection is highly related to finding evidential posts, e.g., the posts expressing specific attitude towards the veracity of a claim, as an extension of the previous RvNN-based detection models that treat every post equally. For this reason, we design discriminative attention mechanisms for the RvNN-based models to selectively attend on the subset of evidential posts during the bottom-up/top-down recursive composition. Experimental results on four datasets collected from real-world microblog platforms confirm that (1) our RvNN-based models achieve much better rumor detection and classification performance than state-of-the-art approaches; (2) the attention mechanisms for focusing on evidential posts can further improve the performance of our RvNN-based method; and (3) our approach possesses superior capacity on detecting rumors at a very early stage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {42},
numpages = {28},
keywords = {neural attention, propagation tree, Rumor detection and classification, social media, recursive neural networks}
}

@article{10.1145/3393619,
author = {Zhang, Xiang and Yao, Lina and Huang, Chaoran and Gu, Tao and Yang, Zheng and Liu, Yunhao},
title = {DeepKey: A Multimodal Biometric Authentication System via Deep Decoding Gaits and Brainwaves},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3393619},
doi = {10.1145/3393619},
abstract = {Biometric authentication involves various technologies to identify individuals by exploiting their unique, measurable physiological and behavioral characteristics. However, traditional biometric authentication systems (e.g., face recognition, iris, retina, voice, and fingerprint) are at increasing risks of being tricked by biometric tools such as anti-surveillance masks, contact lenses, vocoder, or fingerprint films. In this article, we design a multimodal biometric authentication system named DeepKey, which uses both Electroencephalography (EEG) and gait signals to better protect against such risk. DeepKey consists of two key components: an Invalid ID Filter Model to block unauthorized subjects, and an identification model based on attention-based Recurrent Neural Network (RNN) to identify a subject’s EEG IDs and gait IDs in parallel. The subject can only be granted access while all the components produce consistent affirmations to match the user’s proclaimed identity. We implement DeepKey with a live deployment in our university and conduct extensive empirical experiments to study its technical feasibility in practice. DeepKey achieves the False Acceptance Rate (FAR) and the False Rejection Rate (FRR) of 0 and 1.0%, respectively. The preliminary results demonstrate that DeepKey is feasible, shows consistent superior performance compared to a set of methods, and has the potential to be applied to the authentication deployment in real-world settings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {49},
numpages = {24},
keywords = {multimodal, deep learning, biometric authentication, gait, EEG (Electroencephalography)}
}

@article{10.1145/3391229,
author = {Wu, Hanrui and Yan, Yuguang and Ng, Michael K. and Wu, Qingyao},
title = {Domain-Attention Conditional Wasserstein Distance for Multi-Source Domain Adaptation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391229},
doi = {10.1145/3391229},
abstract = {Multi-source domain adaptation has received considerable attention due to its effectiveness of leveraging the knowledge from multiple related sources with different distributions to enhance the learning performance. One of the fundamental challenges in multi-source domain adaptation is how to determine the amount of knowledge transferred from each source domain to the target domain. To address this issue, we propose a new algorithm, called Domain-attention Conditional Wasserstein Distance (DCWD), to learn transferred weights for evaluating the relatedness across the source and target domains. In DCWD, we design a new conditional Wasserstein distance objective function by taking the label information into consideration to measure the distance between a given source domain and the target domain. We also develop an attention scheme to compute the transferred weights of different source domains based on their conditional Wasserstein distances to the target domain. After that, the transferred weights can be used to reweight the source data to determine their importance in knowledge transfer. We conduct comprehensive experiments on several real-world data sets, and the results demonstrate the effectiveness and efficiency of the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {44},
numpages = {19},
keywords = {attention, Domain adaptation, multiple sources, optimal transport}
}

@article{10.1145/3388634,
author = {Singhal, Divya and Gupta, Abhinav and Tripathi, Anurag and Kothari, Ravi},
title = {CNN-Based Multiple Manipulation Detector Using Frequency Domain Features of Image Residuals},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3388634},
doi = {10.1145/3388634},
abstract = {Increasingly sophisticated image editing tools make it easy to modify images. Often these modifications are elaborate, convincing, and undetectable by even careful human inspection. These considerations have prompted the development of forensic algorithms and approaches to detect modifications done to an image. However, these detectors are model-driven (i.e., manipulation-specific) and the choice of a potent detector requires knowledge of the type of manipulation, something that cannot be known (a priori). Thus, the latest effort is directed towards developing model-free (i.e., generalized) detectors capable of detecting multiple manipulation types. In this article, we propose a novel detector capable of exposing seven different manipulation types in low-resolution compressed images. Our proposed approach is based on a two-layer convolutional neural network (CNN) to extract frequency domain features of image median filtered residual that are classified using two different classifiers—softmax and extremely randomized trees. Extensive experiments demonstrate the efficacy of proposed detector over existing state-of-the-art detectors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {40},
numpages = {26},
keywords = {Convolutional neural network (CNN), Multiple manipulation detection, image forensics, two-layer architecture}
}

@article{10.1145/3386090,
author = {Liu, Hui and Wang, Haiou and Wu, Yan and Xing, Lei},
title = {Superpixel Region Merging Based on Deep Network for Medical Image Segmentation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3386090},
doi = {10.1145/3386090},
abstract = {Automatic and accurate semantic segmentation of pathological structures in medical images is challenging because of noisy disturbance, deformable shapes of pathology, and low contrast between soft tissues. Classical superpixel-based classification algorithms suffer from edge leakage due to complexity and heterogeneity inherent in medical images. Therefore, we propose a deep U-Net with superpixel region merging processing incorporated for edge enhancement to facilitate and optimize segmentation. Our approach combines three innovations: (1) different from deep learning--based image segmentation, the segmentation evolved from superpixel region merging via U-Net training getting rich semantic information, in addition to gray similarity; (2) a bilateral filtering module was adopted at the beginning of the network to eliminate external noise and enhance soft tissue contrast at edges of pathogy; and (3) a normalization layer was inserted after the convolutional layer at each feature scale, to prevent overfitting and increase the sensitivity to model parameters. This model was validated on lung CT, brain MR, and coronary CT datasets, respectively. Different superpixel methods and cross validation show the effectiveness of this architecture. The hyperparameter settings were empirically explored to achieve a good trade-off between the performance and efficiency, where a four-layer network achieves the best result in precision, recall, F-measure, and running speed. It was demonstrated that our method outperformed state-of-the-art networks, including FCN-16s, SegNet, PSPNet, DeepLabv3, and traditional U-Net, both quantitatively and qualitatively. Source code for the complete method is available at https://github.com/Leahnawho/Superpixel-network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {39},
numpages = {22},
keywords = {deep U-Net, Medical image segmentation, normalization layer, bilateral filtering, superpixel-based classification algorithm}
}

@article{10.1145/3393671,
author = {Wang, Guang and Zhang, Fan and Sun, Huijun and Wang, Yang and Zhang, Desheng},
title = {Understanding the Long-Term Evolution of Electric Taxi Networks: A Longitudinal Measurement Study on Mobility and Charging Patterns},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3393671},
doi = {10.1145/3393671},
abstract = {Due to the ever-growing concerns over air pollution and energy security, more and more cities have started to replace their conventional taxi fleets with electric ones. Even though environmentally friendly, the rapid promotion of electric taxis raises problems to both taxi drivers and governments, e.g., prolonged waiting/charging time, unbalanced utilization of charging infrastructures, and inadequate taxi supply due to the long charging time. In this article, we conduct the first longitudinal measurement study to understand the long-term evolution of mobility and charging patterns by utilizing 5-year data from one of the largest electric taxi networks in the world, i.e., the Shenzhen electric taxi network in China. In particular, (1) we first perform an electric taxi contextualization about their operation and charging activities; (2) then we design a generic charging event extraction algorithm based on GPS data and charging station data, and (3) based on the contextualization and extracted charging activities, we perform a comprehensive measurement study called ePat to explore the evolution of the electric taxi network from the mobility and charging perspectives. Our ePat is based on 4.8 TB taxi GPS data, 240 GB taxi transaction data, and metadata from 117 charging stations, during an evolution process from 427 electric taxis in 2013 to 13,178 in 2018. Moreover, ePat also explores the impacts of various contexts and benefits during the evolution process. Our ePat as a comprehensive measurement of the electric taxi network mobility and charging evolution has the potential to advance the understanding of the evolution patterns of electric taxi networks and pave the way for analyzing future shared autonomous vehicles.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {48},
numpages = {27},
keywords = {shared autonomous vehicle, Electric taxi, evolution experience, mobility pattern, charging pattern}
}

@article{10.1145/3384675,
author = {Zhang, Yingying and Fang, Quan and Qian, Shengsheng and Xu, Changsheng},
title = {Knowledge-Aware Attentive Wasserstein Adversarial Dialogue Response Generation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3384675},
doi = {10.1145/3384675},
abstract = {Natural language generation has become a fundamental task in dialogue systems. RNN-based natural response generation methods encode the dialogue context and decode it into a response. However, they tend to generate dull and simple responses. In this article, we propose a novel framework, called KAWA-DRG (Knowledge-aware Attentive Wasserstein Adversarial Dialogue Response Generation) to model conversation-specific external knowledge and the importance variances of dialogue context in a unified adversarial encoder-decoder learning framework. In KAWA-DRG, a co-attention mechanism attends to important parts within and among context utterances with word-utterance-level attention. Prior knowledge is integrated into the conditional Wasserstein auto-encoder for learning the latent variable space. The posterior and prior distribution of latent variables are generated and trained through adversarial learning. We evaluate our model on Switchboard, DailyDialog, In-Car Assistant, and Ubuntu Dialogue Corpus. Experimental results show that KAWA-DRG outperforms the existing methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {37},
numpages = {20},
keywords = {adversarial learning, external knowledge, Dialogue system, co-attention}
}

@article{10.1145/3391743,
author = {Yao, Rui and Lin, Guosheng and Xia, Shixiong and Zhao, Jiaqi and Zhou, Yong},
title = {Video Object Segmentation and Tracking: A Survey},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391743},
doi = {10.1145/3391743},
abstract = {Object segmentation and object tracking are fundamental research areas in the computer vision community. These two topics are difficult to handle some common challenges, such as occlusion, deformation, motion blur, scale variation, and more. The former contains heterogeneous object, interacting object, edge ambiguity, and shape complexity; the latter suffers from difficulties in handling fast motion, out-of-view, and real-time processing. Combining the two problems of Video Object Segmentation and Tracking (VOST) can overcome their respective difficulties and improve their performance. VOST can be widely applied to many practical applications such as video summarization, high definition video compression, human computer interaction, and autonomous vehicles. This survey aims to provide a comprehensive review of the state-of-the-art VOST methods, classify these methods into different categories, and identify new trends. First, we broadly categorize VOST methods into Video Object Segmentation (VOS) and Segmentation-based Object Tracking (SOT). Each category is further classified into various types based on the segmentation and tracking mechanism. Moreover, we present some representative VOS and SOT methods of each time node. Second, we provide a detailed discussion and overview of the technical characteristics of the different methods. Third, we summarize the characteristics of the related video dataset and provide a variety of evaluation metrics. Finally, we point out a set of interesting future works and draw our own conclusions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {36},
numpages = {47},
keywords = {semi-supervised methods, object tracking, weakly supervised methods, unsupervised methods, interactive methods, Video object segmentation}
}

@article{10.1145/3391709,
author = {Wang, Min and Lang, Congyan and Liang, Liqian and Feng, Songhe and Wang, Tao and Gao, Yutong},
title = {End-to-End Text-to-Image Synthesis with Spatial Constrains},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391709},
doi = {10.1145/3391709},
abstract = {Although the performance of automatically generating high-resolution realistic images from text descriptions has been significantly boosted, many challenging issues in image synthesis have not been fully investigated, due to shapes variations, viewpoint changes, pose changes, and the relations of multiple objects. In this article, we propose a novel end-to-end approach for text-to-image synthesis with spatial constraints by mining object spatial location and shape information. Instead of learning a hierarchical mapping from text to image, our algorithm directly generates multi-object fine-grained images through the guidance of the generated semantic layouts. By fusing text semantic and spatial information into a synthesis module and jointly fine-tuning them with multi-scale semantic layouts generated, the proposed networks show impressive performance in text-to-image synthesis for complex scenes. We evaluate our method both on single-object CUB dataset and multi-object MS-COCO dataset. Comprehensive experimental results demonstrate that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {47},
numpages = {19},
keywords = {Computer vision, MS-COCO, spatial constrain, text-to-image synthesis, CUB}
}

@article{10.1145/3391230,
author = {Wang, Jun-Zhe and Chen, Yi-Cheng and Shih, Wen-Yueh and Yang, Lin and Liu, Yu-Shao and Huang, Jiun-Long},
title = {Mining High-Utility Temporal Patterns on Time Interval–Based Data},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391230},
doi = {10.1145/3391230},
abstract = {In this article, we propose a novel temporal pattern mining problem, named high-utility temporal pattern mining, to fulfill the needs of various applications. Different from classical temporal pattern mining aimed at discovering frequent temporal patterns, high-utility temporal pattern mining is to find each temporal pattern whose utility is greater than or equal to the minimum-utility threshold. To facilitate efficient high-utility temporal pattern mining, several extension and pruning strategies are proposed to reduce the search space. Algorithm&nbsp;HUTPMiner is then proposed to efficiently mine high-utility temporal patterns with the aid of the proposed extension and pruning strategies. Experimental results show that HUTPMiner is able to prune a large number of candidates, thereby achieving high mining efficiency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {43},
numpages = {31},
keywords = {High-utility temporal pattern, high utility, temporal pattern, interval-based data, data mining}
}

@article{10.1145/3389127,
author = {Li, Lin and Pan, Weike and Ming, Zhong},
title = {CoFi-Points: Collaborative Filtering via Pointwise Preference Learning on User/Item-Set},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3389127},
doi = {10.1145/3389127},
abstract = {With the explosive growth of web resources, an increasingly important task in recommender systems is to provide high-quality personalized services by learning users’ preferences from historically observed information. As an effective preference learning technology, collaborative filtering has been widely extended to model the one-class or implicit feedback data, which is known as one-class collaborative filtering (OCCF). For a long time, pairwise ranking-oriented learning scheme has been viewed as a superior solution than the pointwise scheme for OCCF due to its higher accuracy in most cases. However, we argue that with appropriate model design, pointwise preference learning can achieve comparable or even better performance than the counterpart, i.e., pairwise preference learning. In particular, we propose a new preference assumption, i.e., pointwise preference on user/item-set. Based on this new assumption, we develop a novel, simple, and flexible solution called collaborative filtering via pointwise preference learning on user/item-set (CoFi-points). Furthermore, we derive two specific algorithms of CoFi-points with respect to the involved user-set and item-set, i.e., CoFi-points(u) and CoFi-points(i), referring to preference assumptions defined on user-set and item-set, respectively. Finally, we conduct extensive empirical studies on four real-world datasets with the state-of-the-art methods, and find that our solution can achieve very promising performance with respect to several ranking-oriented evaluation metrics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {41},
numpages = {24},
keywords = {user-set, One-class collaborative filtering, pointwise preference learning, item-set, implicit feedback}
}

@article{10.1145/3391707,
author = {Huang, Yapei and Tian, Yun and Liu, Zhijie and Jin, Xiaowei and Liu, Yanan and Zhao, Shifeng and Tian, Daxin},
title = {A Traffic Density Estimation Model Based on Crowdsourcing Privacy Protection},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391707},
doi = {10.1145/3391707},
abstract = {Acquiring traffic condition information is of great significance in transportation guidance, urban planning, and route recommendation. To date, traffic density data are generally acquired by road sound analysis, video data analysis, or in-vehicle network communication, which are usually financially or temporally expensive. Another way to get traffic conditions is to collect track data by crowdsourcing. However, this way lead to a greater risk of leaking users’ privacy. To avoid the risk, this article proposes a traffic density estimation model based on crowdsourcing privacy protection. First, in the acquisition process of the track data by crowdsourcing, dual servers are employed for transmission, and homomorphic encryption is carried out to encrypt the data to protect the data from being leaked during transmission. Second, sampling is implemented for randomization and anonymization to reduce the spatial continuity and temporal continuity of position data. In this way, the intermediate server cannot acquire users’ original data, and the main server cannot obtain users’ personal information. Finally, before data transmission, Laplace noising is performed on the users’ local position data to further protect the original location information. The proposed algorithm in this study realizes that only users have their original track data, and the servers involved in the work cannot infer the original track data, which ensures the real security of user privacy. The proposed algorithm was verified with the track data from the Didi Gaia Data Opening Plan. The experimental results showed that the proposed algorithm could still maintain the validity of data analysis results and the security of user data privacy after homomorphic encryption, noise addition, and sample collection, and displayed good robustness and scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {46},
numpages = {18},
keywords = {traffic flow, crowdsourcing, sample, Differential privacy, encryption}
}

@article{10.1145/3379500,
author = {Lin, Adi and Lu, Jie and Xuan, Junyu and Zhu, Fujin and Zhang, Guangquan},
title = {A Causal Dirichlet Mixture Model for Causal Inference from Observational Data},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3379500},
doi = {10.1145/3379500},
abstract = {Estimating causal effects by making causal inferences from observational data is common practice in scientific studies, business decision-making, and daily life. In today’s data-driven world, causal inference has become a key part of the evaluation process for many purposes, such as examining the effects of medicine or the impact of an economic policy on society. However, although the literature contains some excellent models, there is room to improve their representation power and their ability to capture complex relationships. For these reasons, we propose a novel prior called Causal DP and a model called CDP. The prior captures the complex relationships between covariates, treatments, and outcomes in observational data using a rational probabilistic dependency structure. The model is Bayesian, nonparametric, and generative and is not based on the assumption of any parametric distribution. CDP is designed to estimate various kinds of causal effects—average, conditional average, average treated, quantile, and so on. It performs well with missing covariates and does not suffer from overfitting. Comparative experiments on synthetic datasets against several state-of-the-art methods demonstrate that CDP has a superior ability to capture complex relationships. Further, a simple evaluation to infer the effect of a job training program on trainee earnings from real-world data shows that CDP is both effective and useful for causal inference.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {33},
numpages = {29},
keywords = {Bayesian nonparametric, Dirichlet process, Causal inference}
}

@article{10.1145/3377553,
author = {Zhang, Lei and Zhang, Yixiang and Zheng, Xiaolong},
title = {WiSign: Ubiquitous American Sign Language Recognition Using Commercial Wi-Fi Devices},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3377553},
doi = {10.1145/3377553},
abstract = {In this article, we propose WiSign that recognizes the continuous sentences of American Sign Language (ASL) with existing WiFi infrastructure. Instead of identifying the individual ASL words from the manually segmented ASL sentence in existing works, WiSign can automatically segment the original channel state information (CSI) based on the power spectral density (PSD) segmentation method. WiSign constructs a five-layer Deep Belief Network (DBN) to automatically extract the features of isolated fragments, and then uses the Hidden Markov Model (HMM) with Gaussian mixture and Forward-Backward algorithm to recognize sign words. In order to further improve the accuracy, WiSign also integrates the language model N-gram, which uses the grammar rules of ASL to calibrate the recognized results of sign words. We implement a prototype of WiSign with commercial WiFi devices and evaluate its performance in real indoor environments. The results show that WiSign achieves satisfactory accuracy when recognizing ASL sentences that involve the movements of the head, arms, hands, and fingers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {24},
keywords = {American Sign Language, channel state information, deep belief network, sentence-level recognition}
}

@article{10.1145/3380537,
author = {Arora, Udit and Dutta, Hridoy Sankar and Joshi, Brihi and Chetan, Aditya and Chakraborty, Tanmoy},
title = {Analyzing and Detecting Collusive Users Involved in Blackmarket Retweeting Activities},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3380537},
doi = {10.1145/3380537},
abstract = {With the rise in popularity of social media platforms like Twitter, having higher influence on these platforms has a greater value attached to it, since it has the power to influence many decisions in the form of brand promotions and shaping opinions. However, blackmarket services that allow users to inorganically gain influence are a threat to the credibility of these social networking platforms. Twitter users can gain inorganic appraisals in the form of likes, retweets, and follows through these blackmarket services either by paying for them or by joining syndicates wherein they gain such appraisals by providing similar appraisals to other users. These customers tend to exhibit a mix of organic and inorganic retweeting behavior, making it tougher to detect them.In this article, we investigate these blackmarket customers engaged in collusive retweeting activities. We collect and annotate a novel dataset containing various types of information about blackmarket customers and use these sources of information to construct multiple user representations. We adopt Weighted Generalized Canonical Correlation Analysis (WGCCA) to combine these individual representations to derive user embeddings that allow us to effectively classify users as: genuine users, bots, promotional customers, and normal customers. Our method significantly outperforms state-of-the-art approaches (32.95% better macro F1-score than the best baseline).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {35},
numpages = {24},
keywords = {blackmarket, Retweeters, collusion, Twitter, OSNs, multiview learning}
}

@article{10.1145/3375788,
author = {Zhao, Yawei and Zhao, Qian and Zhang, Xingxing and Zhu, En and Liu, Xinwang and Yin, Jianping},
title = {Understand Dynamic Regret with Switching Cost for Online Decision Making},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3375788},
doi = {10.1145/3375788},
abstract = {As a metric to measure the performance of an online method, dynamic regret with switching cost has drawn much attention for online decision making problems. Although the sublinear regret has been provided in much previous research, we still have little knowledge about the relation between the dynamic regret and the switching cost. In the article, we investigate the relation for two classic online settings: Online Algorithms (OA) and Online Convex Optimization (OCO). We provide a new theoretical analysis framework that shows an interesting observation; that is, the relation between the switching cost and the dynamic regret is different for settings of OA and OCO. Specifically, the switching cost has significant impact on the dynamic regret in the setting of OA. But it does not have an impact on the dynamic regret in the setting of OCO. Furthermore, we provide a lower bound of regret for the setting of OCO, which is same with the lower bound in the case of no switching cost. It shows that the switching cost does not change the difficulty of online decision making problems in the setting of OCO.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {28},
numpages = {21},
keywords = {online convex optimization, Online decision making, switching cost, online algorithms, online mirror descent, dynamic regret}
}

@article{10.1145/3375787,
author = {Liu, Xueliang and Yang, Xun and Wang, Meng and Hong, Richang},
title = {Deep Neighborhood Component Analysis for Visual Similarity Modeling},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3375787},
doi = {10.1145/3375787},
abstract = {Learning effective visual similarity is an essential problem in multimedia research. Despite the promising progress made in recent years, most existing approaches learn visual features and similarities in two separate stages, which inevitably limits their performance. Once useful information has been lost in the feature extraction stage, it can hardly be recovered later. This article proposes a novel end-to-end approach for visual similarity modeling, called deep neighborhood component analysis, which discriminatively trains deep neural networks to jointly learn visual features and similarities. Specifically, we first formulate a metric learning objective that maximizes the intra-class correlations and minimizes the inter-class correlations under the neighborhood component analysis criterion, and then train deep convolutional neural networks to learn a nonlinear mapping that projects visual instances from original feature space to a discriminative and neighborhood-structure-preserving embedding space, thus resulting in better performance. We conducted extensive evaluations on several widely used and challenging datasets, and the impressive results demonstrate the effectiveness of our proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {29},
numpages = {15},
keywords = {neighborhood component analysis, visual similarity modeling, Metric learning}
}

@article{10.1145/3377552,
author = {Brock, Heike and Law, Felix and Nakadai, Kazuhiro and Nagashima, Yuji},
title = {Learning Three-Dimensional Skeleton Data from Sign Language Video},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3377552},
doi = {10.1145/3377552},
abstract = {Data for sign language research is often difficult and costly to acquire. We therefore present a novel pipeline able to generate motion three-dimensional (3D) skeleton data from single-camera sign language videos only. First, three recurrent neural networks are learned to infer the three-dimensional position data of body, face, and finger joints for a high resolution of the signer’s skeleton. Subsequently, the angular displacements of all joints over time are estimated using inverse kinematics and mapped to a virtual sign avatar for animation. Last, the generated data are evaluated in detail, including a sign language recognition and sign language synthesis scenario. Utilizing a neural word classifier trained on real motion capture data, we reliably classify word segments built from our newly generated position data with similar accuracy as motion capture data (absolute difference 3.8%). Furthermore, qualitative evaluation of sign animations shows that the avatar performs natural movements that are comprehensible and resemble animations created with original motion capture data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {24},
keywords = {sign language recognition, data augmentation, 3D pose estimation, sign language synthesis, recurrent neural networks}
}

@article{10.1145/3373760,
author = {Jin, Di and Li, Bingyi and Jiao, Pengfei and He, Dongxiao and Shan, Hongyu and Zhang, Weixiong},
title = {Modeling with Node Popularities for Autonomous Overlapping Community Detection},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3373760},
doi = {10.1145/3373760},
abstract = {Overlapping community detection has triggered recent research in network analysis. One of the promising techniques for finding overlapping communities is the popular stochastic models, which, unfortunately, have some common drawbacks. They do not support an important observation that highly connected nodes are more likely to reside in the overlapping regions of communities in the network. These methods are in essence not truly unsupervised, since they require a threshold on probabilistic memberships to derive overlapping structures and need the number of communities to be specified a priori. We develop a new method to address these issues for overlapping community detection. We first present a stochastic model to accommodate the relative importance and the expected degree of every node in each community. We then infer every overlapping community by ranking the nodes according to their importance. Second, we determine the number of communities under the Bayesian framework. We evaluate our method and compare it with five state-of-the-art methods. The results demonstrate the superior performance of our method. We also apply this new method to two applications, showing its superb performance on practical problems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {27},
numpages = {23},
keywords = {Community detection, node popularities, model selection, stochastic model}
}

@article{10.1145/3372274,
author = {Horv\'{a}th, G\'{a}bor and Kov\'{a}cs, Edith and Molontay, Roland and Nov\'{a}czki, Szabolcs},
title = {Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372274},
doi = {10.1145/3372274},
abstract = {The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly.The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets.In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {26},
numpages = {26},
keywords = {Anomaly scoring, copula fitting, unsupervised learning}
}

@article{10.1145/3379501,
author = {Lyu, Gengyu and Feng, Songhe and Li, Yidong and Jin, Yi and Dai, Guojun and Lang, Congyan},
title = {HERA: Partial Label Learning by Combining Heterogeneous Loss with Sparse and Low-Rank Regularization},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3379501},
doi = {10.1145/3379501},
abstract = {Partial label learning (PLL) aims to learn from the data where each training instance is associated with a set of candidate labels, among which only one is correct. Most existing methods deal with this type of problem by either treating each candidate label equally or identifying the ground-truth label iteratively. In this article, we propose a novel PLL approach named HERA, which simultaneously incorporates the HeterogEneous Loss and the SpaRse and Low-rAnk procedure to estimate the labeling confidence for each instance while training the desired model. Specifically, the heterogeneous loss integrates the strengths of both the pairwise ranking loss and the pointwise reconstruction loss to provide informative label ranking and reconstruction information for label identification, whereas the embedded sparse and low-rank scheme constrains the sparsity of ground-truth label matrix and the low rank of noise label matrix to explore the global label relevance among the whole training data, for improving the learning model. Comprehensive ablation study demonstrates the effectiveness of our employed heterogeneous loss, and extensive experiments on both artificial and real-world datasets demonstrate that our method achieves superior or comparable performance against state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {19},
keywords = {matrix decomposing, Partial label learning, heterogeneous loss, sparse and low-rank regularization}
}

@article{10.1145/3377554,
author = {Shah, Ankit and Sinha, Arunesh and Ganesan, Rajesh and Jajodia, Sushil and Cam, Hasan},
title = {Two Can Play That Game: An Adversarial Evaluation of a Cyber-Alert Inspection System},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3377554},
doi = {10.1145/3377554},
abstract = {Cyber-security is an important societal concern. Cyber-attacks have increased in numbers as well as in the extent of damage caused in every attack. Large organizations operate a Cyber Security Operation Center (CSOC), which forms the first line of cyber-defense. The inspection of cyber-alerts is a critical part of CSOC operations (defender or blue team). Recent work proposed a reinforcement learning (RL) based approach for the defender’s decision-making to prevent the cyber-alert queue length from growing large and overwhelming the defender. In this article, we perform a red team (adversarial) evaluation of this approach. With the recent attacks on learning-based decision-making systems, it is even more important to test the limits of the defender’s RL approach. Toward that end, we learn several adversarial alert generation policies and the best response against them for various defender’s inspection policy. Surprisingly, we find the defender’s policies to be quite robust to the best response of the attacker. In order to explain this observation, we extend the earlier defender’s RL model to a game model with adversarial RL, and show that there exist defender policies that can be robust against any adversarial policy. We also derive a competitive baseline from the game theory model and compare it to the defender’s RL approach. However, when we go further to exploit the assumptions made in the Markov Decision Process (MDP) in the defender’s RL model, we discover an attacker policy that overwhelms the defender. We use a double oracle like approach to retrain the defender with episodes from this discovered attacker policy. This made the defender robust to the discovered attacker policy and no further harmful attacker policies were discovered. Overall, the adversarial RL and double oracle approach in RL are general techniques that are applicable to other RL usage in adversarial environments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {32},
numpages = {20},
keywords = {Cyber-security operations center, adversarial reinforcement learning, game theory}
}

@article{10.1145/3374217,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
title = {Adversarial Attacks on Deep-Learning Models in Natural Language Processing: A Survey},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3374217},
doi = {10.1145/3374217},
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {41},
keywords = {textual data, natural language processing, adversarial examples, Deep neural networks}
}

@article{10.1145/3372120,
author = {Liu, Zhuang and Xiao, Keli and Jin, Bo and Huang, Kaiyu and Huang, Degen and Zhang, Yunxia},
title = {Unified Generative Adversarial Networks for Multiple-Choice Oriented Machine Comprehension},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372120},
doi = {10.1145/3372120},
abstract = {In this article, we address the multiple-choice machine comprehension (MC) problem in natural language processing. Existing approaches for MC are usually designed for general cases; however, we specially develop a novel method for solving the multiple-choice MC problem. We take the inspiration generative adversarial networks (GANs) and first propose an adversarial framework for multiple-choice oriented MC, named McGAN. Specifically, our approach is designed as a GAN-based method that unifies both generative and discriminative MC models. Working together, the generative model focuses on predicting relevant answer given a passage (text) and a question; the discriminative model focuses on predicting their relevancy given an answer-passage-question set. Based on the competition via adversarial training in a minimize-maximize game, the proposed method takes advantages from both models. To evaluate the performance, we test our McGAN model on three well-known datasets for multiple-choice MC. Our results show that McGAN can achieve a significant increase in accuracy compared to existing models based on all three datasets, and it consistently outperforms all tested baselines, including state-of-the-art techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {25},
numpages = {20},
keywords = {GAN, MC, machine comprehension, RNN, Generative adversarial networks, RACE, recurrent neural networks}
}

@article{10.1145/3375402,
author = {Shmueli, Erez and Tassa, Tamir},
title = {Mediated Secure Multi-Party Protocols for Collaborative Filtering},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3375402},
doi = {10.1145/3375402},
abstract = {Recommender systems have become extremely common in recent years and are utilized in a variety of domains such as movies, music, news, products, restaurants, and so on. While a typical recommender system bases its recommendations solely on users’ preference data collected by the system itself, the quality of recommendations can significantly be improved if several recommender systems (or vendors) share their data. However, such data sharing poses significant privacy and security challenges, both to the vendors and the users. In this article, we propose secure protocols for distributed item-based Collaborative Filtering. Our protocols allow to compute both the predicted ratings of items and their predicted rankings without compromising privacy nor predictions’ accuracy. Unlike previous solutions in which the secure protocols are executed solely by the vendors, our protocols assume the existence of a mediator that performs intermediate computations on encrypted data supplied by the vendors. Such a mediated setting is advantageous over the non-mediated one since it enables each vendor to communicate solely with the mediator. This yields reduced communication costs, and it allows each vendor to issue recommendations to its clients without being dependent on the availability and willingness of the other vendors to collaborate.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {15},
numpages = {25},
keywords = {Item-based collaborative filtering, privacy, distributed computing}
}

@article{10.1145/3360309,
author = {Wang, Jindong and Chen, Yiqiang and Feng, Wenjie and Yu, Han and Huang, Meiyu and Yang, Qiang},
title = {Transfer Learning with Dynamic Distribution Adaptation},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3360309},
doi = {10.1145/3360309},
abstract = {Transfer learning aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Since the source and the target domains are usually from different distributions, existing methods mainly focus on adapting the cross-domain marginal or conditional distributions. However, in real applications, the marginal and conditional distributions usually have different contributions to the domain discrepancy. Existing methods fail to quantitatively evaluate the different importance of these two distributions, which will result in unsatisfactory transfer performance. In this article, we propose a novel concept called Dynamic Distribution Adaptation (DDA), which is capable of quantitatively evaluating the relative importance of each distribution. DDA can be easily incorporated into the framework of structural risk minimization to solve transfer learning problems. On the basis of DDA, we propose two novel learning algorithms: (1) Manifold Dynamic Distribution Adaptation (MDDA) for traditional transfer learning, and (2) Dynamic Distribution Adaptation Network (DDAN) for deep transfer learning. Extensive experiments demonstrate that MDDA and DDAN significantly improve the transfer learning performance and set up a strong baseline over the latest deep and adversarial methods on digits recognition, sentiment analysis, and image classification. More importantly, it is shown that marginal and conditional distributions have different contributions to the domain divergence, and our DDA is able to provide good quantitative evaluation of their relative importance, which leads to better performance. We believe this observation can be helpful for future research in transfer learning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {6},
numpages = {25},
keywords = {subspace learning, Transfer learning, distribution alignment, deep learning, kernel method, domain adaptation}
}

@article{10.1145/3361741,
author = {Fu, Tao-Yang and Lee, Wang-Chien},
title = {Trembr: Exploring Road Networks for Trajectory Representation Learning},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3361741},
doi = {10.1145/3361741},
abstract = {In this article, we propose a novel representation learning framework, namely TRajectory EMBedding via Road networks (Trembr), to learn trajectory embeddings (low-dimensional feature vectors) for use in a variety of trajectory applications. The novelty of Trembr lies in (1) the design of a recurrent neural network--(RNN) based encoder--decoder model, namely Traj2Vec, that encodes spatial and temporal properties inherent in trajectories into trajectory embeddings by exploiting the underlying road networks to constrain the learning process in accordance with the matched road segments obtained using road network matching techniques (e.g., Barefoot [24, 27]), and (2) the design of a neural network--based model, namely Road2Vec, to learn road segment embeddings in road networks that captures various relationships amongst road segments in preparation for trajectory representation learning. In addition to model design, several unique technical issues raising in Trembr, including data preparation in Road2Vec, the road segment relevance-aware loss, and the network topology constraint in Traj2Vec, are examined. To validate our ideas, we learn trajectory embeddings using multiple large-scale real-world trajectory datasets and use them in three tasks, including trajectory similarity measure, travel time prediction, and destination prediction. Empirical results show that Trembr soundly outperforms the state-of-the-art trajectory representation learning models, trajectory2vec and t2vec, by at least one order of magnitude in terms of mean rank in trajectory similarity measure, 23.3% to 41.7% in terms of mean absolute error (MAE) in travel time prediction, and 39.6% to 52.4% in terms of MAE in destination prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {10},
numpages = {25},
keywords = {neural networks, road network, representation learning, Trajectory}
}

@article{10.1145/3372121,
author = {Liu, Wenhe and Chang, Xiaojun and Chen, Ling and Phung, Dinh and Zhang, Xiaoqin and Yang, Yi and Hauptmann, Alexander G.},
title = {Pair-Based Uncertainty and Diversity Promoting Early Active Learning for Person Re-Identification},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372121},
doi = {10.1145/3372121},
abstract = {The effective training of supervised Person Re-identification (Re-ID) models requires sufficient pairwise labeled data. However, when there is limited annotation resource, it is difficult to collect pairwise labeled data. We consider a challenging and practical problem called Early Active Learning, which is applied to the early stage of experiments when there is no pre-labeled sample available as references for human annotating. Previous early active learning methods suffer from two limitations for Re-ID. First, these instance-based algorithms select instances rather than pairs, which can result in missing optimal pairs for Re-ID. Second, most of these methods only consider the representativeness of instances, which can result in selecting less diverse and less informative pairs. To overcome these limitations, we propose a novel pair-based active learning for Re-ID. Our algorithm selects pairs instead of instances from the entire dataset for annotation. Besides representativeness, we further take into account the uncertainty and the diversity in terms of pairwise relations. Therefore, our algorithm can produce the most representative, informative, and diverse pairs for Re-ID data annotation. Extensive experimental results on five benchmark Re-ID datasets have demonstrated the superiority of the proposed pair-based early active learning algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {21},
numpages = {15},
keywords = {person re-identification, Active learning}
}

@article{10.1145/3372117,
author = {Zhang, Shuo and Balog, Krisztian},
title = {Web Table Extraction, Retrieval, and Augmentation: A Survey},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372117},
doi = {10.1145/3372117},
abstract = {Tables are powerful and popular tools for organizing and manipulating data. A vast number of tables can be found on the Web, which represent a valuable knowledge resource. The objective of this survey is to synthesize and present two decades of research on web tables. In particular, we organize existing literature into six main categories of information access tasks: table extraction, table interpretation, table search, question answering, knowledge base augmentation, and table augmentation. For each of these tasks, we identify and describe seminal approaches, present relevant resources, and point out interdependencies among the different tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {13},
numpages = {35},
keywords = {table retrieval, table augmentation, table mining, Table extraction, table search, table interpretation}
}

@article{10.1145/3368271,
author = {Wang, Chien-Chih and Tan, Kent Loong and Lin, Chih-Jen},
title = {Newton Methods for Convolutional Neural Networks},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3368271},
doi = {10.1145/3368271},
abstract = {Deep learning involves a difficult non-convex optimization problem, which is often solved by stochastic gradient (SG) methods. While SG is usually effective, it may not be robust in some situations. Recently, Newton methods have been investigated as an alternative optimization technique, but most existing studies consider only fully connected feedforward neural networks. These studies do not investigate some more commonly used networks such as Convolutional Neural Networks (CNN). One reason is that Newton methods for CNN involve complicated operations, and so far no works have conducted a thorough investigation. In this work, we give details of all building blocks, including the evaluation of function, gradient, Jacobian, and Gauss-Newton matrix-vector products. These basic components are very important not only for practical implementation but also for developing variants of Newton methods for CNN. We show that an efficient MATLAB implementation can be done in just several hundred lines of code. Preliminary experiments indicate that Newton methods are less sensitive to parameters than the stochastic gradient approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {19},
numpages = {30},
keywords = {subsampled Hessian, large-scale classification, newton methods, Convolution neural networks}
}

@article{10.1145/3368270,
author = {Zhuo, Hankz Hankui and Zha, Yantian and Kambhampati, Subbarao and Tian, Xin},
title = {Discovering Underlying Plans Based on Shallow Models},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3368270},
doi = {10.1145/3368270},
abstract = {Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real-world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this article, we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (Recurrent Neural Networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {18},
numpages = {30},
keywords = {shallow model, Plan recognition, recurrent neural networks, action representation}
}

@article{10.1145/3356467,
author = {Nelke, Sofia Amador and Okamoto, Steven and Zivan, Roie},
title = {Market Clearing–Based Dynamic Multi-Agent Task Allocation},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3356467},
doi = {10.1145/3356467},
abstract = {Realistic multi-agent team applications often feature dynamic environments with soft deadlines that penalize late execution of tasks. This puts a premium on quickly allocating tasks to agents. However, when such problems include temporal and spatial constraints that require tasks to be executed sequentially by agents, they are NP-hard, and thus are commonly solved using general and specifically designed incomplete heuristic algorithms.We propose FMC_TA, a novel such incomplete task allocation algorithm that allows tasks to be easily sequenced to yield high-quality solutions. FMC_TA first finds allocations that are fair (envy-free), balancing the load and sharing important tasks among agents, and efficient (Pareto optimal) in a simplified version of the problem. It computes such allocations in polynomial or pseudo-polynomial time (centrally or distributedly, respectively) using a Fisher market with agents as buyers and tasks as goods. It then heuristically schedules the allocations, taking into account inter-agent constraints on shared tasks.We empirically compare our algorithm to state-of-the-art incomplete methods, both centralized and distributed, on law enforcement problems inspired by real police logs. We present a novel formalization of the law enforcement problem, which we use to perform our empirical study. The results show a clear advantage for FMC_TA in total utility and in measures in which law enforcement authorities measure their own performance. Besides problems with realistic properties, the algorithms were compared on synthetic problems in which we increased the size of different elements of the problem to investigate the algorithm’s behavior when the problem scales. The domination of the proposed algorithm was found to be consistent.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {4},
numpages = {25},
keywords = {Distributed Task Allocation, Multi agent system}
}

@article{10.1145/3354287,
author = {Zhou, Binbin and Zhao, Sha and Chen, Longbiao and Li, Shijian and Wu, Zhaohui and Pan, Gang},
title = {Forecasting Price Trend of Bulk Commodities Leveraging Cross-Domain Open Data Fusion},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3354287},
doi = {10.1145/3354287},
abstract = {Forecasting price trend of bulk commodities is important in international trade, not only for markets participants to schedule production and marketing plans but also for government administrators to adjust policies. Previous studies cannot support accurate fine-grained short-term prediction, since they mainly focus on coarse-grained long-term prediction using historical data. Recently, cross-domain open data provides possibilities to conduct fine-grained price forecasting, since they can be leveraged to extract various direct and indirect factors of the price. In this article, we predict the price trend over upcoming days, by leveraging cross-domain open data fusion. More specifically, we formulate the price trend into three classes (rise, slight-change, and fall), and then we predict the specific class in which the price trend of the future day lies. We take three factors into consideration: (1) supply factor considering sources providing bulk commodities,<!--?brk?--> (2) demand factor focusing on vessel transportation with reflection of short time needs, and (3) expectation factor encompassing indirect features (e.g., air quality) with latent influences. A hybrid classification framework is proposed for the price trend forecasting. Evaluation conducted on nine real-world cross-domain open datasets shows that our framework can forecast the price trend accurately, outperforming multiple state-of-the-art baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {1},
numpages = {26},
keywords = {cross-domain data, Price trend, multi-class prediction, data fusion, bulk commodity}
}

@article{10.1145/3364221,
author = {Luo, Ping and Shu, Kai and Wu, Junjie and Wan, Li and Tan, Yong},
title = {Exploring Correlation Network for Cheating Detection},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3364221},
doi = {10.1145/3364221},
abstract = {The correlation network, typically formed by computing pairwise correlations between variables, has recently become a competitive paradigm to discover insights in various application domains, such as climate prediction, financial marketing, and bioinformatics. In this study, we adopt this paradigm to detect cheating behavior hidden in business distribution channels, where falsified big deals are often made by collusive partners to obtain lower product prices—a behavior deemed to be extremely harmful to the sale ecosystem. To this end, we assume that abnormal deals are likely to occur between two partners if their purchase-volume sequences have a strong negative correlation. This seemingly intuitive rule, however, imposes several research challenges. First, existing correlation measures are usually symmetric and thus cannot distinguish the different roles of partners in cheating. Second, the tick-to-tick correspondence between two sequences might be violated due to the possible delay of purchase behavior, which should also be captured by correlation measures. Finally, the fact that any pair of sequences could be correlated may result in a number of false-positive cheating pairs, which need to be corrected in a systematic manner. To address these issues, we propose a correlation network analysis framework for cheating detection. In the framework, we adopt an asymmetric correlation measure to distinguish the two roles, namely, cheating seller and cheating buyer, in a cheating alliance. Dynamic Time Warping is employed to address the time offset between two sequences in computing the correlation. We further propose two graph-cut methods to convert the correlation network into a bipartite graph to rank cheating partners, which simultaneously helps to remove false-positive correlation pairs. Based on a 4-year real-world channel dataset from a worldwide IT company, we demonstrate the effectiveness of the proposed method in comparison to competitive baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {23},
keywords = {time series, distribution channel, Correlation network analysis, graph cut, cheating detection}
}

@article{10.1145/3361740,
author = {Wang, Meng and Li, Hui and Cui, Jiangtao and Bhowmick, Sourav S. and Liu, Ping},
title = {FROST: Movement History–Conscious Facility Relocation},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3361740},
doi = {10.1145/3361740},
abstract = {The facility relocation (FR) problem, which aims to optimize the placement of facilities to accommodate the changes of users’ locations, has a broad spectrum of applications. Despite the significant progress made by existing solutions to the FR problem, they all assume each user is stationary and represented as a single point. Unfortunately, in reality, objects (e.g., people, animals) are mobile. For example, a car-sharing user picks up a vehicle from a station close to where he or she is currently located. Consequently, these efforts may fail to identify a superior solution to the FR problem. In this article, for the first time, we take into account the movement history of users and introduce a novel FR problem, called motion-fr, to address the preceding limitation. Specifically, we present a framework called frost to address it. frost comprises two exact algorithms: index based and index free. The former is designed to address the scenario when facilities and objects are known a priori, whereas the latter solves the motion-fr problem by jettisoning this assumption. Further, we extend the index-based algorithm to solve the general k-motion-fr problem, which aims to relocate k inferior facilities. We devise an approximate solution due to NP-hardness of the problem. Experimental study over both real-world and synthetic datasets demonstrates the superiority of our framework in comparison to state-of-the-art FR techniques in efficiency and effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {9},
numpages = {26},
keywords = {movement history, Facility relocation, spatial database}
}

@article{10.1145/3360312,
author = {Pan, Menghai and Huang, Weixiao and Li, Yanhua and Zhou, Xun and Liu, Zhenming and Song, Rui and Lu, Hui and Tian, Zhihong and Luo, Jun},
title = {DHPA: Dynamic Human Preference Analytics Framework: A Case Study on Taxi Drivers’ Learning Curve Analysis},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3360312},
doi = {10.1145/3360312},
abstract = {Many real-world human behaviors can be modeled and characterized as sequential decision-making processes, such as a taxi driver’s choices of working regions and times. Each driver possesses unique preferences on the sequential choices over time and improves the driver’s working efficiency. Understanding the dynamics of such preferences helps accelerate the learning process of taxi drivers. Prior works on taxi operation management mostly focus on finding optimal driving strategies or routes, lacking in-depth analysis on what the drivers learned during the process and how they affect the performance of the driver. In this work, we make the first attempt to establish Dynamic Human Preference Analytics. We inversely learn the taxi drivers’ preferences from data and characterize the dynamics of such preferences over time. We extract two types of features (i.e., profile features and habit features) to model the decision space of drivers. Then through inverse reinforcement learning, we learn the preferences of drivers with respect to these features. The results illustrate that self-improving drivers tend to keep adjusting their preferences to habit features to increase their earning efficiency while keeping the preferences to profile features invariant. However, experienced drivers have stable preferences over time. The exploring drivers tend to randomly adjust the preferences over time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {8},
numpages = {19},
keywords = {inverse reinforcement learning, Urban computing, preference dynamics}
}

@article{10.1145/3372119,
author = {Pereira, Ramon Fraga and Oren, Nir and Meneguzzi, Felipe},
title = {Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in Discrete Environments},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372119},
doi = {10.1145/3372119},
abstract = {Assessing whether an agent has abandoned a goal or is actively pursuing it is important when multiple agents are trying to achieve joint goals, or when agents commit to achieving goals for each other. Making such a determination for a single goal by observing only plan traces is not trivial, as agents often deviate from optimal plans for various reasons, including the pursuit of multiple goals or the inability to act optimally. In this article, we develop an approach based on domain independent heuristics from automated planning, landmarks, and fact partitions to identify sub-optimal action steps—with respect to a plan—within a fully observable plan execution trace. Such capability is very important in domains where multiple agents cooperate and delegate tasks among themselves, such as through social commitments, and need to ensure that a delegating agent can infer whether or not another agent is actually progressing towards a delegated task. We demonstrate how a creditor can use our technique to determine—by observing a trace—whether a debtor is honouring a commitment. We empirically show, for a number of representative domains, that our approach infers sub-optimal action steps with very high accuracy and detects commitment abandonment in nearly all cases.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {23},
numpages = {26},
keywords = {landmarks, sub-optimal plan, domain-independent heuristics, plan execution, Commitments, plan abandonment, optimal plan}
}

@article{10.1145/3372118,
author = {Chen, Lei and Wu, Zhiang and Cao, Jie and Zhu, Guixiang and Ge, Yong},
title = {Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372118},
doi = {10.1145/3372118},
abstract = {As an e-commerce feature, the personalized recommendation is invariably highly-valued by both consumers and merchants. The e-tourism has become one of the hottest industries with the adoption of recommendation systems. Several lines of evidence have confirmed the travel-product recommendation is quite different from traditional recommendations. Travel products are usually browsed and purchased relatively infrequently compared with other traditional products (e.g., books and food), which gives rise to the extreme sparsity of travel data. Meanwhile, the choice of a suitable travel product is affected by an army of factors such as departure, destination, and financial and time budgets. To address these challenging problems, in this article, we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information (PMF-MAI) model in the context of the travel-product recommendation. In particular, PMF-MAI is able to fuse the probabilistic matrix factorization on the user-item interaction matrix with the linear regression on a suite of features constructed by the multiple auxiliary information. In order to fit the sparse data, PMF-MAI is built by a whole-data based learning approach that utilizes unobserved data to increase the coupling between probabilistic matrix factorization and linear regression. Extensive experiments are conducted on a real-world dataset provided by a large tourism e-commerce company. PMF-MAI shows an overwhelming superiority over all competitive baselines on the recommendation performance. Also, the importance of features is examined to reveal the crucial auxiliary information having a great impact on the adoption of travel products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {22},
numpages = {24},
keywords = {linear regression, probabilistic matrix factorization, Travel product recommendation, multiple auxiliary information, recommender systems}
}

@article{10.1145/3372116,
author = {Huang, Shih-Chia and Jaw, Da-Wei and Chen, Bo-Hao and Kuo, Sy-Yen},
title = {Single Image Snow Removal Using Sparse Representation and Particle Swarm Optimizer},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372116},
doi = {10.1145/3372116},
abstract = {Images are often corrupted by natural obscuration (e.g., snow, rain, and haze) during acquisition in bad weather conditions. The removal of snowflakes from only a single image is a challenging task due to situational variety and has been investigated only rarely. In this article, we propose a novel snow removal framework for a single image, which can be separated into a sparse image approximation module and an adaptive tolerance optimization module. The first proposed module takes the advantage of sparsity-based regularization to reconstruct a potential snow-free image. An auto-tuning mechanism for this framework is then proposed to seek a better reconstruction of a snow-free image via the time-varying inertia weight particle swarm optimizers in the second proposed module. Through collaboration of these two modules iteratively, the number of snowflakes in the reconstructed image is reduced as generations progress. By the experimental results, the proposed method achieves a better efficacy of snow removal than do other state-of-the-art techniques via both objective and subjective evaluations. As a result, the proposed method is able to remove snowflakes successfully from only a single image while preserving most original object structure information.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {20},
numpages = {15},
keywords = {image restoration, Snow removal, sparse representation}
}

@article{10.1145/3368272,
author = {Ye, Juan and Dobson, Simon and Zambonelli, Franco},
title = {<i>XLearn</i>: Learning Activity Labels across Heterogeneous Datasets},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3368272},
doi = {10.1145/3368272},
abstract = {Sensor-driven systems often need to map sensed data into meaningfully labelled activities to classify the phenomena being observed. A motivating and challenging example comes from human activity recognition in which smart home and other datasets are used to classify human activities to support applications such as ambient assisted living, health monitoring, and behavioural intervention. Building a robust and meaningful classifier needs annotated ground truth, labelled with what activities are actually being observed—and acquiring high-quality, detailed, continuous annotations remains a challenging, time-consuming, and error-prone task, despite considerable attention in the literature. In this article, we use knowledge-driven ensemble learning to develop a technique that can combine classifiers built from individually labelled datasets, even when the labels are sparse and heterogeneous. The technique both relieves individual users of the burden of annotation and allows activities to be learned individually and then transferred to a general classifier. We evaluate our approach using four third-party, real-world smart home datasets and show that it enhances activity recognition accuracies even when given only a very small amount of training data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {28},
keywords = {clustering, ensemble learning, Human activity recognition, smart home, transfer learning}
}

@article{10.1145/3365841,
author = {Zhu, Lei and Lu, Xu and Cheng, Zhiyong and Li, Jingjing and Zhang, Huaxiang},
title = {Flexible Multi-Modal Hashing for Scalable Multimedia Retrieval},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3365841},
doi = {10.1145/3365841},
abstract = {Multi-modal hashing methods could support efficient multimedia retrieval by combining multi-modal features for binary hash learning at the both offline training and online query stages. However, existing multi-modal methods cannot binarize the queries, when only one or part of modalities are provided. In this article, we propose a novel Flexible Multi-modal Hashing (FMH) method to address this problem. FMH learns multiple modality-specific hash codes and multi-modal collaborative hash codes simultaneously within a single model. The hash codes are flexibly generated according to the newly coming queries, which provide any one or combination of modality features. Besides, the hashing learning procedure is efficiently supervised by the pair-wise semantic matrix to enhance the discriminative capability. It could successfully avoid the challenging symmetric semantic matrix factorization and O(n2) storage cost of semantic matrix. Finally, we design a fast discrete optimization to learn hash codes directly with simple operations. Experiments validate the superiority of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {14},
numpages = {20},
keywords = {efficient discrete optimization, Multi-modal hashing}
}

@article{10.1145/3365375,
author = {Oliveira, Samuel E. L. and Diniz, Victor and Lacerda, Anisio and Merschmanm, Luiz and Pappa, Gisele L.},
title = {Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3365375},
doi = {10.1145/3365375},
abstract = {Recommender Systems are tools designed to help users find relevant information from the myriad of content available online. They work by actively suggesting items that are relevant to users according to their historical preferences or observed actions. Among recommender systems, top-N recommenders work by suggesting a ranking of N items that can be of interest to a user. Although a significant number of top-N recommenders have been proposed in the literature, they often disagree in their returned rankings, offering an opportunity for improving the final recommendation ranking by aggregating the outputs of different algorithms.Rank aggregation was successfully used in a significant number of areas, but only a few rank aggregation methods have been proposed in the recommender systems literature. Furthermore, there is a lack of studies regarding rankings’ characteristics and their possible impacts on the improvements achieved through rank aggregation. This work presents an extensive two-phase experimental analysis of rank aggregation in recommender systems. In the first phase, we investigate the characteristics of rankings recommended by 15 different top-N recommender algorithms regarding agreement and diversity. In the second phase, we look at the results of 19 rank aggregation methods and identify different scenarios where they perform best or worst according to the input rankings’ characteristics.Our results show that supervised rank aggregation methods provide improvements in the results of the recommended rankings in six out of seven datasets. These methods provide robustness even in the presence of a big set of weak recommendation rankings. However, in cases where there was a set of non-diverse high-quality input rankings, supervised and unsupervised algorithms produced similar results. In these cases, we can avoid the cost of the former in favor of the latter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {16},
numpages = {26},
keywords = {Rank aggregation, machine learning, recommender systems}
}

@article{10.1145/3364222,
author = {Beigi, Ghazaleh and Tang, Jiliang and Liu, Huan},
title = {Social Science–Guided Feature Engineering: A Novel Approach to Signed Link Analysis},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3364222},
doi = {10.1145/3364222},
abstract = {Many real-world relations can be represented by signed networks with positive links (e.g., friendships and trust) and negative links (e.g., foes and distrust). Link prediction helps advance tasks in social network analysis such as recommendation systems. Most existing work on link analysis focuses on unsigned social networks. The existence of negative links piques research interests in investigating whether properties and principles of signed networks differ from those of unsigned networks and mandates dedicated efforts on link analysis for signed social networks. Recent findings suggest that properties of signed networks substantially differ from those of unsigned networks and negative links can be of significant help in signed link analysis in complementary ways. In this article, we center our discussion on a challenging problem of signed link analysis. Signed link analysis faces the problem of data sparsity, i.e., only a small percentage of signed links are given. This problem can even get worse when negative links are much sparser than positive ones as users are inclined more toward positive disposition rather than negative. We investigate how we can take advantage of other sources of information for signed link analysis. This research is mainly guided by three social science theories, Emotional Information, Diffusion of Innovations, and Individual Personality. Guided by these, we extract three categories of related features and leverage them for signed link analysis. Experiments show the significance of the features gleaned from social theories for signed link prediction and addressing the data sparsity challenge.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {11},
numpages = {27},
keywords = {emotional information, feature engineering, data sparsity, diffusion of innovation, individual personality, Signed link analysis, social theory}
}

@article{10.1145/3354189,
author = {Xie, Yiqun and Zhou, Xun and Shekhar, Shashi},
title = {Discovering Interesting Subpaths with Statistical Significance from Spatiotemporal Datasets},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3354189},
doi = {10.1145/3354189},
abstract = {Given a path in a spatial or temporal framework, we aim to find all contiguous subpaths that are both interesting (e.g., abrupt changes) and statistically significant (i.e., persistent trends rather than local fluctuations). Discovering interesting subpaths can provide meaningful information for a variety of domains including Earth science, environmental science, urban planning, and the like. Existing methods are limited to detecting individual points of interest along an input path but cannot find interesting subpaths. Our preliminary work provided a Subpath Enumeration and Pruning (SEP) algorithm to detect interesting subpaths of arbitrary length. However, SEP is not effective in avoiding detections that are random variations rather than meaningful trends, which hampers clear and proper interpretations of the results. In this article, we extend our previous work by proposing a significance testing framework to eliminate these random variations. To compute the statistical significance, we first show a baseline Monte-Carlo method based on our previous work and then propose a Dynamic Search-and-Prune (D-SAP) algorithm to improve its computational efficiency. Our experiments show that the significance testing can greatly suppress the noisy detections in the output and D-SAP can greatly reduce the execution time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {2},
numpages = {24},
keywords = {statistical significance, spatial, Interesting sub-paths, temporal}
}

@article{10.1145/3363818,
author = {Horne, Benjamin D. and N\o{}rregaard, Jeppe and Adali, Sibel},
title = {Robust Fake News Detection Over Time and Attack},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3363818},
doi = {10.1145/3363818},
abstract = {In this study, we examine the impact of time on state-of-the-art news veracity classifiers. We show that, as time progresses, classification performance for both unreliable and hyper-partisan news classification slowly degrade. While this degradation does happen, it happens slower than expected, illustrating that hand-crafted, content-based features, such as style of writing, are fairly robust to changes in the news cycle. We show that this small degradation can be mitigated using online learning. Last, we examine the impact of adversarial content manipulation by malicious news producers. Specifically, we test three types of attack based on changes in the input space and data availability. We show that static models are susceptible to content manipulation attacks, but online models can recover from such attacks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {7},
numpages = {23},
keywords = {biased news, disinformation, misinformation, adversarial machine learning, robust machine learning, concept drift, fake news detection, misleading news, Fake news}
}

@article{10.1145/3356882,
author = {Lopes, Ramon and Assun\c{c}\~{a}o, Renato and Santos, Rodrygo L. T.},
title = {Graph-Based Recommendation Meets Bayes and Similarity Measures},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3356882},
doi = {10.1145/3356882},
abstract = {Graph-based approaches provide an effective memory-based alternative to latent factor models for collaborative recommendation. Modern approaches rely on either sampling short walks or enumerating short paths starting from the target user in a user-item bipartite graph. While the effectiveness of random walk sampling heavily depends on the underlying path sampling strategy, path enumeration is sensitive to the strategy adopted for scoring each individual path. In this article, we demonstrate how both strategies can be improved through Bayesian reasoning. In particular, we propose to improve random walk sampling by exploiting distributional aspects of items’ ratings on the sampled paths. Likewise, we extend existing path enumeration approaches to leverage categorical ratings and to scale the score of each path proportionally to the affinity of pairs of users and pairs of items on the path. Experiments on several publicly available datasets demonstrate the effectiveness of our proposed approaches compared to state-of-the-art graph-based recommenders.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {3},
numpages = {26},
keywords = {graph-based recommendation, similarity measures, Collaborative filtering, Bayesian statistics}
}

@article{10.1145/3345640,
author = {Zhao, Ying and Wang, Lei and Li, Shijie and Zhou, Fangfang and Lin, Xiaoru and Lu, Qiang and Ren, Lei},
title = {A Visual Analysis Approach for Understanding Durability Test Data of Automotive Products},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3345640},
doi = {10.1145/3345640},
abstract = {People face data-rich manufacturing environments in Industry 4.0. As an important technology for explaining and understanding complex data, visual analytics has been increasingly introduced into industrial data analysis scenarios. With the durability test of automotive starters as background, this study proposes a visual analysis approach for understanding large-scale and long-term durability test data. Guided by detailed scenario and requirement analyses, we first propose a migration-adapted clustering algorithm that utilizes a segmentation strategy and a group of matching-updating operations to achieve an efficient and accurate clustering analysis of the data for starting mode identification and abnormal test detection. We then design and implement a visual analysis system that provides a set of user-friendly visual designs and lightweight interactions to help people gain data insights into the test process overview, test data patterns, and durability performance dynamics. Finally, we conduct a quantitative algorithm evaluation, case study, and user interview by using real-world starter durability test datasets. The results demonstrate the effectiveness of the approach and its possible inspiration for the durability test data analysis of other similar industrial products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {70},
numpages = {23},
keywords = {visual analysis, smart manufacturing, durability test, automotive starter, Industry 4.0}
}

@article{10.1145/3357605,
author = {Waniek, Marcin and Michalak, Tomasz P. and Alshamsi, Aamena},
title = {Strategic Attack &amp; Defense in Security Diffusion Games},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3357605},
doi = {10.1145/3357605},
abstract = {Security games model the confrontation between a defender protecting a set of targets and an attacker who tries to capture them. A variant of these games assumes security interdependence between targets, facilitating contagion of an attack. So far, only stochastic spread of an attack has been considered. In this work, we introduce a version of security games, where the attacker strategically drives the entire spread of attack and where interconnections between nodes affect their susceptibility to be captured.We find that the strategies effective in the settings without contagion or with stochastic contagion are no longer feasible when spread of attack is strategic. While in the former settings it was possible to efficiently find optimal strategies of the attacker, doing so in the latter setting turns out to be an NP-complete problem for an arbitrary network. However, for some simpler network structures, such as cliques, stars, and trees, we show that it is possible to efficiently find optimal strategies of both players. For arbitrary networks, we study and compare the efficiency of various heuristic strategies. As opposed to previous works with no or stochastic contagion, we find that centrality-based defense is often effective when spread of attack is strategic, particularly for centrality measures based on the Shapley value.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {5},
numpages = {35},
keywords = {social networks, Security games, network diffusion}
}

@article{10.1145/3339474,
author = {Yang, Po and Liu, Jing and Qi, Jun and Yang, Yun and Wang, Xulong and Lv, Zhihan},
title = {Comparison and Modelling of Country-Level Microblog User and Activity in Cyber-Physical-Social Systems Using Weibo and Twitter Data},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3339474},
doi = {10.1145/3339474},
abstract = {As the rapid growth of social media technologies continues, Cyber-Physical-Social System (CPSS) has been a hot topic in many industrial applications. The use of “microblogging” services, such as Twitter, has rapidly become an influential way to share information. While recent studies have revealed that understanding and modelling microblog user behaviour with massive users’ data in social media are keen to success of many practical applications in CPSS, a key challenge in literatures is that diversity of geography and cultures in social media technologies strongly affect user behaviour and activity. The motivation of this article is to understand differences and similarities between microblogging users from different countries using social media technologies, and to attempt to design a Country-Level Micro-Blog User (CLMB) behaviour and activity model for supporting CPSS applications. We proposed a CLMB model for analysing microblogging user behaviour and their activity across different countries in the CPSS applications. The model has considered three important characteristics of user behaviour in microblogging data, including content of microblogging messages, user emotion index, and user relationship network. We evaluated CLBM model under the collected microblog dataset from 16 countries with the largest number of representative and active users in the world. Experimental results show that (1) for some countries with small population and strong cohesiveness, users pay more attention to social functionalities of microblogging service; (2) for some countries containing mostly large loose social groups, users use microblogging services as a news dissemination platform; (3) users in countries whose social network structure exhibits reciprocity rather than hierarchy will use more linguistic elements to express happiness in microblogging services.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {65},
numpages = {24},
keywords = {Weibo, microblogging}
}

@article{10.1145/3372277,
author = {Jia, Prof. Weijia and Min, Prof. Geyong and Xiang, Prof. Yang and Sangaiah, Dr. Arun Kumar},
title = {Special Issue on Intelligent Edge Computing for Cyber Physical and Cloud Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372277},
doi = {10.1145/3372277},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {60e},
numpages = {4}
}

@article{10.1145/3331146,
author = {Liu, Fang and Guo, Yeting and Cai, Zhiping and Xiao, Nong and Zhao, Ziming},
title = {Edge-Enabled Disaster Rescue: A Case Study of Searching for Missing People},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3331146},
doi = {10.1145/3331146},
abstract = {In the aftermath of earthquakes, floods, and other disasters, photos are increasingly playing more significant roles, such as finding missing people and assessing disasters, in rescue and recovery efforts. These disaster photos are taken in real time by the crowd, unmanned aerial vehicles, and wireless sensors. However, communications equipment is often damaged in disasters, and the very limited communication bandwidth restricts the upload of photos to the cloud center, seriously impeding disaster rescue endeavors. Based on edge computing, we propose Echo, a highly time-efficient disaster rescue framework. By utilizing the computing, storage, and communication abilities of edge servers, disaster photos are preprocessed and analyzed in real time, and more specific visuals are immensely helpful for conducting emergency response and rescue. This article takes the search for missing people as a case study to show that Echo can be more advantageous in terms of disaster rescue. To greatly conserve valuable communication bandwidth, only significantly associated images are extracted and uploaded to the cloud center for subsequent facial recognition. Furthermore, an adaptive photo detector is designed to utilize the precious and unstable communication bandwidth effectively, as well as ensure the photo detection precision and recall rate. The effectiveness and efficiency of the proposed method are demonstrated by simulation experiments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {63},
numpages = {21},
keywords = {time-efficient, finding missing people, face recognition, disaster rescue, Edge computing}
}

@article{10.1145/3321694,
author = {Liu, Yuxin and Liu, Xiao and Liu, Anfeng and Xiong, Neal N. and Liu, Fang},
title = {A Trust Computing-Based Security Routing Scheme for Cyber Physical Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3321694},
doi = {10.1145/3321694},
abstract = {Security is a pivotal issue for the development of Cyber Physical Systems (CPS). The trusted computing of CPS includes the complete protection mechanisms, such as hardware, firmware, and software, the combination of which is responsible for enforcing a system security policy. A Trust Detection-based Secured Routing (TDSR) scheme is proposed to establish security routes from source nodes to the data center under malicious environment to ensure network security. In the TDSR scheme, sensor nodes in the routing path send detection routing to identify relay nodes’ trust. And then, data packets are routed through trustworthy nodes to sink securely. In the TDSR scheme, the detection routing is executed in those nodes that have abundant energy; thus, the network lifetime cannot be affected. Performance evaluation through simulation is carried out for success of routing ratio, compromised node detection ratio, and detection routing overhead. The experiment results show that the performance can be improved in the TDSR scheme compared to previous schemes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {61},
numpages = {27},
keywords = {malicious attacks, data collection, trust-based routing, Cyber physical systems}
}

@article{10.1145/3359995,
author = {Li, Jiuyong and Zhang, Kun and K\i{}c\i{}man, Emre and Cui, Peng},
title = {Introduction to the Special Section on Advances in Causal Discovery and Inference},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3359995},
doi = {10.1145/3359995},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {45},
numpages = {3}
}

@article{10.1145/3331147,
author = {Li, Xiaoming and Xu, Guangquan and Zheng, Xi and Liang, Kaitai and Panaousis, Emmanouil and Li, Tao and Wang, Wei and Shen, Chao},
title = {Using Sparse Representation to Detect Anomalies in Complex WSNs},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3331147},
doi = {10.1145/3331147},
abstract = {In recent years, wireless sensor networks (WSNs) have become an active area of research for monitoring physical and environmental conditions. Due to the interdependence of sensors, a functional anomaly in one sensor can cause a functional anomaly in another sensor, which can further lead to the malfunctioning of the entire sensor network. Existing research work has analysed faulty sensor anomalies but fails to show the effectiveness throughout the entire interdependent network system. In this article, a dictionary learning algorithm based on a non-negative constraint is developed, and a sparse representation anomaly node detection method for sensor networks is proposed based on the dictionary learning. Through experiment on a specific thermal power plant in China, we verify the robustness of our proposed method in detecting abnormal nodes against four state of the art approaches and proved our method is more robust. Furthermore, the experiments are conducted on the obtained abnormal nodes to prove the interdependence of multi-layer sensor networks and reveal the conditions and causes of a system crash.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {64},
numpages = {18},
keywords = {WSNs, anomaly detection, Dependency relationships networks, Sparse Representation}
}

@article{10.1145/3356468,
author = {Li, Jin and Li, Tong and Liu, Zheli and Chen, Xiaofeng},
title = {Secure Deduplication System with Active Key Update and Its Application in IoT},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3356468},
doi = {10.1145/3356468},
abstract = {The rich cloud services in the Internet of Things create certain needs for edge computing, in which devices should be able to handle storage tasks securely, reliably, and efficiently. When processing the storage requests from edge devices, each cloud server is supposed to eliminate duplicate copies of repeating data to reduce the amount of storage space and save on bandwidth. To protect data confidentiality while supporting deduplication, some convergent-encryption-based techniques have been proposed to encrypt the data before uploading. However, all these works cannot meet two requirements while preventing brute-force attacks: (i) power-constrained edge nodes should update encryption keys efficiently when an edge node is abandoned; and (ii) the access privacy of edge nodes should be guaranteed. In this article, we propose a novel encryption scheme for secure chunk-level deduplication. Based on this scheme, we present two constructions of the secure deduplication system that support an efficient key update protocol. The key update protocol does not involve any edge node in computational tasks, so that the deduplication system can adopt an active key update strategy. Moreover, one of our constructions, which is called advance construction, can provide access privacy assurances for edge nodes. The security analysis is given in terms of the proposed threat model. The experimental analysis demonstrates that the proposed deduplication system is practical.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {69},
numpages = {21},
keywords = {Deduplication, edge computing, convergent encryption, key update}
}

@article{10.1145/3347452,
author = {Leyli-Abadi, Milad and sam\'{e}, Allou and Oukhellou, Latifa and Cheifetz, Nicolas and Mandel, Pierre and F\'{e}liers, C\'{e}dric and Chesneau, Olivier},
title = {Mixture of Joint Nonhomogeneous Markov Chains to Cluster and Model Water Consumption Behavior Sequences},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3347452},
doi = {10.1145/3347452},
abstract = {The emergence of smart meters has fostered the collection of massive data that support a better understanding of consumer behaviors and better management of water resources and networks. The main focus of this article is to analyze consumption behavior over time; thus, we first identify the main weekly consumption patterns. This approach allows each meter to be represented by a categorical series, where each category corresponds to a weekly consumption behavior. By considering the resulting consumption behavior sequences, we propose a new methodology based on a mixture of nonhomogeneous Markov models to cluster these categorical time series. Using this method, the meters are described by the Markovian dynamics of their cluster. The latent variable that controls cluster membership is estimated alongside the parameters of the Markov model using a novel classification expectation maximization algorithm. A specific entropy measure is formulated to evaluate the quality of the estimated partition by considering the joint Markovian dynamics. The proposed clustering model can also be used to predict future consumption behaviors within each cluster. Numerical experiments using real water consumption data provided by a water utility in France and gathered over 19 months are conducted to evaluate the performance of the proposed approach in terms of both clustering and prediction. The results demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {21},
keywords = {categorical time series, Nonhomogeneous Markov models, clustering, forecasting, water consumption behavior}
}

@article{10.1145/3341104,
author = {Tang, Wenjuan and Ren, Ju and Zhang, Kuan and Zhang, Deyu and Zhang, Yaoxue and Shen, Xuemin (Sherman)},
title = {Efficient and Privacy-Preserving Fog-Assisted Health Data Sharing Scheme},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3341104},
doi = {10.1145/3341104},
abstract = {Pervasive data collected from e-healthcare devices possess significant medical value through data sharing with professional healthcare service providers. However, health data sharing poses several security issues, such as access control and privacy leakage, as well as faces critical challenges to obtain efficient data analysis and services. In this article, we propose an efficient and privacy-preserving fog-assisted health data sharing (PFHDS) scheme for e-healthcare systems. Specifically, we integrate the fog node to classify the shared data into different categories according to disease risks for efficient health data analysis. Meanwhile, we design an enhanced attribute-based encryption method through combination of a personal access policy on patients and a professional access policy on the fog node for effective medical service provision. Furthermore, we achieve significant encryption consumption reduction for patients by offloading a portion of the computation and storage burden from patients to the fog node. Security discussions show that PFHDS realizes data confidentiality and fine-grained access control with collusion resistance. Performance evaluations demonstrate cost-efficient encryption computation, storage and energy consumption.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {68},
numpages = {23},
keywords = {data sharing, privacy-preservation, e-healthcare, Fog computing, access control}
}

@article{10.1145/3339308,
author = {Zhou, Junhao and Dai, Hong-Ning and Wang, Hao},
title = {Lightweight Convolution Neural Networks for Mobile Edge Computing in Transportation Cyber Physical Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3339308},
doi = {10.1145/3339308},
abstract = {Cloud computing extends Transportation Cyber-Physical Systems (T-CPS) with provision of enhanced computing and storage capability via offloading computing tasks to remote cloud servers. However, cloud computing cannot fulfill the requirements such as low latency and context awareness in T-CPS. The appearance of Mobile Edge Computing (MEC) can overcome the limitations of cloud computing via offloading the computing tasks at edge servers in approximation to users, consequently reducing the latency and improving the context awareness. Although MEC has the potential in improving T-CPS, it is incapable of processing computational-intensive tasks such as deep learning algorithms due to the intrinsic storage and computing-capability constraints. Therefore, we design and develop a lightweight deep learning model to support MEC applications in T-CPS. In particular, we put forth a stacked convolutional neural network (CNN) consisting of factorization convolutional layers alternating with compression layers (namely, lightweight CNN-FC). Extensive experimental results show that our proposed lightweight CNN-FC can greatly decrease the number of unnecessary parameters, thereby reducing the model size while maintaining the high accuracy in contrast to conventional CNN models. In addition, we also evaluate the performance of our proposed model via conducting experiments at a realistic MEC platform. Specifically, experimental results at this MEC platform show that our model can maintain the high accuracy while preserving the portable model size.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {67},
numpages = {20},
keywords = {cyber physical systems, Jetson TX2 module, factorization, Convolutional neural network, model compression, mobile edge computing}
}

@article{10.1145/3324926,
author = {Wang, Tian and Luo, Hao and Zheng, Xi and Xie, Mande},
title = {Crowdsourcing Mechanism for Trust Evaluation in CPCS Based on Intelligent Mobile Edge Computing},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3324926},
doi = {10.1145/3324926},
abstract = {Both academia and industry have directed tremendous interest toward the combination of Cyber Physical Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile edge users with relatively strong computation and storage ability are exploited to provide direct management for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of Quality-Aware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness, respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge users to perform evaluation task and improve the accuracy of trust evaluation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {19},
keywords = {mobile edge computing, trust evaluation, artificial intelligence, Crowdsourcing}
}

@article{10.1145/3336121,
author = {Tariq, Umair Ullah and Ali, Haider and Liu, Lu and Panneerselvam, John and Zhai, Xiaojun},
title = {Energy-Efficient Static Task Scheduling on VFI-Based NoC-HMPSoCs for Intelligent Edge Devices in Cyber-Physical Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3336121},
doi = {10.1145/3336121},
abstract = {The interlinked processing units in modern Cyber-Physical Systems (CPS) creates a large network of connected computing embedded systems. Network-on-Chip (NoC)-based Multiprocessor System-on-Chip (MPSoC) architecture is becoming a de facto computing platform for real-time applications due to its higher performance and Quality-of-Service (QoS). The number of processors has increased significantly on the multiprocessor systems in CPS; therefore, Voltage Frequency Island (VFI) has been recently adopted for effective energy management mechanism in the large-scale multiprocessor chip designs. In this article, we investigated energy-efficient and contention-aware static scheduling for tasks with precedence and deadline constraints on intelligent edge devices deploying heterogeneous VFI-based NoC-MPSoCs (VFI-NoC-HMPSoC) with DVFS-enabled processors. Unlike the existing population-based optimization algorithms, we proposed a novel population-based algorithm called ARSH-FATI that can dynamically switch between explorative and exploitative search modes at run-time. Our static scheduler ARHS-FATI collectively performs task mapping, scheduling, and voltage scaling. Consequently, its performance is superior to the existing state-of-the-art approach proposed for homogeneous VFI-based NoC-MPSoCs. We also developed a communication contention-aware Earliest Edge Consistent Deadline First (EECDF) scheduling algorithm and gradient descent--inspired voltage scaling algorithm called Energy Gradient Decent (EGD). We introduced a notion of Energy Gradient (EG) that guides EGD in its search for island voltage settings and minimize the total energy consumption.We conducted the experiments on eight real benchmarks adopted from Embedded Systems Synthesis Benchmarks (E3S). Our static scheduling approach ARSH-FATI outperformed state-of-the-art technique and achieved an average energy-efficiency of ∼24% and ∼30% over CA-TMES-Search and CA-TMES-Quick, respectively.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {66},
numpages = {22},
keywords = {CPS, VFI-NoC-HMPSoCs, SNS, DAG, heterogeneous, task, energy-efficiency, contention, scheduling, mapping}
}

@article{10.1145/3317572,
author = {Ning, Zhaolong and Dong, Peiran and Wang, Xiaojie and Rodrigues, Joel J. P. C. and Xia, Feng},
title = {Deep Reinforcement Learning for Vehicular Edge Computing: An Intelligent Offloading System},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3317572},
doi = {10.1145/3317572},
abstract = {The development of smart vehicles brings drivers and passengers a comfortable and safe environment. Various emerging applications are promising to enrich users’ traveling experiences and daily life. However, how to execute computing-intensive applications on resource-constrained vehicles still faces huge challenges. In this article, we construct an intelligent offloading system for vehicular edge computing by leveraging deep reinforcement learning. First, both the communication and computation states are modelled by finite Markov chains. Moreover, the task scheduling and resource allocation strategy is formulated as a joint optimization problem to maximize users’ Quality of Experience (QoE). Due to its complexity, the original problem is further divided into two sub-optimization problems. A two-sided matching scheme and a deep reinforcement learning approach are developed to schedule offloading requests and allocate network resources, respectively. Performance evaluations illustrate the effectiveness and superiority of our constructed system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {60},
numpages = {24},
keywords = {edge computing, Vehicular system, deep reinforcement learning, intelligent offloading}
}

@article{10.1145/3335676,
author = {Ling, Zhaolong and Yu, Kui and Wang, Hao and Liu, Lin and Ding, Wei and Wu, Xindong},
title = {BAMB: A Balanced Markov Blanket Discovery Approach to Feature Selection},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3335676},
doi = {10.1145/3335676},
abstract = {The discovery of Markov blanket (MB) for feature selection has attracted much attention in recent years, since the MB of the class attribute is the optimal feature subset for feature selection. However, almost all existing MB discovery algorithms focus on either improving computational efficiency or boosting learning accuracy, instead of both. In this article, we propose a novel MB discovery algorithm for balancing efficiency and accuracy, called <underline>BA</underline>lanced <underline>M</underline>arkov <underline>B</underline>lanket (BAMB) discovery. To achieve this goal, given a class attribute of interest, BAMB finds candidate PC (parents and children) and spouses and removes false positives from the candidate MB set in one go. Specifically, once a feature is successfully added to the current PC set, BAMB finds the spouses with regard to this feature, then uses the updated PC and the spouse set to remove false positives from the current MB set. This makes the PC and spouses of the target as small as possible and thus achieves a trade-off between computational efficiency and learning accuracy. In the experiments, we first compare BAMB with 8 state-of-the-art MB discovery algorithms on 7 benchmark Bayesian networks, then we use 10 real-world datasets and compare BAMB with 12 feature selection algorithms, including 8 state-of-the-art MB discovery algorithms and 4 other well-established feature selection methods. On prediction accuracy, BAMB outperforms 12 feature selection algorithms compared. On computational efficiency, BAMB is close to the IAMB algorithm while it is much faster than the remaining seven MB discovery algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {52},
numpages = {25},
keywords = {classification, Markov blanket, feature selection, Bayesian network}
}

@article{10.1145/3351342,
author = {Strobl, Eric V. and Spirtes, Peter L. and Visweswaran, Shyam},
title = {Estimating and Controlling the False Discovery Rate of the PC Algorithm Using Edge-Specific P-Values},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3351342},
doi = {10.1145/3351342},
abstract = {Many causal discovery algorithms infer graphical structure from observational data. The PC algorithm in particular estimates a completed partially directed acyclic graph (CPDAG), or an acyclic graph containing directed edges identifiable with conditional independence testing. However, few groups have investigated strategies for estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In this article, we introduce PC with p-values (PC-p), a fast algorithm that robustly computes edge-specific p-values and then estimates and controls the FDR across the edges. PC-p specifically uses the p-values returned by many conditional independence (CI) tests to upper bound the p-values of more complex edge-specific hypothesis tests. The algorithm then estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm also help PC-p accurately compute the upper bounds despite non-zero Type II error rates. Experiments show that PC-p yields more accurate FDR estimation and control across the edges in a variety of CPDAGs compared to alternative methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {46},
numpages = {37},
keywords = {Bayesian network, false discovery rate, PC algorithm, directed acyclic graph, causal inference}
}

@article{10.1145/3340268,
author = {Zhang, Yongshan and Wu, Jia and Zhou, Chuan and Cai, Zhihua and Yang, Jian and Yu, Philip S.},
title = {Multi-View Fusion with Extreme Learning Machine for Clustering},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3340268},
doi = {10.1145/3340268},
abstract = {Unlabeled, multi-view data presents a considerable challenge in many real-world data analysis tasks. These data are worth exploring because they often contain complementary information that improves the quality of the analysis results. Clustering with multi-view data is a particularly challenging problem as revealing the complex data structures between many feature spaces demands discriminative features that are specific to the task and, when too few of these features are present, performance suffers. Extreme learning machines (ELMs) are an emerging form of learning model that have shown an outstanding representation ability and superior performance in a range of different learning tasks. Motivated by the promise of this advancement, we have developed a novel multi-view fusion clustering framework based on an ELM, called MVEC. MVEC learns the embeddings from each view of the data via the ELM network, then constructs a single unified embedding according to the correlations and dependencies between each embedding and automatically weighting the contribution of each. This process exposes the underlying clustering structures embedded within multi-view data with a high degree of accuracy. A simple yet efficient solution is also provided to solve the optimization problem within MVEC. Experiments and comparisons on eight different benchmarks from different domains confirm MVEC’s clustering accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {53},
numpages = {23},
keywords = {multi-view embedding, Multi-view clustering, extreme learning machine, unsupervised learning}
}

@article{10.1145/3325708,
author = {Zhang, Hao and Zhou, Shuigeng and Guan, Jihong and Huan, Jun (Luke)},
title = {Measuring Conditional Independence by Independent Residuals for Causal Discovery},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3325708},
doi = {10.1145/3325708},
abstract = {We investigate the relationship between conditional independence (CI) x⫫ y|Z and the independence of two residuals x−E(x|Z)⫫ y−E(y|Z), where x and y are two random variables and Z is a set of random variables. We show that if x, y, and Z are generated by following linear structural equation models and all external influences follow joint Gaussian distribution, then x⫫ y|Z if and only if x−E(x|Z)⫫ y−E(y|Z). That is, the test of x⫫ y|Z can be relaxed to a simpler unconditional independence test of x−E(x|Z)⫫ y−E(y|Z). Furthermore, testing x−E(x|Z)⫫ y−E(y|Z) can be simplified by testing x−E(x|Z)⫫ y or y−E(y|Z)⫫ x. On the other side, if all these external influences follow non-Gaussian distributions and the model satisfies structural faithfulness condition, then we have x⫫ y|Z ⇔ x−E(x|Z)⫫ y−E(y|Z).We apply the results above to the causal discovery problem, where the causal directions are generally determined by a set of V-structures and their consistent propagations, so CI test-based methods can return a set of Markov equivalence classes. We show that in the linear non-Gaussian context, in many cases x−E(x|Z)⫫ z or y−E(y|Z)⫫ z (∀ z∈ Z and Z is a minimal d-separator) is satisfied when x−E(x|Z)⫫ y−E(y|Z), which implies z causes x (or y) if z directly connects to x (or y). Therefore, we conclude that CIs have useful information for distinguishing Markov equivalence classes.In summary, comparing with the existing discretization-based and kernel-based CI testing methods, the proposed method provides a simpler way to measure CI, which needs only one unconditional independence test and two regression operations. When being applied to causal discovery, it can find more causal relationships, which is extensively validated by experiments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {50},
numpages = {19},
keywords = {conditional independence test, independent residual, causal discovery, Causal inference}
}

@article{10.1145/3344257,
author = {Zamani, Hamed and Schedl, Markus and Lamere, Paul and Chen, Ching-Wei},
title = {An Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for Automatic Music Playlist Continuation},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3344257},
doi = {10.1145/3344257},
abstract = {The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the RecSys Challenge, Spotify released a dataset of one million user-generated playlists. Participants could compete in two tracks, i.e., main and creative tracks. Participants in the main track were only allowed to use the provided training set, however, in the creative track, the use of external public sources was permitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team. This article provides an overview of the challenge, including motivation, task definition, dataset description, and evaluation. We further report and analyze the results obtained by the top-performing teams in each track and explore the approaches taken by the winners. We finally summarize our key findings, discuss generalizability of approaches and results to domains other than music, and list the open avenues and possible future directions in the area of automatic playlist continuation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {57},
numpages = {21},
keywords = {challenge, music recommendation systems, evaluation, Recommender systems, benchmark, automatic playlist continuation}
}

@article{10.1145/3322123,
author = {Herd, Benjamin C. and Miles, Simon},
title = {Detecting Causal Relationships in Simulation Models Using Intervention-Based Counterfactual Analysis},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322123},
doi = {10.1145/3322123},
abstract = {Central to explanatory simulation models is their capability to not just show that but also why particular things happen. Explanation is closely related with the detection of causal relationships and is, in a simulation context, typically done by means of controlled experiments. However, for complex simulation models, conventional “blackbox” experiments may be too coarse-grained to cope with spurious relationships. We present an intervention-based causal analysis methodology that exploits the manipulability of computational models, and detects and circumvents spurious effects. The core of the methodology is a formal model that maps basic causal assumptions to causal observations and allows for the identification of combinations of assumptions that have a negative impact on observability. First, experiments indicate that the methodology can successfully deal with notoriously tricky situations involving asymmetric and symmetric overdetermination and detect fine-grained causal relationships between events in the simulation. As illustrated in the article, the methodology can be easily integrated into an existing simulation environment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {47},
numpages = {25},
keywords = {simulation, counterfactuals, Causal analysis}
}

@article{10.1145/3343172,
author = {Pappalardo, Luca and Cintia, Paolo and Ferragina, Paolo and Massucco, Emanuele and Pedreschi, Dino and Giannotti, Fosca},
title = {PlayeRank: Data-Driven Performance Evaluation and Player Ranking in Soccer via a Machine Learning Approach},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3343172},
doi = {10.1145/3343172},
abstract = {The problem of evaluating the performance of soccer players is attracting the interest of many companies and the scientific community, thanks to the availability of massive data capturing all the events generated during a match (e.g., tackles, passes, shots, etc.). Unfortunately, there is no consolidated and widely accepted metric for measuring performance quality in all of its facets. In this article, we design and implement PlayeRank, a data-driven framework that offers a principled multi-dimensional and role-aware evaluation of the performance of soccer players. We build our framework by deploying a massive dataset of soccer-logs and consisting of millions of match events pertaining to four seasons of 18 prominent soccer competitions. By comparing PlayeRank to known algorithms for performance evaluation in soccer, and by exploiting a dataset of players’ evaluations made by professional soccer scouts, we show that PlayeRank significantly outperforms the competitors. We also explore the ratings produced by PlayeRank and discover interesting patterns about the nature of excellent performances and what distinguishes the top players from the others. At the end, we explore some applications of PlayeRank—i.e. searching players and player versatility—showing its flexibility and efficiency, which makes it worth to be used in the design of a scalable platform for soccer analytics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {59},
numpages = {27},
keywords = {data science, ranking, searching, big data, clustering, multi-dimensional analysis, predictive modelling, Sports analytics, football analytics, soccer analytics}
}

@article{10.1145/3313147,
author = {Liu, Yue and Cai, Zheng and Liu, Chunchen and Geng, Zhi},
title = {Local Learning Approaches for Finding Effects of a Specified Cause and Their Causal Paths},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3313147},
doi = {10.1145/3313147},
abstract = {Causal networks are used to describe and to discover causal relationships among variables and data generating mechanisms. There have been many approaches for learning a global causal network of all observed variables. In many applications, we may be interested in finding what are the effects of a specified cause variable and what are the causal paths from the cause variable to its effects. Instead of learning a global causal network, we propose several local learning approaches for finding all effects (or descendants) of the specified cause variable and the causal paths from the cause variable to some effect variable of interest. We discuss the identifiability of the effects and the causal paths from observed data and prior knowledge. For the case that the causal paths are not identifiable, our approaches try to find a path set that contains the causal paths of interest.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {49},
numpages = {15},
keywords = {causes and effects, Causal paths, structural learning, causal networks}
}

@article{10.1145/3342240,
author = {Law, Stephen and Paige, Brooks and Russell, Chris},
title = {Take a Look Around: Using Street View and Satellite Images to Estimate House Prices},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3342240},
doi = {10.1145/3342240},
abstract = {When an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. Some amenities, such as air quality, are measurable while others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. Despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. Two issues have led to this neglect. Not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective.We show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. We propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in London, UK. We make use of traditional housing features such as age, size, and accessibility as well as visual features from Google Street View images and Bing aerial images in estimating the house price model. We find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen London boroughs.We explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract proxy variables for visual desirability of neighborhoods that are both of interest in their own right, and could be used as inputs to other econometric methods. This is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the visual appeal of London streets.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {54},
numpages = {19},
keywords = {Real estate, deep learning, computer vision, hedonic price models, London, convolutional neural network}
}

@article{10.1145/3341557,
author = {Rahmadi, Ridho and Groot, Perry and Heskes, Tom},
title = {Stable Specification Search in Structural Equation Models with Latent Variables},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3341557},
doi = {10.1145/3341557},
abstract = {In our previous study, we introduced stable specification search for cross-sectional data (S3C). It is an exploratory causal method that combines the concept of stability selection and multi-objective optimization to search for stable and parsimonious causal structures across the entire range of model complexities. S3C, however, is designed to model causal relations among observed variables. In this study, we extended S3C to S3C-Latent, to model linear causal relations between latent variables that are measured through observed proxies. We evaluated S3C-Latent on simulated data and compared the results to those of PC-MIMBuild, an extension of the PC algorithm, the state-of-the-art causal discovery method. The comparison shows that S3C-Latent achieved better performance. We also applied S3C-Latent to real-world data of children with attention deficit/hyperactivity disorder and data about measuring mental abilities among pupils. The results are consistent with those of previous studies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {48},
numpages = {23},
keywords = {structural equation model with latent variables, multi-objective evolutionary algorithm, Causal modeling, stability selection, specification search}
}

@article{10.1145/3344211,
author = {Corno, Fulvio and De Russis, Luigi and Monge Roffarello, Alberto},
title = {RecRules: Recommending IF-THEN Rules for End-User Development},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3344211},
doi = {10.1145/3344211},
abstract = {Nowadays, end users can personalize their smart devices and web applications by defining or reusing IF-THEN rules through dedicated End-User Development (EUD) tools. Despite apparent simplicity, such tools present their own set of issues. The emerging and increasing complexity of the Internet of Things, for example, is barely taken into account, and the number of possible combinations between triggers and actions of different smart devices and web applications is continuously growing. Such a large design space makes end-user personalization a complex task for non-programmers, and motivates the need of assisting users in easily discovering and managing rules and functionality, e.g., through recommendation techniques. In this article, we tackle the emerging problem of recommending IF-THEN rules to end users by presenting RecRules, a hybrid and semantic recommendation system. Through a mixed content and collaborative approach, the goal of RecRules is to recommend by functionality: it suggests rules based on their final purposes, thus overcoming details like manufacturers and brands. The algorithm uses a semantic reasoning process to enrich rules with semantic information, with the aim of uncovering hidden connections between rules in terms of shared functionality. Then, it builds a collaborative semantic graph, and it exploits different types of path-based features to train a learning to rank algorithm and compute top-N recommendations. We evaluate RecRules through different experiments on real user data extracted from IFTTT, one of the most popular EUD tools. Results are promising: they show the effectiveness of our approach with respect to other state-of-the-art algorithms and open the way for a new class of recommender systems for EUD that take into account the actual functionality needed by end users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {58},
numpages = {27},
keywords = {Internet of Things, End-user development, trigger-action programming, top-N recommendations, hybrid recommender system}
}

@article{10.1145/3342512,
author = {Braytee, Ali and Liu, Wei and Anaissi, Ali and Kennedy, Paul J.},
title = {Correlated Multi-Label Classification with Incomplete Label Space and Class Imbalance},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3342512},
doi = {10.1145/3342512},
abstract = {Multi-label classification is defined as the problem of identifying the multiple labels or categories of new observations based on labeled training data. Multi-labeled data has several challenges, including class imbalance, label correlation, incomplete multi-label matrices, and noisy and irrelevant features. In this article, we propose an integrated multi-label classification approach with incomplete label space and class imbalance (ML-CIB) for simultaneously training the multi-label classification model and addressing the aforementioned challenges. The model learns a new label matrix and captures new label correlations, because it is difficult to find a complete label vector for each instance in real-world data. We also propose a label regularization to handle the imbalanced multi-labeled issue in the new label, and l1 regularization norm is incorporated in the objective function to select the relevant sparse features. A multi-label feature selection (ML-CIB-FS) method is presented as a variant of the proposed ML-CIB to show the efficacy of the proposed method in selecting the relevant features. ML-CIB is formulated as a constrained objective function. We use the accelerated proximal gradient method to solve the proposed optimisation problem. Last, extensive experiments are conducted on 19 regular-scale and large-scale imbalanced multi-labeled datasets. The promising results show that our method significantly outperforms the state-of-the-art.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {56},
numpages = {26},
keywords = {multi-label feature selection, class imbalance, label correlation, Multi-label classification}
}

@article{10.1145/3342241,
author = {Zhang, Ya-Lin and Zhou, Jun and Zheng, Wenhao and Feng, Ji and Li, Longfei and Liu, Ziqi and Li, Ming and Zhang, Zhiqiang and Chen, Chaochao and Li, Xiaolong and Qi, Yuan (Alan) and Zhou, Zhi-Hua},
title = {Distributed Deep Forest and Its Application to Automatic Detection of Cash-Out Fraud},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3342241},
doi = {10.1145/3342241},
abstract = {Internet companies are facing the need for handling large-scale machine learning applications on a daily basis and distributed implementation of machine learning algorithms which can handle extra-large-scale tasks with great performance is widely needed. Deep forest is a recently proposed deep learning framework which uses tree ensembles as its building blocks and it has achieved highly competitive results on various domains of tasks. However, it has not been tested on extremely large-scale tasks. In this work, based on our parameter server system, we developed the distributed version of deep forest. To meet the need for real-world tasks, many improvements are introduced to the original deep forest model, including MART (Multiple Additive Regression Tree) as base learners for efficiency and effectiveness consideration, the cost-based method for handling prevalent class-imbalanced data, MART based feature selection for high dimension data, and different evaluation metrics for automatically determining the cascade level. We tested the deep forest model on an extra-large-scale task, i.e., automatic detection of cash-out fraud, with more than 100 million training samples. Experimental results showed that the deep forest model has the best performance according to the evaluation metrics from different perspectives even with very little effort for parameter tuning. This model can block fraud transactions in a large amount of money each day. Even compared with the best-deployed model, the deep forest model can additionally bring a significant decrease in economic loss each day.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {55},
numpages = {19},
keywords = {large-scale machine learning, parameter server, Deep forest}
}

@article{10.1145/3309720,
author = {Heckerman, David},
title = {Toward Accounting for Hidden Common Causes When Inferring Cause and Effect from Observational Data},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3309720},
doi = {10.1145/3309720},
abstract = {Hidden common causes make it difficult to infer causal relationships from observational data. Here, we begin an investigation into a new method to account for a hidden common cause that infers its presence from the data. As with other approaches that can account for common causes, this approach is successful only in some cases. We describe such a case taken from the field of genomics, wherein one tries to identify which genomic markers causally influence a trait of interest.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {51},
numpages = {5},
keywords = {linear mixed model, Hidden common cause, genomics}
}

@article{10.1145/3326166,
author = {Cui, Wanqiu and Du, Junping and Wang, Dawei and Yuan, Xunpu and Kou, Feifei and Zhou, Liyan and Zhou, Nan},
title = {Short Text Analysis Based on Dual Semantic Extension and Deep Hashing in Microblog},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326166},
doi = {10.1145/3326166},
abstract = {Short text analysis is a challenging task as far as the sparsity and limitation of semantics. The semantic extension approach learns the meaning of a short text by introducing external knowledge. However, for the randomness of short text descriptions in microblogs, traditional extension methods cannot accurately mine the semantics suitable for the microblog theme. Therefore, we use the prominent and refined hashtag information in microblogs as well as complex social relationships to provide implicit guidance for semantic extension of short text. Specifically, we design a deep hash model based on social and conceptual semantic extension, which consists of dual semantic extension and deep hashing representation. In the extension method, the short text is first conceptualized to achieve the construction of hashtag graph under conceptual space. Then, the associated hashtags are generated by correlation calculation based on the integration of social relationships and concepts to extend the short text. In the deep hash model, we use the semantic hashing model to encode the abundant semantic features and form a compact and meaningful binary encoding. Finally, extensive experiments demonstrate that our method can learn and represent the short texts well by using more meaningful semantic signal. It can effectively enhance and guide the semantic analysis and understanding of short text in microblogs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {38},
numpages = {24},
keywords = {Semantic extension, social and conceptual semantics, conceptual space, deep hash model, hashtag graph}
}

@article{10.1145/3337798,
author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337798},
doi = {10.1145/3337798},
abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {43},
numpages = {25},
keywords = {spatial heterogeneity, Spatial classification, class ambiguity, local models, spatial ensemble}
}

@article{10.1145/3335054,
author = {Tonge, Ashwini and Caragea, Cornelia},
title = {Privacy-Aware Tag Recommendation for Accurate Image Privacy Prediction},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3335054},
doi = {10.1145/3335054},
abstract = {Online images’ tags are very important for indexing, sharing, and searching of images, as well as surfacing images with private or sensitive content, which needs to be protected. Social media sites such as Flickr generate these metadata from user-contributed tags. However, as the tags are at the sole discretion of users, these tags tend to be noisy and incomplete. In this article, we present a privacy-aware approach to automatic image tagging, which aims at improving the quality of user annotations, while also preserving the images’ original privacy sharing patterns. Precisely, we recommend potential tags for each target image by mining privacy-aware tags from the most similar images of the target image, which are obtained from a large collection. Experimental results show that, although the user-input tags compose noise, our privacy-aware approach is able to predict accurate tags that can improve the performance of a downstream application on image privacy prediction and outperforms an existing privacy-oblivious approach to image tagging. The results also show that, even for images that do not have any user tags, our proposed approach can recommend accurate tags. Crowd-sourcing the predicted tags exhibits the quality of our privacy-aware recommended tags. Our code, features, and the dataset used in experiments are available at: https://github.com/ashwinitonge/privacy-aware-tag-rec.git.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {40},
numpages = {28},
keywords = {privacy-aware tags, Social networks, deep learning, tag recommendation, image analysis, image privacy prediction}
}

@article{10.1145/3330138,
author = {Bian, Jiang and Tian, Dayong and Tang, Yuanyan and Tao, Dacheng},
title = {Trajectory Data Classification: A Review},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3330138},
doi = {10.1145/3330138},
abstract = {This article comprehensively surveys the development of trajectory data classification. Considering the critical role of trajectory data classification in modern intelligent systems for surveillance security, abnormal behavior detection, crowd behavior analysis, and traffic control, trajectory data classification has attracted growing attention. According to the availability of manual labels, which is critical to the classification performances, the methods can be classified into three categories, i.e., unsupervised, semi-supervised, and supervised. Furthermore, classification methods are divided into some sub-categories according to what extracted features are used. We provide a holistic understanding and deep insight into three types of trajectory data classification methods and present some promising future directions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {33},
numpages = {34},
keywords = {Trajectory classification, object movement, review, classification algorithms}
}

@article{10.1145/3338123,
author = {Banerjee, Suvadeep and Chatterjee, Abhijit},
title = {ALERA: Accelerated Reinforcement Learning Driven Adaptation to Electro-Mechanical Degradation in Nonlinear Control Systems Using Encoded State Space Error Signatures},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3338123},
doi = {10.1145/3338123},
abstract = {The successful deployment of autonomous real-time systems is contingent on their ability to recover from performance degradation of sensors, actuators, and other electro-mechanical subsystems with low latency. In this article, we introduce ALERA, a novel framework for real-time control law adaptation in nonlinear control systems assisted by system state encodings that generate an error signal when the code properties are violated in the presence of failures. The fundamental contributions of this methodology are twofold—first, we show that the time-domain error signal contains perturbed system parameters’ diagnostic information that can be used for quick control law adaptation to failure conditions and second, this quick adaptation is performed via reinforcement learning algorithms that relearn the control law of the perturbed system from a starting condition dictated by the diagnostic information, thus achieving significantly faster recovery. The fast (up to 80X faster than traditional reinforcement learning paradigms) performance recovery enabled by ALERA is demonstrated on an inverted pendulum balancing problem, a brake-by-wire system, and a self-balancing robot.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {44},
numpages = {25},
keywords = {dependability, real-time systems, control systems, Reinforcement learning}
}

@article{10.1145/3337799,
author = {Thukral, Deepak and Pandey, Adesh and Gupta, Rishabh and Goyal, Vikram and Chakraborty, Tanmoy},
title = {DiffQue: Estimating Relative Difficulty of Questions in Community Question Answering Services},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337799},
doi = {10.1145/3337799},
abstract = {Automatic estimation of relative difficulty of a pair of questions is an important and challenging problem in community question answering (CQA) services. There are limited studies that addressed this problem. Past studies mostly leveraged expertise of users answering the questions and barely considered other properties of CQA services such as metadata of users and posts, temporal information, and textual content. In this article, we propose DiffQue, a novel system that maps this problem to a network-aided edge directionality prediction problem. DiffQue&nbsp;starts by constructing a novel network structure that captures different notions of difficulties among a pair of questions. It then measures the relative difficulty of two questions by predicting the direction of a (virtual) edge connecting these two questions in the network. It leverages features extracted from the network structure, metadata of users/posts, and textual description of questions and answers. Experiments on datasets obtained from two CQA sites (further divided into four datasets) with human annotated ground-truth show that DiffQue&nbsp;outperforms four state-of-the-art methods by a significant margin (28.77% higher F1 score and 28.72% higher AUC than the best baseline). As opposed to the other baselines, (i) DiffQue&nbsp;appropriately responds to the training noise, (ii) DiffQue&nbsp;is capable of adapting multiple domains (CQA datasets), and (iii) DiffQue&nbsp;can efficiently handle the “cold start” problem that may arise due to the lack of information for newly posted questions or newly arrived users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {27},
keywords = {Community question answering, edge directionality prediction, difficulty of questions, time-evolving networks}
}

@article{10.1145/3331450,
author = {He, Suining and Shin, Kang G.},
title = {Spatio-Temporal Adaptive Pricing for Balancing Mobility-on-Demand Networks},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3331450},
doi = {10.1145/3331450},
abstract = {Pricing in mobility-on-demand (MOD) networks, such as Uber, Lyft, and connected taxicabs, is done adaptively by leveraging the price responsiveness of drivers (supplies) and passengers (demands) to achieve such goals as maximizing drivers’ incomes, improving riders’ experience, and sustaining platform operation. Existing pricing policies only respond to short-term demand fluctuations without accurate trip forecast and spatial demand-supply balancing, thus mismatching drivers to riders and resulting in loss of profit.We propose CAPrice, a novel adaptive pricing scheme for urban MOD networks. It uses a new spatio-temporal deep capsule network (STCapsNet) that accurately predicts ride demands and driver supplies with vectorized neuron capsules while accounting for comprehensive spatio-temporal and external factors. Given accurate perception of zone-to-zone traffic flows in a city, CAPrice formulates a joint optimization problem by considering spatial equilibrium to balance the platform, providing drivers and riders/passengers with proactive pricing “signals.” We have conducted an extensive experimental evaluation upon over 4.0\texttimes{} 108 MOD trips (Uber, Didi Chuxing, and connected taxicabs) in New York City, Beijing, and Chengdu, validating the accuracy, effectiveness, and profitability (often 20% ride prediction accuracy and 30% profit improvements over the state-of-the-arts) of CAPrice in managing urban MOD networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {28},
keywords = {deep learning, traffic prediction, adaptive pricing, smart transportation, flow balancing, sharing economy, ride sharing, Mobility-on-demand}
}

@article{10.1145/3326163,
author = {Ao, Xiang and Shi, Haoran and Wang, Jin and Zuo, Luo and Li, Hongwei and He, Qing},
title = {Large-Scale Frequent Episode Mining from Complex Event Sequences with Hierarchies},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326163},
doi = {10.1145/3326163},
abstract = {Frequent Episode Mining (FEM), which aims at mining frequent sub-sequences from a single long event sequence, is one of the essential building blocks for the sequence mining research field. Existing studies about FEM suffer from unsatisfied scalability when faced with complex sequences as it is an NP-complete problem for testing whether an episode occurs in a sequence. In this article, we propose a scalable, distributed framework to support FEM on “big” event sequences. As a rule of thumb, “big” illustrates an event sequence is either very long or with masses of simultaneous events. Meanwhile, the events in this article are arranged in a predefined hierarchy. It derives some abstractive events that can form episodes that may not directly appear in the input sequence. Specifically, we devise an event-centered and hierarchy-aware partitioning strategy to allocate events from different levels of the hierarchy into local processes. We then present an efficient special-purpose algorithm to improve the local mining performance. We also extend our framework to support maximal and closed episode mining in the context of event hierarchy, and to the best of our knowledge, we are the first attempt to define and discover hierarchy-aware maximal and closed episodes. We implement the proposed framework on Apache Spark and conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the efficiency and scalability of the proposed approach and show that we can find practical patterns when taking event hierarchies into account.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {36},
numpages = {26},
keywords = {Frequent episode mining, large-scale sequence mining, hierarchy-aware maximal/closed episode, peak episode miner}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {label propagation, semi-supervised learning, Curriculum learning, multi-modal learning}
}

@article{10.1145/3331449,
author = {Verenich, Ilya and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria and Teinemaa, Irene},
title = {Survey and Cross-Benchmark Comparison of Remaining Time Prediction Methods in Business Process Monitoring},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3331449},
doi = {10.1145/3331449},
abstract = {Predictive business process monitoring methods exploit historical process execution logs to generate predictions about running instances (called cases) of a business process, such as the prediction of the outcome, next activity, or remaining cycle time of a given process case. These insights could be used to support operational managers in taking remedial actions as business processes unfold, e.g., shifting resources from one case onto another to ensure the latter is completed on time. A number of methods to tackle the remaining cycle time prediction problem have been proposed in the literature. However, due to differences in their experimental setup, choice of datasets, evaluation measures, and baselines, the relative merits of each method remain unclear. This article presents a systematic literature review and taxonomy of methods for remaining time prediction in the context of business processes, as well as a cross-benchmark comparison of 16 such methods based on 17 real-life datasets originating from different industry domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {34},
numpages = {34},
keywords = {machine learning, process mining, process performance indicator, predictive monitoring, Business process}
}

@article{10.1145/3326164,
author = {Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet},
title = {Efficient User Guidance for Validating Participatory Sensing Data},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326164},
doi = {10.1145/3326164},
abstract = {Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely <u>G</u>eneralised <u>A</u>uto <u>R</u>egressive <u>C</u>onditional <u>H</u>eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {30},
keywords = {trust management, Participatory sensing, probabilistic database}
}

@article{10.1145/3337967,
author = {Zhao, Guoshuai and Fu, Hao and Song, Ruihua and Sakai, Tetsuya and Chen, Zhongxia and Xie, Xing and Qian, Xueming},
title = {Personalized Reason Generation for Explainable Song Recommendation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337967},
doi = {10.1145/3337967},
abstract = {Personalized recommendation has received a lot of attention as a highly practical research topic. However, existing recommender systems provide the recommendations with a generic statement such as “Customers who bought this item also bought…”. Explainable recommendation, which makes a user aware of why such items are recommended, is in demand. The goal of our research is to make the users feel as if they are receiving recommendations from their friends. To this end, we formulate a new challenging problem called personalized reason generation for explainable recommendation for songs in conversation applications and propose a solution that generates a natural language explanation of the reason for recommending a song to that particular user. For example, if the user is a student, our method can generate an output such as “Campus radio plays this song at noon every day, and I think it sounds wonderful,” which the student may find easy to relate to. In the offline experiments, through manual assessments, the gain of our method is statistically significant on the relevance to songs and personalization to users comparing with baselines. Large-scale online experiments show that our method outperforms manually selected reasons by 8.2% in terms of click-through rate. Evaluation results indicate that our generated reasons are relevant to songs and personalized to users, and they attract users to click the recommendations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {21},
keywords = {recommender system, explainable recommendation, Conversational recommendation, personalization, natural language generation}
}

@article{10.1145/3319403,
author = {Zhuo, Hankz Hankui},
title = {Recognizing Multi-Agent Plans When Action Models and Team Plans Are Both Incomplete},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3319403},
doi = {10.1145/3319403},
abstract = {Multi-Agent Plan Recognition (MAPR) aims to recognize team structures (which are composed of team plans) from the observed team traces (action sequences) of a set of intelligent agents. In this article, we introduce the problem formulation of MAPR based on partially observed team traces, and present a weighted MAX-SAT–based framework to recognize multi-agent plans from partially observed team traces with the help of two types of auxiliary knowledge to help recognize multi-agent plans, i.e., a library of incomplete team plans and a set of incomplete action models. Our framework functions with two phases. We first build a set of hard constraints that encode the correctness property of the team plans, and a set of soft constraints that encode the optimal utility property of team plans based on the input team trace, incomplete team plans, and incomplete action models. After that, we solve all of the constraints using a weighted MAX-SAT solver and convert the solution to a set of team plans that best explain the structure of the observed team trace. We empirically exhibit both effectiveness and efficiency of our framework in benchmark domains from International Planning Competition (IPC).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {30},
numpages = {24},
keywords = {action model, team plan, MAX-SAT, Multi-agent plan recognition}
}

@article{10.1145/3309537,
author = {Wu, Hanrui and Yan, Yuguang and Ye, Yuzhong and Min, Huaqing and Ng, Michael K. and Wu, Qingyao},
title = {Online Heterogeneous Transfer Learning by Knowledge Transition},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3309537},
doi = {10.1145/3309537},
abstract = {In this article, we study the problem of online heterogeneous transfer learning, where the objective is to make predictions for a target data sequence arriving in an online fashion, and some offline labeled instances from a heterogeneous source domain are provided as auxiliary data. The feature spaces of the source and target domains are completely different, thus the source data cannot be used directly to assist the learning task in the target domain. To address this issue, we take advantage of unlabeled co-occurrence instances as intermediate supplementary data to connect the source and target domains, and perform knowledge transition from the source domain into the target domain. We propose a novel online heterogeneous transfer learning algorithm called Online Heterogeneous Knowledge Transition (OHKT) for this purpose. In OHKT, we first seek to generate pseudo labels for the co-occurrence data based on the labeled source data, and then develop an online learning algorithm to classify the target sequence by leveraging the co-occurrence data with pseudo labels. Experimental results on real-world data sets demonstrate the effectiveness and efficiency of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {26},
numpages = {19},
keywords = {Transitive transfer learning, online learning, co-occurrence data, heterogeneous transfer learning}
}

@article{10.1145/3319368,
author = {Zhu, Chunbiao and Zhang, Wenhao and Li, Thomas H. and Liu, Shan and Li, Ge},
title = {Exploiting the Value of the Center-Dark Channel Prior for Salient Object Detection},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3319368},
doi = {10.1145/3319368},
abstract = {Saliency detection aims to detect the most attractive objects in images and is widely used as a foundation for various applications. In this article, we propose a novel salient object detection algorithm for RGB-D images using center-dark channel priors. First, we generate an initial saliency map based on a color saliency map and a depth saliency map of a given RGB-D image. Then, we generate a center-dark channel map based on center saliency and dark channel priors. Finally, we fuse the initial saliency map with the center dark channel map to generate the final saliency map. Extensive evaluations over four benchmark datasets demonstrate that our proposed method performs favorably against most of the state-of-the-art approaches. Besides, we further discuss the application of the proposed algorithm in small target detection and demonstrate the universal value of center-dark channel priors in the field of object detection.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {32},
numpages = {20},
keywords = {Salient object detection, center-dark channel prior}
}

@article{10.1145/3299087,
author = {Yao, Huaxiu and Lian, Defu and Cao, Yi and Wu, Yifan and Zhou, Tao},
title = {Predicting Academic Performance for College Students: A Campus Behavior Perspective},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3299087},
doi = {10.1145/3299087},
abstract = {Detecting abnormal behaviors of students in time and providing personalized intervention and guidance at the early stage is important in educational management. Academic performance prediction is an important building block to enabling this pre-intervention and guidance. Most of the previous studies are based on questionnaire surveys and self-reports, which suffer from small sample size and social desirability bias. In this article, we collect longitudinal behavioral data from the smart cards of 6,597 students and propose three major types of discriminative behavioral factors, diligence, orderliness, and sleep patterns. Empirical analysis demonstrates these behavioral factors are strongly correlated with academic performance. Furthermore, motivated by the social influence theory, we analyze the correlation between each student’s academic performance with his/her behaviorally similar students’. Statistical tests indicate this correlation is significant. Based on these factors, we further build a multi-task predictive framework based on a learning-to-rank algorithm for academic performance prediction. This framework captures inter-semester correlation, inter-major correlation, and integrates student similarity to predict students’ academic performance. The experiments on a large-scale real-world dataset show the effectiveness of our methods for predicting academic performance and the effectiveness of proposed behavioral factors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {24},
numpages = {21},
keywords = {student personality, Campus behavior, academic performance prediction}
}

@article{10.1145/3300198,
author = {Yang, Bailin and Zhang, Luhong and Li, Frederick W. B. and Jiang, Xiaoheng and Deng, Zhigang and Wang, Meng and Xu, Mingliang},
title = {Motion-Aware Compression and Transmission of Mesh Animation Sequences},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3300198},
doi = {10.1145/3300198},
abstract = {With the increasing demand in using 3D mesh data over networks, supporting effective compression and efficient transmission of meshes has caught lots of attention in recent years. This article introduces a novel compression method for 3D mesh animation sequences, supporting user-defined and progressive transmissions over networks. Our motion-aware approach starts with clustering animation frames based on their motion similarities, dividing a mesh animation sequence into fragments of varying lengths. This is done by a novel temporal clustering algorithm, which measures motion similarity based on the curvature and torsion of a space curve formed by corresponding vertices along a series of animation frames. We further segment each cluster based on mesh vertex coherence, representing topological proximity within an object under certain motion. To produce a compact representation, we perform intra-cluster compression based on Graph Fourier Transform (GFT) and Set Partitioning In Hierarchical Trees (SPIHT) coding. Optimized compression results can be achieved by applying GFT due to the proximity in vertex position and motion. We adapt SPIHT to support progressive transmission and design a mechanism to transmit mesh animation sequences with user-defined quality. Experimental results show that our method can obtain a high compression ratio while maintaining a low reconstruction error.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {25},
numpages = {21},
keywords = {3D mesh animation, clustering, compression, graph fourier transform, progressive transmission}
}

@article{10.1145/3319402,
author = {Golpayegani, Fatemeh and Dusparic, Ivana and Clarke, Siobhan},
title = {Using Social Dependence to Enable Neighbourly Behaviour in Open Multi-Agent Systems},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3319402},
doi = {10.1145/3319402},
abstract = {Agents frequently collaborate to achieve a shared goal or to accomplish a task that they cannot do alone. However, collaboration is difficult in open multi-agent systems where agents share constrained resources to achieve both individual and shared goals. In current approaches to collaboration, agents are organised into disjoint groups and social reasoning is used to capture their capabilities when selecting a qualified set of collaborators. These approaches are not useful when agents are in multiple, overlapping groups; depend on each other when using shared resources; have multiple goals to achieve simultaneously; and have to share the overall costs and benefits. In this article, agents use social reasoning to enhance their understanding of other agents’ goals and their dependencies, and self-adaptive techniques to adapt their level of self-interest in a collaborative process, with a view to contributing to lowering shared costs or increasing shared benefits. This model aims at improving the extent to which agents’ goals are met while improving shared resource usage efficiency. For example, in a public transport system where each mode of transport has limited capacity, commuters will be enabled to make choices that avoid over-capacity in different modes, or in a smart energy grid with limited capacity, users can make choices as to when they increase their demand. The model simultaneously helps avoid overloading a shared resource while allowing users to achieve their own goals. The proposed model is evaluated in an open multi-agent system with 100 agents operating in multiple overlapping groups and sharing multiple constrained resources. The impact of agents’ varying levels of social dependencies, mobility, and their groups’ density on their individual and shared goal achievement is analysed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {31},
keywords = {social circles, multi-agent collaboration, social reasoning, Multi-agent systems}
}

@article{10.1145/3319532,
author = {Gou, Jianping and Qiu, Wenmo and Yi, Zhang and Xu, Yong and Mao, Qirong and Zhan, Yongzhao},
title = {A Local Mean Representation-Based <i>K</i>-Nearest Neighbor Classifier},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3319532},
doi = {10.1145/3319532},
abstract = {K-nearest neighbor classification method (KNN), as one of the top 10 algorithms in data mining, is a very simple and yet effective nonparametric technique for pattern recognition. However, due to the selective sensitiveness of the neighborhood size k, the simple majority vote, and the conventional metric measure, the KNN-based classification performance can be easily degraded, especially in the small training sample size cases.&nbsp;In this article, to further improve the classification performance and overcome the main issues in the KNN-based classification, we propose a local mean representation-based k-nearest neighbor classifier (LMRKNN). In the LMRKNN, the categorical k-nearest neighbors of a query sample are first chosen to calculate the corresponding categorical k-local mean vectors, and then the query sample is represented by the linear combination of the categorical k-local mean vectors; finally, the class-specific representation-based distances between the query sample and the categorical k-local mean vectors are adopted to determine the class of the query sample.&nbsp;Extensive experiments on many UCI and KEEL datasets and three popular face databases are carried out by comparing LMRKNN to the state-of-art KNN-based methods. The experimental results demonstrate that the proposed LMRKNN outperforms the related competitive KNN-based methods with more robustness and effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {29},
numpages = {25},
keywords = {representation, pattern recognition, local mean vector, K-nearest neighbor classification}
}

@article{10.1145/3313874,
author = {Li, Zun and Lang, Congyan and Feng, Jiashi and Li, Yidong and Wang, Tao and Feng, Songhe},
title = {Co-Saliency Detection with Graph Matching},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3313874},
doi = {10.1145/3313874},
abstract = {Recently, co-saliency detection, which aims to automatically discover common and salient objects appeared in several relevant images, has attracted increased interest in the computer vision community. In this article, we present a novel graph-matching based model for co-saliency detection in image pairs. A solution of graph matching is proposed to integrate the visual appearance, saliency coherence, and spatial structural continuity for detecting co-saliency collaboratively. Since the saliency and the visual similarity have been seamlessly integrated, such a joint inference schema is able to produce more accurate and reliable results. More concretely, the proposed model first computes the intra-saliency for each image by aggregating multiple saliency cues. The common and salient regions across multiple images are thus discovered via a graph matching procedure. Then, a graph reconstruction scheme is proposed to refine the intra-saliency iteratively. Compared to existing co-saliency detection methods that only utilize visual appearance cues, our proposed model can effectively exploit both visual appearance and structure information to better guide co-saliency detection. Extensive experiments on several challenging image pair databases demonstrate that our model outperforms state-of-the-art baselines significantly.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {22},
numpages = {22},
keywords = {graph reconstruction, image understanding, Co-saliency detection, Computer vision, graph matching model}
}

@article{10.1145/3313778,
author = {Mikhail, Joseph W. and Fossaceca, John M. and Iammartino, Ronald},
title = {A Semi-Boosted Nested Model With Sensitivity-Based Weighted Binarization for Multi-Domain Network Intrusion Detection},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3313778},
doi = {10.1145/3313778},
abstract = {Effective network intrusion detection techniques are required to thwart evolving cybersecurity threats. Historically, traditional enterprise networks have been researched extensively in this regard. However, the cyber threat landscape has grown to include wireless networks. In this article, the authors present a novel model that can be trained on completely different feature sets and applied to two distinct intrusion detection applications: traditional enterprise networks and 802.11 wireless networks. This is the first method that demonstrates superior performance in both aforementioned applications. The model is based on a one-versus-all binary framework comprising multiple nested sub-ensembles. To provide good generalization ability, each sub-ensemble contains a collection of sub-learners, and only a portion of the sub-learners implement boosting. A class weight based on the sensitivity metric (true-positive rate), learned from the training data only, is assigned to the sub-ensembles of each class. The use of pruning to remove sub-learners that do not contribute to or have an adverse effect on overall system performance is investigated as well. The results demonstrate that the proposed system can achieve exceptional performance in applications to both traditional enterprise intrusion detection and 802.11 wireless intrusion detection.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {28},
numpages = {27},
keywords = {802.11 wireless, machine learning, boosting, Intrusion detection, one-versus-all, enterprise network}
}

@article{10.1145/3309993,
author = {Shi, Neng and Tao, Yubo},
title = {CNNs Based Viewpoint Estimation for Volume Visualization},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3309993},
doi = {10.1145/3309993},
abstract = {Viewpoint estimation from 2D rendered images is helpful in understanding how users select viewpoints for volume visualization and guiding users to select better viewpoints based on previous visualizations. In this article, we propose a viewpoint estimation method based on Convolutional Neural Networks (CNNs) for volume visualization. We first design an overfit-resistant image rendering pipeline to generate the training images with accurate viewpoint annotations, and then train a category-specific viewpoint classification network to estimate the viewpoint for the given rendered image. Our method can achieve good performance on images rendered with different transfer functions and rendering parameters in several categories. We apply our model to recover the viewpoints of the rendered images in publications, and show how experts look at volumes. We also introduce a CNN feature-based image similarity measure for similarity voting based viewpoint selection, which can suggest semantically meaningful optimal viewpoints for different volumes and transfer functions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {27},
numpages = {22},
keywords = {Viewpoint estimation, convolutional neural networks, volume visualization}
}

@article{10.1145/3305260,
author = {Sharma, Karishma and Qian, Feng and Jiang, He and Ruchansky, Natali and Zhang, Ming and Liu, Yan},
title = {Combating Fake News: A Survey on Identification and Mitigation Techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3305260},
doi = {10.1145/3305260},
abstract = {The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users’ engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {21},
numpages = {42},
keywords = {misinformation, fake news detection, rumor detection, AI}
}

@article{10.1145/3300199,
author = {Likhyani, Ankita and Bedathur, Srikanta and P., Deepak},
title = {Location-Specific Influence Quantification in Location-Based Social Networks},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3300199},
doi = {10.1145/3300199},
abstract = {Location-based social networks (LBSNs) such as Foursquare offer a platform for users to share and be aware of each other’s physical movements. As a result of such a sharing of check-in information with each other, users can be influenced to visit (or check-in) at the locations visited by their friends. Quantifying such influences in these LBSNs is useful in various settings such as location promotion, personalized recommendations, mobility pattern prediction, and so forth. In this article, we develop a model to quantify the influence specific to a location between a pair of users. Specifically, we develop a framework called LoCaTe, that combines (a) a user mobility model based on kernel density estimates; (b) a model of the semantics of the location using topic models; and (c) a user correlation model that uses an exponential distribution. We further develop LoCaTe+, an advanced model within the same framework where user correlation is quantified using a Mutually Exciting Hawkes Process. We show the applicability of LoCaTe and LoCaTe+ for location promotion and location recommendation tasks using LBSNs. Our models are validated using a long-term crawl of Foursquare data collected between January 2015 and February 2016, as well as other publicly available LBSN datasets. Our experiments demonstrate the efficacy of the LoCaTe framework in capturing location-specific influence between users. We also show that our models improve over state-of-the-art models for the task of location promotion as well as location recommendation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {23},
numpages = {28},
keywords = {Location-based social networks, influence quantification}
}

@article{10.1145/3301274,
author = {Deng, Cheng and Li, Zhao and Gao, Xinbo and Tao, Dacheng},
title = {Deep Multi-Scale Discriminative Networks for Double JPEG Compression Forensics},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3301274},
doi = {10.1145/3301274},
abstract = {As JPEG is the most widely used image format, the importance of tampering detection for JPEG images in blind forensics is self-evident. In this area, extracting effective statistical characteristics from a JPEG image for classification remains a challenge. Effective features are designed manually in traditional methods, suggesting that extensive labor-consuming research and derivation is required. In this article, we propose a novel image tampering detection method based on deep multi-scale discriminative networks (MSD-Nets). The multi-scale module is designed to automatically extract multiple features from the discrete cosine transform (DCT) coefficient histograms of the JPEG image. This module can capture the characteristic information in different scale spaces. In addition, a discriminative module is also utilized to improve the detection effect of the networks in those difficult situations when the first compression quality (QF1) is higher than the second one (QF2). A special network in this module is designed to distinguish the small statistical difference between authentic and tampered regions in these cases. Finally, a probability map can be obtained and the specific tampering area is located using the last classification results. Extensive experiments demonstrate the superiority of our proposed method in both quantitative and qualitative metrics when compared with state-of-the-art approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {20},
keywords = {Blind image forensics, deep neural network, double JPEG compression, tampering detection, multi-scale feature}
}

@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3298981},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {19},
keywords = {GDPR, Federated learning, transfer learning}
}

@article{10.1145/3234464,
author = {Mirsky, Reuth and Gal, Kobi and Stern, Roni and Kalech, Meir},
title = {Goal and Plan Recognition Design for Plan Libraries},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3234464},
doi = {10.1145/3234464},
abstract = {This article provides new techniques for optimizing domain design for goal and plan recognition using plan libraries. We define two new problems: Goal Recognition Design for Plan Libraries (GRD-PL) and Plan Recognition Design (PRD). Solving the GRD-PL helps to infer which goal the agent is trying to achieve, while solving PRD can help to infer how the agent is going to achieve its goal. For each problem, we define a worst-case distinctiveness measure that is an upper bound on the number of observations that are necessary to unambiguously recognize the agent’s goal or plan. This article studies the relationship between these measures, showing that the worst-case distinctiveness of GRD-PL is a lower bound of the worst-case plan distinctiveness of PRD and that they are equal under certain conditions. We provide two complete algorithms for minimizing the worst-case distinctiveness of plan libraries without reducing the agent’s ability to complete its goals: One is a brute-force search over all possible plans and one is a constraint-based search that identifies plans that are most difficult to distinguish in the domain. These algorithms are evaluated in three hierarchical plan recognition settings from the literature. We were able to reduce the worst-case distinctiveness of the domains using our approach, in some cases reaching 100% improvement within a predesignated time window. Our iterative algorithm outperforms the brute-force approach by an order of magnitude in terms of runtime.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {14},
numpages = {23},
keywords = {plan libraries, domain design, Plan recognition, environment design, Conflict Based Search}
}

@article{10.1145/3293318,
author = {Wang, Wei and Zheng, Vincent W. and Yu, Han and Miao, Chunyan},
title = {A Survey of Zero-Shot Learning: Settings, Methods, and Applications},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3293318},
doi = {10.1145/3293318},
abstract = {Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learning works. Third, we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {13},
numpages = {37},
keywords = {Zero-shot learning survey}
}

@article{10.1145/3293319,
author = {Hsieh, Hsun-Ping and Li, Cheng-Te},
title = {Inferring Online Social Ties from Offline Geographical Activities},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3293319},
doi = {10.1145/3293319},
abstract = {As mobile devices are becoming ubiquitous nowadays, the geographical activities and interactions of human beings can be easily recorded and accessed. Each mobile individual can belong to an online social network. Unfortunately, the underlying online social relationships are hidden and only available to service providers. Acquiring the social network of mobile users would enrich lots of mobile applications, such as friend recommendation and energy-saving mobile database management. In this work, we propose to infer online social ties using purely offline geographical activities of users, such as check-in records and spatial meeting events. To tackle the problem, we devise a novel inference framework, O2O-Inf, which consists of two components, Feature Modeling and Link Inference. Feature modeling is to characterize both direct and indirect geographical interactions between nodes from co-location and graph features. Link inference aims to infer the social ties based on a small set of observed social links, and the idea is that pairs of nodes sharing similar geographical behaviors have the same tendency of linkage (i.e., either being friends or non-friends). Experiments conducted on a Gowalla location-based social network and a Meetup event-based social network exhibit a satisfying performance in comparison to state-of-the-art prediction methods under the settings of offline-to-online network inference and geo-link prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {21},
keywords = {location-based services, link prediction, offline geographical activities, Online social ties, network inference}
}

@article{10.1145/3293317,
author = {Wang, Hongjian and Tang, Xianfeng and Kuo, Yu-Hsuan and Kifer, Daniel and Li, Zhenhui},
title = {A Simple Baseline for Travel Time Estimation Using Large-Scale Trip Data},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3293317},
doi = {10.1145/3293317},
abstract = {The increased availability of large-scale trajectory data provides rich information for the study of urban dynamics. For example, New York City Taxi 8 Limousine Commission regularly releases source/destination information of taxi trips, where 173 million taxi trips released for Year 2013 [29]. Such a big dataset provides us potential new perspectives to address the traditional traffic problems. In this article, we study the travel time estimation problem. Instead of following the traditional route-based travel time estimation, we propose to simply use a large amount of taxi trips without using the intermediate trajectory points to estimate the travel time between source and destination. Our experiments show very promising results. The proposed big-data-driven approach significantly outperforms both state-of-the-art route-based method and online map services. Our study indicates that novel simple approaches could be empowered by big data and these approaches could serve as new baselines for some traditional computational problems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {19},
numpages = {22},
keywords = {Travel time estimation, big data, baseline, trajectory data}
}

@article{10.1145/3284174,
author = {Yu, Zeng and Li, Tianrui and Yu, Ning and Pan, Yi and Chen, Hongmei and Liu, Bing},
title = {Reconstruction of Hidden Representation for Robust Feature Extraction},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3284174},
doi = {10.1145/3284174},
abstract = {This article aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretically analyze and summarize the general properties of all algorithms that are based on traditional Auto-Encoders: (1) The reconstruction error of the input cannot be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also cannot be lower than a lower bound. (2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. (3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pretraining methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {18},
numpages = {24},
keywords = {unsupervised learning, reconstruction of hidden representation, Deep architectures, feature representation, auto-encoders}
}

@article{10.1145/3243227,
author = {Zhang, Jason Shuo and Lv, Qin},
title = {Understanding Event Organization at Scale in Event-Based Social Networks},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3243227},
doi = {10.1145/3243227},
abstract = {Understanding real-world event participation behavior has been a subject of active research and can offer valuable insights for event-related recommendation and advertisement. The emergence of event-based social networks (EBSNs), which attracts online users to host/attend offline events, has enabled exciting new research in this domain. However, most existing works focus on understanding or predicting individual users’ event participation behavior or recommending events to individual users. Few studies have addressed the problem of event popularity from the event organizer’s point of view.In this work, we study the latent factors for determining event popularity using large-scale datasets collected from the popular Meetup.com EBSN in five major cities around the world. We analyze and model four contextual factors: spatial factor using location convenience, quality, popularity density, and competitiveness; group factor using group member entropy and loyalty; temporal factor using temporal preference and weekly event patterns; and semantic factor using readability, sentiment, part of speech, and text novelty. In addition, we have developed a group-based social influence propagation network to model group-specific influences on events. By combining the COntextual features and Social Influence NEtwork, our integrated prediction framework COSINE can capture the diverse influential factors of event participation and can be used by event organizers to predict/improve the popularity of their events. Detailed evaluations demonstrate that our COSINE framework achieves high accuracy for event popularity prediction in all five cities with diverse cultures and user event behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {16},
numpages = {23},
keywords = {social influence, Group event organization, user behavior modeling}
}

@article{10.1145/3235026,
author = {Skibski, Oskar and Rahwan, Talal and Michalak, Tomasz P. and Wooldridge, Michael},
title = {Enumerating Connected Subgraphs and Computing the Myerson and Shapley Values in Graph-Restricted Games},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3235026},
doi = {10.1145/3235026},
abstract = {At the heart of multi-agent systems is the ability to cooperate to improve the performance of individual agents and/or the system as a whole. While a widespread assumption in the literature is that such cooperation is essentially unrestricted, in many realistic settings this assumption does not hold. A highly influential approach for modelling such scenarios are graph-restricted games introduced by Myerson [36]. In this approach, agents are represented by nodes in a graph, edges represent communication channels, and a group can generate an arbitrary value only if there exists a direct or indirect communication channel between every pair of agents within the group. Two fundamental solution-concepts that were proposed for such games are the Myerson value and the Shapley value. While an algorithm has been developed to compute the Shapley value in arbitrary graph-restricted games, no such general-purpose algorithm has been developed for the Myerson value to date. With this in mind, we set out to develop for such games a general-purpose algorithm to compute the Myerson value, and a more efficient algorithm to compute the Shapley value. Since the computation of either value involves enumerating all connected induced subgraphs of the game’s underlying graph, we start by developing an algorithm dedicated to this enumeration, and then we show empirically that it is faster than the state of the art in the literature. Finally, we present a sample application of both algorithms, in which we test the Myerson value and the Shapley value as advanced measures of node centrality in networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {15},
numpages = {25},
keywords = {depth-first search, algorithms, Coalitional games, Myerson value}
}

@article{10.1145/3200765,
author = {Xie, Cong and Zhong, Wen and Xu, Wei and Mueller, Klaus},
title = {Visual Analytics of Heterogeneous Data Using Hypergraph Learning},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200765},
doi = {10.1145/3200765},
abstract = {For real-world learning tasks (e.g., classification), graph-based models are commonly used to fuse the information distributed in diverse data sources, which can be heterogeneous, redundant, and incomplete. These models represent the relations in different datasets as pairwise links. However, these links cannot deal with high-order relations which connect multiple objects (e.g., in public health datasets, more than two patient groups admitted by the same hospital in 2014). In this article, we propose a visual analytics approach for the classification on heterogeneous datasets using the hypergraph model. The hypergraph is an extension to traditional graphs in which a hyperedge connects multiple vertices instead of just two. We model various high-order relations in heterogeneous datasets as hyperedges and fuse different datasets with a unified hypergraph structure. We use the hypergraph learning algorithm for predicting missing labels in the datasets. To allow users to inject their domain knowledge into the model-learning process, we augment the traditional learning algorithm in a number of ways. In addition, we also propose a set of visualizations which enable the user to construct the hypergraph structure and the parameters of the learning model interactively during the analysis. We demonstrate the capability of our approach via two real-world cases.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {4},
numpages = {26},
keywords = {data fusion, high-dimensional data, Hypergraph learning}
}

@article{10.1145/3277019,
author = {Cao, Nan and Koch, Steffen and Gotz, David and Wu, Yingcai},
title = {ACM TIST Special Issue on Visual Analytics},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3277019},
doi = {10.1145/3277019},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {1},
numpages = {4}
}

@article{10.1145/3200766,
author = {Chen, Wei and Xia, Jing and Wang, Xumeng and Wang, Yi and Chen, Jun and Chang, Liang},
title = {RelationLines: Visual Reasoning of Egocentric Relations from Heterogeneous Urban Data},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200766},
doi = {10.1145/3200766},
abstract = {The increased accessibility of urban sensor data and the popularity of social network applications is enabling the discovery of crowd mobility and personal communication patterns. However, studying the egocentric relationships of an individual can be very challenging because available data may refer to direct contacts, such as phone calls between individuals, or indirect contacts, such as paired location presence. In this article, we develop methods to integrate three facets extracted from heterogeneous urban data (timelines, calls, and locations) through a progressive visual reasoning and inspection scheme. Our approach uses a detect-and-filter scheme such that, prior to visual refinement and analysis, a coarse detection is performed to extract the target individual and construct the timeline of the target. It then detects spatio-temporal co-occurrences or call-based contacts to develop the egocentric network of the individual. The filtering stage is enhanced with a line-based visual reasoning interface that facilitates a flexible and comprehensive investigation of egocentric relationships and connections in terms of time, space, and social networks. The integrated system, RelationLines, is demonstrated using a dataset that contains taxi GPS data, cell-base mobility data, mobile calling data, microblog data, and point-of-interest (POI) data from a city with millions of citizens. We examine the effectiveness and efficiency of our system with three case studies and user review.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {2},
numpages = {21},
keywords = {egocentric relations, visual reasoning, heterogeneous urban data, Location-based, timeline}
}

@article{10.1145/3230707,
author = {Zhang, Chen and Wang, Hao},
title = {ResumeVis: A Visual Analytics System to Discover Semantic Information in Semi-Structured Resume Data},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230707},
doi = {10.1145/3230707},
abstract = {Massive public resume data emerging on the internet indicates individual-related characteristics in terms of profile and career experiences. Resume Analysis (RA) provides opportunities for many applications, such as recruitment trend predict, talent seeking and evaluation. Existing RA studies either largely rely on the knowledge of domain experts, or leverage classic statistical or data mining models to identify and filter explicit attributes based on pre-defined rules. However, they fail to discover the latent semantic information from semi-structured resume text, i.e., individual career progress trajectory and social-relations, which are otherwise vital to comprehensive understanding of people’s career evolving patterns. Besides, when dealing with large numbers of resumes, how to properly visualize such semantic information to reduce the information load and to support better human cognition is also challenging.To tackle these issues, we propose a visual analytics system called ResumeVis to mine and visualize resume data. First, a text mining-based approach is presented to extract semantic information. Then, a set of visualizations are devised to represent the semantic information in multiple perspectives. Through interactive exploration on ResumeVis performed by domain experts, the following tasks can be accomplished: to trace individual career evolving trajectory; to mine latent social-relations among individuals; and to hold the full picture of massive resumes’ collective mobility. Case studies with over 2,500 government officer resumes demonstrate the effectiveness of our system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {8},
numpages = {25},
keywords = {text visualization, semantic information mining, resume analysis, Visual analytics}
}

@article{10.1145/3230623,
author = {Vogogias, Athanasios and Kennedy, Jessie and Archambault, Daniel and Bach, Benjamin and Smith, V. Anne and Currant, Hannah},
title = {BayesPiles: Visualisation Support for Bayesian Network Structure Learning},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230623},
doi = {10.1145/3230623},
abstract = {We address the problem of exploring, combining, and comparing large collections of scored, directed networks for understanding inferred Bayesian networks used in biology. In this field, heuristic algorithms explore the space of possible network solutions, sampling this space based on algorithm parameters and a network score that encodes the statistical fit to the data. The goal of the analyst is to guide the heuristic search and decide how to determine a final consensus network structure, usually by selecting the top-scoring network or constructing the consensus network from a collection of high-scoring networks. BayesPiles, our visualisation tool, helps with understanding the structure of the solution space and supporting the construction of a final consensus network that is representative of the underlying dataset. BayesPiles builds upon and extends MultiPiles to meet our domain requirements. We developed BayesPiles in conjunction with computational biologists who have used this tool on datasets used in their research. The biologists found our solution provides them with new insights and helps them achieve results that are representative of the underlying data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {5},
numpages = {23},
keywords = {graphs, Visualisation, bioinformatics}
}

@article{10.1145/3200767,
author = {Liang, Haoran and Jiang, Ming and Liang, Ronghua and Zhao, Qi},
title = {CapVis: Toward Better Understanding of Visual-Verbal Saliency Consistency},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200767},
doi = {10.1145/3200767},
abstract = {When looking at an image, humans shift their attention toward interesting regions, making sequences of eye fixations. When describing an image, they also come up with simple sentences that highlight the key elements in the scene. What is the correlation between where people look and what they describe in an image? To investigate this problem intuitively, we develop a visual analytics system, CapVis, to look into visual attention and image captioning, two types of subjective annotations that are relatively task-free and natural. Using these annotations, we propose a word-weighting scheme to extract visual and verbal saliency ranks to compare against each other. In our approach, a number of low-level and semantic-level features relevant to visual-verbal saliency consistency are proposed and visualized for a better understanding of image content. Our method also shows the different ways that a human and a computational model look at and describe images, which provides reliable information for a captioning model. Experiment also shows that the visualized feature can be integrated into a computational model to effectively predict the consistency between the two modalities on an image dataset with both types of annotations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {10},
numpages = {23},
keywords = {visual saliency, Image captioning, visual analytics}
}

@article{10.1145/3200572,
author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
title = {Learning Facial Expressions with 3D Mesh Convolutional Neural Network},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200572},
doi = {10.1145/3200572},
abstract = {Making machines understand human expressions enables various useful applications in human-machine interaction. In this article, we present a novel facial expression recognition approach with 3D Mesh Convolutional Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and optimization scheme. From an RGBD camera, we first reconstruct a 3D face model of a subject with facial expressions and then compute the geometric properties of the surface. Instead of using regular Convolutional Neural Networks (CNNs) to learn intensities of the facial images, we convolve the geometric properties on the surface of the 3D model using 3DMCNN. We design a geodesic distance-based convolution method to overcome the difficulties raised from the irregular sampling of the face surface mesh. We further present interactive visual analytics for the purpose of designing and modifying the networks to analyze the learned features and cluster similar nodes in 3DMCNN. By removing low-activity nodes in the network, the performance of the network is greatly improved. We compare our method with the regular CNN-based method by interactively visualizing each layer of the networks and analyze the effectiveness of our method by studying representative cases. Testing on public datasets, our method achieves a higher recognition accuracy than traditional image-based CNN and other 3D CNNs. The proposed framework, including 3DMCNN and interactive visual analytics of the CNN, can be extended to other applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {7},
numpages = {22},
keywords = {Facial expression analysis, visual analysis, 3D mesh convolutional neural networks}
}

@article{10.1145/3200491,
author = {Xu, Mingliang and Wang, Hua and Chu, Shili and Gan, Yong and Jiang, Xiaoheng and Li, Yafei and Zhou, Bing},
title = {Traffic Simulation and Visual Verification in Smog},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200491},
doi = {10.1145/3200491},
abstract = {Smog causes low visibility on the road and it can impact the safety of traffic. Modeling traffic in smog will have a significant impact on realistic traffic simulations. Most existing traffic models assume that drivers have optimal vision in the simulations, making these simulations are not suitable for modeling smog weather conditions. In this article, we introduce the Smog Full Velocity Difference Model (SMOG-FVDM) for a realistic simulation of traffic in smog weather conditions. In this model, we present a stadia model for drivers in smog conditions. We introduce it into a car-following traffic model using both psychological force and body force concepts, and then we introduce the SMOG-FVDM. Considering that there are lots of parameters in the SMOG-FVDM, we design a visual verification system based on SMOG-FVDM to arrive at an adequate solution which can show visual simulation results under different road scenarios and different degrees of smog by reconciling the parameters. Experimental results show that our model can give a realistic and efficient traffic simulation of smog weather conditions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {3},
numpages = {17},
keywords = {visual verification system, body force, SMOG-FVDM, psychological force, Smog}
}

@article{10.1145/3200490,
author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben},
title = {Visual Interfaces for Recommendation Systems: Finding Similar and Dissimilar Peers},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200490},
doi = {10.1145/3200490},
abstract = {Recommendation applications can guide users in making important life choices by referring to the activities of similar peers. For example, students making academic plans may learn from the data of similar students, while patients and their physicians may explore data from similar patients to select the best treatment. Selecting an appropriate peer group has a strong impact on the value of the guidance that can result from analyzing the peer group data. In this article, we describe a visual interface that helps users review the similarity and differences between a seed record and a group of similar records and refine the selection. We introduce the LikeMeDonuts, Ranking Glyph, and History Heatmap visualizations. The interface was refined through three rounds of formative usability evaluation with 12 target users, and its usefulness was evaluated by a case study with a student review manager using real student data. We describe three analytic workflows observed during use and summarize how users’ input shaped the final design.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {9},
numpages = {23},
keywords = {decision making, multidimensional data visualization, Similarity, temporal visualization, visual analytics, personal record}
}

@article{10.1145/3200489,
author = {Liu, Dongyu and Cui, Weiwei and Jin, Kai and Guo, Yuxiao and Qu, Huamin},
title = {DeepTracker: Visualizing the Training Process of Convolutional Neural Networks},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200489},
doi = {10.1145/3200489},
abstract = {Deep Convolutional Neural Networks (CNNs) have achieved remarkable success in various fields. However, training an excellent CNN is practically a trial-and-error process that consumes a tremendous amount of time and computer resources. To accelerate the training process and reduce the number of trials, experts need to understand what has occurred in the training process and why the resulting CNN behaves as it does. However, current popular training platforms, such as TensorFlow, only provide very little and general information, such as training/validation errors, which is far from enough to serve this purpose. To bridge this gap and help domain experts with their training tasks in a practical environment, we propose a visual analytics system, DeepTracker, to facilitate the exploration of the rich dynamics of CNN training processes and to identify the unusual patterns that are hidden behind the huge amount of information in training log. Specifically, we combine a hierarchical index mechanism and a set of hierarchical small multiples to help experts explore the entire training log from different levels of detail. We also introduce a novel cube-style visualization to reveal the complex correlations among multiple types of heterogeneous training data, including neuron weights, validation images, and training iterations. Three case studies are conducted to demonstrate how DeepTracker provides its users with valuable knowledge in an industry-level CNN training process; namely, in our case, training ResNet-50 on the ImageNet dataset. We show that our method can be easily applied to other state-of-the-art “very deep” CNN models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {6},
numpages = {25},
keywords = {correlation analysis, Deep learning, visual analytics, multiple time series, training process}
}

@article{10.1145/3183347,
author = {Chen, Siming and Chen, Shuai and Wang, Zhenhuang and Liang, Jie and Wu, Yadong and Yuan, Xiaoru},
title = {D-Map+: Interactive Visual Analysis and Exploration of Ego-Centric and Event-Centric Information Diffusion Patterns in Social Media},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3183347},
doi = {10.1145/3183347},
abstract = {Information diffusion analysis is important in social media. In this work, we present a coherent ego-centric and event-centric model to investigate diffusion patterns and user behaviors. Applying the model, we propose Diffusion Map+ (D-Maps+), a novel visualization method to support exploration and analysis of user behaviors and diffusion patterns through a map metaphor. For ego-centric analysis, users who participated in reposting (i.e., resending a message initially posted by others) one central user’s posts (i.e., a series of original tweets) are collected. Event-centric analysis focuses on multiple central users discussing a specific event, with all the people participating and reposting messages about it. Social media users are mapped to a hexagonal grid based on their behavior similarities and in the chronological order of repostings. With the additional interactions and linkings, D-Map+ is capable of providing visual profiling of influential users, describing their social behaviors and analyzing the evolution of significant events in social media. A comprehensive visual analysis system is developed to support interactive exploration with D-Map+. We evaluate our work with real-world social media data and find interesting patterns among users and events. We also perform evaluations including user studies and expert feedback to certify the capabilities of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {11},
numpages = {26},
keywords = {information diffusion, map, Social media}
}

@article{10.1145/3209686,
author = {Wang, Pengyang and Fu, Yanjie and Zhang, Jiawei and Li, Xiaolin and Lin, Dan},
title = {Learning Urban Community Structures: A Collective Embedding Perspective with Periodic Spatial-Temporal Mobility Graphs},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3209686},
doi = {10.1145/3209686},
abstract = {Learning urban community structures refers to the efforts of quantifying, summarizing, and representing an urban community’s (i) static structures, e.g., Point-Of-Interests (POIs) buildings and corresponding geographic allocations, and (ii) dynamic structures, e.g., human mobility patterns among POIs. By learning the community structures, we can better quantitatively represent urban communities and understand their evolutions in the development of cities. This can help us boost commercial activities, enhance public security, foster social interactions, and, ultimately, yield livable, sustainable, and viable environments. However, due to the complex nature of urban systems, it is traditionally challenging to learn the structures of urban communities. To address this problem, in this article, we propose a collective embedding framework to learn the community structure from multiple periodic spatial-temporal graphs of human mobility. Specifically, we first exploit a probabilistic propagation-based approach to create a set of mobility graphs from periodic human mobility records. In these mobility graphs, the static POIs are regarded as vertexes, the dynamic mobility connectivities between POI pairs are regarded as edges, and the edge weights periodically evolve over time. A collective deep auto-encoder method is then developed to collaboratively learn the embeddings of POIs from multiple spatial-temporal mobility graphs. In addition, we develop a Unsupervised Graph based Weighted Aggregation method to align and aggregate the POI embeddings into the representation of the community structures. We apply the proposed embedding framework to two applications (i.e., spotting vibrant communities and predicting housing price return rates) to evaluate the performance of our proposed method. Extensive experimental results on real-world urban communities and human mobility data demonstrate the effectiveness of the proposed collective embedding framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {63},
numpages = {28},
keywords = {community structure, Urban communities, collective embedding, periodic mobility graphs}
}

@article{10.1145/3230708,
author = {Anaissi, Ali and Khoa, Nguyen Lu Dang and Rakotoarivelo, Thierry and Alamdari, Mehrisadat Makki and Wang, Yang},
title = {Adaptive Online One-Class Support Vector Machines with Applications in Structural Health Monitoring},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230708},
doi = {10.1145/3230708},
abstract = {One-class support vector machine (OCSVM) has been widely used in the area of structural health monitoring, where only data from one class (i.e., healthy) are available. Incremental learning of OCSVM is critical for online applications in which huge data streams continuously arrive and the healthy data distribution may vary over time. This article proposes a novel adaptive self-advised online OCSVM that incrementally tunes the kernel parameter and decides whether a model update is required or not. As opposed to existing methods, this novel online algorithm does not rely on any fixed threshold, but it uses the slack variables in the OCSVM to determine which new data points should be included in the training set and trigger a model update. The algorithm also incrementally tunes the kernel parameter of OCSVM automatically based on the spatial locations of the edge and interior samples in the training data with respect to the constructed hyperplane of OCSVM. This new online OCSVM algorithm was extensively evaluated using synthetic data and real data from case studies in structural health monitoring. The results showed that the proposed method significantly improved the classification error rates, was able to assimilate the changes in the positive data distribution over time, and maintained a high damage detection accuracy in all case studies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {64},
numpages = {20},
keywords = {online learning, structural health monitoring, One-class support vector machine, anomaly detection, incremental learning}
}

@article{10.1145/3226111,
author = {Liu, Xiaobai and Xu, Qian and Mu, Yadong and Yang, Jiadi and Lin, Liang and Yan, Shuicheng},
title = {High-Precision Camera Localization in Scenes with Repetitive Patterns},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3226111},
doi = {10.1145/3226111},
abstract = {This article presents a high-precision multi-modal approach for localizing moving cameras with monocular videos, which has wide potentials in many intelligent applications, including robotics, autonomous vehicles, and so on. Existing visual odometry methods often suffer from symmetric or repetitive scene patterns, e.g., windows on buildings or parking stalls. To address this issue, we introduce a robust camera localization method that contributes in two aspects. First, we formulate feature tracking, the critical step of visual odometry, as a hierarchical min-cost network flow optimization task, and we regularize the formula with flow constraints, cross-scale consistencies, and motion heuristics. The proposed regularized formula is capable of adaptively selecting distinctive features or feature combinations, which is more effective than traditional methods that detect and group repetitive patterns in a separate step. Second, we develop a joint formula for integrating dense visual odometry and sparse GPS readings in a common reference coordinate. The fusion process is guided with high-order statistics knowledge to suppress the impacts of noises, clusters, and model drifting. We evaluate the proposed camera localization method on both public video datasets and a newly created dataset that includes scenes full of repetitive patterns. Results with comparisons show that our method can achieve comparable performance to state-of-the-art methods and is particularly effective for addressing repetitive pattern issues.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {66},
numpages = {21},
keywords = {feature matching, Visual odometry, flow optimization}
}

@article{10.1145/3232229,
author = {Du, Bowen and Cui, Yifeng and Fu, Yanjie and Zhong, Runxing and Xiong, Hui},
title = {SmartTransfer: Modeling the Spatiotemporal Dynamics of Passenger Transfers for Crowdedness-Aware Route Recommendations},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3232229},
doi = {10.1145/3232229},
abstract = {In urban transportation systems, transfer stations refer to hubs connecting a variety of bus and subway lines and, thus, are the most important nodes in transportation networks. The pervasive availability of large-scale travel traces of passengers, collected from automated fare collection (AFC) systems, has provided unprecedented opportunities for understanding citywide transfer patterns, which can benefit smart transportation, such as smart route recommendation to avoid crowded lines, and dynamic bus scheduling to enhance transportation efficiency. To this end, in this article, we provide a systematic study of the measurement, patterns, and modeling of spatiotemporal dynamics of passenger transfers. Along this line, we develop a data-driven analytical system for modeling the transfer volumes of each transfer station. More specifically, we first identify and quantify the discriminative patterns of spatiotemporal dynamics of passenger transfers by utilizing heterogeneous sources of transfer related data for each station. Also, we develop a multi-task spatiotemporal learning model for predicting the transfer volumes of a specific station at a specific time period. Moreover, we further leverage the predictive model of passenger transfers to provide crowdedness-aware route recommendations. Finally, we conduct the extensive evaluations with a variety of real-world data. Experimental results demonstrate the effectiveness of our proposed modeling method and its applications for smart transportation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {70},
numpages = {26},
keywords = {transit behavior, Automated fare collection, route recommendation, crowdedness detection, spatiotemporal}
}

@article{10.1145/3231601,
author = {Li, Cheng-Te and Hsu, Chia-Tai and Shan, Man-Kwan},
title = {A Cross-Domain Recommendation Mechanism for Cold-Start Users Based on Partial Least Squares Regression},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3231601},
doi = {10.1145/3231601},
abstract = {Recommender systems are common in e-commerce platforms in recent years. Recommender systems are able to help users find preferential items among a large amount of products so that users’ time is saved and sellers’ profits are increased. Cross-domain recommender systems aim to recommend items based on users’ different tastes across domains. While recommender systems usually suffer from the user cold-start problem that leads to unsatisfying recommendation performance, cross-domain recommendation can remedy such a problem. This article proposes a novel cross-domain recommendation model based on regression analysis, partial least squares regression (PLSR). The proposed recommendation models, PLSR-CrossRec and PLSR-Latent, are able to purely use source-domain ratings to predict the ratings for cold-start users who never rated items in the target domains. Experiments conducted on the Epinions dataset with ten various domains’ rating records demonstrate that PLSR-Latent can outperform several matrix factorization-based competing methods under a variety of cross-domain settings. The time efficiency of PLSR-Latent is also satisfactory.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {67},
numpages = {26},
keywords = {transfer learning, partial least square regression, Cross-domain recommendation, cold start}
}

@article{10.1145/3230706,
author = {Wang, Weiqing and Yin, Hongzhi and Du, Xingzhong and Nguyen, Quoc Viet Hung and Zhou, Xiaofang},
title = {TPM: A Temporal Personalized Model for Spatial Item Recommendation},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230706},
doi = {10.1145/3230706},
abstract = {With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important way of helping users discover interesting locations to increase their engagement with location-based services. The availability of spatial, temporal, and social information in LBSNs offers an unprecedented opportunity to enhance the spatial item recommendation. Many previous works studied spatial and social influences on spatial item recommendation in LBSNs. Due to the strong correlations between a user’s check-in time and the corresponding check-in location, which include the sequential influence and temporal cyclic effect, it is essential for spatial item recommender system to exploit the temporal effect to improve the recommendation accuracy. Leveraging temporal information in spatial item recommendation is, however, very challenging, considering (1) when integrating sequential influences, users’ check-in data in LBSNs has a low sampling rate in both space and time, which renders existing location prediction techniques on GPS trajectories ineffective, and the prediction space is extremely large, with millions of distinct locations as the next prediction target, which impedes the application of classical Markov chain models; (2) there are various temporal cyclic patterns (i.e., daily, weekly, and monthly) in LBSNs, but existing work is limited to one specific pattern; and (3) there is no existing framework that unifies users’ personal interests, temporal cyclic patterns, and the sequential influence of recently visited locations in a principled manner.In light of the above challenges, we propose a Temporal Personalized Model (TPM), which introduces a novel latent variable topic-region to model and fuse sequential influence, cyclic patterns with personal interests in the latent and exponential space. The advantages of modeling the temporal effect at the topic-region level include a significantly reduced prediction space, an effective alleviation of data sparsity, and a direct expression of the semantic meaning of users’ spatial activities. Moreover, we introduce two methods to model the effect of various cyclic patterns. The first method is a time indexing scheme that encodes the effect of various cyclic patterns into a binary code. However, the indexing scheme faces the data sparsity problem in each time slice. To deal with this data sparsity problem, the second method slices the time according to each cyclic pattern separately and explores these patterns in a joint additive model.Furthermore, we design an asymmetric Locality Sensitive Hashing (ALSH) technique to speed up the online top-k recommendation process by extending the traditional LSH. We evaluate the performance of TPM on two real datasets and one large-scale synthetic dataset. The performance of TPM in recommending cold-start items is also evaluated. The results demonstrate a significant improvement in TPM’s ability to recommend spatial items, in terms of both effectiveness and efficiency, compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {61},
numpages = {25},
keywords = {online learning, POI, spatial-temporal modeling, location-based service}
}

@article{10.1145/3229051,
author = {Li, Xuelong and Cui, Guosheng and Dong, Yongsheng},
title = {Discriminative and Orthogonal Subspace Constraints-Based Nonnegative Matrix Factorization},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3229051},
doi = {10.1145/3229051},
abstract = {Nonnegative matrix factorization (NMF) is one widely used feature extraction technology in the tasks of image clustering and image classification. For the former task, various unsupervised NMF methods based on the data distribution structure information have been proposed. While for the latter task, the label information of the dataset is one very important guiding. However, most previous proposed supervised NMF methods emphasis on imposing the discriminant constraints on the coefficient matrix. When dealing with new coming samples, the transpose or the pseudoinverse of the basis matrix is used to project these samples to the low dimension space. In this way, the label influence to the basis matrix is indirect. Although, there are also some methods trying to constrain the basis matrix in NMF framework, either they only restrict within-class samples or impose improper constraint on the basis matrix. To address these problems, in this article a novel NMF framework named discriminative and orthogonal subspace constraints-based nonnegative matrix factorization (DOSNMF) is proposed. In DOSNMF, the discriminative constraints are imposed on the projected subspace instead of the directly learned representation. In this manner, the discriminative information is directly connected with the projected subspace. At the same time, an orthogonal term is incorporated in DOSNMF to adjust the orthogonality of the learned basis matrix, which can ensure the orthogonality of the learned subspace and improve the sparseness of the basis matrix at the same time. This framework can be implemented in two ways. The first way is based on the manifold learning theory. In this way, two graphs, i.e., the intrinsic graph and the penalty graph, are constructed to capture the intra-class structure and the inter-class distinctness. With this design, both the manifold structure information and the discriminative information of the dataset are utilized. For convenience, we name this method as the name of the framework, i.e., DOSNMF. The second way is based on the Fisher’s criterion, we name it Fisher’s criterion-based DOSNMF (FDOSNMF). The objective functions of DOSNMF and FDOSNMF can be easily optimized using multiplicative update (MU) rules. The new methods are tested on five datasets and compared with several supervised and unsupervised variants of NMF. The experimental results reveal the effectiveness of the proposed methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {65},
numpages = {24},
keywords = {Data representation, nonnegative matrix factorization (NMF), discriminative graphs, image classification, fisher’s criterion}
}

@article{10.1145/3232231,
author = {Yang, Longqi and Fang, Chen and Jin, Hailin and Hoffman, Matthew D. and Estrin, Deborah},
title = {Characterizing User Skills from Application Usage Traces with Hierarchical Attention Recurrent Networks},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3232231},
doi = {10.1145/3232231},
abstract = {Predicting users’ proficiencies is a critical component of AI-powered personal assistants. This article introduces a novel approach for the prediction based on users’ diverse, noisy, and passively generated application usage histories. We propose a novel bi-directional recurrent neural network with hierarchical attention mechanism to extract sequential patterns and distinguish informative traces from noise. Our model is able to attend to the most discriminative actions and sessions to make more accurate and directly interpretable predictions while requiring 50\texttimes{} less training data than the state-of-the-art sequential learning approach. We evaluate our model with two large scale datasets collected from 68K Photoshop users: a digital design skill dataset where the user skill is determined by the quality of the end products and a software skill dataset where users self-disclose their software usage skill levels. The empirical results demonstrate our model’s superior performance compared to existing user representation learning techniques that leverage action frequencies and sequential patterns. In addition, we qualitatively illustrate the model’s significant interpretative power. The proposed approach is broadly relevant to applications that generate user time-series analytics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {68},
numpages = {18},
keywords = {recurrent neural network, user modeling, User skill, hierarchical attention}
}

@article{10.1145/3232230,
author = {Wang, Suhang and Aggarwal, Charu and Liu, Huan},
title = {Random-Forest-Inspired Neural Networks},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3232230},
doi = {10.1145/3232230},
abstract = {Neural networks have become very popular in recent years, because of the astonishing success of deep learning in various domains such as image and speech recognition. In many of these domains, specific architectures of neural networks, such as convolutional networks, seem to fit the particular structure of the problem domain very well and can therefore perform in an astonishingly effective way. However, the success of neural networks is not universal across all domains. Indeed, for learning problems without any special structure, or in cases where the data are somewhat limited, neural networks are known not to perform well with respect to traditional machine-learning methods such as random forests. In this article, we show that a carefully designed neural network with random forest structure can have better generalization ability. In fact, this architecture is more powerful than random forests, because the back-propagation algorithm reduces to a more powerful and generalized way of constructing a decision tree. Furthermore, the approach is efficient to train and requires a small constant factor of the number of training examples. This efficiency allows the training of multiple neural networks to improve the generalization accuracy. Experimental results on real-world benchmark datasets demonstrate the effectiveness of the proposed enhancements for classification and regression.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {69},
numpages = {25},
keywords = {random forest, regression, Neural network, classification}
}

@article{10.1145/3230710,
author = {Chin, Wei-Sheng and Yuan, Bo-Wen and Yang, Meng-Yuan and Lin, Chih-Jen},
title = {An Efficient Alternating Newton Method for Learning Factorization Machines},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230710},
doi = {10.1145/3230710},
abstract = {To date, factorization machines (FMs) have emerged as a powerful model in many applications. In this work, we study the training of FM with the logistic loss for binary classification, which is a nonlinear extension of the linear model with the logistic loss (i.e., logistic regression). For the training of large-scale logistic regression, Newton methods have been shown to be an effective approach, but it is difficult to apply such methods to FM because of the nonconvexity. We consider a modification of FM that is multiblock convex and propose an alternating minimization algorithm based on Newton methods. Some novel optimization techniques are introduced to reduce the running time. Our experiments demonstrate that the proposed algorithm is more efficient than stochastic gradient algorithms and coordinate descent methods. The parallelism of our method is also investigated for the acceleration in multithreading environments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {72},
numpages = {31},
keywords = {preconditioned conjugate gradient methods, Newton methods, subsampled Hessian matrix}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {Transfer learning, classification}
}

@article{10.1145/3205453,
author = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Silvestri, Fabrizio and Trani, Salvatore},
title = {X-CLE<span class="smallcaps SmallerCapital">a</span>VER: Learning Ranking Ensembles by Growing and Pruning Trees},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3205453},
doi = {10.1145/3205453},
abstract = {Learning-to-Rank (LtR) solutions are commonly used in large-scale information retrieval systems such as Web search engines, which have to return highly relevant documents in response to user query within fractions of seconds. The most effective LtR algorithms adopt a gradient boosting approach to build additive ensembles of weighted regression trees. Since the required ranking effectiveness is achieved with very large ensembles, the impact on response time and query throughput of these solutions is not negligible. In this article, we propose X-CLEaVER, an iterative meta-algorithm able to build more efficient and effective ranking ensembles. X-CLEaVER interleaves the iterations of a given gradient boosting learning algorithm with pruning and re-weighting phases. First, redundant trees are removed from the given ensemble, then the weights of the remaining trees are fine-tuned by optimizing the desired ranking quality metric. We propose and analyze several pruning strategies and we assess their benefits showing that interleaving pruning and re-weighting phases during learning is more effective than applying a single post-learning optimization step. Experiments conducted using two publicly available LtR datasets show that X-CLEaVER can be successfully exploited on top of several LtR algorithms as it is effective in optimizing the effectiveness of the learnt ensembles, thus obtaining more compact forests that hence are much more efficient at scoring time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {26},
keywords = {efficiency, Learning to rank, pruning}
}

@article{10.1145/3200764,
author = {Rossi, Ryan A. and Ahmed, Nesreen K. and Zhou, Rong and Eldardiry, Hoda},
title = {Interactive Visual Graph Mining and Learning},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200764},
doi = {10.1145/3200764},
abstract = {This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {59},
numpages = {25},
keywords = {interactive relational machine learning, visual graph analytics, node embeddings, interactive graph generation, interactive visual graph mining, direct manipulation, network analysis, higher-order network analysis, Statistical relational learning, interactive graph learning, rapid visual feedback, interactive network visualization, real-time performance, interactive role discovery, link prediction}
}

@article{10.1145/3158675,
author = {Jian, Ling and Li, Jundong and Liu, Huan},
title = {Exploiting Multilabel Information for Noise-Resilient Feature Selection},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3158675},
doi = {10.1145/3158675},
abstract = {In a conventional supervised learning paradigm, each data instance is associated with one single class label. Multilabel learning differs in the way that data instances may belong to multiple concepts simultaneously, which naturally appear in a variety of high impact domains, ranging from bioinformatics and information retrieval to multimedia analysis. It targets leveraging the multiple label information of data instances to build a predictive learning model that can classify unlabeled instances into one or multiple predefined target classes. In multilabel learning, even though each instance is associated with a rich set of class labels, the label information could be noisy and incomplete as the labeling process is both time consuming and labor expensive, leading to potential missing annotations or even erroneous annotations. The existence of noisy and missing labels could negatively affect the performance of underlying learning algorithms. More often than not, multilabeled data often has noisy, irrelevant, and redundant features of high dimensionality. The existence of these uninformative features may also deteriorate the predictive power of the learning model due to the curse of dimensionality. Feature selection, as an effective dimensionality reduction technique, has shown to be powerful in preparing high-dimensional data for numerous data mining and machine-learning tasks. However, a vast majority of existing multilabel feature selection algorithms either boil down to solving multiple single-labeled feature selection problems or directly make use of the imperfect labels to guide the selection of representative features. As a result, they may not be able to obtain discriminative features shared across multiple labels. In this article, to bridge the gap between a rich source of multilabel information and its blemish in practical usage, we propose a novel noise-resilient multilabel informed feature selection framework (MIFS) by exploiting the correlations among different labels. In particular, to reduce the negative effects of imperfect label information in obtaining label correlations, we decompose the multilabel information of data instances into a low-dimensional space and then employ the reduced label representation to guide the feature selection phase via a joint sparse regression framework. Empirical studies on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of the proposed MIFS framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {52},
numpages = {23},
keywords = {noise resilient, Multilabel learning, label correlations, feature selection}
}

@article{10.1145/3200751,
author = {Peng, Xuefeng and Chi, Li-Kai and Luo, Jiebo},
title = {The Effect of Pets on Happiness: A Large-Scale Multi-Factor Analysis Using Social Multimedia},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200751},
doi = {10.1145/3200751},
abstract = {From reducing stress and loneliness, to boosting productivity and overall well-being, pets are believed to play a significant role in people’s daily lives. Many traditional studies have identified that frequent interactions with pets could make individuals become healthier and more optimistic, and ultimately enjoy a happier life. However, most of those studies are not only restricted in scale, but also may carry biases by using subjective self-reports, interviews, and questionnaires as the major approaches. In this article, we leverage large-scale data collected from social media and the state-of-the-art deep learning technologies to study this phenomenon in depth and breadth. Our study includes five major steps: (1) collecting timeline posts from around 20,000 Instagram users; (2) using face detection and recognition on 2 million photos to infer users’ demographics, relationship status, and whether having children, (3) analyzing a user’s degree of happiness based on images and captions via smiling classification and textual sentiment analysis; (4) applying transfer learning techniques to retrain the final layer of the Inception v3 model for pet classification; and (5) analyzing the effects of pets on happiness in terms of multiple factors of user demographics. Our main results have demonstrated the efficacy of our proposed method with many new insights. We believe this method is also applicable to other domains as a scalable, efficient, and effective methodology for modeling and analyzing social behaviors and psychological well-being. In addition, to facilitate the research involving human faces, we also release our dataset of 700K analyzed faces.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {60},
numpages = {15},
keywords = {social media, Happiness analysis, social multimedia, happiness, pet and happiness, user demographics}
}

@article{10.1145/3200692,
author = {Huang, Dingjiang and Yu, Shunchang and Li, Bin and Hoi, Steven C. H. and Zhou, Shuigeng},
title = {Combination Forecasting Reversion Strategy for Online Portfolio Selection},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200692},
doi = {10.1145/3200692},
abstract = {Machine learning and artificial intelligence techniques have been applied to construct online portfolio selection strategies recently. A popular and state-of-the-art family of strategies is to explore the reversion phenomenon through online learning algorithms and statistical prediction models. Despite gaining promising results on some benchmark datasets, these strategies often adopt a single model based on a selection criterion (e.g., breakdown point) for predicting future price. However, such model selection is often unstable and may cause unnecessarily high variability in the final estimation, leading to poor prediction performance in real datasets and thus non-optimal portfolios. To overcome the drawbacks, in this article, we propose to exploit the reversion phenomenon by using combination forecasting estimators and design a novel online portfolio selection strategy, named Combination Forecasting Reversion&nbsp;(CFR), which outputs optimal portfolios based on the improved reversion estimator. We further present two efficient CFR implementations based on online Newton step (ONS) and online gradient descent (OGD) algorithms, respectively, and theoretically analyze their regret bounds, which guarantee that the online CFR model performs as well as the best CFR model in hindsight. We evaluate the proposed algorithms on various real markets with extensive experiments. Empirical results show that CFR can effectively overcome the drawbacks of existing reversion strategies and achieve the state-of-the-art performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {58},
numpages = {22},
keywords = {combination forecasting reversion, combination forecasting estimators, online learning, Portfolio selection, mean reversion}
}

@article{10.1145/3200488,
author = {Peng, Chong and Kang, Zhao and Cai, Shuting and Cheng, Qiang},
title = {Integrate and Conquer: Double-Sided Two-Dimensional <i>k</i>-Means Via Integrating of Projection and Manifold Construction},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3200488},
doi = {10.1145/3200488},
abstract = {In this article, we introduce a novel, general methodology, called integrate and conquer, for simultaneously accomplishing the tasks of feature extraction, manifold construction, and clustering, which is taken to be superior to building a clustering method as a single task. When the proposed novel methodology is used on two-dimensional (2D) data, it naturally induces a new clustering method highly effective on 2D data. Existing clustering algorithms usually need to convert 2D data to vectors in a preprocessing step, which, unfortunately, severely damages 2D spatial information and omits inherent structures and correlations in the original data. The induced new clustering method can overcome the matrix-vectorization-related issues to enhance the clustering performance on 2D matrices. More specifically, the proposed methodology mutually enhances three tasks of finding subspaces, learning manifolds, and constructing data representation in a seamlessly integrated fashion. When used on 2D data, we seek two projection matrices with optimal numbers of directions to project the data into low-rank, noise-mitigated, and the most expressive subspaces, in which manifolds are adaptively updated according to the projections, and new data representation is built with respect to the projected data by accounting for nonlinearity via adaptive manifolds. Consequently, the learned subspaces and manifolds are clean and intrinsic, and the new data representation is discriminative and robust. Extensive experiments have been conducted and the results confirm the effectiveness of the proposed methodology and algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {57},
numpages = {25},
keywords = {unsupervised learning, feature extraction, Clustering, two-dimensional data}
}

@article{10.1145/3183891,
author = {Zde\v{s}ar, Andrej and \v{S}krjanc, Igor},
title = {Optimum Velocity Profile of Multiple Bernstein-B\'{e}Zier Curves Subject to Constraints for Mobile Robots},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3183891},
doi = {10.1145/3183891},
abstract = {This article deals with trajectory planning that is suitable for nonholonomic differentially driven wheeled mobile robots. The path is approximated with a spline that consists of multiple Bernstein-B\'{e}zier curves that are merged together in a way that continuous curvature of the spline is achieved. The article presents the approach for optimization of velocity profile of Bernstein-B\'{e}zier spline subject to velocity and acceleration constraints. For the purpose of optimization, velocity and turning points are introduced. Based on these singularity points, local segments are defined where local velocity profiles are optimized independently of each other. From the locally optimum velocity profiles, the global optimum velocity profile is determined. Since each local velocity profile can be evaluated independently, the algorithm is suitable for concurrent implementation and modification of one part of the curve does not require recalculation of all local velocity profiles. These properties enable efficient implementation of the optimization algorithm. The optimization algorithm is also suitable for the splines that consist of Bernstein-B\'{e}zier curves that have substantially different lengths. The proposed optimization approach was experimentally evaluated and validated in simulation environment and on real mobile robots.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {56},
numpages = {23},
keywords = {Mobile robots, path planning, velocity profile, parametric curves, trajectory optimization}
}

@article{10.1145/3178119,
author = {Shen, Xiaobo and Shen, Fumin and Liu, Li and Yuan, Yun-Hao and Liu, Weiwei and Sun, Quan-Sen},
title = {Multiview Discrete Hashing for Scalable Multimedia Search},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3178119},
doi = {10.1145/3178119},
abstract = {Hashing techniques have recently gained increasing research interest in multimedia studies. Most existing hashing methods only employ single features for hash code learning. Multiview data with each view corresponding to a type of feature generally provides more comprehensive information. How to efficiently integrate multiple views for learning compact hash codes still remains challenging. In this article, we propose a novel unsupervised hashing method, dubbed multiview discrete hashing (MvDH), by effectively exploring multiview data. Specifically, MvDH performs matrix factorization to generate the hash codes as the latent representations shared by multiple views, during which spectral clustering is performed simultaneously. The joint learning of hash codes and cluster labels enables that MvDH can generate more discriminative hash codes, which are optimal for classification. An efficient alternating algorithm is developed to solve the proposed optimization problem with guaranteed convergence and low computational complexity. The binary codes are optimized via the discrete cyclic coordinate descent (DCC) method to reduce the quantization errors. Extensive experimental results on three large-scale benchmark datasets demonstrate the superiority of the proposed method over several state-of-the-art methods in terms of both accuracy and scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {53},
numpages = {21},
keywords = {Hashing, multi-view, multimedia search}
}

@article{10.1145/3178116,
author = {Li, Chen and Cheung, William K. and Liu, Jiming and Ng, Joseph K.},
title = {Automatic Extraction of Behavioral Patterns for Elderly Mobility and Daily Routine Analysis},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3178116},
doi = {10.1145/3178116},
abstract = {The elderly living in smart homes can have their daily movement recorded and analyzed. As different elders can have their own living habits, a methodology that can automatically identify their daily activities and discover their daily routines will be useful for better elderly care and support. In this article, we focus on automatic detection of behavioral patterns from the trajectory data of an individual for activity identification as well as daily routine discovery. The underlying challenges lie in the need to consider longer-range dependency of the sensor triggering events and spatiotemporal variations of the behavioral patterns exhibited by humans. We propose to represent the trajectory data using a behavior-aware flow graph that is a probabilistic finite state automaton with its nodes and edges attributed with some local behavior-aware features. We identify the underlying subflows as the behavioral patterns using the kernel k-means algorithm. Given the identified activities, we propose a novel nominal matrix factorization method under a Bayesian framework with Lasso to extract highly interpretable daily routines. For empirical evaluation, the proposed methodology has been compared with a number of existing methods based on both synthetic and publicly available real smart home datasets with promising results obtained. We also discuss how the proposed unsupervised methodology can be used to support exploratory behavior analysis for elderly care.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {54},
numpages = {26},
keywords = {Nominal matrix factorization, Bayesian inference, probabilistic hierarchical model, routine pattern discovery}
}

@article{10.1145/3178114,
author = {Wang, Jun-Zhe and Huang, Jiun-Long},
title = {On Incremental High Utility Sequential Pattern Mining},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3178114},
doi = {10.1145/3178114},
abstract = {High utility sequential pattern (HUSP) mining is an emerging topic in pattern mining, and only a few algorithms have been proposed to address it. In practice, most sequence databases usually grow over time, and it is inefficient for existing algorithms to mine HUSPs from scratch when databases grow with a small portion of updates. In view of this, we propose the IncUSP-Miner+ algorithm to mine HUSPs incrementally. Specifically, to avoid redundant re-computations, we propose a tighter upper bound of the utility of a sequence, called Tight Sequence Utility (TSU), and then we design a novel data structure, called the candidate pattern tree, to buffer the sequences whose TSU values are greater than or equal to the minimum utility threshold in the original database. Accordingly, to avoid keeping a huge amount of utility information for each sequence, a set of concise utility information is designed to be stored in each tree node. To improve the mining efficiency, several strategies are proposed to reduce the amount of computation for utility update and the scopes of database scans. Moreover, several strategies are also proposed to properly adjust the candidate pattern tree for the support of multiple database updates. Experimental results on some real and synthetic datasets show that IncUSP-Miner+ is able to efficiently mine HUSPs incrementally.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {55},
numpages = {26},
keywords = {incremental mining, incremental high utility sequential pattern mining, High utility sequential pattern mining, utility mining}
}

@article{10.1145/3178115,
author = {Zhang, Zixing and Geiger, J\"{u}rgen and Pohjalainen, Jouni and Mousa, Amr El-Desoky and Jin, Wenyu and Schuller, Bj\"{o}rn},
title = {Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3178115},
doi = {10.1145/3178115},
abstract = {Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge. Data-driven supervised approaches, especially the ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. In the meanwhile, we discuss the pros and cons of these approaches and provide their experimental results on benchmark databases. We expect that this overview can facilitate the development of the robustness of speech recognition systems in acoustic noisy environments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {49},
numpages = {28},
keywords = {multi-channel speech recognition, non-stationary noise, neural networks, deep learning, Robust speech recognition}
}

@article{10.1145/3173458,
author = {Liu, Qiang and Yu, Feng and Wu, Shu and Wang, Liang},
title = {Mining Significant Microblogs for Misinformation Identification: An Attention-Based Approach},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3173458},
doi = {10.1145/3173458},
abstract = {With the rapid growth of social media, massive misinformation is also spreading widely on social media, e.g., Weibo and Twitter, and brings negative effects to human life. Today, automatic misinformation identification has drawn attention from academic and industrial communities. Whereas an event on social media usually consists of multiple microblogs, current methods are mainly constructed based on global statistical features. However, information on social media is full of noise, which should be alleviated. Moreover, most of the microblogs about an event have little contribution to the identification of misinformation, where useful information can be easily overwhelmed by useless information. Thus, it is important to mine significant microblogs for constructing a reliable misinformation identification method. In this article, we propose an attention-based approach for identification of misinformation (AIM). Based on the attention mechanism, AIM can select microblogs with the largest attention values for misinformation identification. The attention mechanism in AIM contains two parts: content attention and dynamic attention. Content attention is the calculated-based textual features of each microblog. Dynamic attention is related to the time interval between the posting time of a microblog and the beginning of the event. To evaluate AIM, we conduct a series of experiments on the Weibo and Twitter datasets, and the experimental results show that the proposed AIM model outperforms the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {50},
numpages = {20},
keywords = {social media, Misinformation identification, significant microblogs, attention model}
}

@article{10.1145/3173457,
author = {Shah, Ankit and Ganesan, Rajesh and Jajodia, Sushil and Cam, Hasan},
title = {Dynamic Optimization of the Level of Operational Effectiveness of a CSOC Under Adverse Conditions},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3173457},
doi = {10.1145/3173457},
abstract = {The analysts at a cybersecurity operations center (CSOC) analyze the alerts that are generated by intrusion detection systems (IDSs). Under normal operating conditions, sufficient numbers of analysts are available to analyze the alert workload. For the purpose of this article, this means that the cybersecurity analysts in each shift can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time and perform their normal tasks in a shift. Normal tasks include analysis time, time to attend training programs, report writing time, personal break time, and time to update the signatures on new patterns in alerts as detected by the IDS. There are several disruptive factors that occur randomly and can adversely impact the normal operating condition of a CSOC, such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decrease the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of the preceding factors is that the alerts wait for a long duration before being analyzed, which impacts the level of operational effectiveness (LOE) of the CSOC. To return the CSOC to normal operating conditions, the manager of a CSOC can take several actions, such as increasing the alert analysis time spent by analysts in a shift by canceling a training program, spending some of his own time to assist the analysts in alert investigation, and calling upon the on-call analyst workforce to boost the service rate of alerts. However, additional resources are limited in quantity over a 14-day work cycle, and the CSOC manager must determine when and how much action to take in the face of uncertainty, which arises from both the intensity and the random occurrences of the disruptive factors. The preceding decision by the CSOC manager is nontrivial and is often made in an ad hoc manner using prior experiences. This work develops a reinforcement learning (RL) model for optimizing the LOE throughout the entire 14-day work cycle of a CSOC in the face of uncertainties due to disruptive events. Results indicate that the RL model is able to assist the CSOC manager with a decision support tool to make better decisions than current practices in determining when and how much resource to allocate when the LOE of a CSOC deviates from the normal operating condition.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {51},
numpages = {20},
keywords = {on-call analysts, allocate resources, reinforcement learning, average time to analyze alerts, analysts, Cybersecurity, resource allocation, level of operational effectiveness, absenteeism in shift, stochastic optimization}
}

@article{10.1145/3080575,
author = {Varakantham, Pradeep and Kumar, Akshat and Lau, Hoong Chuin and Yeoh, William},
title = {Risk-Sensitive Stochastic Orienteering Problems for Trip Optimization in Urban Environments},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3080575},
doi = {10.1145/3080575},
abstract = {Orienteering Problems (OPs) are used to model many routing and trip planning problems. OPs are a variant of the well-known traveling salesman problem where the goal is to compute the highest reward path that includes a subset of vertices and has an overall travel time less than a specified deadline. However, the applicability of OPs is limited due to the assumption of deterministic and static travel times. To that end, Campbell et al. extended OPs to Stochastic OPs (SOPs) to represent uncertain travel times (Campbell et al. 2011). In this article, we make the following key contributions: (1) We extend SOPs to Dynamic SOPs (DSOPs), which allow for time-dependent travel times; (2) we introduce a new objective criterion for SOPs and DSOPs to represent a percentile measure of risk; (3) we provide non-linear optimization formulations along with their linear equivalents for solving the risk-sensitive SOPs and DSOPs; (4) we provide a local search mechanism for solving the risk-sensitive SOPs and DSOPs; and (5) we provide results on existing benchmark problems and a real-world theme park trip planning problem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {25},
keywords = {risk-sensitive optimization, sample average approximation, Orienteering problems}
}

@article{10.1145/3168361,
author = {Liu, Qi and Wu, Runze and Chen, Enhong and Xu, Guandong and Su, Yu and Chen, Zhigang and Hu, Guoping},
title = {Fuzzy Cognitive Diagnosis for Modelling Examinee Performance},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3168361},
doi = {10.1145/3168361},
abstract = {Recent decades have witnessed the rapid growth of educational data mining (EDM), which aims at automatically extracting valuable information from large repositories of data generated by or related to people’s learning activities in educational settings. One of the key EDM tasks is cognitive modelling with examination data, and cognitive modelling tries to profile examinees by discovering their latent knowledge state and cognitive level (e.g. the proficiency of specific skills). However, to the best of our knowledge, the problem of extracting information from both objective and subjective examination problems to achieve more precise and interpretable cognitive analysis remains underexplored. To this end, we propose a fuzzy cognitive diagnosis framework (FuzzyCDF) for examinees’ cognitive modelling with both objective and subjective problems. Specifically, to handle the partially correct responses on subjective problems, we first fuzzify the skill proficiency of examinees. Then we combine fuzzy set theory and educational hypotheses to model the examinees’ mastery on the problems based on their skill proficiency. Finally, we simulate the generation of examination score on each problem by considering slip and guess factors. In this way, the whole diagnosis framework is built. For further comprehensive verification, we apply our FuzzyCDF to three classical cognitive assessment tasks, i.e., predicting examinee performance, slip and guess detection, and cognitive diagnosis visualization. Extensive experiments on three real-world datasets for these assessment tasks prove that FuzzyCDF can reveal the knowledge states and cognitive level of the examinees effectively and interpretatively.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {48},
numpages = {26},
keywords = {graphic model, Cognitive, educational data mining}
}

@article{10.1145/3161607,
author = {Chen, Qin and Hu, Qinmin and Huang, Jimmy Xiangji and He, Liang},
title = {Modeling Queries with Contextual Snippets for Information Retrieval},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3161607},
doi = {10.1145/3161607},
abstract = {Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of single terms, which can generate plenty of irrelevant query terms and decrease retrieval performance. To alleviate this problem, we propose an approach that adapts the PRF-based contextual snippets into a context-aware topic model to enhance query representations. Specifically, instead of selecting a series of independent terms, we make full use of the query contextual information and focus on the snippets with the length of n in the PRF documents. Furthermore, we propose a context-aware topic (CAT) model to mine the topic distributions of the query-relevant snippets, namely, fine contextual snippets. In contrast to the traditional topic models that infer the topics from the whole corpus, we establish a bridge between the snippets and the corresponding PRF documents, which can be used for modeling the topics more precisely and efficiently. Finally, the topic distributions of the fine snippets are used for context-aware and topic-sensitive query representations. To evaluate the performance of our approach, we integrate the obtained queries into a topic-based hybrid retrieval model and conduct extensive experiments on various TREC collections. The experimental results show that our query-modeling approach is more effective in boosting retrieval performance compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {47},
numpages = {26},
keywords = {topic modeling, Contextual snippet, query representation}
}

@article{10.1145/3151957,
author = {Wang, Pengwei and Ji, Lei and Yan, Jun and Dou, Dejing and Silva, Nisansa De and Zhang, Yong and Jin, Lianwen},
title = {Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3151957},
doi = {10.1145/3151957},
abstract = {Question retrieval, which aims to find similar versions of a given question, is playing a pivotal role in various question answering (QA) systems. This task is quite challenging, mainly in regard to five aspects: synonymy, polysemy, word order, question length, and data sparsity. In this article, we propose a unified framework to simultaneously handle these five problems. We use the word combined with corresponding concept information to handle the synonymy problem and the polysemous problem. Concept embedding and word embedding are learned at the same time from both the context-dependent and context-independent views. To handle the word-order problem, we propose a high-level feature-embedded convolutional semantic model to learn question embedding by inputting concept embedding and word embedding. Due to the fact that the lengths of some questions are long, we propose a value-based convolutional attentional method to enhance the proposed high-level feature-embedded convolutional semantic model in learning the key parts of the question and the answer. The proposed high-level feature-embedded convolutional semantic model nicely represents the hierarchical structures of word information and concept information in sentences with their layer-by-layer convolution and pooling. Finally, to resolve data sparsity, we propose using the multi-view learning method to train the attention-based convolutional semantic model on question–answer pairs. To the best of our knowledge, we are the first to propose simultaneously handling the above five problems in question retrieval using one framework. Experiments on three real question-answering datasets show that the proposed framework significantly outperforms the state-of-the-art solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {41},
numpages = {24},
keywords = {Question retrieval, question embedding, value-based convolutional attentional method, concept embedding}
}

@article{10.1145/3161606,
author = {Reyes, Oscar and Ventura, Sebasti\'{a}n},
title = {Evolutionary Strategy to Perform Batch-Mode Active Learning on Multi-Label Data},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3161606},
doi = {10.1145/3161606},
abstract = {Multi-label learning has become an important area of research owing to the increasing number of real-world problems that contain multi-label data. Data labeling is an expensive process that requires expert handling. The annotation of multi-label data is laborious since a human expert needs to consider the presence/absence of each possible label. Consequently, numerous modern multi-label problems may involve a small number of labeled examples and plentiful unlabeled examples simultaneously. Active learning methods allow us to induce better classifiers by selecting the most useful unlabeled data, thus considerably reducing the labeling effort and the cost of training an accurate model. Batch-mode active learning methods focus on selecting a set of unlabeled examples in each iteration in such a way that the selected examples are informative and as diverse as possible. This article presents a strategy to perform batch-mode active learning on multi-label data. The batch-mode active learning is formulated as a multi-objective problem, and it is solved by means of an evolutionary algorithm. Extensive experiments were conducted in a large collection of datasets, and the experimental results confirmed the effectiveness of our proposal for better batch-mode multi-label active learning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {46},
numpages = {26},
keywords = {evolutionary algorithm, multi-objective problem, multi-label learning, Batch-mode active learning}
}

@article{10.1145/3158674,
author = {Zhang, Dingwen and Fu, Huazhu and Han, Junwei and Borji, Ali and Li, Xuelong},
title = {A Review of Co-Saliency Detection Algorithms: Fundamentals, Applications, and Challenges},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3158674},
doi = {10.1145/3158674},
abstract = {Co-saliency detection is a newly emerging and rapidly growing research area in the computer vision community. As a novel branch of visual saliency, co-saliency detection refers to the discovery of common and salient foregrounds from two or more relevant images, and it can be widely used in many computer vision tasks. The existing co-saliency detection algorithms mainly consist of three components: extracting effective features to represent the image regions, exploring the informative cues or factors to characterize co-saliency, and designing effective computational frameworks to formulate co-saliency. Although numerous methods have been developed, the literature is still lacking a deep review and evaluation of co-saliency detection techniques. In this article, we aim at providing a comprehensive review of the fundamentals, challenges, and applications of co-saliency detection. Specifically, we provide an overview of some related computer vision works, review the history of co-saliency detection, summarize and categorize the major algorithms in this research area, discuss some open issues in this area, present the potential applications of co-saliency detection, and finally point out some unsolved challenges and promising future works. We expect this review to be beneficial to both fresh and senior researchers in this field and to give insights to researchers in other related areas regarding the utility of co-saliency detection algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {38},
numpages = {31},
keywords = {image understanding, Computer vision, (Co-)saliency detection}
}

@article{10.1145/3156684,
author = {Lu, Jing and Sahoo, Doyen and Zhao, Peilin and Hoi, Steven C. H.},
title = {Sparse Passive-Aggressive Learning for Bounded Online Kernel Methods},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3156684},
doi = {10.1145/3156684},
abstract = {One critical deficiency of traditional online kernel learning methods is their unbounded and growing number of support vectors in the online learning process, making them inefficient and non-scalable for large-scale applications. Recent studies on scalable online kernel learning have attempted to overcome this shortcoming, e.g., by imposing a constant budget on the number of support vectors. Although they attempt to bound the number of support vectors at each online learning iteration, most of them fail to bound the number of support vectors for the final output hypothesis, which is often obtained by averaging the series of hypotheses over all the iterations. In this article, we propose a novel framework for bounded online kernel methods, named “Sparse Passive-Aggressive (SPA)” learning, which is able to yield a final output kernel-based hypothesis with a bounded number of support vectors. Unlike the common budget maintenance strategy used by many existing budget online kernel learning approaches, the idea of our approach is to attain the bounded number of support vectors using an efficient stochastic sampling strategy that samples an incoming training example as a new support vector with a probability proportional to its loss suffered. We theoretically prove that SPA achieves an optimal mistake bound in expectation, and we empirically show that it outperforms various budget online kernel learning algorithms. Finally, in addition to general online kernel learning tasks, we also apply SPA to derive bounded online multiple-kernel learning algorithms, which can significantly improve the scalability of traditional Online Multiple-Kernel Classification (OMKC) algorithms while achieving satisfactory learning accuracy as compared with the existing unbounded OMKC algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {45},
numpages = {27},
keywords = {online multiple-kernel learning, Online learning, kernel methods}
}

@article{10.1145/3156683,
author = {Kulev, Igor and Pu, Pearl and Faltings, Boi},
title = {A Bayesian Approach to Intervention-Based Clustering},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3156683},
doi = {10.1145/3156683},
abstract = {An important task for intelligent healthcare systems is to predict the effect of a new intervention on individuals. This is especially true for medical treatments. For example, consider patients who do not respond well to a new drug or have adversary reactions. Predicting the likelihood of positive or negative response before trying the drug on the patient can potentially save his or her life. We are therefore interested in identifying distinctive subpopulations that respond differently to a given intervention. For this purpose, we have developed a novel technique, Intervention-based Clustering, based on a Bayesian mixture model. Compared to the baseline techniques, the novelty of our approach lies in its ability to model complex decision boundaries by using soft clustering, thus predicting the effect for individuals more accurately. It can also incorporate prior knowledge, making the method useful even for smaller datasets. We demonstrate how our method works by applying it to both simulated and real data. Results of our evaluation show that our model has strong predictive power and is capable of producing high-quality clusters compared to the baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {44},
numpages = {23},
keywords = {personalized medicine, mixture model, heterogeneous treatment effects, cross-validation, subgroup analysis, randomized controlled trial, Clustering, Bayesian analysis}
}

@article{10.1145/3156682,
author = {Huang, Michael Xuelin and Li, Jiajia and Ngai, Grace and Leong, Hong Va},
title = {Quick Bootstrapping of a Personalized Gaze Model from Real-Use Interactions},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3156682},
doi = {10.1145/3156682},
abstract = {Understanding human visual attention is essential for understanding human cognition, which in turn benefits human--computer interaction. Recent work has demonstrated a Personalized, Auto-Calibrating Eye-tracking (PACE) system, which makes it possible to achieve accurate gaze estimation using only an off-the-shelf webcam by identifying and collecting data implicitly from user interaction events. However, this method is constrained by the need for large amounts of well-annotated data. We thus present fast-PACE, an adaptation to PACE that exploits knowledge from existing data from different users to accelerate the learning speed of the personalized model. The result is an adaptive, data-driven approach that continuously “learns” its user and recalibrates, adapts, and improves with additional usage by a user. Experimental evaluations of fast-PACE demonstrate its competitive accuracy in iris localization, validity of alignment identification between gaze and interactions, and effectiveness of gaze transfer. In general, fast-PACE achieves an initial visual error of 3.98 degrees and then steadily improves to 2.52 degrees given incremental interaction-informed data. Our performance is comparable to state-of-the-art, but without the need for explicit training or calibration. Our technique addresses the data quality and quantity problems. It therefore has the potential to enable comprehensive gaze-aware applications in the wild.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {43},
numpages = {25},
keywords = {data validation, Gaze estimation, implicit modeling, gaze-interaction alignment, gaze transfer learning}
}

@article{10.1145/3152875,
author = {Khan, Naimul Mefraz and Ksantini, Riadh and Guan, Ling},
title = {A Novel Image-Centric Approach Toward Direct Volume Rendering},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3152875},
doi = {10.1145/3152875},
abstract = {Transfer function (TF) generation is a fundamental problem in direct volume rendering (DVR). A TF maps voxels to color and opacity values to reveal inner structures. Existing TF tools are complex and unintuitive for the users who are more likely to be medical professionals than computer scientists. In this article, we propose a novel image-centric method for TF generation where instead of complex tools, the user directly manipulates volume data to generate DVR. The user’s work is further simplified by presenting only the most informative volume slices for selection. Based on the selected parts, the voxels are classified using our novel sparse nonparametric support vector machine classifier, which combines both local and near-global distributional information of the training data. The voxel classes are mapped to aesthetically pleasing and distinguishable color and opacity values using harmonic colors. Experimental results on several benchmark datasets and a detailed user survey show the effectiveness of the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {42},
numpages = {18},
keywords = {intelligent systems, support vector machine, Volume visualization, pattern recognition, medical imaging}
}

@article{10.1145/3151937,
author = {Fu, Yanjie and Liu, Junming and Li, Xiaolin and Xiong, Hui},
title = {A Multi-Label Multi-View Learning Framework for In-App Service Usage Analysis},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3151937},
doi = {10.1145/3151937},
abstract = {The service usage analysis, aiming at identifying customers’ messaging behaviors based on encrypted App traffic flows, has become a challenging and emergent task for service providers. Prior literature usually starts from segmenting a traffic sequence into single-usage subsequences, and then classify the subsequences into different usage types. However, they could suffer from inaccurate traffic segmentations and mixed-usage subsequences. To address this challenge, we exploit a multi-label multi-view learning strategy and develop an enhanced framework for in-App usage analytics. Specifically, we first devise an enhanced traffic segmentation method to reduce mixed-usage subsequences. Besides, we develop a multi-label multi-view logistic classification method, which comprises two alignments. The first alignment is to make use of the classification consistency between packet-length view and time-delay view of traffic subsequences and improve classification accuracy. The second alignment is to combine the classification of single-usage subsequence and the post-classification of mixed-usage subsequences into a unified multi-label logistic classification problem. Finally, we present extensive experiments with real-world datasets to demonstrate the effectiveness of our approach. We find that the proposed multi-label multi-view framework can help overcome the pain of mixed-usage subsequences and can be generalized to latent activity analysis in sequential data, beyond in-App usage analytics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {40},
numpages = {24},
keywords = {service usage, Internet traffic, In-App analytics, multi-view, multi-label}
}

@article{10.1145/3141770,
author = {Wang, Bingsheng and Chen, Zhiqian and Boedihardjo, Arnold P. and Lu, Chang-Tien},
title = {Virtual Metering: An Efficient Water Disaggregation Algorithm via Nonintrusive Load Monitoring},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3141770},
doi = {10.1145/3141770},
abstract = {The scarcity of potable water is a critical challenge in many regions around the world. Previous studies have shown that knowledge of device-level water usage can lead to significant conservation. Although there is considerable interest in determining discriminative features via sparse coding for water disaggregation to separate whole-house consumption into its component appliances, existing methods lack a mechanism for fitting coefficient distributions and are thus unable to accurately discriminate parallel devices’ consumption. This article proposes a Bayesian discriminative sparse coding model, referred to as Virtual Metering (VM), for this disaggregation task. Mixture-of-Gammas is employed for the prior distribution of coefficients, contributing two benefits: (i) guaranteeing the coefficients’ sparseness and non-negativity, and (ii) capturing the distribution of active coefficients. The resulting method effectively adapts the bases to aggregated consumption to facilitate discriminative learning in the proposed model, and devices’ shape features are formalized and incorporated into Bayesian sparse coding to direct the learning of basis functions. Compact Gibbs Sampling (CGS) is developed to accelerate the inference process by utilizing the sparse structure of coefficients. The empirical results obtained from applying the new model to large-scale real and synthetic datasets revealed that VM significantly outperformed the benchmark methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {39},
numpages = {30},
keywords = {non-intrusive load monitoring, Bayesian discriminative learning, Mixture-of-Gammas, sparse coding, Computational sustainability, low-sampling-rate disaggregation}
}

@article{10.1145/3108413,
author = {Li, Liangda and Zha, Hongyuan},
title = {Energy Usage Behavior Modeling in Energy Disaggregation via Hawkes Processes},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3108413},
doi = {10.1145/3108413},
abstract = {Energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances, has been proved to be essential in energy conservation research. One powerful cue for breaking down the entire household’s energy consumption is user’s daily energy usage behavior, which has so far received little attention: existing works on energy disaggregation mostly ignored the relationship between the energy usages of various appliances by householders across different time slots. The major challenge in modeling such a relationship in that, with ambiguous appliance usage membership of householders, we find it difficult to appropriately model the influence between appliances, since such influence is determined by human behaviors in energy usage. To address this problem, we propose to model the influence between householders’ energy usage behaviors directly through a novel probabilistic model, which combines topic models with the Hawkes processes. The proposed model simultaneously disaggregates the whole home electricity signal into each component appliance and infers the appliance usage membership of household members and enables those two tasks to mutually benefit each other. Experimental results on both synthetic data and four real-world data sets demonstrate the effectiveness of our model, which outperforms state-of-the-art approaches in not only decomposing the entire consumed energy to each appliance in houses but also the inference of household structures. We further analyze the inferred appliance-householder assignment and the corresponding influence within the appliance usage of each householder and across different householders, which provides insight into appealing human behavior patterns in appliance usage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {36},
numpages = {22},
keywords = {Hawkes process, latent dirichlet allocation, household structure analysis, Energy disaggregation, energy usage behavior}
}

@article{10.1145/3066167,
author = {Muralidhar, Nikhil and Wang, Chen and Self, Nathan and Momtazpour, Marjan and Nakayama, Kiyoshi and Sharma, Ratnesh and Ramakrishnan, Naren},
title = {Illiad: InteLLigent Invariant and Anomaly Detection in Cyber-Physical Systems},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3066167},
doi = {10.1145/3066167},
abstract = {Cyber-physical systems (CPSs) are today ubiquitous in urban environments. Such systems now serve as the backbone to numerous critical infrastructure applications, from smart grids to IoT installations. Scalable and seamless operation of such CPSs requires sophisticated tools for monitoring the time series progression of the system, dynamically tracking relationships, and issuing alerts about anomalies to operators. We present an online monitoring system (illiad) that models the state of the CPS as a function of its relationships between constituent components, using a combination of model-based and data-driven strategies. In addition to accurate inference for state estimation and anomaly tracking, illiad also exploits the underlying network structure of the CPS (wired or wireless) for state estimation purposes. We demonstrate the application of illiad to two diverse settings: a wireless sensor motes application and an IEEE 33-bus microgrid.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {35},
numpages = {20},
keywords = {Urban computing, state-estimation, IoT, big-data, urban informatics}
}

@article{10.1145/3102302,
author = {Kaminka, Gal A. and Fridman, Natalie},
title = {Simulating Urban Pedestrian Crowds of Different Cultures},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3102302},
doi = {10.1145/3102302},
abstract = {Models of crowd dynamics are critically important for urban planning and management. They support analysis, facilitate qualitative and quantitative predictions, and synthesize behaviors for simulations. One promising approach to crowd modeling relies on micro-level agent-based simulations, where the interactions of simulated individual agents in the crowd result in macro-level crowd dynamics which are the object of study. This article reports on an agent-based model of urban pedestrian crowds, where culture is explicitly modeled. We extend an established agent-based social agent model, inspired by social psychology, to account for individual cultural attributes discussed in social science literature. We then embed the model in a simulation of pedestrians and explore the resulting macro-level crowd behaviors, such as pedestrian flow, lane changes rate, and so on. We validate the model by quantitatively comparing the simulation results to the pedestrian dynamics in movies of human crowds in five different countries: Iraq, Israel, England, Canada, and France. We conclude that the model can faithfully replicate urban pedestrians in different cultures. Encouraged by these results, we explore simulations of mixed-culture pedestrian crowds.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {27},
numpages = {27},
keywords = {agent-based simulation, culture modeling, crowd modeling, pedestrian, Social simulation}
}

@article{10.1145/3078853,
author = {Tran, Luan and To, Hien and Fan, Liyue and Shahabi, Cyrus},
title = {A Real-Time Framework for Task Assignment in Hyperlocal Spatial Crowdsourcing},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078853},
doi = {10.1145/3078853},
abstract = {Spatial Crowdsourcing (SC) is a novel platform that engages individuals in the act of collecting various types of spatial data. This method of data collection can significantly reduce cost and turnover time and is particularly useful in urban environmental sensing, where traditional means fail to provide fine-grained field data. In this study, we introduce hyperlocal spatial crowdsourcing, where all workers who are located within the spatiotemporal vicinity of a task are eligible to perform the task (e.g., reporting the precipitation level at their area and time). In this setting, there is often a budget constraint, either for every time period or for the entire campaign, on the number of workers to activate to perform tasks. The challenge is thus to maximize the number of assigned tasks under the budget constraint despite the dynamic arrivals of workers and tasks. We introduce a taxonomy of several problem variants, such as budget-per-time-period vs. budget-per-campaign and binary-utility vs. distance-based-utility. We study the hardness of the task assignment problem in the offline setting and propose online heuristics which exploit the spatial and temporal knowledge acquired over time. Our experiments are conducted with spatial crowdsourcing workloads generated by the SCAWG tool, and extensive results show the effectiveness and efficiency of our proposed solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {37},
numpages = {26},
keywords = {participatory sensing, GIS, Spatial crowdsourcing, budget constraints, crowdsensing, online task assignment}
}

@article{10.1145/3066166,
author = {Zhang, Chao and Lei, Dongming and Yuan, Quan and Zhuang, Honglei and Kaplan, Lance and Wang, Shaowen and Han, Jiawei},
title = {GeoBurst+: Effective and Real-Time Local Event Detection in Geo-Tagged Tweet Streams},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3066166},
doi = {10.1145/3066166},
abstract = {The real-time discovery of local events (e.g., protests, disasters) has been widely recognized as a fundamental socioeconomic task. Recent studies have demonstrated that the geo-tagged tweet stream serves as an unprecedentedly valuable source for local event detection. Nevertheless, how to effectively extract local events from massive geo-tagged tweet streams in real time remains challenging. To bridge the gap, we propose a method for effective and real-time local event detection from geo-tagged tweet streams. Our method, named GeoBurst+, first leverages a novel cross-modal authority measure to identify several pivots in the query window. Such pivots reveal different geo-topical activities and naturally attract similar tweets to form candidate events. GeoBurst+ further summarizes the continuous stream and compares the candidates against the historical summaries to pinpoint truly interesting local events. Better still, as the query window shifts, GeoBurst+ is capable of updating the event list with little time cost, thus achieving continuous monitoring of the stream. We used crowdsourcing to evaluate GeoBurst+ on two million-scale datasets and found it significantly more effective than existing methods while being orders of magnitude faster.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {34},
numpages = {24},
keywords = {social media, Event detection, spatiotemporal data mining, data stream, location-based service, local event}
}

@article{10.1145/3108364,
author = {Ranganath, Suhas and Hu, Xia and Tang, Jiliang and Wang, Suhang and Liu, Huan},
title = {Understanding and Identifying Rhetorical Questions in Social Media},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3108364},
doi = {10.1145/3108364},
abstract = {Social media provides a platform for seeking information from a large user base. Information seeking in social media, however, occurs simultaneously with users expressing their viewpoints by making statements. Rhetorical questions have the form of a question but serve the function of a statement and are an important tool employed by users to express their viewpoints. Therefore, rhetorical questions might mislead platforms assisting information seeking in social media. It becomes difficult to identify rhetorical questions as they are not syntactically different from other questions. In this article, we develop a framework to identify rhetorical questions by modeling some motivations of the users to post them. We focus on two motivations of the users drawing from linguistic theories to implicitly convey a message and to modify the strength of a statement previously made. We develop a quantitative framework from these motivations to identify rhetorical questions in social media. We evaluate the framework using two datasets of questions posted on a social media platform Twitter and demonstrate its effectiveness in identifying rhetorical questions. This is the first framework, to the best of our knowledge, to model the possible motivations for posting rhetorical questions to identify them on social media platforms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {22},
keywords = {information seeking, Rhetorical, social media, political campaigns, persuasion, question answering}
}

@article{10.1145/3078845,
author = {Liu, Jie and Liu, Bin and Liu, Yanchi and Chen, Huipeng and Feng, Lina and Xiong, Hui and Huang, Yalou},
title = {Personalized Air Travel Prediction: A Multi-Factor Perspective},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078845},
doi = {10.1145/3078845},
abstract = {Human mobility analysis is one of the most important research problems in the field of urban computing. Existing research mainly focuses on the intra-city ground travel behavior modeling, while the inter-city air travel behavior modeling has been largely ignored. Actually, the inter-city travel analysis can be of equivalent importance and complementary to the intra-city travel analysis. Understanding massive passenger-air-travel behavior delivers intelligence for airlines’ precision marketing and related socioeconomic activities, such as airport planning, emergency management, local transportation planning, and tourism-related businesses. Moreover, it provides opportunities to study the characteristics of cities and the mutual relationships between them. However, modeling and predicting air traveler behavior is challenging due to the complex factors of the market situation and individual characteristics of customers (e.g., airlines’ market share, customer membership, and travelers’ intrinsic interests on destinations). To this end, in this article, we present a systematic study on the personalized air travel prediction problem, namely where a customer will fly to and which airline carrier to fly with, by leveraging real-world anonymized Passenger Name Record (PNR) data. Specifically, we first propose a relational travel topic model, which combines the merits of latent factor model with a neighborhood-based method, to uncover the personal travel preferences of aviation customers and the latent travel topics of air routes and airline carriers simultaneously. Then we present a multi-factor travel prediction framework, which fuses complex factors of the market situation and individual characteristics of customers, to predict airline customers’ personalized travel demands. Experimental results on two real-world PNR datasets demonstrate the effectiveness of our approach on both travel topic discovery and customer travel prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {30},
numpages = {26},
keywords = {urban computing, travel topic model, latent dirichlet allocation, Air travel demand}
}

@article{10.1145/3149409,
author = {Dong, Xiaowen and Suhara, Yoshihiko and Bozkaya, Bur\c{c}in and Singh, Vivek K. and Lepri, Bruno and Pentland, Alex ‘Sandy’},
title = {Social Bridges in Urban Purchase Behavior},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3149409},
doi = {10.1145/3149409},
abstract = {The understanding and modeling of human purchase behavior in city environment can have important implications in the study of urban economy and in the design and organization of cities. In this article, we study human purchase behavior at the community level and argue that people who live in different communities but work at close-by locations could act as “social bridges” between the respective communities and that they are correlated with similarity in community purchase behavior. We provide empirical evidence by studying millions of credit card transaction records for tens of thousands of individuals in a city environment during a period of three months. More specifically, we show that the number of social bridges between communities is a much stronger indicator of similarity in their purchase behavior than traditionally considered factors such as income and sociodemographic variables. Our findings also suggest that such an effect varies across different merchant categories, that the presence of female customers in social bridges is a stronger indicator compared to that of their male counterparts, and that there seems to be a geographical constraint for this effect, all of which may have implications in the studies of urban economy and data-driven urban planning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {33},
numpages = {29},
keywords = {Purchase behavior, social bridge, credit card transaction, physical environment}
}

@article{10.1145/3106774,
author = {Pellungrini, Roberto and Pappalardo, Luca and Pratesi, Francesca and Monreale, Anna},
title = {A Data Mining Approach to Assess Privacy Risk in Human Mobility Data},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106774},
doi = {10.1145/3106774},
abstract = {Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {31},
numpages = {27},
keywords = {Human mobility, data mining, privacy}
}

@article{10.1145/3092692,
author = {Zhang, Desheng and He, Tian and Zhang, Fan},
title = {Real-Time Human Mobility Modeling with Multi-View Learning},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3092692},
doi = {10.1145/3092692},
abstract = {Real-time human mobility modeling is essential to various urban applications. To model such human mobility, numerous data-driven techniques have been proposed. However, existing techniques are mostly driven by data from a single view, for example, a transportation view or a cellphone view, which leads to over-fitting of these single-view models. To address this issue, we propose a human mobility modeling technique based on a generic multi-view learning framework called coMobile. In coMobile, we first improve the performance of single-view models based on tensor decomposition with correlated contexts, and then we integrate these improved single-view models together for multi-view learning to iteratively obtain mutually reinforced knowledge for real-time human mobility at urban scale. We implement coMobile based on an extremely large dataset in the Chinese city Shenzhen, including data about taxi, bus, and subway passengers along with cellphone users, capturing more than 27 thousand vehicles and 10 million urban residents. The evaluation results show that our approach outperforms a single-view model by 51% on average. More importantly, we design a novel application where urban taxis are dispatched based on unaccounted mobility demand inferred by coMobile.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {22},
numpages = {25},
keywords = {model integration, mobility model, Smart cities}
}

@article{10.1145/3078851,
author = {Zhang, Yingjie and Li, Beibei and Hong, Jason},
title = {Using Online Geotagged and Crowdsourced Data to Understand Human Offline Behavior in the City: An Economic Perspective},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078851},
doi = {10.1145/3078851},
abstract = {The pervasiveness of mobile technologies today has facilitated the creation of massive online crowdsourced and geotagged data from individual users at different locations in a city. Such ubiquitous user-generated data allow us to study the social and behavioral trajectories of individuals across both digital and physical environments. This information, combined with traditional economic and behavioral indicators in the city (e.g., store purchases, restaurant visits, parking), can help us better understand human behavior and interactions with cities. In this study, we take an economic perspective and focus on understanding human economic behavior in the city by examining the performance of local businesses based on the values learned from crowsourced and geotagged data. Specifically, we extract multiple traffic and human mobility features from publicly available data source geomapping and geo-social-tagging techniques and examine the effects of both static and dynamic features on booking volume of local restaurants. Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3,187 restaurants in New York City from November 2013 to March 2014. Our results suggest that foot traffic can increase local popularity and business performance, while mobility and traffic from automobiles may hurt local businesses, especially the well-established chains and high-end restaurants. We also find that, on average, one or more street closure (caused by events or construction projects) nearby leads to a 4.7% decrease in the probability of a restaurant being fully booked during the dinner peak. Our study demonstrates the potential to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast, cheap, accurate, and meaningful.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {32},
numpages = {24},
keywords = {econometric analysis, econometrics, location-based service, mobility analytic, Geotagged social media, city demand, crowdsourced user behavior}
}

@article{10.1145/3078849,
author = {Wang, Pengfei and Liu, Guannan and Fu, Yanjie and Zhou, Yuanchun and Li, Jianhui},
title = {Spotting Trip Purposes from Taxi Trajectories: A General Probabilistic Model},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078849},
doi = {10.1145/3078849},
abstract = {What is the purpose of a trip? What are the unique human mobility patterns and spatial contexts in or near the pickup points and delivery points of trajectories for a specific trip purpose? Many prior studies have modeled human mobility patterns in urban regions; however, these analytics mainly focus on interpreting the semantic meanings of geographic topics at an aggregate level. Given the lack of information about human activities at pick-up and dropoff points, it is challenging to convert the prior studies into effective tools for inferring trip purposes. To address this challenge, in this article, we study large-scale taxi trajectories from an unsupervised perspective in light of the following observations. First, the POI configurations of origin and destination regions closely relate to the urban functionality of these regions and further indicate various human activities. Second, with respect to the functionality of neighborhood environments, trip purposes can be discerned from the transitions between regions with different functionality at particular time periods.Along these lines, we develop a general probabilistic framework for spotting trip purposes from massive taxi GPS trajectories. Specifically, we first augment the origin and destination regions of trajectories by attaching neighborhood POIs. Then, we introduce a latent factor, POI Topic, to represent the mixed functionality of the regions, such that each origin or destination point in the city can be modeled as a mixture over POI Topics. In addition, considering the transitions from origins to destinations at specific time periods, the trip time is generated collaboratively from the pairwise POI Topics at both ends of the O-D pairs, constituting POI Links, and hence the trip purpose can be explained semantically by the POI Links. Finally, we present extensive experiments with the real-world data of New York City to demonstrate the effectiveness of our proposed method for spotting trip purposes, and moreover, the model is validated to perform well in predicting the destinations and trip time among all the baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {29},
numpages = {26},
keywords = {trip purposes, Human mobility, taxi trajectories, probabilistic model}
}

@article{10.1145/3078847,
author = {Cao, Zhiguang and Guo, Hongliang and Zhang, Jie},
title = {A Multiagent-Based Approach for Vehicle Routing by Considering Both Arriving on Time and Total Travel Time},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078847},
doi = {10.1145/3078847},
abstract = {Arriving on time and total travel time are two important properties for vehicle routing. Existing route guidance approaches always consider them independently, because they may conflict with each other. In this article, we develop a semi-decentralized multiagent-based vehicle routing approach where vehicle agents follow the local route guidance by infrastructure agents at each intersection, and infrastructure agents perform the route guidance by solving a route assignment problem. It integrates the two properties by expressing them as two objective terms of the route assignment problem. Regarding arriving on time, it is formulated based on the probability tail model, which aims to maximize the probability of reaching destination before deadline. Regarding total travel time, it is formulated as a weighted quadratic term, which aims to minimize the expected travel time from the current location to the destination based on the potential route assignment. The weight for total travel time is designed to be comparatively large if the deadline is loose. Additionally, we improve the proposed approach in two aspects, including travel time prediction and computational efficiency. Experimental results on real road networks justify its ability to increase the average probability of arriving on time, reduce total travel time, and enhance the overall routing performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {25},
numpages = {21},
keywords = {probability tail model, total travel time, arriving on time, multiagent-based route guidance, Intelligent transportation systems}
}

@article{10.1145/3078842,
author = {Cheng, Shih-Fen and Chen, Cen and Kandappu, Thivya and Lau, Hoong Chuin and Misra, Archan and Jaiman, Nikita and Tandriansyah, Randy and Koh, Desmond},
title = {Scalable Urban Mobile Crowdsourcing: Handling Uncertainty in Worker Movement},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078842},
doi = {10.1145/3078842},
abstract = {In this article, we investigate effective ways of utilizing crowdworkers in providing various urban services. The task recommendation platform that we design can match tasks to crowdworkers based on workers’ historical trajectories and time budget limits, thus making recommendations personal and efficient. One major challenge we manage to address is the handling of crowdworker’s trajectory uncertainties. In this article, we explicitly allow multiple routine routes to be probabilistically associated with each worker. We formulate this problem as an integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structures of the formulation and apply the Lagrangian relaxation technique to scale up computation. Numerical experiments have been performed over the instances generated using the realistic public transit dataset in Singapore. The results show that we can find significantly better solutions than the deterministic formulation, and in most cases we can find solutions that are very close to the theoretical performance limit. To demonstrate the practicality of our approach, we deployed our recommendation engine to a campus-scale field trial, and we demonstrate that workers receiving our recommendations incur fewer detours and complete more tasks, and are more efficient against workers relying on their own planning (25% more for top workers who receive recommendations). This is achieved despite having highly uncertain worker trajectories. We also demonstrate how to further improve the robustness of the system by using a simple multi-coverage mechanism.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {26},
numpages = {24},
keywords = {Mobile crowdsourcing, user behavior, empirical study, uncertainty modeling, spatial crowdsourcing, participatory sensing, context-aware}
}

@article{10.1145/3057730,
author = {Auffenberg, Frederik and Snow, Stephen and Stein, Sebastian and Rogers, Alex},
title = {A Comfort-Based Approach to Smart Heating and Air Conditioning},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057730},
doi = {10.1145/3057730},
abstract = {In this article, we address the interrelated challenges of predicting user comfort and using this to reduce energy consumption in smart heating, ventilation, and air conditioning (HVAC) systems. At present, such systems use simple models of user comfort when deciding on a set-point temperature. Being built using broad population statistics, these models generally fail to represent individual users’ preferences, resulting in poor estimates of the users’ preferred temperatures. To address this issue, we propose the Bayesian Comfort Model (BCM). This personalised thermal comfort model uses a Bayesian network to learn from a user’s feedback, allowing it to adapt to the users’ individual preferences over time. We further propose an alternative to the ASHRAE 7-point scale used to assess user comfort. Using this model, we create an optimal HVAC control algorithm that minimizes energy consumption while preserving user comfort. Through an empirical evaluation based on the ASHRAE RP-884 dataset and data collected in a separate deployment by us, we show that our model is consistently 13.2% to 25.8% more accurate than current models and how using our alternative comfort scale can increase our model’s accuracy. Through simulations we show that using this model, our HVAC control algorithm can reduce energy consumption by 7.3% to 13.5% while decreasing user discomfort by 24.8% simultaneously.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {28},
numpages = {20},
keywords = {computational sustainability, agents, machine learning, User behaviour modeling and learning}
}

@article{10.1145/3154942,
author = {An, Bo and Jennings, Nick and Li, Zhenhui Jessie},
title = {ACM TIST Special Issue on Urban Intelligence},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3154942},
doi = {10.1145/3154942},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {23},
numpages = {4}
}

@article{10.1145/3137114,
author = {Katz, Gilad and Caragea, Cornelia and Shabtai, Asaf},
title = {Vertical Ensemble Co-Training for Text Classification},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3137114},
doi = {10.1145/3137114},
abstract = {High-quality, labeled data is essential for successfully applying machine learning methods to real-world text classification problems. However, in many cases, the amount of labeled data is very small compared to that of the unlabeled, and labeling additional samples could be expensive and time consuming. Co-training algorithms, which make use of unlabeled data to improve classification, have proven to be very effective in such cases. Generally, co-training algorithms work by using two classifiers, trained on two different views of the data, to label large amounts of unlabeled data. Doing so can help minimize the human effort required for labeling new data, as well as improve classification performance. In this article, we propose an ensemble-based co-training approach that uses an ensemble of classifiers from different training iterations to improve labeling accuracy. This approach, which we call vertical ensemble, incurs almost no additional computational cost. Experiments conducted on six textual datasets show a significant improvement of over 45% in AUC compared with the original co-training algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {21},
numpages = {23},
keywords = {Co-training, ensemble, text classification}
}

@article{10.1145/3125645,
author = {Zhang, Yexun and Cai, Wenbin and Wang, Wenquan and Zhang, Ya},
title = {Stopping Criterion for Active Learning with Model Stability},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3125645},
doi = {10.1145/3125645},
abstract = {Active learning selectively labels the most informative instances, aiming to reduce the cost of data annotation. While much effort has been devoted to active sampling functions, relatively limited attention has been paid to when the learning process should stop. In this article, we focus on the stopping criterion of active learning and propose a model stability--based criterion, that is, when a model does not change with inclusion of additional training instances. The challenge lies in how to measure the model change without labeling additional instances and training new models. Inspired by the stochastic gradient update rule, we use the gradient of the loss function at each candidate example to measure its effect on model change. We propose to stop active learning when the model change brought by any of the remaining unlabeled examples is lower than a given threshold. We apply the proposed stopping criterion to two popular classifiers: logistic regression (LR) and support vector machines (SVMs). In addition, we theoretically analyze the stability and generalization ability of the model obtained by our stopping criterion. Substantial experiments on various UCI benchmark datasets and ImageNet datasets have demonstrated that the proposed approach is highly effective.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {19},
numpages = {26},
keywords = {stopping criterion, support vector machines, Active learning, logistic regression, model stability}
}

@article{10.1145/3131671,
author = {Wang, Leye and Zhang, Daqing and Yang, Dingqi and Pathak, Animesh and Chen, Chao and Han, Xiao and Xiong, Haoyi and Wang, Yasha},
title = {SPACE-TA: Cost-Effective Task Allocation Exploiting Intradata and Interdata Correlations in Sparse Crowdsensing},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3131671},
doi = {10.1145/3131671},
abstract = {Data quality and budget are two primary concerns in urban-scale mobile crowdsensing. Traditional research on mobile crowdsensing mainly takes sensing coverage ratio as the data quality metric rather than the overall sensed data error in the target-sensing area. In this article, we propose to leverage spatiotemporal correlations among the sensed data in the target-sensing area to significantly reduce the number of sensing task assignments. In particular, we exploit both intradata correlations within the same type of sensed data and interdata correlations among different types of sensed data in the sensing task. We propose a novel crowdsensing task allocation framework called SPACE-TA (SPArse Cost-Effective Task Allocation), combining compressive sensing, statistical analysis, active learning, and transfer learning, to dynamically select a small set of subareas for sensing in each timeslot (cycle), while inferring the data of unsensed subareas under a probabilistic data quality guarantee. Evaluations on real-life temperature, humidity, air quality, and traffic monitoring datasets verify the effectiveness of SPACE-TA. In the temperature-monitoring task leveraging intradata correlations, SPACE-TA requires data from only 15.5% of the subareas while keeping the inference error below 0.25°C in 95% of the cycles, reducing the number of sensed subareas by 18.0% to 26.5% compared to baselines. When multiple tasks run simultaneously, for example, for temperature and humidity monitoring, SPACE-TA can further reduce ∼10% of the sensed subareas by exploiting interdata correlations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {20},
numpages = {28},
keywords = {task allocation, data quality, Crowdsensing}
}

@article{10.1145/3122802,
author = {Wu, Ou and Mao, Xue and Hu, Weiming},
title = {Iteratively Divide-and-Conquer Learning for Nonlinear Classification and Ranking},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3122802},
doi = {10.1145/3122802},
abstract = {Nonlinear classifiers (i.e., kernel support vector machines (SVMs)) are effective for nonlinear data classification. However, nonlinear classifiers are usually prohibitively expensive when dealing with large nonlinear data. Ensembles of linear classifiers have been proposed to address this inefficiency, which is called the ensemble linear classifiers for nonlinear data problem. In this article, a new iterative learning approach is introduced that involves two steps at each iteration: partitioning the data into clusters according to Gaussian mixture models with local consistency and then training basic classifiers (i.e., linear SVMs) for each cluster. The two divide-and-conquer steps are combined into a graphical model. Meanwhile, with training, each classifier is regarded as a task; clustered multitask learning is employed to capture the relatedness among different tasks and avoid overfitting in each task. In addition, two novel extensions are introduced based on the proposed approach. First, the approach is extended for quality-aware web data classification. In this problem, the types of web data vary in terms of information quality. The ignorance of the variations of information quality of web data leads to poor classification models. The proposed approach can effectively integrate quality-aware factors into web data classification. Second, the approach is extended for listwise learning to rank to construct an ensemble of linear ranking models, whereas most existing listwise ranking methods construct a solely linear ranking model. Experimental results on benchmark datasets show that our approach outperforms state-of-the-art algorithms. During prediction for nonlinear classification, it also obtains comparable classification performance to kernel SVMs, with much higher efficiency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {18},
numpages = {26},
keywords = {multi-task learning, classification, clustering, Divide-and-conquer, listwise learning to rank}
}

@article{10.1145/3108257,
author = {Zhuang, Fuzhen and Cheng, Xiaohu and Luo, Ping and Pan, Sinno Jialin and He, Qing},
title = {Supervised Representation Learning with Double Encoding-Layer Autoencoder for Transfer Learning},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3108257},
doi = {10.1145/3108257},
abstract = {Transfer learning has gained a lot of attention and interest in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higher-level features for transfer learning. In this article, we adapt the autoencoder technique to transfer learning and propose a supervised representation learning method based on double encoding-layer autoencoder. The proposed framework consists of two encoding layers: one for embedding and the other one for label encoding. In the embedding layer, the distribution distance of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Moreover, to empirically explore why the proposed framework can work well for transfer learning, we propose a new effective measure based on autoencoder to compute the distribution distance between different domains. Experimental results show that the proposed new measure can better reflect the degree of transfer difficulty and has stronger correlation with the performance from supervised learning algorithms (e.g., Logistic Regression), compared with previous ones, such as KL-Divergence and Maximum Mean Discrepancy. Therefore, in our model, we have incorporated two distribution distance measures to minimize the difference between source and target domains in the embedding representations. Extensive experiments conducted on three real-world image datasets and one text data demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {16},
numpages = {17},
keywords = {representation learning, distribution difference measure, Double encoding-layer autoencoder}
}

@article{10.1145/3106775,
author = {Sun, Guodao and Tang, Tan and Peng, Tai-Quan and Liang, Ronghua and Wu, Yingcai},
title = {SocialWave: Visual Analysis of Spatio-Temporal Diffusion of Information on Social Media},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106775},
doi = {10.1145/3106775},
abstract = {Rapid advancement of social media tremendously facilitates and accelerates the information diffusion among users around the world. How and to what extent will the information on social media achieve widespread diffusion across the world? How can we quantify the interaction between users from different geolocations in the diffusion process? How will the spatial patterns of information diffusion change over time? To address these questions, a dynamic social gravity model (SGM) is proposed to quantify the dynamic spatial interaction behavior among social media users in information diffusion. The dynamic SGM includes three factors that are theoretically significant to the spatial diffusion of information: geographic distance, cultural proximity, and linguistic similarity. Temporal dimension is also taken into account to help detect recency effect, and ground-truth data is integrated into the model to help measure the diffusion power. Furthermore, SocialWave, a visual analytic system, is developed to support both spatial and temporal investigative tasks. SocialWave provides a temporal visualization that allows users to quickly identify the overall temporal diffusion patterns, which reflect the spatial characteristics of the diffusion network. When a meaningful temporal pattern is identified, SocialWave utilizes a new occlusion-free spatial visualization, which integrates a node-link diagram into a circular cartogram for further analysis. Moreover, we propose a set of rich user interactions that enable in-depth, multi-faceted analysis of the diffusion on social media. The effectiveness and efficiency of the mathematical model and visualization system are evaluated with two datasets on social media, namely, Ebola Epidemics and Ferguson Unrest.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {15},
numpages = {23},
keywords = {information diffusion, Spatio-temporal visualization, social media visualization}
}

@article{10.1145/3106745,
author = {Goodwin, Travis R. and Harabagiu, Sanda M.},
title = {Knowledge Representations and Inference Techniques for Medical Question Answering},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106745},
doi = {10.1145/3106745},
abstract = {Answering medical questions related to complex medical cases, as required in modern Clinical Decision Support (CDS) systems, imposes (1) access to vast medical knowledge and (2) sophisticated inference techniques. In this article, we examine the representation and role of combining medical knowledge automatically derived from (a) clinical practice and (b) research findings for inferring answers to medical questions. Knowledge from medical practice was distilled from a vast Electronic Medical Record (EMR) system, while research knowledge was processed from biomedical articles available in PubMed Central. The knowledge automatically acquired from the EMR system took into account the clinical picture and therapy recognized from each medical record to generate a probabilistic Markov network denoted as a Clinical Picture and Therapy Graph (CPTG). Moreover, we represented the background of medical questions available from the description of each complex medical case as a medical knowledge sketch. We considered three possible representations of medical knowledge sketches that were used by four different probabilistic inference methods to pinpoint the answers from the CPTG. In addition, several answer-informed relevance models were developed to provide a ranked list of biomedical articles containing the answers. Evaluations on the TREC-CDS data show which of the medical knowledge representations and inference methods perform optimally. The experiments indicate an improvement of biomedical article ranking by 49% over state-of-the-art results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {14},
numpages = {26},
keywords = {medical question answering, probabilistic inference, medical knowledge representation, medical information retrieval, Clinical decision support}
}

@article{10.1145/3104984,
author = {Zhu, Wenwu and Walrand, Jean and Guo, Yike and Wang, Zhi},
title = {ACM TIST Special Issue on Data-Driven Intelligence for Wireless Networking},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3104984},
doi = {10.1145/3104984},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {4},
numpages = {2}
}

@article{10.1145/3035968,
author = {Fan, Xiaoyi and Gong, Wei and Liu, Jiangchuan},
title = {I<sup>2</sup>Tag: RFID Mobility and Activity Identification Through Intelligent Profiling},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3035968},
doi = {10.1145/3035968},
abstract = {Many radio frequency identification (RFID) applications, such as virtual shopping cart and tag-assisted gaming, involve sensing and recognizing tag mobility. However, existing RFID localization methods are mostly designed for static or slowly moving targets (less than 0.3m/sec). More importantly, we observe that prior methods suffer from serious performance degradation for detecting real-world moving tags in typical indoor environments with multipath interference. In this article, we present i2tag, an intelligent mobility-aware activity identification system for RFID tags in multipath-rich environments (e.g., indoors). i2tag employs a supervised learning framework based on our novel fine-grain mobility provile, which can quantify different levels of mobility. Unlike previous methods that mostly rely on phase measurement, i2tag takes into account various measurements, including RSSI variance, packet loss rate, and our novel relative phase--based fingerprint. Additionally, we design a multidimensional dynamic time warping--based algorithm to robustly detect mobility and the associated activities. We show that i2tag is readily deployable using off-the-shelf RFID devices. A prototype has been implemented using a ThingMagic reader and standard-compatible tags. Experimental results demonstrate its superiority in mobility detection and activity identification in various indoor environments.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {5},
numpages = {21},
keywords = {backscatter, mobility detection, RFID, activity identification}
}

@article{10.1145/3102292,
author = {Yin, Hao and Wang, Wei and Zhang, Xu and Lyu, Yongqiang and Min, Geyong and Guo, Dongchao},
title = {UMCR: User Interaction-Driven Mobile Content Retrieval},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3102292},
doi = {10.1145/3102292},
abstract = {Although mobile application ecosystems have experienced tremendous growth in recent years, retrieving content of mobile applications that serves a key to mobile content search engines still faces grand challenges. Compared to web content retrieval, it is much more difficult to capture content in mobile applications due to the diversity of applications and the lack of Uniform Resource Locator indices. In this study, we propose and implement a <underline>u</underline>ser interaction-driven <underline>m</underline>obile <underline>c</underline>ontent <underline>r</underline>etrieval (UMCR) system to address such issues, which is the first mobile content crawler in the current literature. UMCR is a distributed system that contains many measurement nodes, each of which combines the user interaction path traversing (UIPT) and Deep Package Inspection (DPI) together to obtain mobile content. UIPT determines the events of user interactions in various applications to capture the static content such as text and images, in which a traversal depth termination scheme and an optional cut-off component are adopted to balance the content coverage and traversing efficiency. Meanwhile, the analysis based on DPI is responsible for extracting the videos as well as digging the infrastructural information and performance metrics. In addition, a distributed traversal scheduling method is designed for UIPT tasks to improve the throughput and scalability in large-scale content retrieval. Experiments on retrieving content of 64 real mobile applications demonstrate that UMCR can handle diverse mobile applications efficiently. The scheduler can improve throughput by 3 times compared to the legacy arbitrary task assignment strategy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {7},
numpages = {19},
keywords = {Mobile network, content retrieval, task scheduling, user interaction-driven}
}

@article{10.1145/3063716,
author = {Yeh, Lo-Yao and Tsaur, Woei-Jiunn and Huang, Hsin-Han},
title = {Secure IoT-Based, Incentive-Aware Emergency Personnel Dispatching Scheme with Weighted Fine-Grained Access Control},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3063716},
doi = {10.1145/3063716},
abstract = {Emergency response times following a traffic accident are extremely crucial in reducing the number of traffic-related deaths. Existing emergency vehicle dispatching systems rely heavily on manual assignments. Although some technology-assisted emergency systems engage in emergency message dissemination and path planning, efficient emergency response is one of the main factors that can decrease traffic-related deaths. Obviously, effective emergency response often plays a far more important role in a successful rescue. In this article, we propose a secure IoT-based and incentive-aware emergency personnel dispatching scheme (EPDS) with weighted fine-grained access control. Our EPDS can recruit available medical personnel on-the-fly, such as physicians driving in the vicinity of the accident scene. An appropriate incentive, such as paid leave, can be offered to encourage medical personnel to join rescue missions. Furthermore, IoT-based devices are installed in vehicles or wearable on drivers to gather biometric signals from the driver, which can be used to decide precisely which divisions or physicians are needed to administer the appropriate remedy. Additionally, our scheme can cryptographically authorize the assigned rescue vehicle to control traffic to increase rescue efficacy. Our scheme also takes advantage of adjacent roadside units to organize the appropriate rescue personnel without requiring long-distance communication with a trusted traffic authority. Proof of security is provided and extensive analyses, including qualitative and quantitative analyses and simulations, show that the proposed scheme can significantly improve rescue response time and effectiveness. To the best of our knowledge, this is the first work to make use of medical personnel that are close by in emergency rescue missions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {10},
numpages = {23},
keywords = {biometric authentication, fine-grained access control, Security, emergency personnel dispatch<!--?pgbrk?-->, internet of vehicles}
}

@article{10.1145/3086635,
author = {Cui, Chaoran and Shen, Jialie and Nie, Liqiang and Hong, Richang and Ma, Jun},
title = {Augmented Collaborative Filtering for Sparseness Reduction in Personalized POI Recommendation},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3086635},
doi = {10.1145/3086635},
abstract = {As mobile device penetration increases, it has become pervasive for images to be associated with locations in the form of geotags. Geotags bridge the gap between the physical world and the cyberspace, giving rise to new opportunities to extract further insights into user preferences and behaviors. In this article, we aim to exploit geotagged photos from online photo-sharing sites for the purpose of personalized Point-of-Interest (POI) recommendation. Owing to the fact that most users have only very limited travel experiences, data sparseness poses a formidable challenge to personalized POI recommendation. To alleviate data sparseness, we propose to augment current collaborative filtering algorithms along from multiple perspectives. Specifically, hybrid preference cues comprising user-uploaded and user-favored photos are harvested to study users’ tastes. Moreover, heterogeneous high-order relationship information is jointly captured from user social networks and POI multimodal contents with hypergraph models. We also build upon the matrix factorization algorithm to integrate the disparate sources of preference and relationship information, and apply our approach to directly optimize user preference rankings. Extensive experiments on a large and publicly accessible dataset well verified the potential of our approach for addressing data sparseness and offering quality recommendations to users, especially for those who have only limited travel experiences.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {71},
numpages = {23},
keywords = {Geotagged photos, hypergraph-based learning, personalized POI recommendation}
}

@article{10.1145/3090311,
author = {Li, Zhifeng and Gong, Dihong and Zhu, Kai and Tao, Dacheng and Li, Xuelong},
title = {Multifeature Anisotropic Orthogonal Gaussian Process for Automatic Age Estimation},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3090311},
doi = {10.1145/3090311},
abstract = {Automatic age estimation is an important yet challenging problem. It has many promising applications in social media. Of the existing age estimation algorithms, the personalized approaches are among the most popular ones. However, most person-specific approaches rely heavily on the availability of training images across different ages for a single subject, which is usually difficult to satisfy in practical application of age estimation. To address this limitation, we first propose a new model called Orthogonal Gaussian Process (OGP), which is not restricted by the number of training samples per person. In addition, without sacrifice of discriminative power, OGP is much more computationally efficient than the standard Gaussian Process. Based on OGP, we then develop an effective age estimation approach, namely anisotropic OGP (A-OGP), to further reduce the estimation error. A-OGP is based on an anisotropic noise level learning scheme that contributes to better age estimation performance. To finally optimize the performance of age estimation, we propose a multifeature A-OGP fusion framework that uses multiple features combined with a random sampling method in the feature space. Extensive experiments on several public domain face aging datasets (FG-NET, MORPH Album1, and MORPH Album 2) are conducted to demonstrate the state-of-the-art estimation accuracy of our new algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {2},
numpages = {15},
keywords = {Age estimation, face image}
}

@article{10.1145/3078855,
author = {Wang, Xuyu and Yang, Chao and Mao, Shiwen},
title = {TensorBeat: Tensor Decomposition for Monitoring Multiperson Breathing Beats with Commodity WiFi},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078855},
doi = {10.1145/3078855},
abstract = {Breathing signal monitoring can provide important clues for health problems. Compared to existing techniques that require wearable devices and special equipment, a more desirable approach is to provide contact-free and long-term breathing rate monitoring by exploiting wireless signals. In this article, we propose TensorBeat, a system to employ channel state information (CSI) phase difference data to intelligently estimate breathing rates for multiple persons with commodity WiFi devices. The main idea is to leverage the tensor decomposition technique to handle the CSI phase difference data. The proposed TensorBeat scheme first obtains CSI phase difference data between pairs of antennas at the WiFi receiver to create CSI tensors. Then canonical polyadic (CP) decomposition is applied to obtain the desired breathing signals. A stable signal matching algorithm is developed to identify the decomposed signal pairs, and a peak detection method is applied to estimate the breathing rates for multiple persons. Our experimental study shows that TensorBeat can achieve high accuracy under different environments for multiperson breathing rate monitoring.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {8},
numpages = {27},
keywords = {Healthcare Internet of Things (IoT), vital sign monitoring, tensor decomposition, channel state information, stable roommate matching, commodity WiFi}
}

@article{10.1145/3078844,
author = {Zhang, Ruide and Zhang, Ning and Du, Changlai and Lou, Wenjing and Hou, Y. Thomas and Kawamoto, Yuichi},
title = {From Electromyogram to Password: Exploring the Privacy Impact of Wearables in Augmented Reality},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078844},
doi = {10.1145/3078844},
abstract = {With the increasing popularity of augmented reality (AR) services, providing seamless human-computer interactions in the AR setting has received notable attention in the industry. Gesture control devices have recently emerged to be the next great gadgets for AR due to their unique ability to enable computer interaction with day-to-day gestures. While these AR devices are bringing revolutions to our interaction with the cyber world, it is also important to consider potential privacy leakages from these always-on wearable devices. Specifically, the coarse access control on current AR systems could lead to possible abuse of sensor data.Although the always-on gesture sensors are frequently quoted as a privacy concern, there has not been any study on information leakage of these devices. In this article, we present our study on side-channel information leakage of the most popular gesture control device, Myo. Using signals recorded from the electromyography (EMG) sensor and accelerometers on Myo, we can recover sensitive information such as passwords typed on a keyboard and PIN sequence entered through a touchscreen. EMG signal records subtle electric currents of muscle contractions. We design novel algorithms based on dynamic cumulative sum and wavelet transform to determine the exact time of finger movements. Furthermore, we adopt the Hudgins feature set in a support vector machine to classify recorded signal segments into individual fingers or numbers. We also apply coordinate transformation techniques to recover fine-grained spatial information with low-fidelity outputs from the sensor in keystroke recovery.We evaluated the information leakage using data collected from a group of volunteers. Our results show that there is severe privacy leakage from these commodity wearable sensors. Our system recovers complex passwords constructed with lowercase letters, uppercase letters, numbers, and symbols with a mean success rate of 91%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {13},
numpages = {20},
keywords = {augmented reality, keystroke detection, PIN sequence inference, Information leakage, EMG side-channel}
}

@article{10.1145/3078843,
author = {Wu, Yanqiu and Minkus, Tehila and Ross, Keith W.},
title = {Taking the Pulse of US College Campuses with Location-Based Anonymous Mobile Apps},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078843},
doi = {10.1145/3078843},
abstract = {We deploy GPS hacking in conjunction with location-based mobile apps to passively survey users in targeted geographical regions. Specifically, we investigate surveying students at different college campuses with Yik Yak, an anonymous mobile app that is popular on US college campuses. In addition to being campus centric, Yik Yak’s anonymity allows students to express themselves candidly without self-censorship.We collect nearly 1.6 million Yik Yak messages (“yaks”) from a diverse set of 45 college campuses in the United States. We use natural language processing to determine the sentiment (positive, negative, or neutral) of all of the yaks. We employ supervised machine learning to predict the gender of the authors of the yaks and then analyze how sentiment differs among the two genders on college campuses. We also use supervised machine learning to classify all the yaks into nine topics and then investigate which topics are most popular throughout the US and how topic popularity varies on the different campuses. The results in this article provide significant insight into how campus culture and student’s thinking varies among US colleges and universities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {12},
numpages = {18},
keywords = {Social networks, data mining}
}

@article{10.1145/3022471,
author = {Huang, Chao and Wang, Dong and Tao, Jun},
title = {An Unsupervised Approach to Inferring the Localness of People Using Incomplete Geotemporal Online Check-In Data},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3022471},
doi = {10.1145/3022471},
abstract = {Inferring the localness of people is to classify people who are local residents in a city from people who visit the city by analyzing online check-in points that are contributed by online users. This information is critical for the urban planning, user profiling, and localized recommendation systems. Supervised learning approaches have been developed to infer the location of people in a city by assuming the availability of high-quality training datasets with complete geotemporal information. In this article, we develop an unsupervised model to accurately identify local people in a city by using the incomplete online check-in data that are publicly available. In particular, we develop an incomplete geotemporal expectation maximization (IGT-EM) scheme, which incorporates a set of hidden variables to represent the localness of people and a set of estimation parameters to represent the likelihood of venues to attract local and nonlocal people, respectively. Our solution can accurately classify local people from nonlocal nones without requiring any training data. We also implement a parallel IGT-EM algorithm by leveraging the computing power of a graphic processing unit (GPU) that consists of 2,496 cores. In the evaluation, we compare our new approach with the existing solutions through four real-world case studies using data from the New York City, Chicago, Boston, and Washington, DC. The results show that our approach can identify the local people and significantly outperform the compared baselines in estimation accuracy and execution time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {80},
numpages = {18},
keywords = {maximum likelihood estimation, Localness of people, crowdsourcing, unsupervised learning, online social networks, GPU implementation}
}

@article{10.1145/2932192,
author = {Li, Yang and Jiang, Jing and Liu, Ting and Qiu, Minghui and Sun, Xiaofei},
title = {Personalized Microtopic Recommendation on Microblogs},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/2932192},
doi = {10.1145/2932192},
abstract = {Microblogging services such as Sina Weibo and Twitter allow users to create tags explicitly indicated by the # symbol. In Sina Weibo, these tags are called microtopics, and in Twitter, they are called hashtags. In Sina Weibo, each microtopic has a designate page and can be directly visited or commented on. Recommending these microtopics to users based on their interests can help users efficiently acquire information. However, it is non-trivial to recommend microtopics to users to satisfy their information needs. In this article, we investigate the task of personalized microtopic recommendation, which exhibits two challenges. First, users usually do not give explicit ratings to microtopics. Second, there exists rich information about users and microtopics, for example, users' published content and biographical information, but it is not clear how to best utilize such information. To address the above two challenges, we propose a joint probabilistic latent factor model to integrate rich information into a matrix factorization-based solution to microtopic recommendation. Our model builds on top of collaborative filtering, content analysis, and feature regression. Using two real-world datasets, we evaluate our model with different kinds of content and contextual information. Experimental results show that our model significantly outperforms a few competitive baseline methods, especially in the circumstance where users have few adoption behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {77},
numpages = {21},
keywords = {microtopic recommendation, topic model, collaborative filtering, Microblogs}
}

@article{10.1145/3102301,
author = {Zhang, Wei and Fan, Rui and Wen, Yonggang and Liu, Fang},
title = {Energy-Efficient Mobile Video Streaming: A Location-Aware Approach},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3102301},
doi = {10.1145/3102301},
abstract = {Video streaming is one of the most widely used mobile applications today, and it also accounts for a large fraction of mobile battery usage. Much of the energy consumption is for wireless data transmission and is highly correlated to network bandwidth conditions. In periods of poor connectivity, up to 90% of mobile energy can be used for wireless data transfer. In this article, we study the problem of energy-efficient mobile video streaming. We make use of the observed correlation between bandwidth and user location, and also observe that a user’s location is predictable in many situations, such as when commuting to a known destination. Based on the user’s predicted locations and bandwidth conditions, we optimize wireless transmission times to achieve high quality video playback while minimizing energy use. We propose an optimal offline algorithm for this problem, which runs in O(Tk) time, where T is the duration of the video and k is the size of the video buffer. We also propose LAWS, a Location AWare Streaming algorithm. LAWS learns from historical location-aware bandwidth conditions and predicts future bandwidths along a planned route to make online wireless download decisions. We evaluate LAWS using real bandwidth traces, and show that LAWS closely approximates the performance of the optimal offline algorithm, achieving 90.6% of the optimal performance on average, and 97% in certain cases. LAWS also outperforms three popular strategies used in practice by, on average, 69%, 63%, and 38%, respectively. Lastly, we show that LAWS is able to deal with noisy data and can attain the stated performance after sampling bandwidth conditions only five times.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {6},
numpages = {16},
keywords = {media cloud, video streaming, Energy efficiency, mobile device}
}

@article{10.1145/3094786,
author = {Gao, Yang and Li, Yuefeng and Lau, Raymond Y. K. and Xu, Yue and Bashar, Md Abul},
title = {Finding Semantically Valid and Relevant Topics by Association-Based Topic Selection Model},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3094786},
doi = {10.1145/3094786},
abstract = {Topic modelling methods such as Latent Dirichlet Allocation (LDA) have been successfully applied to various fields, since these methods can effectively characterize document collections by using a mixture of semantically rich topics. So far, many models have been proposed. However, the existing models typically outperform on full analysis on the whole collection to find all topics but difficult to capture coherent and specifically meaningful topic representations. Furthermore, it is very challenging to incorporate user preferences into existing topic modelling methods to extract relevant topics. To address these problems, we develop a novel personalized Association-based Topic Selection (ATS) model, which can identify semantically valid and relevant topics from a set of raw topics based on the semantical relatedness between users’ preferences and the structured patterns captured in topics. The advantage of the proposed ATS model is that it enables an interactive topic modelling process driven by users’ specific interests. Based on three benchmark datasets, namely, RCV1, R8, and WT10G under the context of information filtering (IF) and information retrieval (IR), our rigorous experiments show that the proposed ATS model can effectively identify relevant topics with respect to users’ specific interests, and hence to improve the performance of IF and IR.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {3},
numpages = {22},
keywords = {topic evaluation, information filtering, topic components, Topic selection}
}

@article{10.1145/3090312,
author = {Li, Xuelong and Cui, Guosheng and Dong, Yongsheng},
title = {Refined-Graph Regularization-Based Nonnegative Matrix Factorization},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3090312},
doi = {10.1145/3090312},
abstract = {Nonnegative matrix factorization (NMF) is one of the most popular data representation methods in the field of computer vision and pattern recognition. High-dimension data are usually assumed to be sampled from the submanifold embedded in the original high-dimension space. To preserve the locality geometric structure of the data, k-nearest neighbor (k-NN) graph is often constructed to encode the near-neighbor layout structure. However, k-NN graph is based on Euclidean distance, which is sensitive to noise and outliers. In this article, we propose a refined-graph regularized nonnegative matrix factorization by employing a manifold regularized least-squares regression (MRLSR) method to compute the refined graph. In particular, each sample is represented by the whole dataset regularized with ℓ2-norm and Laplacian regularizer. Then a MRLSR graph is constructed based on the representative coefficients of each sample. Moreover, we present two optimization schemes to generate refined-graphs by employing a hard-thresholding technique. We further propose two refined-graph regularized nonnegative matrix factorization methods and use them to perform image clustering. Experimental results on several image datasets reveal that they outperform 11 representative methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {1},
numpages = {21},
keywords = {refined-graph, nonnegative matrix factorization (NMF), Data representation, least squares regression, image clustering}
}

@article{10.1145/3065949,
author = {Shen, Jiaxing and Cao, Jiannong and Liu, Xuefeng and Zhang, Chisheng},
title = {DMAD: Data-Driven Measuring of Wi-Fi Access Point Deployment in Urban Spaces},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3065949},
doi = {10.1145/3065949},
abstract = {Wireless networks offer many advantages over wired local area networks such as scalability and mobility. Strategically deployed wireless networks can achieve multiple objectives like traffic offloading, network coverage, and indoor localization. To this end, various mathematical models and optimization algorithms have been proposed to find optimal deployments of access points (APs).However, wireless signals can be blocked by the human body, especially in crowded urban spaces. As a result, the real coverage of an on-site AP deployment may shrink to some degree and lead to unexpected dead spots (areas without wireless coverage). Dead spots are undesirable, since they degrade the user experience in network service continuity, on one hand, and, on the other hand paralyze some applications and services like tracking and monitoring when users are in these areas. Nevertheless, it is nontrivial for existing methods to analyze the impact of human beings on wireless coverage. Site surveys are too time consuming and labor intensive to conduct. It is also infeasible for simulation methods to predict the number of on-site people.In this article, we propose DMAD, a Data-driven Measuring of Wi-Fi Access point Deployment, which not only estimates potential dead spots of an on-site AP deployment but also quantifies their severity, using simple Wi-Fi data collected from the on-site deployment and shop profiles from the Internet. DMAD first classifies static devices and mobile devices with a decision-tree classifier. Then it locates mobile devices to grid-level locations based on shop popularities, wireless signal, and visit duration. Last, DMAD estimates the probability of dead spots for each grid during different time slots and derives their severity considering the probability and the number of potential users.The analysis of Wi-Fi data from static devices indicates that the Pearson Correlation Coefficient of wireless coverage status and the number of on-site people is over 0.7, which confirms that human beings may have a significant impact on wireless coverage. We also conduct extensive experiments in a large shopping mall in Shenzhen. The evaluation results demonstrate that DMAD can find around 70% of dead spots with a precision of over 70%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {11},
numpages = {29},
keywords = {data-driven approach, Wi-Fi AP, room-level localization, AP deployment measuring}
}

@article{10.1145/3059149,
author = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
title = {Exploring Indoor White Spaces in Metropolises},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3059149},
doi = {10.1145/3059149},
abstract = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels, to meet the rapidly growing demand for wireless data services in both outdoor and indoor scenarios. While most prior works have focused on outdoor white space, the indoor story is largely open for investigation. Motivated by this observation and discovering that 70% of the spectrum demand comes from indoor environment, we carry out a comprehensive study to explore indoor white spaces. We first conduct a large-scale measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse locations in a typical metropolis—Hong Kong. Our results show that abundant white spaces are available in different areas in Hong Kong, which account for more than 50% and 70% of the entire TV spectrum in outdoor and indoor scenarios, respectively. Although there are substantially more white spaces indoors than outdoors, there have been very few solutions for identifying indoor white space. To fill in this gap, we develop the first data-driven, low-cost indoor white space identification system for White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify white spaces for communication without sensing the spectrum themselves. We design the architecture and algorithms to address the inherent challenges. We build a WISER prototype and carry out real-world experiments to evaluate its performance. Our results show that WISER can identify 30%--40% more indoor white spaces with negligible false alarms, as compared to alternative baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {9},
numpages = {25},
keywords = {clustering algorithms, TV white spaces, sensor placement}
}

@article{10.1145/3086637,
author = {Fu, Hao and Xie, Xing and Rui, Yong and Gong, Neil Zhenqiang and Sun, Guangzhong and Chen, Enhong},
title = {Robust Spammer Detection in Microblogs: Leveraging User Carefulness},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3086637},
doi = {10.1145/3086637},
abstract = {Microblogging Web sites, such as Twitter and Sina Weibo, have become popular platforms for socializing and sharing information in recent years. Spammers have also discovered this new opportunity to unfairly overpower normal users with unsolicited content, namely social spams. Although it is intuitive for everyone to follow legitimate users, recent studies show that both legitimate users and spammers follow spammers for different reasons. Evidence of users seeking spammers on purpose is also observed. We regard this behavior as useful information for spammer detection. In this article, we approach the problem of spammer detection by leveraging the “carefulness” of users, which indicates how careful a user is when she is about to follow a potential spammer. We propose a framework to measure the carefulness and develop a supervised learning algorithm to estimate it based on known spammers and legitimate users. We illustrate how the robustness of the detection algorithms can be improved with aid of the proposed measure. Evaluation on two real datasets from Sina Weibo and Twitter with millions of users are performed, as well as an online test on Sina Weibo. The results show that our approach indeed captures the carefulness, and it is effective for detecting spammers. In addition, we find that our measure is also beneficial for other applications, such as link prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {83},
numpages = {31},
keywords = {social network, microblog, Spammer detection}
}

@article{10.1145/3070665,
author = {Tu, Cunchao and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
title = {PRISM: Profession Identification in Social Media},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3070665},
doi = {10.1145/3070665},
abstract = {Profession is an important social attribute of people. It plays a crucial role in commercial services such as personalized recommendation and targeted advertising. In practice, profession information is usually unavailable due to privacy and other reasons. In this article, we explore the task of identifying user professions according to their behaviors in social media. The task confronts the following challenges that make it non-trivial: how to incorporate heterogeneous information of user behaviors, how to effectively utilize both labeled and unlabeled data, and how to exploit community structure. To address these challenges, we present a framework called Profession Identification in Social Media. It takes advantage of both personal information and community structure of users in the following aspects: (1) We present a cascaded two-level classifier with heterogeneous personal features to measure the confidence of users belonging to different professions. (2) We present a multi-training process to take advantages of both labeled and unlabeled data to enhance classification performance. (3) We design a profession identification method synthetically considering the confidences from personal features and community structure. We collect a real-world dataset to conduct experiments, and experimental results demonstrate the significant effectiveness of our method compared with other baseline methods. By applying prediction on large-scale users, we also analyze characteristics of microblog users, finding that there are significant diversities among users of different professions in demographics, social network structures, and linguistic styles.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {81},
numpages = {16},
keywords = {Profession identification, social media, heterogeneous information, community detection}
}

@article{10.1145/3070658,
author = {Chikhaoui, Belkacem and Chiazzaro, Mauricio and Wang, Shengrui and Sotir, Martin},
title = {Detecting Communities of Authority and Analyzing Their Influence in Dynamic Social Networks},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3070658},
doi = {10.1145/3070658},
abstract = {Users in real-world social networks are organized into communities that differ from each other in terms of influence, authority, interest, size, etc. This article addresses the problems of detecting communities of authority and of estimating the influence of such communities in dynamic social networks. These are new issues that have not yet been addressed in the literature, and they are important in applications such as marketing and recommender systems. To facilitate the identification of communities of authority, our approach first detects communities sharing common interests, which we call “meta-communities,” by incorporating topic modeling based on users’ community memberships. Then, communities of authority are extracted with respect to each meta-community, using a new measure based on the betweenness centrality. To assess the influence between communities over time, we propose a new model based on the Granger causality method. Through extensive experiments on a variety of social network datasets, we empirically demonstrate the suitability of our approach for community-of-authority detection and assessment of the influence between communities over time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {82},
numpages = {28},
keywords = {granger causality, meta-community, betweenness centrality, community influence, topic modeling, Community of authority}
}

@article{10.1145/3022472,
author = {Chen, Chien-Cheng and Hsu, Kuo-Wei and Peng, Wen-Chih},
title = {Exploring Communication Behaviors of Users to Target Potential Users in Mobile Social Networks},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3022472},
doi = {10.1145/3022472},
abstract = {In mobile communication services, users can communicate with each other over different telecommunication carriers. For telecom operators, how to acquire and retain users is a significant and practical task. Note that telecom operators only have their own customer profiles. For the users from other telecom operators, their information is sparse. Thus, given a set of communication logs, the main theme of our work is to identify the potential users who will possibly join the target services in the near future. Since only a limited amount of information is available, one challenging issue is how to extract features from the communication logs. In this article, we propose a Communication-Based Feature Generation (CBFG) framework that extracts features and builds models to infer the potential users. Explicitly, we construct a heterogeneous information network from the communication logs of users. Then, we extract the explicit features, which refer to those calling features of users, from the potential users’ interaction behaviors in the heterogeneous information network. Moreover, from the calling behaviors of users, one could extract the possible community structures of users. Based on the community structures, we further extract the implicit features of users. In light of both explicit and implicit features, we propose an information-gain-based method to select the effective features. According to the features selected, we utilize three popular classifiers (i.e., AdaBoost, Random Forest, and SVM) to build models to target the potential users. In addition, we have designed a sampling approach to extract training data for classifiers. To evaluate our methods, we have conducted experiments on a real dataset. The results of our experiments show that the features extracted by our proposed method can be effective for targeting the potential users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {79},
numpages = {22},
keywords = {feature engineering, Communication behaviors, mobile social network}
}

@article{10.1145/3086636,
author = {Assem, Haytham and Buda, Teodora Sandra and O’sullivan, Declan},
title = {RCMC: Recognizing Crowd-Mobility Patterns in Cities Based on Location Based Social Networks Data},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3086636},
doi = {10.1145/3086636},
abstract = {During the past few years, the analysis of data generated from Location-Based Social Networks (LBSNs) have aided in the identification of urban patterns, understanding activity behaviours in urban areas, as well as producing novel recommender systems that facilitate users’ choices. Recognizing crowd-mobility patterns in cities is very important for public safety, traffic managment, disaster management, and urban planning. In this article, we propose a framework for Recognizing the Crowd Mobility Patterns in Cities using LBSN data. Our proposed framework comprises four main components: data gathering, recurrent crowd-mobility patterns extraction, temporal functional regions detection, and visualization component. More specifically, we employ a novel approach based on Non-negative Matrix Factorization and Gaussian Kernel Density Estimation for extracting the recurrent crowd-mobility patterns in cities illustrating how crowd shifts from one area to another during each day across various time slots. Moreover, the framework employs a hierarchical clustering-based algorithm for identifying what we refer to as temporal functional regions by modeling functional areas taking into account temporal variation by means of check-ins’ categories. We build the framework using a spatial-temporal dataset crawled from Twitter for two entire years (2013 and 2014) for the area of Manhattan in New York City. We perform a detailed analysis of the extracted crowd patterns with an exploratory visualization showing that our proposed approach can identify clearly obvious mobility patterns that recur over time and location in the urban scenario. Using same time interval, we show that correlating the temporal functional regions with the recognized recurrent crowd-mobility patterns can yield to a deeper understanding of city dynamics and the motivation behind the crowd mobility. We are confident that our proposed framework not only can help in managing complex city environments and better allocation of resources based on the expected crowd mobility and temporal functional regions but also can have a direct implication on a variety of applications such as personalized recommender systems, anomalous event detection, disaster resilience management systems, and others.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {70},
numpages = {30},
keywords = {Crowd patterns, urban computing, urban mobility, smart cities}
}

@article{10.1145/3065950,
author = {Gao, Xingyu and Hoi, Steven C. H. and Zhang, Yongdong and Zhou, Jianshe and Wan, Ji and Chen, Zhenyu and Li, Jintao and Zhu, Jianke},
title = {Sparse Online Learning of Image Similarity},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3065950},
doi = {10.1145/3065950},
abstract = {Learning image similarity plays a critical role in real-world multimedia information retrieval applications, especially in Content-Based Image Retrieval (CBIR) tasks, in which an accurate retrieval of visually similar objects largely relies on an effective image similarity function. Crafting a good similarity function is very challenging because visual contents of images are often represented as feature vectors in high-dimensional spaces, for example, via bag-of-words (BoW) representations, and traditional rigid similarity functions, for example, cosine similarity, are often suboptimal for CBIR tasks. In this article, we address this fundamental problem, that is, learning to optimize image similarity with sparse and high-dimensional representations from large-scale training data, and propose a novel scheme of Sparse Online Learning of Image Similarity (SOLIS). In contrast to many existing image-similarity learning algorithms that are designed to work with low-dimensional data, SOLIS is able to learn image similarity from large-scale image data in sparse and high-dimensional spaces. Our encouraging results showed that the proposed new technique achieves highly competitive accuracy as compared to the state-of-the-art approaches but enjoys significant advantages in computational efficiency, model sparsity, and retrieval scalability, making it more practical for real-world multimedia retrieval applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {64},
numpages = {22},
keywords = {bag-of-words representation, distance metric, Online learning, image retrieval, similarity learning, metric learning}
}

@article{10.1145/3110318,
author = {Burt, Ronald and Tang, Jie and Vazirgiannis, Michalis and Yang, Shuang},
title = {Introduction to Special Issue on <i>Social Media Processing</i> (<i>TIST</i>-<i>SMP</i>)},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3110318},
doi = {10.1145/3110318},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {76},
numpages = {2}
}

@article{10.1145/3078852,
author = {Feyisetan, Oluwaseyi and Simperl, Elena},
title = {Social Incentives in Paid Collaborative Crowdsourcing},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078852},
doi = {10.1145/3078852},
abstract = {Paid microtask crowdsourcing has traditionally been approached as an individual activity, with units of work created and completed independently by the members of the crowd. Other forms of crowdsourcing have, however, embraced more varied models, which allow for a greater level of participant interaction and collaboration. This article studies the feasibility and uptake of such an approach in the context of paid microtasks. Specifically, we compare engagement, task output, and task accuracy in a paired-worker model with the traditional, single-worker version. Our experiments indicate that collaboration leads to better accuracy and more output, which, in turn, translates into lower costs. We then explore the role of the social flow and social pressure generated by collaborating partners as sources of incentives for improved performance. We utilise a Bayesian method in conjunction with interface interaction behaviours to detect when one of the workers in a pair tries to exit the task. Upon this realisation, the other worker is presented with the opportunity to contact the exiting partner to stay: either for personal financial reasons (i.e., they have not completed enough tasks to qualify for a payment) or for fun (i.e., they are enjoying the task). The findings reveal that: (1) these socially motivated incentives can act as furtherance mechanisms to help workers attain and exceed their task requirements and produce better results than baseline collaborations; (2) microtask crowd workers are empathic (as opposed to selfish) agents, willing to go the extra mile to help their partners get paid; and, (3) social furtherance incentives create a win-win scenario for the requester and for the workers by helping more workers get paid by re-engaging them before they drop out.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {73},
numpages = {31},
keywords = {Crowdsourcing, incentives, social flow, social pressure}
}

@article{10.1145/3078850,
author = {Khezerlou, Amin Vahedian and Zhou, Xun and Li, Lufan and Shafiq, Zubair and Liu, Alex X. and Zhang, Fan},
title = {A Traffic Flow Approach to Early Detection of Gathering Events: Comprehensive Results},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078850},
doi = {10.1145/3078850},
abstract = {Given a spatial field and the traffic flow between neighboring locations, the early detection of gathering events (edge) problem aims to discover and localize a set of most likely gathering events. It is important for city planners to identify emerging gathering events that might cause public safety or sustainability concerns. However, it is challenging to solve the edge problem due to numerous candidate gathering footprints in a spatial field and the nontrivial task of balancing pattern quality and computational efficiency. Prior solutions to model the edge problem lack the ability to describe the dynamic flow of traffic and the potential gathering destinations because they rely on static or undirected footprints. In our recent work, we modeled the footprint of a gathering event as a Gathering Graph (G-Graph), where the root of the directed acyclic G-Graph is the potential destination and the directed edges represent the most likely paths traffic takes to move toward the destination. We also proposed an efficient algorithm called SmartEdge to discover the most likely nonoverlapping G-Graphs in the given spatial field. However, it is challenging to perform a systematic performance study of the proposed algorithm, due to unavailability of the ground truth of gathering events. In this article, we introduce an event simulation mechanism, which makes it possible to conduct a comprehensive performance study of the SmartEdge algorithm. We measure the quality of the detected patterns, in a systematic way, in terms of timeliness and location accuracy. The results show that, on average, the SmartEdge algorithm is able to detect patterns within a grid cell away (less than 500 meters) of the simulated events and detect patterns of the simulated events as early as 10 minutes prior to the first arrival to the gathering event.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {74},
numpages = {24},
keywords = {spatial data mining, Gathering event, early detection}
}

@article{10.1145/3078848,
author = {Zhao, Hongke and Ge, Yong and Liu, Qi and Wang, Guifeng and Chen, Enhong and Zhang, Hefu},
title = {P2P Lending Survey: Platforms, Recent Advances and Prospects},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078848},
doi = {10.1145/3078848},
abstract = {P2P lending is an emerging Internet-based application where individuals can directly borrow money from each other. The past decade has witnessed the rapid development and prevalence of online P2P lending platforms, examples of which include Prosper, LendingClub, and Kiva. Meanwhile, extensive research has been done that mainly focuses on the studies of platform mechanisms and transaction data. In this article, we provide a comprehensive survey on the research about P2P lending, which, to the best of our knowledge, is the first focused effort in this field. Specifically, we first provide a systematic taxonomy for P2P lending by summarizing different types of mainstream platforms and comparing their working mechanisms in detail. Then, we review and organize the recent advances on P2P lending from various perspectives (e.g., economics and sociology perspective, and data-driven perspective). Finally, we propose our opinions on the prospects of P2P lending and suggest some future research directions in this field. Meanwhile, throughout this paper, some analysis on real-world data collected from Prosper and Kiva are also conducted.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {72},
numpages = {28},
keywords = {prospects, micro-finance, P2P lending, online loans, platforms}
}

@article{10.1145/3078846,
author = {Feng, Xiaodong and Wu, Sen and Zhou, Wenjun},
title = {Multi-Hypergraph Consistent Sparse Coding},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078846},
doi = {10.1145/3078846},
abstract = {Sparse representation has been a powerful technique for modeling high-dimensional data. As an unsupervised technique to extract sparse representations, sparse coding encodes the original data into a new sparse code space and simultaneously learns a dictionary representing high-level semantics. Existing methods have considered local manifold within high-dimensional data using graph/hypergraph Laplacian regularization, and more from the manifold could be utilized to improve the performance. In this article, we propose to further regulate the sparse coding so that the learned sparse codes can well reconstruct the hypergraph structure. In particular, we add a novel hypergraph consistency regularization term (HC) by minimizing the reconstruction error of the hypergraph incidence or weight matrix. Moreover, we extend the HC term to multi-hypergraph consistent sparse coding (MultiCSC) and automatically select the optimal manifold structure under the multi-hypergraph learning framework. We show that the optimization of MultiCSC can be solved efficiently, and that several existing sparse coding methods can fit into the general framework of MultiCSC as special cases. As a case study, hypergraph incidence consistent sparse coding is applied to perform semi-auto image tagging, demonstrating the effectiveness of hypergraph consistency regulation. We perform further experiments using MultiCSC for image clustering, which outperforms a number of baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {75},
numpages = {25},
keywords = {multiple-hypergraph learning, Sparse coding, hypergraph consistency, image tagging, image clustering}
}

@article{10.1145/2963104,
author = {Glenski, Maria and Weninger, Tim},
title = {Rating Effects on Social News Posts and Comments},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/2963104},
doi = {10.1145/2963104},
abstract = {At a time when information seekers first turn to digital sources for news and opinion, it is critical that we understand the role that social media plays in human behavior. This is especially true when information consumers also act as information producers and editors through their online activity. In order to better understand the effects that editorial ratings have on online human behavior, we report the results of a two large-scale in vivo experiments in social media. We find that small, random rating manipulations on social media posts and comments created significant changes in downstream ratings, resulting in significantly different final outcomes. We found positive herding effects for positive treatments on posts, increasing the final rating by 11.02% on average, but not for positive treatments on comments. Contrary to the results of related work, we found negative herding effects for negative treatments on posts and comments, decreasing the final ratings, on average, of posts by 5.15% and of comments by 37.4%. Compared to the control group, the probability of reaching a high rating ( ⩾ 2,000) for posts is increased by 24.6% when posts receive the positive treatment and for comments it is decreased by 46.6% when comments receive the negative treatment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {78},
numpages = {19},
keywords = {herding effects, Social media, social news, voting}
}

@article{10.1145/3066156,
author = {Ottens, Brammert and Dimitrakakis, Christos and Faltings, Boi},
title = {DUCT: An Upper Confidence Bound Approach to Distributed Constraint Optimization Problems},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3066156},
doi = {10.1145/3066156},
abstract = {We propose a distributed upper confidence bound approach, DUCT, for solving distributed constraint optimization problems. We compare four variants of this approach with a baseline random sampling algorithm, as well as other complete and incomplete algorithms for DCOPs. Under general assumptions, we theoretically show that the solution found by DUCT after T steps is approximately T−1-close to the optimal. Experimentally, we show that DUCT matches the optimal solution found by the well-known DPOP and O-DPOP algorithms on moderate-size problems, while always requiring less agent communication. For larger problems, where DPOP fails, we show that DUCT produces significantly better solutions than local, incomplete algorithms. Overall, we believe that DUCT is a practical, scalable algorithm for complex DCOPs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {69},
numpages = {27},
keywords = {Distributed constraint optimization, tree search, multiagent systems, coordination}
}

@article{10.1145/3057733,
author = {Ou, Xinyu and Ling, Hefei and Yu, Han and Li, Ping and Zou, Fuhao and Liu, Si},
title = {Adult Image and Video Recognition by a Deep Multicontext Network and Fine-to-Coarse Strategy},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057733},
doi = {10.1145/3057733},
abstract = {Adult image and video recognition is an important and challenging problem in the real world. Low-level feature cues do not produce good enough information, especially when the dataset is very large and has various data distributions. This issue raises a serious problem for conventional approaches. In this article, we tackle this problem by proposing a deep multicontext network with fine-to-coarse strategy for adult image and video recognition. We employ a deep convolution networks to model fusion features of sensitive objects in images. Global contexts and local contexts are both taken into consideration and are jointly modeled in a unified multicontext deep learning framework. To make the model more discriminative for diverse target objects, we investigate a novel hierarchical method, and a task-specific fine-to-coarse strategy is designed to make the multicontext modeling more suitable for adult object recognition. Furthermore, some recently proposed deep models are investigated. Our approach is extensively evaluated on four different datasets. One dataset is used for ablation experiments, whereas others are used for generalization experiments. Results show significant and consistent improvements over the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {68},
numpages = {25},
keywords = {fine-to-coarse strategy, deep convolutional network, multicontext modeling, Adult image and video recognition}
}

@article{10.1145/3057731,
author = {Agrawal, Rakesh and Golshan, Behzad and Papalexakis, Evangelos E.},
title = {Homogeneity in Web Search Results: Diagnosis and Mitigation},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057731},
doi = {10.1145/3057731},
abstract = {Access to diverse perspectives nurtures an informed citizenry. Google and Bing have emerged as the duopoly that largely arbitrates which English-language documents are seen by web searchers. We present our empirical study over the search results produced by Google and Bing that shows a large overlap. Thus, citizens may not gain different perspectives by simultaneously probing them for the same query. Fortunately, our study also shows that by mining Twitter data, one can obtain search results that are quite distinct from those produced by Google, Bing, and Bing News. Additionally, the users found those results to be quite informative.We also present two novel tools we designed for this study. One uses tensor analysis to derive low-dimensional compact representation of search results and study their behavior over time. The other uses machine learning and quantifies the similarity of results between two search engines by framing it as a prediction problem. Although these tools have different underpinnings, the analytical results obtained using them corroborate each other, which reinforces the confidence one can place in them for finding meaningful insights from big data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {66},
numpages = {35},
keywords = {social media search, Google, search result comparison, big data, tensor, Bing, Web search, search engine, prediction}
}

@article{10.1145/3065951,
author = {Hu, Zhenhen and Wen, Yonggang and Liu, Luoqi and Jiang, Jianguo and Hong, Richang and Wang, Meng and Yan, Shuicheng},
title = {Visual Classification of Furniture Styles},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3065951},
doi = {10.1145/3065951},
abstract = {Furniture style describes the discriminative appearance characteristics of furniture. It plays an important role in real-world indoor decoration. In this article, we explore the furniture style features and study the problem of furniture style classification. Differing from traditional object classification, furniture style classification aims at classifying different furniture in terms of the “style” that describes its appearance (e.g., American style, Gothic style, Rococo style, etc.) rather than the “kind” that is more related to its functional structure (e.g., bed, desk, etc.). To pursue efficient furniture style features, we construct a novel dataset of furniture styles that contains 16 common style categories and implement three strategies with respect to two categories of classification, that is, handcrafted classification and learning-based classification. First, we follow the typical image classification pipeline to extract the handcrafted features and train the classifier by support vector machine. Then we use the convolutional neural network to extract learning-based features from training images. To obtain comprehensive furniture style features, we finally combine the handcrafted image classification pipeline and the learning-based network. We experimentally evaluate the performances of handcrafted features and learning-based features of each strategy, and the results show the superiority of learning-based features and also the comprehensiveness of handcrafted features.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {67},
numpages = {20},
keywords = {image classification, convolutional neural network, Furniture style}
}

@article{10.1145/3057732,
author = {Pan, Weike and Yang, Qiang and Duan, Yuchao and Tan, Ben and Ming, Zhong},
title = {Transfer Learning for Behavior Ranking},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057732},
doi = {10.1145/3057732},
abstract = {Intelligent recommendation has been well recognized as one of the major approaches to address the information overload problem in the big data era. A typical intelligent recommendation engine usually consists of three major components, that is, data as the main input, algorithms for preference learning, and system for user interaction and high-performance computation. We observe that the data (e.g., users’ behavior) are usually in different forms, such as examinations (e.g., browse and collection) and ratings, where the former are often much more abundant than the latter. Although the data are in different representations, they are both related to users’ true preferences and are also deemed complementary to each other for preference learning. However, very few ranking or recommendation algorithms have been developed to exploit such two types of user behavior.In this article, we focus on jointly modeling the examination behavior and rating behavior and develop a novel and efficient ranking-oriented recommendation algorithm accordingly. First, we formally define a new recommendation problem termed behavior ranking, which aims to build a ranking-oriented model by exploiting both the examination behavior and rating behavior. Second, we develop a simple and generic transfer to rank (ToR) algorithm for behavior ranking, which transfers knowledge of candidate items from a global preference learning task to a local preference learning task. Compared with the previous work on integrating heterogeneous user behavior, our ToR algorithm is the first ranking-oriented solution, which can effectively generate recommendations in a more direct manner than those regression-oriented methods. Extensive empirical studies show that our ToR algorithm performs significantly more accurately than the state-of-the-art methods in most cases. Furthermore, our ToR algorithm is very efficient in terms of the time complexity, which is similar to those for homogeneous user behavior alone.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {65},
numpages = {23},
keywords = {behavior ranking, preference learning, Transfer to rank, collaborative recommendation}
}

@article{10.1145/3035969,
author = {Lagr\'{e}e, Paul and Cautis, Bogdan and Vahabi, Hossein},
title = {As-You-Type Social Aware Search},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3035969},
doi = {10.1145/3035969},
abstract = {Modern search applications feature real-time as-you-type query search. In its elementary form, the problem consists in retrieving a set of k search results, that is, performing a search with a given prefix, and showing the top-ranked results. In this article, we focus on as-you-type keyword search over social media, that is, data published by users who are interconnected through a social network. We adopt a “network-aware” interpretation for information relevance, by which information produced by users who are closer to the user issuing a request is considered more relevant. This query model raises new challenges for effectiveness and efficiency in online search, even when the intent of the user is fully specified, as a complete query given as input in one keystroke. This is mainly because it requires a joint exploration of the social space and traditional IR indexes, such as inverted lists. We describe a memory-efficient and incremental prefix-based retrieval algorithm, which also exhibits an anytime behavior, allowing output of the most likely answer within any chosen runtime limit. We evaluate our approach through extensive experiments for several applications and search scenarios. We consider searching for posts in microblogging (Twitter and Tumblr), for businesses (Yelp), as well as for movies (Amazon) based on reviews. We also conduct a series of experiments comparing our algorithm with baselines using state-of-the-art techniques and measuring the improvements brought by several key optimizations. They show that our solution is effective in answering real-time as-you-type searches over social media.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {63},
numpages = {31},
keywords = {micro-blogging applications, network-aware search, social networks, As-you-type search}
}

@article{10.1145/3035967,
author = {Yao, Lina and Sheng, Quan Z. and Ngu, Anne H. H. and Li, Xue and Benattalah, Boualem},
title = {Unveiling Correlations via Mining Human-Thing Interactions in the Web of Things},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3035967},
doi = {10.1145/3035967},
abstract = {With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations among ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based approach for discovering underlying connections of things via mining the rich content embodied in the human-thing interactions in terms of user, temporal, and spatial information. We model this various information using two graphs, namely a spatio-temporal graph and a social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management and analytics. To demonstrate the utility of the proposed approach, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {62},
numpages = {25},
keywords = {random walk with restart, correlation discovery, Web of Things}
}

@article{10.1145/2998550,
author = {Kolman, Eyal and Pinkas, Benny},
title = {Securely Computing a Ground Speed Model},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2998550},
doi = {10.1145/2998550},
abstract = {Consider a server offering risk assessment services and potential clients of these services. The risk assessment model that is run by the server is based on current and historical data of the clients. However, the clients might prefer not sharing such sensitive data with external parties such as the server, and the server might consider the possession of this data as a liability rather than an asset. Secure multi-party computation (MPC) enables one, in principle, to compute any function while hiding the inputs to the function, and would thus enable the computation of the risk assessment model while hiding the client’s data from the server. However, a direct application of a generic MPC solution to this problem is rather inefficient due to the large scale of the data and the complexity of the function.We examine a specific case of risk assessment—the ground speed model. In this model, the geographical locations of successive user-authentication attempts are compared, and a warning flag is raised if the physical speed required to move between these locations is greater than some threshold, and some other conditions, such as authentication from two related networks, do not hold. We describe a very efficient secure computation solution that is tailored for this problem. This solution demonstrates that a risk model can be applied over encrypted data with sufficient efficiency to fit the requirements of commercial systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {54},
numpages = {13},
keywords = {Secure multi-party computation, risk models}
}

@article{10.1145/3057729,
author = {Harel, Yaniv and Gal, Irad Ben and Elovici, Yuval},
title = {Cyber Security and the Role of Intelligent Systems in Addressing Its Challenges},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057729},
doi = {10.1145/3057729},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {49},
numpages = {12},
keywords = {cyber phenomenon, internet of things, cyber technology trends, cyber collaboration, Cyber security, autonomous systems, cyber definition, cyber challenges, intelligent systems, artificial intelligence, machine learning}
}

@article{10.1145/3057727,
author = {Ben-Israel, Isaac},
title = {The Letter from Prof. Maj. Gen. (Ret.) Isaac Ben-Israel},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3057727},
doi = {10.1145/3057727},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {49e},
numpages = {1}
}

@article{10.1145/3046676,
author = {Hirschprung, Ron and Toch, Eran and Schwartz-Chassidim, Hadas and Mendel, Tamir and Maimon, Oded},
title = {Analyzing and Optimizing Access Control Choice Architectures in Online Social Networks},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3046676},
doi = {10.1145/3046676},
abstract = {The way users manage access to their information and computers has a tremendous effect on the overall security and privacy of individuals and organizations. Usually, access management is conducted using a choice architecture, a behavioral economics concept that describes the way decisions are framed to users. Studies have consistently shown that the design of choice architectures, mainly the selection of default options, has a strong effect on the final decisions users make by nudging them toward certain behaviors. In this article, we propose a method for optimizing access control choice architectures in online social networks. We empirically evaluate the methodology on Facebook, the world's largest online social network, by measuring how well the default options cover the existing user choices and preferences and toward which outcome the choice architecture nudges users. The evaluation includes two parts: (a) collecting access control decisions made by 266 users of Facebook for a period of 3 months; and (b) surveying 533 participants who were asked to express their preferences regarding default options. We demonstrate how optimal defaults can be algorithmically identified from users’ decisions and preferences, and we measure how existing defaults address users’ preferences compared with the optimal ones. We analyze how access control defaults can better serve existing users, and we discuss how our method can be used to establish a common measuring tool when examining the effects of default options.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {57},
numpages = {22},
keywords = {privacy, choice architecture, social networks, Access control}
}

@article{10.1145/3041216,
author = {Panagopoulos, Athanasios Aris and Maleki, Sasan and Rogers, Alex and Venanzi, Matteo and Jennings, Nicholas R.},
title = {Advanced Economic Control of Electricity-Based Space Heating Systems in Domestic Coalitions with Shared Intermittent Energy Resources},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3041216},
doi = {10.1145/3041216},
abstract = {Over the past few years, Domestic Heating Automation Systems (DHASs) that optimize the domestic space heating control process with minimum user input, utilizing appropriate occupancy prediction technology, have emerged as commercial products (e.g., the smart thermostats from Nest and Honeywell). At the same time, many houses are being equipped with, potentially grid-connected, Intermittent Energy Resources (IERs), such as rooftop photovoltaic systems and/or small wind turbine generators. Now, in many regions of the world, such houses can sell energy to the grid but at a lower price than the price of buying it. In this context, and given the anticipated increase in electrification of heating, the next generation DHASs need to incorporate Advanced Economic Control (AEC). Such AEC can exploit the energy buffer that heating loads provide, in order to shift the consumption of electricity-based heating systems to follow the intermittent energy generation of the house. By so doing, the energy imported from the grid can be minimized and considerable monetary gains for the household can be achieved, without affecting the occupants’ schedule. These benefits can be amplified still further in domestic coalitions, where a number of houses come together and share their IER generation to minimize their cumulative grid energy import.Given the above, in this work we extend a state-of-the-art DHAS, to propose AdaHeat+, a practical DHAS, that, for the first time, incorporates AEC. Our work is applicable to both individual houses and domestic coalitions and comes complete with an allocation mechanism to share the coalition gains. Importantly, we propose an effective heuristic heating schedule planning approach for collective AEC that (i) has a complexity that scales in a linear and parallelizable manner with the coalition size, and (ii) enables AdaHeat+ to handle the distinct preferences, in balancing heating cost and thermal discomfort, of the households. Our approach relies on stochastic IER power output predictions. In this context, we propose a simple and effective formulation for the site-specific calibration of such predictions based on adaptive Gaussian process modeling. Finally, we demonstrate the effectiveness of AdaHeat+ through real data evaluation, to show that collective AEC can improve heating cost-efficiency by up to 60%, compared to independent AEC (and even more when compared to no-AEC).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {59},
numpages = {27},
keywords = {energy systems, control, smart grid, Advanced economic control, space heating}
}

@article{10.1145/3040966,
author = {Maltinsky, Alex and Giladi, Ran and Shavitt, Yuval},
title = {On Network Neutrality Measurements},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3040966},
doi = {10.1145/3040966},
abstract = {Network level surveillance, censorship, and various man-in-the-middle attacks target only specific types of network traffic (e.g., HTTP, HTTPS, VoIP, or Email). Therefore, packets of these types will likely receive “special” treatment by a transit network or a man-in-the-middle attacker. A transit Internet Service Provider (ISP) or an attacker may pass the targeted traffic through special software or equipment to gather data or perform an attack. This creates a measurable difference between the performance of the targeted traffic versus the general case. In networking terms, it violates the principle of “network neutrality,” which states that all traffic should be treated equally. Many techniques were designed to detect network neutrality violations, and some have naturally suggested using them to detect surveillance and censorship. In this article, we show that the existing network neutrality measurement techniques can be easily detected and therefore circumvented. We then briefly propose a new approach to overcome the drawbacks of current measurement techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {56},
numpages = {22},
keywords = {Adversarial model}
}

@article{10.1145/2928274,
author = {Neria, Michal Ben and Yacovzada, Nancy-Sarah and Ben-Gal, Irad},
title = {A Risk-Scoring Feedback Model for Webpages and Web Users Based on Browsing Behavior},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2928274},
doi = {10.1145/2928274},
abstract = {It has been claimed that many security breaches are often caused by vulnerable (na\"{\i}ve) employees within the organization [Ponemon Institute LLC 2015a]. Thus, the weakest link in security is often not the technology itself but rather the people who use it [Schneier 2003]. In this article, we propose a machine learning scheme for detecting risky webpages and risky browsing behavior, performed by na\"{\i}ve users in the organization. The scheme analyzes the interaction between two modules: one represents na\"{\i}ve users, while the other represents risky webpages. It implements a feedback loop between these modules such that if a webpage is exposed to a lot of traffic from risky users, its “risk score” increases, while in a similar manner, as the user is exposed to risky webpages (with a high “risk score”), his own “risk score” increases. The proposed scheme is tested on a real-world dataset of HTTP logs provided by a large American toolbar company. The results suggest that a feedback learning process involving webpages and users can improve the scoring accuracy and lead to the detection of unknown malicious webpages.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {53},
numpages = {21},
keywords = {malware detection, spectral clustering, Machine learning, na\"{\i}ve user behavior, link-based ranking algorithms}
}

@article{10.1145/2870641,
author = {Guri, Mordechai and Monitz, Matan and Elovici, Yuval},
title = {Bridging the Air Gap between Isolated Networks and Mobile Phones in a Practical Cyber-Attack},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870641},
doi = {10.1145/2870641},
abstract = {Information is the most critical asset of modern organizations, and accordingly it is one of the resources most coveted by adversaries. When highly sensitive data is involved, an organization may resort to air gap isolation in which there is no networking connection between the inner network and the external world. While infiltrating an air-gapped network has been proven feasible in recent years, data exfiltration from an air-gapped network is still considered one of the most challenging phases of an advanced cyber-attack. In this article, we present “AirHopper,” a bifurcated malware that bridges the air gap between an isolated network and nearby infected mobile phones using FM signals. While it is known that software can intentionally create radio emissions from a video card, this is the first time that mobile phones serve as the intended receivers of the maliciously crafted electromagnetic signals. We examine the attack model and its limitations and discuss implementation considerations such as modulation methods, signal collision, and signal reconstruction. We test AirHopper in an existing workplace at a typical office building and demonstrate how valuable data such as keylogging and files can be exfiltrated from physically isolated computers to mobile phones at a distance of 1--7 meters, with an effective bandwidth of 13--60 bytes per second.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {50},
numpages = {25},
keywords = {TEMPEST, FM Radio, data exfiltration, EMSEC, cyber-attack, Air-gap, APT, bridging the air-gap}
}

@article{10.1145/3014431,
author = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
title = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3014431},
doi = {10.1145/3014431},
abstract = {The unprecedent growth of microblog services poses significant challenges on network traffic and service latency to the underlay infrastructure (i.e., geo-distributed data centers). Furthermore, the dynamic evolution in microblog status generates a huge workload on data consistence maintenance. In this article, motivated by insights of cross-media analysis-based propagation patterns, we propose a novel cache strategy for microblog service systems to reduce the inter-data center traffic and consistence maintenance cost, while achieving low service latency. Specifically, we first present a microblog classification method, which utilizes the external knowledge from correlated domains, to categorize microblogs. Then we conduct a large-scale measurement on a representative online social network system to study the category-based propagation diversity on region and time scales. These insights illustrate social common habits on creating and consuming microblogs and further motivate our architecture design. Finally, we formulate the content cache problem as a constrained optimization problem. By jointly using the Lyapunov optimization framework and simplex gradient method, we find the optimal online control strategy. Extensive trace-driven experiments further demonstrate that our algorithm reduces the system cost by 24.5% against traditional approaches with the same service latency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {40},
numpages = {18},
keywords = {data center, Social media analytics, performance optimization, cross-media analysis}
}

@article{10.1145/3011019,
author = {Wang, Weiqing and Yin, Hongzhi and Chen, Ling and Sun, Yizhou and Sadiq, Shazia and Zhou, Xiaofang},
title = {ST-SAGE: A Spatial-Temporal Sparse Additive Generative Model for Spatial Item Recommendation},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3011019},
doi = {10.1145/3011019},
abstract = {With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important mobile application, especially when users travel away from home. However, this type of recommendation is very challenging compared to traditional recommender systems. A user may visit only a limited number of spatial items, leading to a very sparse user-item matrix. This matrix becomes even sparser when the user travels to a distant place, as most of the items visited by a user are usually located within a short distance from the user’s home. Moreover, user interests and behavior patterns may vary dramatically across different time and geographical regions. In light of this, we propose ST-SAGE, a spatial-temporal sparse additive generative model for spatial item recommendation in this article. ST-SAGE considers both personal interests of the users and the preferences of the crowd in the target region at the given time by exploiting both the co-occurrence patterns and content of spatial items. To further alleviate the data-sparsity issue, ST-SAGE exploits the geographical correlation by smoothing the crowd’s preferences over a well-designed spatial index structure called the spatial pyramid. To speed up the training process of ST-SAGE, we implement a parallel version of the model inference algorithm on the GraphLab framework. We conduct extensive experiments; the experimental results clearly demonstrate that ST-SAGE outperforms the state-of-the-art recommender systems in terms of recommendation effectiveness, model training efficiency, and online recommendation efficiency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {48},
numpages = {25},
keywords = {location-based service, real-time recommendation, efficient retrieval algorithm, online learning, Point of interest (POI)}
}

@article{10.1145/3001593,
author = {Zhang, Jiaming and Wang, Shuhui and Huang, Qingming},
title = {Location-Based Parallel Tag Completion for Geo-Tagged Social Image Retrieval},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3001593},
doi = {10.1145/3001593},
abstract = {Having benefited from tremendous growth of user-generated content, social annotated tags get higher importance in the organization and retrieval of large-scale image databases on Online Sharing Websites (OSW). To obtain high-quality tags from existing community contributed tags with missing information and noise, tag-based annotation or recommendation methods have been proposed for performance promotion of tag prediction. While images from OSW contain rich social attributes, they have not taken full advantage of rich social attributes and auxiliary information associated with social images to construct global information completion models. In this article, beyond the image-tag relation, we take full advantage of the ubiquitous GPS locations and image-user relationship to enhance the accuracy of tag prediction and improve the computational efficiency. For GPS locations, we define the popular geo-locations where people tend to take more images as Points of Interests (POI), which are discovered by mean shift approach. For image-user relationship, we integrate a localized prior constraint, expecting the completed tag sub-matrix in each POI to maintain consistency with users’ tagging behaviors. Based on these two key issues, we propose a unified tag matrix completion framework, which learns the image-tag relation within each POI. To solve the optimization problem, an efficient proximal sub-gradient descent algorithm is designed. The model optimization can be easily parallelized and distributed to learn the tag sub-matrix for each POI. Extensive experimental results reveal that the learned tag sub-matrix of each POI reflects the major trend of users’ tagging results with respect to different POIs and users, and the parallel learning process provides strong support for processing large-scale online image databases. To fit the response time requirement and storage limitations of Tag-based Image Retrieval (TBIR) on mobile devices, we introduce Asymmetric Locality Sensitive Hashing (ALSH) to reduce the time cost and meanwhile improve the efficiency of retrieval.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {38},
numpages = {21},
keywords = {geo-location information, social image retrieval, Tag matrix completion, asymmetric locality sensitive hashing}
}

@article{10.1145/2996200,
author = {Mirsky, Reuth and Gal, Ya’akov (Kobi) and Shieber, Stuart M.},
title = {CRADLE: An Online Plan Recognition Algorithm for Exploratory Domains},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2996200},
doi = {10.1145/2996200},
abstract = {In exploratory domains, agents’ behaviors include switching between activities, extraneous actions, and mistakes. Such settings are prevalent in real world applications such as interaction with open-ended software, collaborative office assistants, and integrated development environments. Despite the prevalence of such settings in the real world, there is scarce work in formalizing the connection between high-level goals and low-level behavior and inferring the former from the latter in these settings. We present a formal grammar for describing users’ activities in such domains. We describe a new top-down plan recognition algorithm called CRADLE (Cumulative Recognition of Activities and Decreasing Load of Explanations) that uses this grammar to recognize agents’ interactions in exploratory domains. We compare the performance of CRADLE with state-of-the-art plan recognition algorithms in several experimental settings consisting of real and simulated data. Our results show that CRADLE was able to output plans exponentially more quickly than the state-of-the-art without compromising its correctness, as determined by domain experts. Our approach can form the basis of future systems that use plan recognition to provide real-time support to users in a growing class of interesting and challenging domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {45},
numpages = {22},
keywords = {Plan recognition, tree languages, interactive learning environments, grammars and context-free languages}
}

@article{10.1145/2990507,
author = {Hoang, Tuan-Anh and Lim, Ee-Peng},
title = {Modeling Topics and Behavior of Microbloggers: An Integrated Approach},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2990507},
doi = {10.1145/2990507},
abstract = {Microblogging encompasses both user-generated content and behavior. When modeling microblogging data, one has to consider personal and background topics, as well as how these topics generate the observed content and behavior. In this article, we propose the Generalized Behavior-Topic (GBT) model for simultaneously modeling background topics and users’ topical interest in microblogging data. GBT considers multiple topical communities (or realms) with different background topical interests while learning the personal topics of each user and the user’s dependence on realms to generate both content and behavior. This differentiates GBT from other previous works that consider either one realm only or content data only. By associating user behavior with the latent background and personal topics, GBT helps to model user behavior by the two types of topics. GBT also distinguishes itself from other earlier works by modeling multiple types of behavior together. Our experiments on two Twitter datasets show that GBT can effectively mine the representative topics for each realm. We also demonstrate that GBT significantly outperforms other state-of-the-art models in modeling content topics and user profiling.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {44},
numpages = {37},
keywords = {microblogging, probabilistic graphic model, Social media, behavior mining, topic modeling, user behavior}
}

@article{10.1145/2987379,
author = {Tao, Dapeng and Tao, Dacheng and Li, Xuelong and Gao, Xinbo},
title = {Large Sparse Cone Non-Negative Matrix Factorization for Image Annotation},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2987379},
doi = {10.1145/2987379},
abstract = {Image annotation assigns relevant tags to query images based on their semantic contents. Since Non-negative Matrix Factorization (NMF) has the strong ability to learn parts-based representations, recently, a number of algorithms based on NMF have been proposed for image annotation and have achieved good performance. However, most of the efforts have focused on the representations of images and annotations. The properties of the semantic parts have not been well studied. In this article, we revisit the sparseness-constrained NMF (sNMF) proposed by Hoyer [2004]. By endowing the sparseness constraint with a geometric interpretation and sNMF with theoretical analyses of the generalization ability, we show that NMF with such a sparseness constraint has three advantages for image annotation tasks: (i) The sparseness constraint is more ℓ0-norm oriented than the ℓ1-norm-based sparseness, which significantly enhances the ability of NMF to robustly learn semantic parts. (ii) The sparseness constraint has a large cone interpretation and thus allows the reconstruction error of NMF to be smaller, which means that the learned semantic parts are more powerful to represent images for tagging. (iii) The learned semantic parts are less correlated, which increases the discriminative ability for annotating images. Moreover, we present a new efficient large sparse cone NMF (LsCNMF) algorithm to optimize the sNMF problem by employing the Nesterov’s optimal gradient method. We conducted experiments on the PASCAL VOC07 dataset and demonstrated the effectiveness of LsCNMF for image annotation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {sparseness constraint, image annotation, Non-negative matrix factorization, Nesterovs optimal gradient}
}

@article{10.1145/3040934,
author = {Ji, Rongrong and Liu, Wei and Xie, Xing and Chen, Yiqiang and Luo, Jiebo},
title = {Mobile Social Multimedia Analytics in the Big Data Era: An Introduction to the Special Issue},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3040934},
doi = {10.1145/3040934},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {3}
}

@article{10.1145/2963105,
author = {Nie, Liqiang and Zhang, Luming and Wang, Meng and Hong, Richang and Farseev, Aleksandr and Chua, Tat-Seng},
title = {Learning User Attributes via Mobile Social Multimedia Analytics},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2963105},
doi = {10.1145/2963105},
abstract = {Learning user attributes from mobile social media is a fundamental basis for many applications, such as personalized and targeting services. A large and growing body of literature has investigated the user attributes learning problem. However, far too little attention has been paid to jointly consider the dual heterogeneities of user attributes learning by harvesting multiple social media sources. In particular, user attributes are complementarily and comprehensively characterized by multiple social media sources, including footprints from Foursqare, daily updates from Twitter, professional careers from Linkedin, and photo posts from Instagram. On the other hand, attributes are inter-correlated in a complex way rather than independent to each other, and highly related attributes may share similar feature sets. Towards this end, we proposed a unified model to jointly regularize the source consistency and graph-constrained relatedness among tasks. As a byproduct, it is able to learn the attribute-specific and attribute-sharing features via graph-guided fused lasso penalty. Besides, we have theoretically demonstrated its optimization. Extensive evaluations on a real-world dataset thoroughly demonstrated the effectiveness of our proposed model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {36},
numpages = {19},
keywords = {mobile social multimedia analytic, Learning user attributes, occupation inference}
}

@article{10.1145/3041217,
author = {An, Bo and Chen, Haipeng and Park, Noseong and Subrahmanian, V. S.},
title = {Data-Driven Frequency-Based Airline Profit Maximization},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3041217},
doi = {10.1145/3041217},
abstract = {Although numerous traditional models predict market share and demand along airline routes, the prediction of existing models is not precise enough, and to the best of our knowledge, there is no use of data mining--based forecasting techniques for improving airline profitability. We propose the maximizing airline profits (MAP) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared to past methods used to forecast market share and demand along airline routes, we introduce a novel ensemble forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson correlation coefficients (greater than 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared to three state-of-the-art works for forecasting market share and demand while showing much lower variance. Using the results of MAP-EF, we develop MAP--bilevel branch and bound (MAP-BBB) and MAP-greedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes to maximize an airline’s profit. We also study two extensions of the profit maximization problem considering frequency constraints and long-term profits. Furthermore, we develop algorithms for computing Nash equilibrium frequencies when there are multiple strategic airlines. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: the U.S. Bureau of Transportation Statistics (BTS), the U.S. Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the U.S. Census Bureau (CB).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {61},
numpages = {28},
keywords = {Ensemble prediction, regression, airline demand and market share prediction, airline profit maximization}
}

@article{10.1145/2890509,
author = {Ovelg\"{o}nne, Michael and Dumitra\c{s}, Tudor and Prakash, B. Aditya and Subrahmanian, V. S. and Wang, Benjamin},
title = {Understanding the Relationship between Human Behavior and Susceptibility to Cyber Attacks: A Data-Driven Approach},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2890509},
doi = {10.1145/2890509},
abstract = {Despite growing speculation about the role of human behavior in cyber-security of machines, concrete data-driven analysis and evidence have been lacking. Using Symantec’s WINE platform, we conduct a detailed study of 1.6 million machines over an 8-month period in order to learn the relationship between user behavior and cyber attacks against their personal computers. We classify users into 4 categories (gamers, professionals, software developers, and others, plus a fifth category comprising everyone) and identify a total of 7 features that act as proxies for human behavior. For each of the 35 possible combinations (5 categories times 7 features), we studied the relationship between each of these seven features and one dependent variable, namely the number of attempted malware attacks detected by Symantec on the machine. Our results show that there is a strong relationship between several features and the number of attempted malware attacks. Had these hosts not been protected by Symantec’s anti-virus product or a similar product, they would likely have been infected. Surprisingly, our results show that software developers are more at risk of engaging in risky cyber-behavior than other categories.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {51},
numpages = {25},
keywords = {user behavior, computer virus, Malware}
}

@article{10.1145/3040967,
author = {Bistaffa, Filippo and Farinelli, Alessandro and Cerquides, Jes\'{u}s and Rodr\'{\i}guez-Aguilar, Juan and Ramchurn, Sarvapali D.},
title = {Algorithms for Graph-Constrained Coalition Formation in the Real World},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3040967},
doi = {10.1145/3040967},
abstract = {Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this article, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (Coalition Formation for Sparse Synergies (CFSS)), which is particularly efficient when applied to a general class of characteristic functions called m + a functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a nonredundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is four orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2,700 agents).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {60},
numpages = {24},
keywords = {collective energy purchasing, networks, Coalition formation, graphs}
}

@article{10.1145/3011871,
author = {Yang, Xitong and Luo, Jiebo},
title = {Tracking Illicit Drug Dealing and Abuse on Instagram Using Multimodal Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3011871},
doi = {10.1145/3011871},
abstract = {Illicit drug trade via social media sites, especially photo-oriented Instagram, has become a severe problem in recent years. As a result, tracking drug dealing and abuse on Instagram is of interest to law enforcement agencies and public health agencies. However, traditional approaches are based on manual search and browsing by trained domain experts, which suffers from the problem of poor scalability and reproducibility. In this article, we propose a novel approach to detecting drug abuse and dealing automatically by utilizing multimodal data on social media. This approach also enables us to identify drug-related posts and analyze the behavior patterns of drug-related user accounts. To better utilize multimodal data on social media, multimodal analysis methods including multi-task learning and decision-level fusion are employed in our framework. We collect three datasets using Instagram and web search engine for training and testing our models. Experiment results on expertly labeled data have demonstrated the effectiveness of our approach, as well as its scalability and reproducibility over labor-intensive conventional approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {58},
numpages = {15},
keywords = {Multimodal analysis, illicit drug, social media}
}

@article{10.1145/3011018,
author = {Kleinmann, Amit and Wool, Avishai},
title = {Automatic Construction of Statechart-Based Anomaly Detection Models for Multi-Threaded Industrial Control Systems},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3011018},
doi = {10.1145/3011018},
abstract = {Traffic of Industrial Control System (ICS) between the Human Machine Interface (HMI) and the Programmable Logic Controller (PLC) is known to be highly periodic. However, it is sometimes multiplexed, due to asynchronous scheduling. Modeling the network traffic patterns of multiplexed ICS streams using Deterministic Finite Automata (DFA) for anomaly detection typically produces a very large DFA and a high false-alarm rate. In this article, we introduce a new modeling approach that addresses this gap. Our Statechart DFA modeling includes multiple DFAs, one per cyclic pattern, together with a DFA-selector that de-multiplexes the incoming traffic into sub-channels and sends them to their respective DFAs. We demonstrate how to automatically construct the statechart from a captured traffic stream. Our unsupervised learning algorithms first build a Discrete-Time Markov Chain (DTMC) from the stream. Next, we split the symbols into sets, one per multiplexed cycle, based on symbol frequencies and node degrees in the DTMC graph. Then, we create a sub-graph for each cycle and extract Euler cycles for each sub-graph. The final statechart is comprised of one DFA per Euler cycle. The algorithms allow for non-unique symbols, which appear in more than one cycle, and also for symbols that appear more than once in a cycle.We evaluated our solution on traces from a production ICS using the Siemens S7-0x72 protocol. We also stress-tested our algorithms on a collection of synthetically-generated traces that simulated multiplexed ICS traces with varying levels of symbol uniqueness and time overlap. The algorithms were able to split the symbols into sets with 99.6% accuracy. The resulting statechart modeled the traces with a median false-alarm rate of as low as 0.483%. In all but the most extreme scenarios, the Statechart model drastically reduced both the false-alarm rate and the learned model size in comparison with the naive single-DFA model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {55},
numpages = {21},
keywords = {Siemens, network-intrusion-detection-system, ICS, S7, Statechart, SCADA}
}

@article{10.1145/2914795,
author = {Ganesan, Rajesh and Jajodia, Sushil and Cam, Hasan},
title = {Optimal Scheduling of Cybersecurity Analysts for Minimizing Risk},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2914795},
doi = {10.1145/2914795},
abstract = {Cybersecurity threats are on the rise with evermore digitization of the information that many day-to-day systems depend upon. The demand for cybersecurity analysts outpaces supply, which calls for optimal management of the analyst resource. Therefore, a key component of the cybersecurity defense system is the optimal scheduling of its analysts. Sensor data is analyzed by automatic processing systems, and alerts are generated. A portion of these alerts is considered to be significant, which requires thorough examination by a cybersecurity analyst. Risk, in this article, is defined as the percentage of unanalyzed or not thoroughly analyzed alerts among the significant alerts by analysts. The article presents a generalized optimization model for scheduling cybersecurity analysts to minimize risk (a.k.a., maximize significant alert coverage by analysts) and maintain risk under a pre-determined upper bound. The article tests the optimization model and its scalability on a set of given sensors with varying analyst experiences, alert generation rates, system constraints, and system requirements. Results indicate that the optimization model is scalable and is capable of identifying both the right mix of analyst expertise in an organization and the sensor-to-analyst allocation in order to maintain risk below a given upper bound. Several meta-principles are presented, which are derived from the optimization model, and they further serve as guiding principles for hiring and scheduling cybersecurity analysts. The simulation studies (validation) of the optimization model outputs indicate that risk varies non-linearly with an analyst/sensor ratio, and for a given analyst/sensor ratio, the risk is independent of the number of sensors in the system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {52},
numpages = {32},
keywords = {Cybersecurity analysts, risk mitigation, simulation, scheduling, optimization, resource allocation}
}

@article{10.1145/3007195,
author = {Xiong, Haoyi and Zhang, Jinghe and Huang, Yu and Leach, Kevin and Barnes, Laura E.},
title = {<i>Daehr</i>: A Discriminant Analysis Framework for Electronic Health Record Data and an Application to Early Detection of Mental Health Disorders},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3007195},
doi = {10.1145/3007195},
abstract = {Electronic health records (EHR) provide a rich source of temporal data that present a unique opportunity to characterize disease patterns and risk of imminent disease. While many data-mining tools have been adopted for EHR-based disease early detection, linear discriminant analysis (LDA) is one of the most commonly used statistical methods. However, it is difficult to train an accurate LDA model for early disease diagnosis when too few patients are known to have the target disease. Furthermore, EHR data are heterogeneous with significant noise. In such cases, the covariance matrices used in LDA are usually singular and estimated with a large variance.This article presents Daehr, an extension of the LDA framework using electronic health record data to address these issues. Beyond existing LDA analyzers, we propose Daehr to (1) eliminate the data noise caused by the manual encoding of EHR data and (2) lower the variance of parameter (covariance matrices) estimation for LDA models when only a few patients’ EHR are available for training. To achieve these two goals, we designed an iterative algorithm to improve the covariance matrix estimation with embedded data-noise/parameter-variance reduction for LDA. We evaluated Daehr extensively using the College Health Surveillance Network, a large, real-world EHR dataset. Specifically, our experiments compared the performance of LDA to three baselines (i.e., LDA and its derivatives) in identifying college students at high risk for mental health disorders from 23 U.S. universities. Experimental results demonstrate Daehr significantly outperforms the three baselines by achieving 1.4%--19.4% higher accuracy and a 7.5%--43.5% higher F1-score.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {47},
numpages = {21},
keywords = {anxiety/depression, temporal order, electronic health data, Predictive models, early detection}
}

@article{10.1145/2967502,
author = {Gao, Yue and Zhang, Hanwang and Zhao, Xibin and Yan, Shuicheng},
title = {Event Classification in Microblogs via Social Tracking},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2967502},
doi = {10.1145/2967502},
abstract = {Social media websites have become important information sharing platforms. The rapid development of social media platforms has led to increasingly large-scale social media data, which has shown remarkable societal and marketing values. There are needs to extract important events in live social media streams. However, microblogs event classification is challenging due to two facts, i.e., the short/conversational nature and the incompatible meanings between the text and the corresponding image in social posts, and the rapidly evolving contents. In this article, we propose to conduct event classification via deep learning and social tracking. First, we introduce a Multi-modal Multi-instance Deep Network (M2DN) for microblogs classification, which is able to handle the weakly labeled microblogs data oriented from the incompatible meanings inside microblogs. Besides predicting each microblogs as predefined events, we propose to employ social tracking to extract social-related auxiliary information to enrich the testing samples. We extract a set of candidate-relevant microblogs in a short time window by using social connections, such as related users and geographical locations. All these selected microblogs and the testing data are formulated in a Markov Random Field model. The inference on the Markov Random Field is conducted to update the classification results of the testing microblogs. This method is evaluated on the Brand-Social-Net dataset for classification of 20 events. Experimental results and comparison with the state of the arts show that the proposed method can achieve better performance for the event classification task.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {14},
keywords = {Event classification, multi-instance, social tracking, Markov Random Field (MRF), multi-modal}
}

@article{10.1145/3001594,
author = {Sang, Jitao and Fang, Quan and Xu, Changsheng},
title = {Exploiting Social-Mobile Information for Location Visualization},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3001594},
doi = {10.1145/3001594},
abstract = {With a smart phone at hand, it becomes easy now to snap pictures and publish them online with few lines of texts. The GPS coordinates and User-Generated Content (UGC) data embedded in the shared photos provide opportunities to exploit important knowledge to tackle interesting tasks like geographically organizing photos and location visualization. In this work, we propose to organize photos both geographically and semantically, and investigate the problem of location visualization from multiple semantic themes. The novel visualization scheme provides a rich display landscape for geographical exploration from versatile views. A two-level solution is presented, where we first identify the highly photographed places of interest (POI) and discover their focused themes, and then aggregate the lower-level POI themes to generate the higher-level city themes for location visualization. We have conducted experiments on crawled Flickr and Instagram data and exhibited the visualization for the cities of Singapore and Sydney. The experimental results have validated the proposed method and demonstrated the potentials of location visualization from multiple themes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {39},
numpages = {19},
keywords = {point of interest, Geotagged photo, location visualization}
}

@article{10.1145/2994608,
author = {Zhang, Peng and Yu, Qian and Hou, Yuexian and Song, Dawei and Li, Jingfei and Hu, Bin},
title = {A Distribution Separation Method Using Irrelevance Feedback Data for Information Retrieval},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2994608},
doi = {10.1145/2994608},
abstract = {In many research and application areas, such as information retrieval and machine learning, we often encounter dealing with a probability distribution that is mixed by one distribution that is relevant to our task in hand and the other that is irrelevant and that we want to get rid of. Thus, it is an essential problem to separate the irrelevant distribution from the mixture distribution. This article is focused on the application in Information Retrieval, where relevance feedback is a widely used technique to build a refined query model based on a set of feedback documents. However, in practice, the relevance feedback set, even provided by users explicitly or implicitly, is often a mixture of relevant and irrelevant documents. Consequently, the resultant query model (typically a term distribution) is often a mixture rather than a true relevance term distribution, leading to a negative impact on the retrieval performance. To tackle this problem, we recently proposed a Distribution Separation Method (DSM), which aims to approximate the true relevance distribution by separating a seed irrelevance distribution from the mixture one. While it achieved a promising performance in an empirical evaluation with simulated explicit irrelevance feedback data, it has not been deployed in the scenario where one should automatically obtain the irrelevance feedback data. In this article, we propose a substantial extension of the basic DSM from two perspectives: developing a further regularization framework and deploying DSM in the automatic irrelevance feedback scenario. Specifically, in order to avoid the output distribution of DSM drifting away from the true relevance distribution when the quality of seed irrelevant distribution (as the input to DSM) is not guaranteed, we propose a DSM regularization framework to constrain the estimation for the relevance distribution. This regularization framework includes three algorithms, each corresponding to a regularization strategy incorporated in the objective function of DSM. In addition, we exploit DSM in automatic (i.e., pseudo) irrelevance feedback, by automatically detecting the seed irrelevant documents via three different document reranking methods. We have carried out extensive experiments based on various TREC datasets, in order to systematically evaluate the proposed methods. The experimental results demonstrate the effectiveness of our proposed approaches in comparison with various strong baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {46},
numpages = {26},
keywords = {irrelevant term distribution, Relevance feedback, distribution separation method, mixture distribution, regularization}
}

@article{10.1145/2990508,
author = {Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Cheng, Debo},
title = {Learning <i>k</i> for KNN Classification},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2990508},
doi = {10.1145/2990508},
abstract = {The K Nearest Neighbor (kNN) method has widely been used in the applications of data mining and machine learning due to its simple implementation and distinguished performance. However, setting all test data with the same k value in the previous kNN methods has been proven to make these methods impractical in real applications. This article proposes to learn a correlation matrix to reconstruct test data points by training data to assign different k values to different test data points, referred to as the Correlation Matrix kNN (CM-kNN for short) classification. Specifically, the least-squares loss function is employed to minimize the reconstruction error to reconstruct each test data point by all training data points. Then, a graph Laplacian regularizer is advocated to preserve the local structure of the data in the reconstruction process. Moreover, an ℓ1-norm regularizer and an ℓ2, 1-norm regularizer are applied to learn different k values for different test data and to result in low sparsity to remove the redundant/noisy feature from the reconstruction process, respectively. Besides for classification tasks, the kNN methods (including our proposed CM-kNN method) are further utilized to regression and missing data imputation. We conducted sets of experiments for illustrating the efficiency, and experimental results showed that the proposed method was more accurate and efficient than existing kNN methods in data-mining applications, such as classification, regression, and missing data imputation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {43},
numpages = {19},
keywords = {missing data imputation, kNN method, sparse learning}
}

@article{10.1145/2987378,
author = {Peng, Chong and Kang, Zhao and Hu, Yunhong and Cheng, Jie and Cheng, Qiang},
title = {Nonnegative Matrix Factorization with Integrated Graph and Feature Learning},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2987378},
doi = {10.1145/2987378},
abstract = {Matrix factorization is a useful technique for data representation in many data mining and machine learning tasks. Particularly, for data sets with all nonnegative entries, matrix factorization often requires that factor matrices be nonnegative, leading to nonnegative matrix factorization (NMF). One important application of NMF is for clustering with reduced dimensions of the data represented in the new feature space. In this paper, we propose a new graph regularized NMF method capable of feature learning and apply it to clustering. Unlike existing NMF methods that treat all features in the original feature space equally, our method distinguishes features by incorporating a feature-wise sparse approximation error matrix in the formulation. It enables important features to be more closely approximated by the factor matrices. Meanwhile, the graph of the data is constructed using cleaner features in the feature learning process, which integrates feature learning and manifold learning procedures into a unified NMF model. This distinctly differs from applying the existing graph-based NMF models after feature selection in that, when these two procedures are independently used, they often fail to align themselves toward obtaining a compact and most expressive data representation. Comprehensive experimental results demonstrate the effectiveness of the proposed method, which outperforms state-of-the-art algorithms when applied to clustering.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {42},
numpages = {29},
keywords = {feature learning, clustering, Non-negative matrix factorization, manifold learning}
}

@article{10.1145/2983921,
author = {Xu, Jun and Xia, Long and Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi},
title = {Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversification},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2983921},
doi = {10.1145/2983921},
abstract = {The queries issued to search engines are often ambiguous or multifaceted, which requires search engines to return diverse results that can fulfill as many different information needs as possible; this is called search result diversification. Recently, the relational learning to rank model, which designs a learnable ranking function following the criterion of maximal marginal relevance, has shown effectiveness in search result diversification [Zhu et al. 2014]. The goodness of a diverse ranking model is usually evaluated with diversity evaluation measures such as α-NDCG [Clarke et al. 2008], ERR-IA [Chapelle et al. 2009], and D#-NDCG [Sakai and Song 2011]. Ideally the learning algorithm would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing relational learning to rank algorithms, however, only train the ranking models by optimizing loss functions that loosely relate to the evaluation measures. To deal with the problem, we propose a general framework for learning relational ranking models via directly optimizing any diversity evaluation measure. In learning, the loss function upper-bounding the basic loss function defined on a diverse ranking measure is minimized. We can derive new diverse ranking algorithms under the framework, and several diverse ranking algorithms are created based on different upper bounds over the basic loss function. We conducted comparisons between the proposed algorithms with conventional diverse ranking methods using the TREC benchmark datasets. Experimental results show that the algorithms derived under the diverse learning to rank framework always significantly outperform the state-of-the-art baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {41},
numpages = {26},
keywords = {relational learning to rank, diversity evaluation measure, Search result diversification}
}

@article{10.1145/2974024,
author = {Liu, Yan and Liu, Yang and Zhong, Shenghua and Wu, Songtao},
title = {Implicit Visual Learning: Image Recognition via Dissipative Learning Model},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2974024},
doi = {10.1145/2974024},
abstract = {According to consciousness involvement, human’s learning can be roughly classified into explicit learning and implicit learning. Contrasting strongly to explicit learning with clear targets and rules, such as our school study of mathematics, learning is implicit when we acquire new information without intending to do so. Research from psychology indicates that implicit learning is ubiquitous in our daily life. Moreover, implicit learning plays an important role in human visual perception. But in the past 60 years, most of the well-known machine-learning models aimed to simulate explicit learning while the work of modeling implicit learning was relatively limited, especially for computer vision applications. This article proposes a novel unsupervised computational model for implicit visual learning by exploring dissipative system, which provides a unifying macroscopic theory to connect biology with physics. We test the proposed Dissipative Implicit Learning Model (DILM) on various datasets. The experiments show that DILM not only provides a good match to human behavior but also improves the explicit machine-learning performance obviously on image classification tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {31},
numpages = {24},
keywords = {image recognition, Implicit learning, visual data analysis, dissipative theory, dissipative implicit learning model}
}

@article{10.1145/2979682,
author = {Barbieri, Nicola and Bonchi, Francesco and Manco, Giuseppe},
title = {Efficient Methods for Influence-Based Network-Oblivious Community Detection},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2979682},
doi = {10.1145/2979682},
abstract = {We study the problem of detecting social communities when the social graph is not available but instead we have access to a log of user activity, that is, a dataset of tuples (u, i, t) recording the fact that user u “adopted” item i at time t. We propose a stochastic framework that assumes that the adoption of items is governed by an underlying diffusion process over the unobserved social network and that such a diffusion model is based on community-level influence. That is, we aim at modeling communities through the lenses of social contagion. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. The general framework is instantiated with two different diffusion models, one with discrete time and one with continuous time, and we show that the computational complexity of both approaches is linear in the number of users and in the size of the propagation log. Experiments on synthetic data with planted community structure show that our methods outperform non-trivial baselines. The effectiveness of the proposed techniques is further validated on real-word data, on which our methods are able to detect high-quality communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {32},
numpages = {31},
keywords = {network-oblivious community detection, information diffusion, social network analysis, Social influence}
}

@article{10.1145/2898363,
author = {Ramamohanarao, Kotagiri and Xie, Hairuo and Kulik, Lars and Karunasekera, Shanika and Tanin, Egemen and Zhang, Rui and Khunayn, Eman Bin},
title = {SMARTS: Scalable Microscopic Adaptive Road Traffic Simulator},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2898363},
doi = {10.1145/2898363},
abstract = {Microscopic traffic simulators are important tools for studying transportation systems as they describe the evolution of traffic to the highest level of detail. A major challenge to microscopic simulators is the slow simulation speed due to the complexity of traffic models. We have developed the Scalable Microscopic Adaptive Road Traffic Simulator (SMARTS), a distributed microscopic traffic simulator that can utilize multiple independent processes in parallel. SMARTS can perform fast large-scale simulations. For example, when simulating 1 million vehicles in an area the size of Melbourne, the system runs 1.14 times faster than real time with 30 computing nodes and 0.2s simulation timestep. SMARTS supports various driver models and traffic rules, such as the car-following model and lane-changing model, which can be driver dependent. It can simulate multiple vehicle types, including bus and tram. The simulator is equipped with a wide range of features that help to customize, calibrate, and monitor simulations. Simulations are accurate and confirm with real traffic behaviours. For example, it achieves 79.1% accuracy in predicting traffic on a 10km freeway 90 minutes into the future. The simulator can be used for predictive traffic advisories as well as traffic management decisions as simulations complete well ahead of real time. SMARTS can be easily deployed to different operating systems as it is developed with the standard Java libraries.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {26},
numpages = {22},
keywords = {Microscopic traffic simulation, distributed computing}
}

@article{10.1145/2979681,
author = {Wu, Zhonggang and Lu, Zhao and Ho, Shan-Yuan},
title = {Community Detection with Topological Structure and Attributes in Information Networks},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2979681},
doi = {10.1145/2979681},
abstract = {Information networks contain objects connected by multiple links and described by rich attributes. Detecting community for these networks is a challenging research problem, because there is a scarcity of effective approaches that balance the features of the network structure and the characteristics of the nodes. Some methods detect communities by considering topological structures while ignoring the contributions of attributes. Other methods have considered both topological structure and attributes but pay a high price in time complexity. We establish a new community detection algorithm which explores both topological <u>S</u>tructure and <u>A</u>ttributes using <u>G</u>lobal structure and <u>L</u>ocal neighborhood features (SAGL) which also has low computational complexity. The first step of SAGL evaluates the global importance of every node and calculates the similarity of each node pair by combining edge strength and node attribute similarity. The second step of SAGL uses a clustering algorithm that identifies communities by measuring the similarity of two nodes, calculated by the distance of their neighbors. Experimental results on three real-world datasets show the effectiveness of SAGL, particularly its fast convergence compared to current state-of-the-art attributed graph clustering methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {33},
numpages = {17},
keywords = {attribute similarity, global importance, topological structure, Community detection}
}

@article{10.1145/2972957,
author = {Peng, Chong and Cheng, Jie and Cheng, Qiang},
title = {A Supervised Learning Model for High-Dimensional and Large-Scale Data},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2972957},
doi = {10.1145/2972957},
abstract = {We introduce a new supervised learning model using a discriminative regression approach. This new model estimates a regression vector to represent the similarity between a test example and training examples while seamlessly integrating the class information in the similarity estimation. This distinguishes our model from usual regression models and locally linear embedding approaches, rendering our method suitable for supervised learning problems in high-dimensional settings. Our model is easily extensible to account for nonlinear relationship and applicable to general data, including both high- and low-dimensional data. The objective function of the model is convex, for which two optimization algorithms are provided. These two optimization approaches induce two scalable solvers that are of mathematically provable, linear time complexity. Experimental results verify the effectiveness of the proposed method on various kinds of data. For example, our method shows comparable performance on low-dimensional data and superior performance on high-dimensional data to several widely used classifiers; also, the linear solvers obtain promising performance on large-scale classification.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {30},
numpages = {23},
keywords = {high dimension, Discriminative regression, supervised learning, large-scale data, classification}
}

@article{10.1145/2970819,
author = {Song, Xuan and Zhang, Quanshi and Sekimoto, Yoshihide and Shibasaki, Ryosuke and Yuan, Nicholas Jing and Xie, Xing},
title = {Prediction and Simulation of Human Mobility Following Natural Disasters},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2970819},
doi = {10.1145/2970819},
abstract = {In recent decades, the frequency and intensity of natural disasters has increased significantly, and this trend is expected to continue. Therefore, understanding and predicting human behavior and mobility during a disaster will play a vital role in planning effective humanitarian relief, disaster management, and long-term societal reconstruction. However, such research is very difficult to perform owing to the uniqueness of various disasters and the unavailability of reliable and large-scale human mobility data. In this study, we collect big and heterogeneous data (e.g., GPS records of 1.6 million users1 over 3 years, data on earthquakes that have occurred in Japan over 4 years, news report data, and transportation network data) to study human mobility following natural disasters. An empirical analysis is conducted to explore the basic laws governing human mobility following disasters, and an effective human mobility model is developed to predict and simulate population movements. The experimental results demonstrate the efficiency of our model, and they suggest that human mobility following disasters can be significantly more predictable and be more easily simulated than previously thought.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {29},
numpages = {23},
keywords = {urban computing, spatiotemporal data mining, disaster informatics, Human mobility}
}

@article{10.1145/2948071,
author = {Marrella, Andrea and Mecella, Massimo and Sardina, Sebastian},
title = {Intelligent Process Adaptation in the SmartPM System},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2948071},
doi = {10.1145/2948071},
abstract = {The increasing application of process-oriented approaches in new challenging dynamic domains beyond business computing (e.g., healthcare, emergency management, factories of the future, home automation, etc.) has led to reconsider the level of flexibility and support required to manage complex knowledge-intensive processes in such domains. A knowledge-intensive process is influenced by user decision making and coupled with contextual data and knowledge production, and involves performing complex tasks in the “physical” real world to achieve a common goal. The physical world, however, is not entirely predictable, and knowledge-intensive processes must be robust to unexpected conditions and adaptable to unanticipated exceptions, recognizing that in real-world environments it is not adequate to assume that all possible recovery activities can be predefined for dealing with the exceptions that can ensue. To tackle this issue, in this paper we present SmartPM, a model and a prototype Process Management System featuring a set of techniques providing support for automated adaptation of knowledge-intensive processes at runtime. Such techniques are able to automatically adapt process instances when unanticipated exceptions occur, without explicitly defining policies to recover from exceptions and without the intervention of domain experts at runtime, aiming at reducing error-prone and costly manual ad-hoc changes, and thus at relieving users from complex adaptations tasks. To accomplish this, we make use of well-established techniques and frameworks from Artificial Intelligence, such as situation calculus, IndiGolog and classical planning. The approach, which is backed by a formal model, has been implemented and validated with a case study based on real knowledge-intensive processes coming from an emergency management domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {25},
numpages = {43},
keywords = {Classical planning, process modeling and execution, knowledge-intensive processes, pervasive applications, process adaptation, situation calculus, indiGolog}
}

@article{10.1145/2956556,
author = {Xie, Liping and Tao, Dacheng and Wei, Haikun},
title = {Joint Structured Sparsity Regularized Multiview Dimension Reduction for Video-Based Facial Expression Recognition},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2956556},
doi = {10.1145/2956556},
abstract = {Video-based facial expression recognition (FER) has recently received increased attention as a result of its widespread application. Using only one type of feature to describe facial expression in video sequences is often inadequate, because the information available is very complex. With the emergence of different features to represent different properties of facial expressions in videos, an appropriate combination of these features becomes an important, yet challenging, problem. Considering that the dimensionality of these features is usually high, we thus introduce multiview dimension reduction (MVDR) into video-based FER. In MVDR, it is critical to explore the relationships between and within different feature views. To achieve this goal, we propose a novel framework of MVDR by enforcing joint structured sparsity at both inter- and intraview levels. In this way, correlations on and between the feature spaces of different views tend to be well-exploited. In addition, a transformation matrix is learned for each view to discover the patterns contained in the original features, so that the different views are comparable in finding a common representation. The model can be not only performed in an unsupervised manner, but also easily extended to a semisupervised setting by incorporating some domain knowledge. An alternating algorithm is developed for problem optimization, and each subproblem can be efficiently solved. Experiments on two challenging video-based FER datasets demonstrate the effectiveness of the proposed framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {28},
numpages = {21},
keywords = {structured sparsity, dimension reduction, multiple features, Video-based facial expression recognition}
}

@article{10.1145/2926719,
author = {Schindler, Alexander and Rauber, Andreas},
title = {Harnessing Music-Related Visual Stereotypes for Music Information Retrieval},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2926719},
doi = {10.1145/2926719},
abstract = {Over decades, music labels have shaped easily identifiable genres to improve recognition value and subsequently market sales of new music acts. Referring to print magazines and later to music television as important distribution channels, the visual representation thus played and still plays a significant role in music marketing. Visual stereotypes developed over decades that enable us to quickly identify referenced music only by sight without listening. Despite the richness of music-related visual information provided by music videos and album covers as well as T-shirts, advertisements, and magazines, research towards harnessing this information to advance existing or approach new problems of music retrieval or recommendation is scarce or missing. In this article, we present our research on visual music computing that aims to extract stereotypical music-related visual information from music videos. To provide comprehensive and reproducible results, we present the Music Video Dataset, a thoroughly assembled suite of datasets with dedicated evaluation tasks that are aligned to current Music Information Retrieval tasks. Based on this dataset, we provide evaluations of conventional low-level image processing and affect-related features to provide an overview of the expressiveness of fundamental visual properties such as color, illumination, and contrasts. Further, we introduce a high-level approach based on visual concept detection to facilitate visual stereotypes. This approach decomposes the semantic content of music video frames into concrete concepts such as vehicles, tools, and so on, defined in a wide visual vocabulary. Concepts are detected using convolutional neural networks and their frequency distributions as semantic descriptions for a music video. Evaluations showed that these descriptions show good performance in predicting the music genre of a video and even outperform audio-content descriptors on cross-genre thematic tags. Further, highly significant performance improvements were observed by augmenting audio-based approaches through the introduced visual approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {20},
numpages = {21},
keywords = {Music videos, video analysis, visual concept detection}
}

@article{10.1145/2898362,
author = {Kim, Jungeun and Lee, Jae-Gil and Lim, Sungsu},
title = {Differential Flattening: A Novel Framework for Community Detection in Multi-Layer Graphs},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2898362},
doi = {10.1145/2898362},
abstract = {A multi-layer graph consists of multiple layers of weighted graphs, where the multiple layers represent the different aspects of relationships. Considering multiple aspects (i.e., layers) together is essential to achieve a comprehensive and consolidated view. In this article, we propose a novel framework of differential flattening, which facilitates the analysis of multi-layer graphs, and apply this framework to community detection. Differential flattening merges multiple graphs into a single graph such that the graph structure with the maximum clustering coefficient is obtained from the single graph. It has two distinct features compared with existing approaches. First, dealing with multiple layers is done independently of a specific community detection algorithm, whereas previous approaches rely on a specific algorithm. Thus, any algorithm for a single graph becomes applicable to multi-layer graphs. Second, the contribution of each layer to the single graph is determined automatically for the maximum clustering coefficient. Since differential flattening is formulated by an optimization problem, the optimal solution is easily obtained by well-known algorithms such as interior point methods. Extensive experiments were conducted using the Lancichinetti-Fortunato-Radicchi (LFR) benchmark networks as well as the DBLP, 20 Newsgroups, and MIT Reality Mining networks. The results show that our approach of differential flattening leads to discovery of higher-quality communities than baseline approaches and the state-of-the-art algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {27},
numpages = {23},
keywords = {community detection, differential flattening, social networks, multi-layer graphs, Clustering coefficient}
}

@article{10.1145/2953886,
author = {Sandouk, Ubai and Chen, Ke},
title = {Learning Contextualized Music Semantics from Tags Via a Siamese Neural Network},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2953886},
doi = {10.1145/2953886},
abstract = {Music information retrieval faces a challenge in modeling contextualized musical concepts formulated by a set of co-occurring tags. In this article, we investigate the suitability of our recently proposed approach based on a Siamese neural network in fighting off this challenge. By means of tag features and probabilistic topic models, the network captures contextualized semantics from tags via unsupervised learning. This leads to a distributed semantics space and a potential solution to the out of vocabulary problem, which has yet to be sufficiently addressed. We explore the nature of the resultant music-based semantics and address computational needs. We conduct experiments on three public music tag collections—namely, CAL500, MagTag5K and Million Song Dataset—and compare our approach to a number of state-of-the-art semantics learning approaches. Comparative results suggest that this approach outperforms previous approaches in terms of semantic priming and music tag completion.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {24},
numpages = {21},
keywords = {contextualized concept modeling, million song dataset, semantic priming, Distributed semantics learning, out of vocabulary, tag representation}
}

@article{10.1145/2950066,
author = {Tian, Mi and Sandler, Mark B.},
title = {Towards Music Structural Segmentation across Genres: Features, Structural Hypotheses, and Annotation Principles},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2950066},
doi = {10.1145/2950066},
abstract = {This article faces the problem of how different audio features and segmentation methods work with different music genres. A new annotated corpus of Chinese traditional Jingju music is presented. We incorporate this dataset with two existing music datasets from the literature in an integrated retrieval system to evaluate existing features, structural hypotheses, and segmentation algorithms outside a Western bias. A harmonic-percussive source separation technique is introduced to the feature extraction process and brings significant improvement to the segmentation. Results show that different features capture the structural patterns of different music genres in different ways. Novelty- or homogeneity-based segmentation algorithms and timbre features can surpass the investigated alternatives for the structure analysis of Jingju due to their lack of harmonic repetition patterns. Findings indicate that the design of audio features and segmentation algorithms as well as the consideration of contextual information related to the music corpora should be accounted dependently in an effective segmentation system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {23},
numpages = {19},
keywords = {music structural segmentation, evaluation, Music information retrieval, non-western music, data collection, harmonic-percussive source separation}
}

@article{10.1145/2926718,
author = {Oramas, Sergio and Ostuni, Vito Claudio and Noia, Tommaso Di and Serra, Xavier and Sciascio, Eugenio Di},
title = {Sound and Music Recommendation with Knowledge Graphs},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2926718},
doi = {10.1145/2926718},
abstract = {The Web has moved, slowly but steadily, from a collection of documents towards a collection of structured data. Knowledge graphs have then emerged as a way of representing the knowledge encoded in such data as well as a tool to reason on them in order to extract new and implicit information. Knowledge graphs are currently used, for example, to explain search results, to explore knowledge spaces, to semantically enrich textual documents, or to feed knowledge-intensive applications such as recommender systems. In this work, we describe how to create and exploit a knowledge graph to supply a hybrid recommendation engine with information that builds on top of a collections of documents describing musical and sound items. Tags and textual descriptions are exploited to extract and link entities to external graphs such as WordNet and DBpedia, which are in turn used to semantically enrich the initial data. By means of the knowledge graph we build, recommendations are computed using a feature combination hybrid approach. Two explicit graph feature mappings are formulated to obtain meaningful item feature representations able to catch the knowledge embedded in the graph. Those content features are further combined with additional collaborative information deriving from implicit user feedback. An extensive evaluation on historical data is performed over two different datasets: a dataset of sounds composed of tags, textual descriptions, and user’s download information gathered from Freesound.org and a dataset of songs that mixes song textual descriptions with tags and user’s listening habits extracted from Songfacts.com and Last.fm, respectively. Results show significant improvements with respect to state-of-the-art collaborative algorithms in both datasets. In addition, we show how the semantic expansion of the initial descriptions helps in achieving much better recommendation quality in terms of aggregated diversity and novelty.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {21},
numpages = {21},
keywords = {recommender systems, diversity, novelty, music, entity linking, Knowledge graphs}
}

@article{10.1145/2926717,
author = {Rodriguez-Serrano, Francisco Jose and Carabias-Orti, Julio Jose and Vera-Candeas, Pedro and Martinez-Munoz, Damian},
title = {Tempo Driven Audio-to-Score Alignment Using Spectral Decomposition and Online Dynamic Time Warping},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2926717},
doi = {10.1145/2926717},
abstract = {In this article, we present an online score following framework designed to deal with automatic accompaniment. The proposed framework is based on spectral factorization and online Dynamic Time Warping (DTW) and has two separated stages: preprocessing and alignment. In the first one, we convert the score into a reference audio signal using a MIDI synthesizer software and we analyze the provided information in order to obtain the spectral patterns (i.e., basis functions) associated to each score unit. In this work, a score unit represents the occurrence of concurrent or isolated notes in the score. These spectral patterns are learned from the synthetic MIDI signal using a method based on Non-negative Matrix Factorization (NMF) with Beta-divergence, where the gains are initialized as the ground-truth transcription inferred from the MIDI. On the second stage, a non-iterative signal decomposition method with fixed spectral patterns per score unit is used over the magnitude spectrogram of the input signal resulting in a distortion matrix that can be interpreted as the cost of the matching for each score unit at each frame. Finally, the relation between the performance and the musical score times is obtained using a strategy based on online DTW, where the optimal path is biased by the speed of interpretation. Our system has been evaluated and compared to other systems, yielding reliable results and performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {22},
numpages = {20},
keywords = {Accompaniment, non-negative matrix factorization (NMF), speed of interpretation, online algorithm, beta-divergence, dynamic time warping (DTW), audio-to-score alignment, score following, tempo}
}

@article{10.1145/2991468,
author = {Schedl, Markus and Yang, Yi-Hsuan and Herrera-Boyer, Perfecto},
title = {Introduction to Intelligent Music Systems and Applications},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2991468},
doi = {10.1145/2991468},
abstract = {Intelligent technologies have become an essential part of music systems and applications. This is evidenced by today's omnipresence of digital online music stores and streaming services, which rely on music recommenders, automatic playlist generators, and music browsing interfaces. A large amount of research leading to intelligent music applications deals with the extraction of musical and acoustic information directly from the audio signal using signal processing techniques. Other strategies exploit contextual aspects of music, not present in the signal, for example, community meta-data and trails of user interaction, as found, for instance, on social media platforms. In this editorial, we discuss the notion of “intelligent music system” and give an overview of the papers selected to this special issue.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {17},
numpages = {8},
keywords = {Intelligent music systems, music information retrieval, machine learning}
}

@article{10.1145/2915921,
author = {Papalexakis, Evangelos E. and Faloutsos, Christos and Sidiropoulos, Nicholas D.},
title = {Tensors for Data Mining and Data Fusion: Models, Applications, and Scalable Algorithms},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2915921},
doi = {10.1145/2915921},
abstract = {Tensors and tensor decompositions are very powerful and versatile tools that can model a wide variety of heterogeneous, multiaspect data. As a result, tensor decompositions, which extract useful latent information out of multiaspect data tensors, have witnessed increasing popularity and adoption by the data mining community. In this survey, we present some of the most widely used tensor decompositions, providing the key insights behind them, and summarizing them from a practitioner’s point of view. We then provide an overview of a very broad spectrum of applications where tensors have been instrumental in achieving state-of-the-art performance, ranging from social network analysis to brain data analysis, and from web mining to healthcare. Subsequently, we present recent algorithmic advances in scaling tensor decompositions up to today’s big data, outlining the existing systems and summarizing the key ideas behind them. Finally, we conclude with a list of challenges and open problems that outline exciting future research directions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {16},
numpages = {44},
keywords = {multi-way analysis, multi-aspect data, Tensors, tensor factorization, tensor decomposition}
}

@article{10.1145/2899004,
author = {Widmer, Gerhard},
title = {Getting Closer to the Essence of Music: The <i>Con Espressione</i> Manifesto},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2899004},
doi = {10.1145/2899004},
abstract = {This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on but often ignored in everyday research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {19},
numpages = {13},
keywords = {music perception, MIR, musical expressivity}
}

@article{10.1145/2897738,
author = {Pachet, Fran\c{c}ois},
title = {A Joyful Ode to Automatic Orchestration},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897738},
doi = {10.1145/2897738},
abstract = {Most works in automatic music generation have addressed so far specific tasks. Such a reductionist approach has been extremely successful and some of these tasks have been solved once and for all. However, few works have addressed the issue of generating automatically fully fledged music material, of human-level quality. In this article, we report on a specific experiment in holistic music generation: the reorchestration of Beethoven’s Ode to Joy, the European anthem, in seven styles. These reorchestrations were produced with algorithms developed in the Flow Machines project and within a short time frame. We stress the benefits of having had such a challenging and unifying goal, and the interesting problems and challenges it raised along the way.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {18},
numpages = {13},
keywords = {generation, music, Machine learning}
}

@article{10.1145/2890510,
author = {Towne, W. Ben and Ros\'{e}, Carolyn P. and Herbsleb, James D.},
title = {Measuring Similarity Similarly: LDA and Human Perception},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2890510},
doi = {10.1145/2890510},
abstract = {Several intelligent technologies designed to improve navigability in and digestibility of text corpora use topic modeling such as the state-of-the-art Latent Dirichlet Allocation (LDA). This model and variants on it provide lower-dimensional document representations used in visualizations and in computing similarity between documents. This article contributes a method for validating such algorithms against human perceptions of similarity, especially applicable to contexts in which the algorithm is intended to support navigability between similar documents via dynamically generated hyperlinks. Such validation enables researchers to ground their methods in context of intended use instead of relying on assumptions of fit. In addition to the methodology, this article presents the results of an evaluation using a corpus of short documents and the LDA algorithm. We also present some analysis of potential causes of differences between cases in which this model matches human perceptions of similarity more or less well.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {7},
numpages = {28},
keywords = {algorithm validation, similarity metrics, Perceived similarity}
}

@article{10.1145/2932193,
author = {Wang, Jialei and Zhao, Peilin and Hoi, Steven C. H.},
title = {Soft Confidence-Weighted Learning},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2932193},
doi = {10.1145/2932193},
abstract = {Online learning plays an important role in many big data mining problems because of its high efficiency and scalability. In the literature, many online learning algorithms using gradient information have been applied to solve online classification problems. Recently, more effective second-order algorithms have been proposed, where the correlation between the features is utilized to improve the learning efficiency. Among them, Confidence-Weighted (CW) learning algorithms are very effective, which assume that the classification model is drawn from a Gaussian distribution, which enables the model to be effectively updated with the second-order information of the data stream. Despite being studied actively, these CW algorithms cannot handle nonseparable datasets and noisy datasets very well. In this article, we propose a family of Soft Confidence-Weighted (SCW) learning algorithms for both binary classification and multiclass classification tasks, which is the first family of online classification algorithms that enjoys four salient properties simultaneously: (1) large margin training, (2) confidence weighting, (3) capability to handle nonseparable data, and (4) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW, and NHERD), we found that SCW in general achieves better or at least comparable predictive performance, but enjoys considerably better efficiency advantage (i.e., using a smaller number of updates and lower time cost). To facilitate future research, we release all the datasets and source code to the public at http://libol.stevenhoi.org/.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {15},
numpages = {32},
keywords = {Confidence weighted, second-order algorithms, multiclass classification, binary classification}
}

@article{10.1145/2901299,
author = {Cheng, Chen and Yang, Haiqin and King, Irwin and Lyu, Michael R.},
title = {A Unified Point-of-Interest Recommendation Framework in Location-Based Social Networks},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2901299},
doi = {10.1145/2901299},
abstract = {Location-based social networks (LBSNs), such as Gowalla, Facebook, Foursquare, Brightkite, and so on, have attracted millions of users to share their social friendship and their locations via check-ins in the past few years. Plenty of valuable information is accumulated based on the check-in behaviors, which makes it possible to learn users’ moving patterns as well as their preferences. In LBSNs, point-of-interest (POI) recommendation is one of the most significant tasks because it can help targeted users explore their surroundings as well as help third-party developers provide personalized services. Matrix factorization is a promising method for this task because it can capture users’ preferences to locations and is widely adopted in traditional recommender systems such as movie recommendation. However, the sparsity of the check-in data makes it difficult to capture users’ preferences accurately. Geographical influence can help alleviate this problem and have a large impact on the final recommendation result. By studying users’ moving patterns, we find that users tend to check in around several centers and different users have different numbers of centers. Based on this, we propose a Multi-center Gaussian Model (MGM) to capture this pattern via modeling the probability of a user’s check-in on a location. Moreover, users are usually more interested in the top 20 or even top 10 recommended POIs, which makes personalized ranking important in this task. From previous work, directly optimizing for pairwise ranking like Bayesian Personalized Ranking (BPR) achieves better performance in the top-k recommendation than directly using matrix matrix factorization that aims to minimize the point-wise rating error. To consider users’ preferences, geographical influence and personalized ranking, we propose a unified POI recommendation framework, which unifies all of them together. Specifically, we first fuse MGM with matrix factorization methods and further with BPR using two different approaches. We conduct experiments on Gowalla and Foursquare datasets, which are two large-scale real-world LBSN datasets publicly available online. The results on both datasets show that our unified POI recommendation framework can produce better performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {10},
numpages = {21},
keywords = {location recommendation, location-based social networks, Data mining, recommender systems}
}

@article{10.1145/2899005,
author = {Noia, Tommaso Di and Ostuni, Vito Claudio and Tomeo, Paolo and Sciascio, Eugenio Di},
title = {SPrank: Semantic Path-Based Ranking for Top-<i>N</i> Recommendations Using Linked Open Data},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2899005},
doi = {10.1145/2899005},
abstract = {In most real-world scenarios, the ultimate goal of recommender system applications is to suggest a short ranked list of items, namely top-N recommendations, that will appeal to the end user. Often, the problem of computing top-N recommendations is mainly tackled with a two-step approach. The system focuses first on predicting the unknown ratings, which are eventually used to generate a ranked recommendation list. Actually, the top-N recommendation task can be directly seen as a ranking problem where the main goal is not to accurately predict ratings but to directly find the best-ranked list of items to recommend. In this article we present SPrank, a novel hybrid recommendation algorithm able to compute top-N recommendations exploiting freely available knowledge in the Web of Data. In particular, we employ DBpedia, a well-known encyclopedic knowledge base in the Linked Open Data cloud, to extract semantic path-based features and to eventually compute top-N recommendations in a learning-to-rank fashion. Experiments with three datasets related to different domains (books, music, and movies) prove the effectiveness of our approach compared to state-of-the-art recommendation algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {9},
numpages = {34},
keywords = {DBpedia, hybrid recommender systems, Learning to rank}
}

@article{10.1145/2912148,
author = {Nanni, Mirco and Trasarti, Roberto and Monreale, Anna and Grossi, Valerio and Pedreschi, Dino},
title = {Driving Profiles Computation and Monitoring for Car Insurance CRM},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2912148},
doi = {10.1145/2912148},
abstract = {Customer segmentation is one of the most traditional and valued tasks in customer relationship management (CRM). In this article, we explore the problem in the context of the car insurance industry, where the mobility behavior of customers plays a key role: Different mobility needs, driving habits, and skills imply also different requirements (level of coverage provided by the insurance) and risks (of accidents). In the present work, we describe a methodology to extract several indicators describing the driving profile of customers, and we provide a clustering-oriented instantiation of the segmentation problem based on such indicators. Then, we consider the availability of a continuous flow of fresh mobility data sent by the circulating vehicles, aiming at keeping our segments constantly up to date. We tackle a major scalability issue that emerges in this context when the number of customers is large—namely, the communication bottleneck—by proposing and implementing a sophisticated distributed monitoring solution that reduces communications between vehicles and company servers to the essential. We validate the framework on a large database of real mobility data coming from GPS devices on private cars. Finally, we analyze the privacy risks that the proposed approach might involve for the users, providing and evaluating a countermeasure based on data perturbation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {14},
numpages = {26},
keywords = {distributed clustering, Driving profiles, privacy}
}

@article{10.1145/2912147,
author = {Sintsova, Valentina and Pu, Pearl},
title = {Dystemo: Distant Supervision Method for Multi-Category Emotion Recognition in Tweets},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2912147},
doi = {10.1145/2912147},
abstract = {Emotion recognition in text has become an important research objective. It involves building classifiers capable of detecting human emotions for a specific application, for example, analyzing reactions to product launches, monitoring emotions at sports events, or discerning opinions in political debates. Most successful approaches rely heavily on costly manual annotation. To alleviate this burden, we propose a distant supervision method—Dystemo—for automatically producing emotion classifiers from tweets labeled using existing or easy-to-produce emotion lexicons. The goal is to obtain emotion classifiers that work more accurately for specific applications than available emotion lexicons. The success of this method depends mainly on a novel classifier—Balanced Weighted Voting (BWV)—designed to overcome the imbalance in emotion distribution in the initial dataset, and on novel heuristics for detecting neutral tweets. We demonstrate how Dystemo works using Twitter data about sports events, a fine-grained 20-category emotion model, and three different initial emotion lexicons. Through a series of carefully designed experiments, we confirm that Dystemo is effective both in extending initial emotion lexicons of small coverage to find correctly more emotional tweets and in correcting emotion lexicons of low accuracy to perform more accurately.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {13},
numpages = {22},
keywords = {emotion recognition, semi-supervised learning, twitter, Distant supervision, natural language processing, text mining}
}

@article{10.1145/2890511,
author = {Wang, Tianben and Wang, Zhu and Zhang, Daqing and Gu, Tao and Ni, Hongbo and Jia, Jiangbo and Zhou, Xingshe and Lv, Jing},
title = {Recognizing Parkinsonian Gait Pattern by Exploiting Fine-Grained Movement Function Features},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2890511},
doi = {10.1145/2890511},
abstract = {Parkinson's disease (PD) is one of the typical movement disorder diseases among elderly people, which has a serious impact on their daily lives. In this article, we propose a novel computation framework to recognize gait patterns in patients with PD. The key idea of our approach is to distinguish gait patterns in PD patients from healthy individuals by accurately extracting gait features that capture all three aspects of movement functions, that is, stability, symmetry, and harmony. The proposed framework contains three steps: gait phase discrimination, feature extraction and selection, and pattern classification. In the first step, we put forward a sliding window--based method to discriminate four gait phases from plantar pressure data. Based on the gait phases, we extract and select gait features that characterize stability, symmetry, and harmony of movement functions. Finally, we recognize PD gait patterns by applying a hybrid classification model. We evaluate the framework using an open dataset that contains real plantar pressure data of 93 PD patients and 72 healthy individuals. Experimental results demonstrate that our framework significantly outperforms the four baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {6},
numpages = {22},
keywords = {Parkinson’s disease, gait symmetry, gait harmony, gait pattern recognition, gait phases, gait stability}
}

@article{10.1145/2873066,
author = {Phan, Nhathai and Ebrahimi, Javid and Kil, David and Piniewski, Brigitte and Dou, Dejing},
title = {Topic-Aware Physical Activity Propagation with Temporal Dynamics in a Health Social Network},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873066},
doi = {10.1145/2873066},
abstract = {Modeling physical activity propagation, such as activity level and intensity, is a key to preventing obesity from cascading through communities, and to helping spread wellness and healthy behavior in a social network. However, there have not been enough scientific and quantitative studies to elucidate how social communication may deliver physical activity interventions. In this work, we introduce a novel model named Topic-aware Community-level Physical Activity Propagation with Temporal Dynamics (TCPT) to analyze physical activity propagation and social influence at different granularities (i.e., individual level and community level). Given a social network, the TCPT model first integrates the correlations between the content of social communication, social influences, and temporal dynamics. Then, a hierarchical approach is utilized to detect a set of communities and their reciprocal influence strength of physical activities. The experimental evaluation shows not only the effectiveness of our approach but also the correlation of the detected communities with various health outcome measures. Our promising results pave a way for knowledge discovery in health social networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {2},
numpages = {20},
keywords = {Physical activity propagation, health social network}
}

@article{10.1145/2905373,
author = {He, Tieke and Yin, Hongzhi and Chen, Zhenyu and Zhou, Xiaofang and Sadiq, Shazia and Luo, Bin},
title = {A Spatial-Temporal Topic Model for the Semantic Annotation of POIs in LBSNs},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2905373},
doi = {10.1145/2905373},
abstract = {Semantic tags of points of interest (POIs) are a crucial prerequisite for location search, recommendation services, and data cleaning. However, most POIs in location-based social networks (LBSNs) are either tag-missing or tag-incomplete. This article aims to develop semantic annotation techniques to automatically infer tags for POIs. We first analyze two LBSN datasets and observe that there are two types of tags, category-related ones and sentimental ones, which have unique characteristics. Category-related tags are hierarchical, whereas sentimental ones are category-aware. All existing related work has adopted classification methods to predict high-level category-related tags in the hierarchy, but they cannot apply to infer either low-level category tags or sentimental ones.In light of this, we propose a latent-class probabilistic generative model, namely the spatial-temporal topic model (STM), to infer personal interests, the temporal and spatial patterns of topics/semantics embedded in users’ check-in activities, the interdependence between category-topic and sentiment-topic, and the correlation between sentimental tags and rating scores from users’ check-in and rating behaviors. Then, this learned knowledge is utilized to automatically annotate all POIs with both category-related and sentimental tags in a unified way. We conduct extensive experiments to evaluate the performance of the proposed STM on a real large-scale dataset. The experimental results show the superiority of our proposed STM, and we also observe that the real challenge of inferring category-related tags for POIs lies in the low-level ones of the hierarchy and that the challenge of predicting sentimental tags are those with neutral ratings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {12},
numpages = {24},
keywords = {semantic annotation, topic model, POI tagging, location-based social network}
}

@article{10.1145/2903725,
author = {Deng, Zhaohong and Jiang, Yizhang and Ishibuchi, Hisao and Choi, Kup-Sze and Wang, Shitong},
title = {Enhanced Knowledge-Leverage-Based TSK Fuzzy System Modeling for Inductive Transfer Learning},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2903725},
doi = {10.1145/2903725},
abstract = {The knowledge-leverage-based Takagi--Sugeno--Kang fuzzy system (KL-TSK-FS) modeling method has shown promising performance for fuzzy modeling tasks where transfer learning is required. However, the knowledge-leverage mechanism of the KL-TSK-FS can be further improved. This is because available training data in the target domain are not utilized for the learning of antecedents and the knowledge transfer mechanism from a source domain to the target domain is still too simple for the learning of consequents when a Takagi--Sugeno--Kang fuzzy system (TSK-FS) model is trained in the target domain. The proposed method, that is, the enhanced KL-TSK-FS (EKL-TSK-FS), has two knowledge-leverage strategies for enhancing the parameter learning of the TSK-FS model for the target domain using available information from the source domain. One strategy is used for the learning of antecedent parameters, while the other is for consequent parameters. It is demonstrated that the proposed EKL-TSK-FS has higher transfer learning abilities than the KL-TSK-FS. In addition, the EKL-TSK-FS has been further extended for the scene of the multisource domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {11},
numpages = {21},
keywords = {transfer learning, fuzzy systems, fuzzy modeling, missing data, Enhanced KL-TSK-FS, knowledge leverage}
}

@article{10.1145/2894759,
author = {Jiang, Yexi and Perng, Chang-Shing and Sailer, Anca and Silva-Lepe, Ignacio and Zhou, Yang and Li, Tao},
title = {CSM: A Cloud Service Marketplace for Complex Service Acquisition},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2894759},
doi = {10.1145/2894759},
abstract = {The cloud service marketplace (CSM) is an exploratory project aiming to provide “an AppStore for Services.” It is an intelligent online marketplace that facilitates service discovery and acquisition for enterprise customers. Traditional service discovery and acquisition are time-consuming. In the era of OneClick Checkout and pay-as-you-go service plans, users expect services to be purchased online efficiently and conveniently. However, as services are complex and different from software apps, the currently prevailing App Store based on keyword search is inadequate for services.In CSM, exploring and configuring services are an iterative process. Customers provide their requirements in natural language and interact with the system through questioning and answering. Learning from the input, the system can incrementally clarify users’ intention, narrow down the candidate services, and profile the configuration information for the candidates at the same time. CSM’s back end is built around the Services Knowledge Graph (SKG) and leverages data mining technologies to enable the semantic understanding of customers’ requirements. To quantitatively assess the value of CSM, empirical evaluation on real and synthetic datasets and case studies are given to demonstrate the efficacy and effectiveness of the proposed system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {8},
numpages = {25},
keywords = {interactive search, semantic web, Cloud service}
}

@article{10.1145/2882969,
author = {Ganesan, Rajesh and Jajodia, Sushil and Shah, Ankit and Cam, Hasan},
title = {Dynamic Scheduling of Cybersecurity Analysts for Minimizing Risk Using Reinforcement Learning},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2882969},
doi = {10.1145/2882969},
abstract = {An important component of the cyber-defense mechanism is the adequate staffing levels of its cybersecurity analyst workforce and their optimal assignment to sensors for investigating the dynamic alert traffic. The ever-increasing cybersecurity threats faced by today’s digital systems require a strong cyber-defense mechanism that is both reactive in its response to mitigate the known risk and proactive in being prepared for handling the unknown risks. In order to be proactive for handling the unknown risks, the above workforce must be scheduled dynamically so the system is adaptive to meet the day-to-day stochastic demands on its workforce (both size and expertise mix). The stochastic demands on the workforce stem from the varying alert generation and their significance rate, which causes an uncertainty for the cybersecurity analyst scheduler that is attempting to schedule analysts for work and allocate sensors to analysts. Sensor data are analyzed by automatic processing systems, and alerts are generated. A portion of these alerts is categorized to be significant, which requires thorough examination by a cybersecurity analyst. Risk, in this article, is defined as the percentage of significant alerts that are not thoroughly analyzed by analysts. In order to minimize risk, it is imperative that the cyber-defense system accurately estimates the future significant alert generation rate and dynamically schedules its workforce to meet the stochastic workload demand to analyze them. The article presents a reinforcement learning-based stochastic dynamic programming optimization model that incorporates the above estimates of future alert rates and responds by dynamically scheduling cybersecurity analysts to minimize risk (i.e., maximize significant alert coverage by analysts) and maintain the risk under a pre-determined upper bound. The article tests the dynamic optimization model and compares the results to an integer programming model that optimizes the static staffing needs based on a daily-average alert generation rate with no estimation of future alert rates (static workforce model). Results indicate that over a finite planning horizon, the learning-based optimization model, through a dynamic (on-call) workforce in addition to the static workforce, (a) is capable of balancing risk between days and reducing overall risk better than the static model, (b) is scalable and capable of identifying the quantity and the right mix of analyst expertise in an organization, and (c) is able to determine their dynamic (on-call) schedule and their sensor-to-analyst allocation in order to maintain risk below a given upper bound. Several meta-principles are presented, which are derived from the optimization model, and they further serve as guiding principles for hiring and scheduling cybersecurity analysts. Days-off scheduling was performed to determine analyst weekly work schedules that met the cybersecurity system’s workforce constraints and requirements.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {4},
numpages = {21},
keywords = {Cybersecurity analysts, risk mitigation, optimization, genetic algorithm, reinforcement learning, integer programming, dynamic scheduling, resource allocation}
}

@article{10.1145/2898361,
author = {Leskovec, Jure and Sosi\v{c}, Rok},
title = {SNAP: A General-Purpose Network Analysis and Graph-Mining Library},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2898361},
doi = {10.1145/2898361},
abstract = {Large networks are becoming a widely used abstraction for studying complex systems in a broad set of disciplines, ranging from social-network analysis to molecular biology and neuroscience. Despite an increasing need to analyze and manipulate large networks, only a limited number of tools are available for this task.Here, we describe the Stanford Network Analysis Platform (SNAP), a general-purpose, high-performance system that provides easy-to-use, high-level operations for analysis and manipulation of large networks. We present SNAP functionality, describe its implementational details, and give performance benchmarks. SNAP has been developed for single big-memory machines, and it balances the trade-off between maximum performance, compact in-memory graph representation, and the ability to handle dynamic graphs in which nodes and edges are being added or removed over time. SNAP can process massive networks with hundreds of millions of nodes and billions of edges. SNAP offers over 140 different graph algorithms that can efficiently manipulate large graphs, calculate structural properties, generate regular and random graphs, and handle attributes and metadata on nodes and edges. Besides being able to handle large graphs, an additional strength of SNAP is that networks and their attributes are fully dynamic; they can be modified during the computation at low cost. SNAP is provided as an open-source library in C++ as well as a module in Python.We also describe the Stanford Large Network Dataset, a set of social and information real-world networks and datasets, which we make publicly available. The collection is a complementary resource to our SNAP software and is widely used for development and benchmarking of graph analytics algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {1},
numpages = {20},
keywords = {graph analytics, Networks, graphs, open-source software, data mining}
}

@article{10.1145/2888402,
author = {Belcastro, Loris and Marozzo, Fabrizio and Talia, Domenico and Trunfio, Paolo},
title = {Using Scalable Data Mining for Predicting Flight Delays},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2888402},
doi = {10.1145/2888402},
abstract = {Flight delays are frequent all over the world (about 20% of airline flights arrive more than 15min late) and they are estimated to have an annual cost of billions of dollars. This scenario makes the prediction of flight delays a primary issue for airlines and travelers. The main goal of this work is to implement a predictor of the arrival delay of a scheduled flight due to weather conditions. The predicted arrival delay takes into consideration both flight information (origin airport, destination airport, scheduled departure and arrival time) and weather conditions at origin airport and destination airport according to the flight timetable. Airline flight and weather observation datasets have been analyzed and mined using parallel algorithms implemented as MapReduce programs executed on a Cloud platform. The results show a high accuracy in predicting delays above a given threshold. For instance, with a delay threshold of 15min, we achieve an accuracy of 74.2% and 71.8% recall on delayed flights, while with a threshold of 60min, the accuracy is 85.8% and the delay recall is 86.9%. Furthermore, the experimental results demonstrate the predictor scalability that can be achieved performing data preparation and mining tasks as MapReduce applications on the Cloud.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {5},
numpages = {20},
keywords = {scalability, big data, flight delay, Cloud computing, open data}
}

@article{10.1145/2875441,
author = {Doucette, John A. and Pinhey, Graham and Cohen, Robin},
title = {Multiagent Resource Allocation for Dynamic Task Arrivals with Preemption},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2875441},
doi = {10.1145/2875441},
abstract = {In this article, we present a distributed algorithm for allocating resources to tasks in multiagent systems, one that adapts well to dynamic task arrivals where new work arises at short notice. Our algorithm is designed to leverage preemption if it is available, revoking resource allocations to tasks in progress if new opportunities arise that those resources are better suited to handle. Our multiagent model assigns a task agent to each task that must be completed and a proxy agent to each resource that is available. Preemption occurs when a task agent approaches a proxy agent with a sufficiently compelling need that the proxy agent determines the newcomer derives more benefit from the proxy agent’s resource than the task agent currently using that resource. Task agents reason about which resources to request based on a learning of churn and congestion. We compare to a well-established multiagent resource allocation framework that permits preemption under more conservative assumptions and show through simulation that our model allows for improved allocations through more permissive preemption. In all, we offer a novel approach for multiagent resource allocation that is able to cope well with dynamic task arrivals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {3},
numpages = {27},
keywords = {humans and agents-agents for improving human cooperative activities, Agent cooperation-teamwork, coalitions and coordination, agent cooperation-distributed problem solving}
}

@article{10.1145/2920522,
author = {Chen, Kuan-Ta and Alonso, Omar and Larson, Martha and King, Irwin},
title = {Introduction to the Special Issue on Crowd in Intelligent Systems},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2920522},
doi = {10.1145/2920522},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {44},
numpages = {2}
}

@article{10.1145/2897370,
author = {Chen, Chen and Woundefinedniak, Pawe\l{} W. and Romanowski, Andrzej and Obaid, Mohammad and Jaworski, Tomasz and Kucharski, Jacek and Grudzie\'{n}, Krzysztof and Zhao, Shengdong and Fjeld, Morten},
title = {Using Crowdsourcing for Scientific Analysis of Industrial Tomographic Images},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897370},
doi = {10.1145/2897370},
abstract = {In this article, we present a novel application domain for human computation, specifically for crowdsourcing, which can help in understanding particle-tracking problems. Through an interdisciplinary inquiry, we built a crowdsourcing system designed to detect tracer particles in industrial tomographic images, and applied it to the problem of bulk solid flow in silos. As images from silo-sensing systems cannot be adequately analyzed using the currently available computational methods, human intelligence is required. However, limited availability of experts, as well as their high cost, motivates employing additional nonexperts. We report on the results of a study that assesses the task completion time and accuracy of employing nonexpert workers to process large datasets of images in order to generate data for bulk flow research. We prove the feasibility of this approach by comparing results from a user study with data generated from a computational algorithm. The study shows that the crowd is more scalable and more economical than an automatic solution. The system can help analyze and understand the physics of flow phenomena to better inform the future design of silos, and is generalized enough to be applicable to other domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {25},
keywords = {particle tracking, tomography, Silo, crowdsourcing}
}

@article{10.1145/2897368,
author = {Kim, Yubin and Collins-Thompson, Kevyn and Teevan, Jaime},
title = {Using the Crowd to Improve Search Result Ranking and the Search Experience},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897368},
doi = {10.1145/2897368},
abstract = {Despite technological advances, algorithmic search systems still have difficulty with complex or subtle information needs. For example, scenarios requiring deep semantic interpretation are a challenge for computers. People, on the other hand, are well suited to solving such problems. As a result, there is an opportunity for humans and computers to collaborate during the course of a search in a way that takes advantage of the unique abilities of each. While search tools that rely on human intervention will never be able to respond as quickly as current search engines do, recent research suggests that there are scenarios where a search engine could take more time if it resulted in a much better experience. This article explores how crowdsourcing can be used at query time to augment key stages of the search pipeline. We first explore the use of crowdsourcing to improve search result ranking. When the crowd is used to replace or augment traditional retrieval components such as query expansion and relevance scoring, we find that we can increase robustness against failure for query expansion and improve overall precision for results filtering. However, the gains that we observe are limited and unlikely to make up for the extra cost and time that the crowd requires. We then explore ways to incorporate the crowd into the search process that more drastically alter the overall experience. We find that using crowd workers to support rich query understanding and result processing appears to be a more worthwhile way to make use of the crowd during search. Our results confirm that crowdsourcing can positively impact the search experience but suggest that significant changes to the search process may be required for crowdsourcing to fulfill its potential in search systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {24},
keywords = {Slow search, crowdsourcing, information retrieval}
}

@article{10.1145/2850422,
author = {Schulz, Sarah and Pauw, Guy De and Clercq, Orph\'{e}e De and Desmet, Bart and Hoste, V\'{e}ronique and Daelemans, Walter and Macken, Lieve},
title = {Multimodular Text Normalization of Dutch User-Generated Content},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2850422},
doi = {10.1145/2850422},
abstract = {As social media constitutes a valuable source for data analysis for a wide range of applications, the need for handling such data arises. However, the nonstandard language used on social media poses problems for natural language processing (NLP) tools, as these are typically trained on standard language material. We propose a text normalization approach to tackle this problem. More specifically, we investigate the usefulness of a multimodular approach to account for the diversity of normalization issues encountered in user-generated content (UGC). We consider three different types of UGC written in Dutch (SNS, SMS, and tweets) and provide a detailed analysis of the performance of the different modules and the overall system. We also apply an extrinsic evaluation by evaluating the performance of a part-of-speech tagger, lemmatizer, and named-entity recognizer before and after normalization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {61},
numpages = {22},
keywords = {Social media, user-generated content, text normalization}
}

@article{10.1145/2847421,
author = {Xin, Bo and Kawahara, Yoshinobu and Wang, Yizhou and Hu, Lingjing and Gao, Wen},
title = {Efficient Generalized Fused Lasso and Its Applications},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2847421},
doi = {10.1145/2847421},
abstract = {Generalized fused lasso (GFL) penalizes variables with l1 norms based both on the variables and their pairwise differences. GFL is useful when applied to data where prior information is expressed using a graph over the variables. However, the existing GFL algorithms incur high computational costs and do not scale to high-dimensional problems. In this study, we propose a fast and scalable algorithm for GFL. Based on the fact that fusion penalty is the Lov\'{a}sz extension of a cut function, we show that the key building block of the optimization is equivalent to recursively solving graph-cut problems. Thus, we use a parametric flow algorithm to solve GFL in an efficient manner. Runtime comparisons demonstrate a significant speedup compared to existing GFL algorithms. Moreover, the proposed optimization framework is very general; by designing different cut functions, we also discuss the extension of GFL to directed graphs. Exploiting the scalability of the proposed algorithm, we demonstrate the applications of our algorithm to the diagnosis of Alzheimer’s disease (AD) and video background subtraction (BS). In the AD problem, we formulated the diagnosis of AD as a GFL regularized classification. Our experimental evaluations demonstrated that the diagnosis performance was promising. We observed that the selected critical voxels were well structured, i.e., connected, consistent according to cross validation, and in agreement with prior pathological knowledge. In the BS problem, GFL naturally models arbitrary foregrounds without predefined grouping of the pixels. Even by applying simple background models, e.g., a sparse linear combination of former frames, we achieved state-of-the-art performance on several public datasets.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {60},
numpages = {22},
keywords = {background subtraction, Alzheimer’s disease, parametric cut, Generalized fused lasso}
}

@article{10.1145/2776896,
author = {Siddharthan, Advaith and Lambin, Christopher and Robinson, Anne-Marie and Sharma, Nirwan and Comont, Richard and O'mahony, Elaine and Mellish, Chris and Wal, Ren\'{e} Van Der},
title = {Crowdsourcing Without a Crowd: Reliable Online Species Identification Using Bayesian Models to Minimize Crowd Size},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2776896},
doi = {10.1145/2776896},
abstract = {We present an incremental Bayesian model that resolves key issues of crowd size and data quality for consensus labeling. We evaluate our method using data collected from a real-world citizen science program, BeeWatch, which invites members of the public in the United Kingdom to classify (label) photographs of bumblebees as one of 22 possible species. The biological recording domain poses two key and hitherto unaddressed challenges for consensus models of crowdsourcing: (1) the large number of potential species makes classification difficult, and (2) this is compounded by limited crowd availability, stemming from both the inherent difficulty of the task and the lack of relevant skills among the general public. We demonstrate that consensus labels can be reliably found in such circumstances with very small crowd sizes of around three to five users (i.e., through group sourcing). Our incremental Bayesian model, which minimizes crowd size by re-evaluating the quality of the consensus label following each species identification solicited from the crowd, is competitive with a Bayesian approach that uses a larger but fixed crowd size and outperforms majority voting. These results have important ecological applicability: biological recording programs such as BeeWatch can sustain themselves when resources such as taxonomic experts to confirm identifications by photo submitters are scarce (as is typically the case), and feedback can be provided to submitters in a timely fashion. More generally, our model provides benefits to any crowdsourced consensus labeling task where there is a cost (financial or otherwise) associated with soliciting a label.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {45},
numpages = {20},
keywords = {consensus model, biological recording, citizen science, Bayesian reasoning, Crowdsourcing, bumblebee identification}
}

@article{10.1145/2897371,
author = {Wang, Senzhang and Xie, Sihong and Zhang, Xiaoming and Li, Zhoujun and Yu, Philip S. and He, Yueying},
title = {Coranking the Future Influence of Multiobjects in Bibliographic Network Through Mutual Reinforcement},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897371},
doi = {10.1145/2897371},
abstract = {Scientific literature ranking is essential to help researchers find valuable publications from a large literature collection. Recently, with the prevalence of webpage ranking algorithms such as PageRank and HITS, graph-based algorithms have been widely used to iteratively rank papers and researchers through the networks formed by citation and coauthor relationships. However, existing graph-based ranking algorithms mostly focus on ranking the current importance of literature. For researchers who enter an emerging research area, they might be more interested in new papers and young researchers that are likely to become influential in the future, since such papers and researchers are more helpful in letting them quickly catch up on the most recent advances and find valuable research directions. Meanwhile, although some works have been proposed to rank the prestige of a certain type of objects with the help of multiple networks formed of multiobjects, there still lacks a unified framework to rank multiple types of objects in the bibliographic network simultaneously. In this article, we propose a unified ranking framework MRCoRank to corank the future popularity of four types of objects: papers, authors, terms, and venues through mutual reinforcement. Specifically, because the citation data of new publications are sparse and not efficient to characterize their innovativeness, we make the first attempt to extract the text features to help characterize innovative papers and authors. With the observation that the current trend is more indicative of the future trend of citation and coauthor relationships, we then construct time-aware weighted graphs to quantify the importance of links established at different times on both citation and coauthor graphs. By leveraging both the constructed text features and time-aware graphs, we finally fuse the rich information in a mutual reinforcement ranking framework to rank the future importance of multiobjects simultaneously. We evaluate the proposed model through extensive experiments on the ArnetMiner dataset containing more than 1,500,000 papers. Experimental results verify the effectiveness of MRCoRank in coranking the future influence of multiobjects in a bibliographic network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {64},
numpages = {28},
keywords = {literature ranking, Influence mining, mutual reinforcement}
}

@article{10.1145/2897369,
author = {Katsimerou, Christina and Albeda, Joris and Huldtgren, Alina and Heynderickx, Ingrid and Redi, Judith A.},
title = {Crowdsourcing Empathetic Intelligence: The Case of the Annotation of EMMA Database for Emotion and Mood Recognition},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897369},
doi = {10.1145/2897369},
abstract = {Unobtrusive recognition of the user's mood is an essential capability for affect-adaptive systems. Mood is a subtle, long-term affective state, often misrecognized even by humans. The challenge to train a machine to recognize it from, for example, a video of the user, is significant, and already begins with the lack of ground truth for supervised learning. Existing affective databases consist mainly of short videos, annotated in terms of expressed emotions rather than mood. In very few cases, we encounter perceived mood annotations, of questionable reliability, however, due to the subjectivity of mood estimation and the small number of coders involved. In this work, we introduce a new database for mood recognition from video. Our database contains 180 long, acted videos, depicting typical daily scenarios, and subtle facial and bodily expressions. The videos cover three visual modalities (face, body, Kinect data), and are annotated in terms of emotions (via G-trace) and mood (via the Self-Assessment Manikin and the AffectButton). To annotate the database exhaustively, we exploit crowdsourcing to reach out to an extensive number of nonexpert coders. We validate the reliability of our crowdsourced annotations by (1) adopting a number of criteria to filter out unreliable coders, and (2) comparing the annotations of a subset of our videos with those collected in a controlled lab setting.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {51},
numpages = {27},
keywords = {crowdsourcing, Multimodal database, mood recognition, emotion recognition, affective annotation}
}

@article{10.1145/2856057,
author = {Luo, Chen and Zeng, Jia and Yuan, Mingxuan and Dai, Wenyuan and Yang, Qiang},
title = {Telco User Activity Level Prediction with Massive Mobile Broadband Data},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2856057},
doi = {10.1145/2856057},
abstract = {Telecommunication (telco) operators aim to provide users with optimized services and bandwidth in a timely manner. The goal is to increase user experience while retaining profit. To do this, knowing the changing behavior patterns of users through their activity levels in advance can be a great help for operators to adjust their management strategies and reduce operational risk. To achieve this goal, the operators can make use of knowledge discovered from telco’s historical mobile broadband (MBB) records to predict mobile access activity level at an early stage. In this article, we report our research in a real-world telco setting involving more than one million telco users. Our novel contribution includes representing users as documents containing a collection of changing spatiotemporal “words” that express user behavior. By extracting users’ space-time access records in MBB data, we use latent Dirichlet allocation (LDA) to learn user-specific compact topic features for user activity level prediction. We propose a scalable online expectation-maximization (OEM) algorithm that can scale LDA to massive MBB data, which is significantly faster than several state-of-the-art online LDA algorithms. Using these real-world MBB data, we confirm high performance in user activity level prediction. In addition, we show that the inferred topics indicate that future activity level anomalies correlate highly with early skewed bandwidth supply and demand relations. Thus, our prediction system can also guide the telco operators to balance the telecommunication network in terms of supply-demand relations, saving deployment costs and energy of cell towers in the future.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {63},
numpages = {30},
keywords = {Mobile broadband, OEM algorithm, latent Dirichlet allocation, activity level prediction, user-specific topic features, big spatiotemporal data}
}

@article{10.1145/2854157,
author = {Hegedundefineds, Istv\'{a}n and Berta, \'{A}rp\'{a}d and Kocsis, Levente and Bencz\'{u}r, Andr\'{a}s A. and Jelasity, M\'{a}rk},
title = {Robust Decentralized Low-Rank Matrix Decomposition},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2854157},
doi = {10.1145/2854157},
abstract = {Low-rank matrix approximation is an important tool in data mining with a wide range of applications, including recommender systems, clustering, and identifying topics in documents. When the matrix to be approximated originates from a large distributed system, such as a network of mobile phones or smart meters, a challenging problem arises due to the strongly conflicting yet essential requirements of efficiency, robustness, and privacy preservation. We argue that although collecting sensitive data in a centralized fashion may be efficient, it is not an option when considering privacy and efficiency at the same time. Thus, we do not allow any sensitive data to leave the nodes of the network. The local information at each node (personal attributes, documents, media ratings, etc.) defines one row in the matrix. This means that all computations have to be performed at the edge of the network. Known parallel methods that respect the locality constraint, such as synchronized parallel gradient search or distributed iterative methods, require synchronized rounds or have inherent issues with load balancing, and thus they are not robust to failure. Our distributed stochastic gradient descent algorithm overcomes these limitations. During the execution, any sensitive information remains local, whereas the global features (e.g., the factor model of movies) converge to the correct value at all nodes. We present a theoretical derivation and a thorough experimental evaluation of our algorithm. We demonstrate that the convergence speed of our method is competitive while not relying on synchronization and being robust to extreme and realistic failure scenarios. To demonstrate the feasibility of our approach, we present trace-based simulations, real smartphone user behavior analysis, and tests over real movie recommender system data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {62},
numpages = {24},
keywords = {decentralized matrix factorization, singular value decomposition, privacy, stochastic gradient descent, Data mining, online learning, decentralized recommender systems}
}

@article{10.1145/2897367,
author = {Boer, Patrick M. De and Bernstein, Abraham},
title = {PPLib: Toward the Automated Generation of Crowd Computing Programs Using Process Recombination and Auto-Experimentation},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897367},
doi = {10.1145/2897367},
abstract = {Crowdsourcing is increasingly being adopted to solve simple tasks such as image labeling and object tagging, as well as more complex tasks, where crowd workers collaborate in processes with interdependent steps. For the whole range of complexity, research has yielded numerous patterns for coordinating crowd workers in order to optimize crowd accuracy, efficiency, and cost. Process designers, however, often don't know which pattern to apply to a problem at hand when designing new applications for crowdsourcing.In this article, we propose to solve this problem by systematically exploring the design space of complex crowdsourced tasks via automated recombination and auto-experimentation for an issue at hand. Specifically, we propose an approach to finding the optimal process for a given problem by defining the deep structure of the problem in terms of its abstract operators, generating all possible alternatives via the (re)combination of the abstract deep structure with concrete implementations from a Process Repository, and then establishing the best alternative via auto-experimentation.To evaluate our approach, we implemented PPLib (pronounced “People Lib”), a program library that allows for the automated recombination of known processes stored in an easily extensible Process Repository. We evaluated our work by generating and running a plethora of process candidates in two scenarios on Amazon's Mechanical Turk followed by a meta-evaluation, where we looked at the differences between the two evaluations. Our first scenario addressed the problem of text translation, where our automatic recombination produced multiple processes whose performance almost matched the benchmark established by an expert translation. In our second evaluation, we focused on text shortening; we automatically generated 41 crowd process candidates, among them variations of the well-established Find-Fix-Verify process. While Find-Fix-Verify performed well in this setting, our recombination engine produced five processes that repeatedly yielded better results. We close the article by comparing the two settings where the Recombinator was used, and empirically show that the individual processes performed differently in the two settings, which led us to contend that there is no unifying formula, hence emphasizing the necessity for recombination.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {49},
numpages = {20},
keywords = {Human computation algorithms}
}

@article{10.1145/2897365,
author = {Semertzidis, Theodoros and Novak, Jasminko and Lazaridis, Michalis and Melenhorst, Mark and Micheel, Isabel and Michalopoulos, Dimitrios and B\"{o}ckle, Martin and Strintzis, Michael G. and Daras, Petros},
title = {A Crowd-Powered System for Fashion Similarity Search},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897365},
doi = {10.1145/2897365},
abstract = {Driven by the needs of customers and industry, online fashion search and analytics are recently gaining much attention. As fashion is mostly expressed by visual content, the analysis of fashion images in online social networks is a rich source of possible insights on evolving trends and customer preferences. Although a plethora of visual content is available, the modeling of clothes’ physics and movement, the implicit semantics in fashion designs, and the subjectivity of their interpretation pose difficulties to fully automated solutions for fashion search and analysis. In this article, we present the design and evaluation of a crowd-powered system for fashion similarity search from Twitter, supporting trend analysis for fashion professionals. The system enables fashion similarity search based on specific human-based similarity criteria. This is achieved by implementing a novel machine--crowd workflow that supports complex tasks requiring highly subjective judgments where multiple true solutions may coexist. We discuss how this leads to a novel class of crowd-powered systems for which the output of the crowd is not used to verify the automatic analysis but is the desired outcome. Finally, we show how this kind of crowd involvement enables a novel kind of similarity search and represents a crucial factor for the acceptance of system results by the end user.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {46},
numpages = {24},
keywords = {human--machine cooperation, Real-time crowd, crowdsourcing, social multimedia retrieval, interactive systems, similarity dimensions, fashion search}
}

@article{10.1145/2870649,
author = {Han, Shuguang and Dai, Peng and Paritosh, Praveen and Huynh, David},
title = {Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870649},
doi = {10.1145/2870649},
abstract = {Parsing the semantic structure of a web page is a key component of web information extraction. Successful extraction algorithms usually require large-scale training and evaluation datasets, which are difficult to acquire. Recently, crowdsourcing has proven to be an effective method of collecting large-scale training data in domains that do not require much domain knowledge. For more complex domains, researchers have proposed sophisticated quality control mechanisms to replicate tasks in parallel or sequential ways and then aggregate responses from multiple workers. Conventional annotation integration methods often put more trust in the workers with high historical performance; thus, they are called performance-based methods. Recently, Rzeszotarski and Kittur have demonstrated that behavioral features are also highly correlated with annotation quality in several crowdsourcing applications. In this article, we present a new crowdsourcing system, called Wernicke, to provide annotations for web information extraction. Wernicke collects a wide set of behavioral features and, based on these features, predicts annotation quality for a challenging task domain: annotating web page structure. We evaluate the effectiveness of quality control using behavioral features through a case study where 32 workers annotate 200 Q&amp;A web pages from five popular websites. In doing so, we discover several things: (1) Many behavioral features are significant predictors for crowdsourcing quality. (2) The behavioral-feature-based method outperforms performance-based methods in recall prediction, while performing equally with precision prediction. In addition, using behavioral features is less vulnerable to the cold-start problem, and the corresponding prediction model is more generalizable for predicting recall than precision for cross-website quality analysis. (3) One can effectively combine workers’ behavioral information and historical performance information to further reduce prediction errors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {56},
numpages = {25},
keywords = {worker performance, Crowdsourcing, behavioral features, quality control}
}

@article{10.1145/2838738,
author = {Morris, Robert and Johnson, Matthew and Venable, K. Brent and Lindsey, James},
title = {Designing Noise-Minimal Rotorcraft Approach Trajectories},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2838738},
doi = {10.1145/2838738},
abstract = {NASA and the international aviation community are investing in the development of a commercial transportation infrastructure that includes the increased use of rotorcraft, specifically helicopters and civil tilt rotors. However, there is significant concern over the impact of noise on the communities surrounding the transportation facilities. One way to address the rotorcraft noise problem is by exploiting powerful search techniques coming from artificial intelligence to design low-noise flight profiles that can be then validated though field tests. This article investigates the use of discrete heuristic search methods to design low-noise approach trajectories for rotorcraft. Our work builds on a long research tradition in trajectory optimization using either numerical methods or discrete search. Novel features of our approach include the use of a discrete search space with a resolution that can be varied, and the coupling of search with a robust simulator to evaluate candidates. The article includes a systematic comparison of different search techniques; in particular, in the experiments, we are able to do a trade study that compares complete search algorithms such as A* with faster but approximate methods such as local search.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {58},
numpages = {25},
keywords = {optimization, sustainability, Path planning, rotorcraft noise reduction}
}

@article{10.1145/2873065,
author = {Rao, Huaming and Huang, Shih-Wen and Fu, Wai-Tat},
title = {Leveraging Human Computations to Improve Schematization of Spatial Relations from Imagery},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873065},
doi = {10.1145/2873065},
abstract = {The process of generating schematic maps of salient objects from a set of pictures of an indoor environment is challenging. It has been an active area of research as it is crucial to a wide range of context- and location-aware services, as well as for general scene understanding. Although many automated systems have been developed to solve the problem, most of them either require predefining labels or expensive equipment, such as RGBD sensors or lasers, to scan the environment. In this article, we introduce a prototype system to show how human computations can be utilized to generate schematic maps from a set of pictures, without making strong assumptions or demanding extra devices. The system requires humans (crowd workers from Amazon Mechanical Turks) to do simple spatial mapping tasks in various conditions, and their data are aggregated by filtering and clustering techniques that allow salient cues to be identified in the pictures and their spatial relations to be inferred and projected on a two-dimensional map. In particular, we tested and demonstrated the effectiveness of two methods that improved the quality of the generated schematic map: (1) We encouraged humans to adopt an allocentric representations of salient objects by guiding them to perform mental rotations of these objects and (2) we sensitized human perception by guided arrows superimposed on the imagery to improve the accuracy of depth and width estimation. We demonstrated the feasibility of our system by evaluating the results of schematic maps generated from indoor pictures taken from an office building. By calculating Riemannian shape distances between the generated maps to the ground truth, we found that the generated schematic maps captured the spatial relations well. Our results showed that the combination of human computations and machine clustering could lead to more-accurate schematized maps from imagery. We also discuss how our approach may have important insights on methods that leverage human computations in other areas.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {54},
numpages = {21},
keywords = {spatial cognition, Human computations, crowdsourcing, picture schematization}
}

@article{10.1145/2873064,
author = {You, Linlin and Motta, Gianmario and Liu, Kaixu and Ma, Tianyi},
title = {CITY FEED: A Pilot System of Citizen-Sourcing for City Issue Management},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873064},
doi = {10.1145/2873064},
abstract = {Crowdsourcing implies user collaboration and engagement, which fosters a renewal of city governance processes. In this article, we address a subset of crowdsourcing, named citizen-sourcing, where citizens interact with authorities collaboratively and actively. Many systems have experimented citizen-sourcing in city governance processes; however, their maturity levels are mixed. In order to focus on the service maturity, we introduce a city service maturity framework that contains five levels of service support and two levels of information integration. As an example, we introduce CITY FEED, which implements citizen-sourcing in city issue management process. In order to support such process, CITY FEED supports all levels of the maturity framework (publishing, transacting, interacting, collaborating, and evaluating) and integrates related information relationally and heterogeneously. In order to integrate heterogeneous information, it implements a threefold feed deduplication mechanism based on the geographic, text semantic, and image similarities of feeds. Currently, CITY FEED is in a pilot stage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {53},
numpages = {25},
keywords = {Crowdsourcing, city service maturity framework, feed deduplication, citizen-sourced city issue management, citizen-sourcing}
}

@article{10.1145/2873063,
author = {Moshfeghi, Yashar and Rosero, Alvaro Francisco Huertas and Jose, Joemon M.},
title = {A Game-Theory Approach for Effective Crowdsource-Based Relevance Assessment},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873063},
doi = {10.1145/2873063},
abstract = {Despite the ever-increasing popularity of crowdsourcing (CS) in both industry and academia, procedures that ensure quality in its results are still elusive. We hypothesise that a CS design based on game theory can persuade workers to perform their tasks as quickly as possible with the highest quality. In order to do so, in this article we propose a CS framework inspired by the n-person Chicken game. Our aim is to address the problem of CS quality without compromising on CS benefits such as low monetary cost and high task completion speed. With that goal in mind, we study the effects of knowledge updates as well as incentives for good workers to continue playing. We define a general task with the characteristics of relevance assessment as a case study, because it has been widely explored in the past with CS due to its potential cost and complexity. In order to investigate our hypotheses, we conduct a simulation where we study the effect of the proposed framework on data accuracy, task completion time, and total monetary rewards. Based on a game-theoretical analysis, we study how different types of individuals would behave under a particular game scenario. In particular, we simulate a population comprised of different types of workers with varying ability to formulate optimal strategies and learn from their experiences. A simulation of the proposed framework produced results that support our hypothesis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {55},
numpages = {25},
keywords = {relevance assessment, crowdsourcing, Game theory}
}

@article{10.1145/2856102,
author = {Radanovic, Goran and Faltings, Boi and Jurca, Radu},
title = {Incentives for Effort in Crowdsourcing Using the Peer Truth Serum},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2856102},
doi = {10.1145/2856102},
abstract = {Crowdsourcing is widely proposed as a method to solve a large variety of judgment tasks, such as classifying website content, peer grading in online courses, or collecting real-world data. As the data reported by workers cannot be verified, there is a tendency to report random data without actually solving the task. This can be countered by making the reward for an answer depend on its consistency with answers given by other workers, an approach called peer consistency. However, it is obvious that the best strategy in such schemes is for all workers to report the same answer without solving the task.Dasgupta and Ghosh [2013] show that, in some cases, exerting high effort can be encouraged in the highest-paying equilibrium. In this article, we present a general mechanism that implements this idea and is applicable to most crowdsourcing settings. Furthermore, we experimentally test the novel mechanism, and validate its theoretical properties.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {48},
numpages = {28},
keywords = {Crowdsourcing, mechanism design, peer prediction}
}

@article{10.1145/2856058,
author = {Li, Teng and Cheng, Bin and Ni, Bingbing and Liu, Guangchan and Yan, Shuicheng},
title = {Multitask Low-Rank Affinity Graph for Image Segmentation and Image Annotation},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2856058},
doi = {10.1145/2856058},
abstract = {This article investigates a low-rank representation--based graph, which can used in graph-based vision tasks including image segmentation and image annotation. It naturally fuses multiple types of image features in a framework named multitask low-rank affinity pursuit. Given the image patches described with multiple types of features, we aim at inferring a unified affinity matrix that implicitly encodes the relations among these patches. This is achieved by seeking the sparsity-consistent low-rank affinities from the joint decompositions of multiple feature matrices into pairs of sparse and low-rank matrices, the latter of which is expressed as the production of the image feature matrix and its corresponding image affinity matrix. The inference process is formulated as a minimization problem and solved efficiently with the augmented Lagrange multiplier method. Considering image patches as vertices, a graph can be built based on the resulted affinity matrix. Compared to previous methods, which are usually based on a single type of feature, the proposed method seamlessly integrates multiple types of features to jointly produce the affinity matrix in a single inference step. The proposed method is applied to graph-based image segmentation and graph-based image annotation. Experiments on benchmark datasets well validate the superiority of using multiple features over single feature and also the superiority of our method over conventional methods for feature fusion.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {65},
numpages = {18},
keywords = {image segmentation, low rank, Multitask, image annotation}
}

@article{10.1145/2842631,
author = {Fang, Quan and Xu, Changsheng and Hossain, M. Shamim and Muhammad, G.},
title = {STCAPLRS: A Spatial-Temporal Context-Aware Personalized Location Recommendation System},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2842631},
doi = {10.1145/2842631},
abstract = {Newly emerging location-based social media network services (LBSMNS) provide valuable resources to understand users’ behaviors based on their location histories. The location-based behaviors of a user are generally influenced by both user intrinsic interest and the location preference, and moreover are spatial-temporal context dependent. In this article, we propose a spatial-temporal context-aware personalized location recommendation system (STCAPLRS), which offers a particular user a set of location items such as points of interest or venues (e.g., restaurants and shopping malls) within a geospatial range by considering personal interest, local preference, and spatial-temporal context influence. STCAPLRS can make accurate recommendation and facilitate people’s local visiting and new location exploration by exploiting the context information of user behavior, associations between users and location items, and the location and content information of location items. Specifically, STCAPLRS consists of two components: offline modeling and online recommendation. The core module of the offline modeling part is a context-aware regression mixture model that is designed to model the location-based user behaviors in LBSMNS to learn the interest of each individual user, the local preference of each individual location, and the context-aware influence factors. The online recommendation part takes a querying user along with the corresponding querying spatial-temporal context as input and automatically combines the learned interest of the querying user, the local preference of the querying location, and the context-aware influence factor to produce the top-k recommendations. We evaluate the performance of STCAPLRS on two real-world datasets: Dianping and Foursquare. The results demonstrate the superiority of STCAPLRS in recommending location items for users in terms of both effectiveness and efficiency. Moreover, the experimental analysis results also illustrate the excellent interpretability of STCAPLRS.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {59},
numpages = {30},
keywords = {topic model, matrix factorization, Location recommendation}
}

@article{10.1145/2775109,
author = {Wei, Yunchao and Zhao, Yao and Zhu, Zhenfeng and Wei, Shikui and Xiao, Yanhui and Feng, Jiashi and Yan, Shuicheng},
title = {Modality-Dependent Cross-Media Retrieval},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2775109},
doi = {10.1145/2775109},
abstract = {In this article, we investigate the cross-media retrieval between images and text, that is, using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based on the 4,096-dimensional convolutional neural network (CNN) visual feature and 100-dimensional Latent Dirichlet Allocation (LDA) textual feature, the mAP of the proposed method achieves the mAP score of 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {57},
numpages = {13},
keywords = {Cross-media retrieval, canonical correlation analysis, subspace learning}
}

@article{10.1145/2897366,
author = {Borish, Michael and Lok, Benjamin},
title = {Rapid Low-Cost Virtual Human Bootstrapping via the Crowd},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897366},
doi = {10.1145/2897366},
abstract = {Virtual human interactions provide an important avenue for training as emergent opportunities arise. In response to a new training need, we propose a framework to rapidly create experiential learning opportunities in the form of a question--answer chat interaction with virtual humans. This framework takes quickly generated case documents and breaks down the case into small tasks that can be crowdsourced by nonexperts. This framework can serve as a first step to rapidly bootstrapping new virtual humans. We have applied our framework to the task of preparing health care students and professionals to infrequent, but high-stakes, situations such as infectious diseases, cranial nerve disorders, and stroke.Our framework was utilized by medical professionals interested in providing new training experiences to students and colleagues. Over the course of two months, these professionals created seven scenarios on a diverse range of topics that included Ebola, cancer, and neurological disorders. These scenarios were developed for multiple target audiences such as medical students, residents, and fellows. As a first step, each scenario utilized our framework and crowdsourced workers to create an initial corpus over the course of two days.From these seven cases, we selected two to evaluate the quality of the resulting virtual-human corpuses. The two scenarios were compared to preexisting reference scenarios that have been in curricular use for several years. We found a reduction in author time commitment of at least 92% while creating a character that was at least 75% as accurate as its reference counterparts. The commitment reduction and accuracy achieved by our framework represents a first step towards rapid development of a virtual human. Our framework can then be combined with other creation processes for further virtual-human development in order to create a mature virtual human. As part of a virtual-human development process, our framework can help to rapidly develop new scenarios in response to emergent training opportunities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {47},
numpages = {20},
keywords = {crowdsource bootstrap framework, corpus generation, virtual human, Crowdsourcing}
}

@article{10.1145/2870627,
author = {Cremonesi, Paolo and Said, Alan and Tikk, Domonkos and Zhou, Michelle X.},
title = {Introduction to the Special Issue on Recommender System Benchmarking},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870627},
doi = {10.1145/2870627},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {38},
numpages = {4},
keywords = {benchmarking, Recommender systems, evaluation}
}

@article{10.1145/2807705,
author = {Li, Zhifeng and Gong, Dihong and Li, Qiang and Tao, Dacheng and Li, Xuelong},
title = {Mutual Component Analysis for Heterogeneous Face Recognition},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2807705},
doi = {10.1145/2807705},
abstract = {Heterogeneous face recognition, also known as cross-modality face recognition or intermodality face recognition, refers to matching two face images from alternative image modalities. Since face images from different image modalities of the same person are associated with the same face object, there should be mutual components that reflect those intrinsic face characteristics that are invariant to the image modalities. Motivated by this rationality, we propose a novel approach called Mutual Component Analysis (MCA) to infer the mutual components for robust heterogeneous face recognition. In the MCA approach, a generative model is first proposed to model the process of generating face images in different modalities, and then an Expectation Maximization (EM) algorithm is designed to iteratively learn the model parameters. The learned generative model is able to infer the mutual components (which we call the hidden factor, where hidden means the factor is unreachable and invisible, and can only be inferred from observations) that are associated with the person’s identity, thus enabling fast and effective matching for cross-modality face recognition. To enhance recognition performance, we propose an MCA-based multiclassifier framework using multiple local features. Experimental results show that our new approach significantly outperforms the state-of-the-art results on two typical application scenarios: sketch-to-photo and infrared-to-visible face recognition.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {28},
numpages = {23},
keywords = {heterogeneous face recognition, Face recognition, mutual component analysis (MCA)}
}

@article{10.1145/2837029,
author = {Luo, Tie and Das, Sajal K. and Tan, Hwee Pink and Xia, Lirong},
title = {Incentive Mechanism Design for Crowdsourcing: An All-Pay Auction Approach},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2837029},
doi = {10.1145/2837029},
abstract = {Crowdsourcing can be modeled as a principal-agent problem in which the principal (crowdsourcer) desires to solicit a maximal contribution from a group of agents (participants) while agents are only motivated to act according to their own respective advantages. To reconcile this tension, we propose an all-pay auction approach to incentivize agents to act in the principal’s interest, i.e., maximizing profit, while allowing agents to reap strictly positive utility. Our rationale for advocating all-pay auctions is based on two merits that we identify, namely all-pay auctions (i) compress the common, two-stage “bid-contribute” crowdsourcing process into a single “bid-cum-contribute” stage, and (ii) eliminate the risk of task nonfulfillment. In our proposed approach, we enhance all-pay auctions with two additional features: an adaptive prize and a general crowdsourcing environment. The prize or reward adapts itself as per a function of the unknown winning agent’s contribution, and the environment or setting generally accommodates incomplete and asymmetric information, risk-averse (and risk-neutral) agents, and a stochastic (and deterministic) population. We analytically derive this all-pay auction-based mechanism and extensively evaluate it in comparison to classic and optimized mechanisms. The results demonstrate that our proposed approach remarkably outperforms its counterparts in terms of the principal’s profit, agent’s utility, and social welfare.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {26},
keywords = {shading effect, incomplete information, risk aversion, Mobile crowd sensing, Bayesian Nash equilibrium, participatory sensing}
}

@article{10.1145/2819000,
author = {Li, Xiaoyan and Liu, Tongliang and Deng, Jiankang and Tao, Dacheng},
title = {Video Face Editing Using Temporal-Spatial-Smooth Warping},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2819000},
doi = {10.1145/2819000},
abstract = {Editing faces in videos is a popular yet challenging task in computer vision and graphics that encompasses various applications, including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Directly applying the existing warping methods to video face editing has the major problem of temporal incoherence in the synthesized videos, which cannot be addressed by simply employing face tracking techniques or manual interventions, as it is difficult to eliminate the subtly temporal incoherence of the facial feature point localizations in a video sequence. In this article, we propose a temporal-spatial-smooth warping (TSSW) method to achieve a high temporal coherence for video face editing. TSSW is based on two observations: (1) the control lattices are critical for generating warping surfaces and achieving the temporal coherence between consecutive video frames, and (2) the temporal coherence and spatial smoothness of the control lattices can be simultaneously and effectively preserved. Based upon these observations, we impose the temporal coherence constraint on the control lattices on two consecutive frames, as well as the spatial smoothness constraint on the control lattice on the current frame. TSSW calculates the control lattice (in either the horizontal or vertical direction) by updating the control lattice (in the corresponding direction) on its preceding frame, i.e., minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. The contributions of this article are twofold: (1) we develop TSSW, which is robust to the subtly temporal incoherence of the facial feature point localizations and is effective to preserve the temporal coherence and spatial smoothness of the control lattices for editing faces in videos, and (2) we present a new unified video face editing framework that is capable for improving the performances of facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {32},
numpages = {28},
keywords = {Video face editing, temporal coherence, spatial smoothness, warping}
}

@article{10.1145/2809433,
author = {Ye, Jintao and Ming, Zhao Yan and Chua, Tat Seng},
title = {Generating Incremental Length Summary Based on Hierarchical Topic Coverage Maximization},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2809433},
doi = {10.1145/2809433},
abstract = {Document summarization is playing an important role in coping with information overload on the Web. Many summarization models have been proposed recently, but few try to adjust the summary length and sentence order according to application scenarios. With the popularity of handheld devices, presenting key information first in summaries of flexible length is of great convenience in terms of faster reading and decision-making and network consumption reduction. Targeting this problem, we introduce a novel task of generating summaries of incremental length. In particular, we require that the summaries should have the ability to automatically adjust the coverage of general-detailed information when the summary length varies. We propose a novel summarization model that incrementally maximizes topic coverage based on the document’s hierarchical topic model. In addition to the standard Rouge-1 measure, we define a new evaluation metric based on the similarity of the summaries’ topic coverage distribution in order to account for sentence order and summary length. Extensive experiments on Wikipedia pages, DUC 2007, and general noninverted writing style documents from multiple sources show the effectiveness of our proposed approach. Moreover, we carry out a user study on a mobile application scenario to show the usability of the produced summary in terms of improving judgment accuracy and speed, as well as reducing the reading burden and network traffic.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {29},
numpages = {33},
keywords = {Multi-document summarization, data reconstruction}
}

@article{10.1145/2700485,
author = {Doerfel, Stephan and J\"{a}schke, Robert and Stumme, Gerd},
title = {The Role of Cores in Recommender Benchmarking for Social Bookmarking Systems},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700485},
doi = {10.1145/2700485},
abstract = {Social bookmarking systems have established themselves as an important part in today’s Web. In such systems, tag recommender systems support users during the posting of a resource by suggesting suitable tags. Tag recommender algorithms have often been evaluated in offline benchmarking experiments. Yet, the particular setup of such experiments has rarely been analyzed. In particular, since the recommendation quality usually suffers from difficulties such as the sparsity of the data or the cold-start problem for new resources or users, datasets have often been pruned to so-called cores (specific subsets of the original datasets), without much consideration of the implications on the benchmarking results.In this article, we generalize the notion of a core by introducing the new notion of a set-core, which is independent of any graph structure, to overcome a structural drawback in the previous constructions of cores on tagging data. We show that problems caused by some types of cores can be eliminated using set-cores. Further, we present a thorough analysis of tag recommender benchmarking setups using cores. To that end, we conduct a large-scale experiment on four real-world datasets, in which we analyze the influence of different cores on the evaluation of recommendation algorithms. We can show that the results of the comparison of different recommendation approaches depends on the selection of core type and level. For the benchmarking of tag recommender algorithms, our results suggest that the evaluation must be set up more carefully and should not be based on one arbitrarily chosen core type and level.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {40},
numpages = {33},
keywords = {evaluation, Recommender, preprocessing, core, graph, benchmarking}
}

@article{10.1145/2845089,
author = {Ding, Changxing and Tao, Dacheng},
title = {A Comprehensive Survey on Pose-Invariant Face Recognition},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2845089},
doi = {10.1145/2845089},
abstract = {The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, Pose-Invariant Face Recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this article, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, that is, pose-robust feature extraction approaches, multiview subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {37},
numpages = {42},
keywords = {pose-robust feature, survey, face synthesis, multiview learning, Pose-invariant face recognition}
}

@article{10.1145/2835496,
author = {Ben-Shimon, David and Rokach, Lior and Shani, Guy and Shapira, Bracha},
title = {Anytime Algorithms for Recommendation Service Providers},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2835496},
doi = {10.1145/2835496},
abstract = {Recommender systems (RS) can now be found in many commercial Web sites, often presenting customers with a short list of additional products that they might purchase. Many commercial sites do not typically have the ability and resources to develop their own system and may outsource the RS to a third party. This had led to the growth of a recommendation as a service industry, where companies, referred to as RS providers, provide recommendation services. These companies must carefully balance the cost of building recommendation models and the payment received from the e-business, as these payments are expected to be low. In such a setting, restricting the computational time required for model building is critical for the RS provider to be profitable.In this article, we propose anytime algorithms as an attractive method for balancing computational time and the recommendation model performance, thus tackling the RS provider problem. In an anytime setting, an algorithm can be stopped after any amount of computational time, always ensuring that a valid, although suboptimal, solution will be returned. Given sufficient time, however, the algorithm should converge to an optimal solution. In this setting, it is important to evaluate the quality of the returned solution over time, monitoring quality improvement. This is significantly different from traditional evaluation methods, which mostly estimate the performance of the algorithm only after its convergence is given sufficient time.We show that the popular item-item top-N recommendation approach can be brought into the anytime framework by smartly considering the order by which item pairs are being evaluated. We experimentally show that the time-accuracy trade-off can be significantly improved for this specific problem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {43},
numpages = {26},
keywords = {Recommender systems, collaborative filtering, anytime algorithms}
}

@article{10.1145/2824286,
author = {Hardegger, Michael and Roggen, Daniel and Calatroni, Alberto and Tr\"{o}ster, Gerhard},
title = {S-SMART: A Unified Bayesian Framework for Simultaneous Semantic Mapping, Activity Recognition, and Tracking},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2824286},
doi = {10.1145/2824286},
abstract = {The machine recognition of user trajectories and activities is fundamental to devise context-aware applications for support and monitoring in daily life. So far, tracking and activity recognition were mostly considered as orthogonal problems, which limits the richness of possible context inference. In this work, we introduce the novel unified computational and representational framework S-SMART that simultaneously models the environment state (semantic mapping), localizes the user within this map (tracking), and recognizes interactions with the environment (activity recognition). Thus, S-SMART identifies which activities the user executes where (e.g., turning a handle next to a window), and reflects the outcome of these actions by updating the world model (e.g., the window is now open). This in turn conditions the future possibility of executing actions at specific places (e.g., closing the window is likely to be the next action at this location). S-SMART works in a self-contained manner and iteratively builds the semantic map from wearable sensors only. This enables the seamless deployment to new environments.We characterize S-SMART in an experimental dataset with people performing hand actions as part of their usual routines at home and in office buildings. The framework combines dead reckoning from a foot-worn motion sensor with template-matching-based action recognition, identifying objects in the environment (windows, doors, water taps, phones, etc.) and tracking their state (open/closed, etc.). In real-life recordings with up to 23 action classes, S-SMART consistently outperforms independent systems for positioning and activity recognition, and constructs accurate semantic maps. This environment representation enables novel applications that build upon information about the arrangement and state of the user’s surroundings. For example, it may be possible to remind elderly people of a window that they left open before leaving the house, or of a plant they did not water yet, using solely wearable sensors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {34},
numpages = {28},
keywords = {Semantic mapping, localization, wearable sensors, context awareness, particle filter, SLAM, template matching, activity recognition}
}

@article{10.1145/2816824,
author = {Jia, Yantao and Wang, Yuanzhuo and Jin, Xiaolong and Cheng, Xueqi},
title = {Location Prediction: A Temporal-Spatial Bayesian Model},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2816824},
doi = {10.1145/2816824},
abstract = {In social networks, predicting a user’s location mainly depends on those of his/her friends, where the key lies in how to select his/her most influential friends. In this article, we analyze the theoretically maximal accuracy of location prediction based on friends’ locations and compare it with the practical accuracy obtained by the state-of-the-art location prediction methods. Upon observing a big gap between the theoretical and practical accuracy, we propose a new strategy for selecting influential friends in order to improve the practical location prediction accuracy. Specifically, several features are defined to measure the influence of the friends on a user’s location, based on which we put forth a sequential random-walk-with-restart procedure to rank the friends of the user in terms of their influence. By dynamically selecting the top N most influential friends of the user per time slice, we develop a temporal-spatial Bayesian model to characterize the dynamics of friends’ influence for location prediction. Finally, extensive experimental results on datasets of real social networks demonstrate that the proposed influential friend selection method and temporal-spatial Bayesian model can significantly improve the accuracy of location prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {31},
numpages = {25},
keywords = {Location prediction, dynamic selection, influential friends, temporal-spatial evolution, dynamic Bayesian network}
}

@article{10.1145/2751565,
author = {Dooms, Simon and Bellog\'{\i}n, Alejandro and Pessemier, Toon De and Martens, Luc},
title = {A Framework for Dataset Benchmarking and Its Application to a New Movie Rating Dataset},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2751565},
doi = {10.1145/2751565},
abstract = {Rating datasets are of paramount importance in recommender systems research. They serve as input for recommendation algorithms, as simulation data, or for evaluation purposes. In the past, public accessible rating datasets were not abundantly available, leaving researchers no choice but to work with old and static datasets like MovieLens and Netflix. More recently, however, emerging trends as social media and smartphones are found to provide rich data sources which can be turned into valuable research datasets. While dataset availability is growing, a structured way for introducing and comparing new datasets is currently still lacking. In this work, we propose a five-step framework to introduce and benchmark new datasets in the recommender systems domain. We illustrate our framework on a new movie rating dataset—called MovieTweetings—collected from Twitter. Following our framework, we detail the origin of the dataset, provide basic descriptive statistics, investigate external validity, report the results of a number of reproducible benchmarks, and conclude by discussing some interesting advantages and appropriate research use cases.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {41},
numpages = {28},
keywords = {movielens, twitter, dataset, reproducibility, imdb, Benchmark, movietweetings, evaluation}
}

@article{10.1145/2842630,
author = {Ibrahim, Azhar Mohd and Venkat, Ibrahim and Subramanian, K. G. and Khader, Ahamad Tajudin and Wilde, Philippe De},
title = {Intelligent Evacuation Management Systems: A Review},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2842630},
doi = {10.1145/2842630},
abstract = {Crowd and evacuation management have been active areas of research and study in the recent past. Various developments continue to take place in the process of efficient evacuation of crowds in mass gatherings. This article is intended to provide a review of intelligent evacuation management systems covering the aspects of crowd monitoring, crowd disaster prediction, evacuation modelling, and evacuation path guidelines. Soft computing approaches play a vital role in the design and deployment of intelligent evacuation applications pertaining to crowd control management. While the review deals with video and nonvideo based aspects of crowd monitoring and crowd disaster prediction, evacuation techniques are reviewed via the theme of soft computing, along with a brief review on the evacuation navigation path. We believe that this review will assist researchers in developing reliable automated evacuation systems that will help in ensuring the safety of the evacuees especially during emergency evacuation scenarios.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {36},
numpages = {27},
keywords = {evacuation modelling, evacuation path guidelines, crowd management, prediction of crowd disaster, Crowd monitoring}
}

@article{10.1145/2822907,
author = {Li, Zechao and Tang, Jinhui and Wang, Xueming and Liu, Jing and Lu, Hanqing},
title = {Multimedia News Summarization in Search},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2822907},
doi = {10.1145/2822907},
abstract = {It is a necessary but challenging task to relieve users from the proliferative news information and allow them to quickly and comprehensively master the information of the whats and hows that are happening in the world every day. In this article, we develop a novel approach of multimedia news summarization for searching results on the Internet, which uncovers the underlying topics among query-related news information and threads the news events within each topic to generate a query-related brief overview. First, the hierarchical latent Dirichlet allocation (hLDA) model is introduced to discover the hierarchical topic structure from query-related news documents, and a new approach based on the weighted aggregation and max pooling is proposed to identify one representative news article for each topic. One representative image is also selected to visualize each topic as a complement to the text information. Given the representative documents selected for each topic, a time-bias maximum spanning tree (MST) algorithm is proposed to thread them into a coherent and compact summary of their parent topic. Finally, we design a friendly interface to present users with the hierarchical summarization of their required news information. Extensive experiments conducted on a large-scale news dataset collected from multiple news Web sites demonstrate the encouraging performance of the proposed solution for news summarization in news retrieval.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {33},
numpages = {20},
keywords = {News summarization, topic structure, multimodal, maximum spanning tree, hierarchical latent Dirichlet allocation}
}

@article{10.1145/2801130,
author = {Bel\'{e}m, Fabiano M. and Batista, Carolina S. and Santos, Rodrygo L. T. and Almeida, Jussara M. and Gon\c{c}alves, Marcos A.},
title = {Beyond Relevance: Explicitly Promoting Novelty and Diversity in Tag Recommendation},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2801130},
doi = {10.1145/2801130},
abstract = {The design and evaluation of tag recommendation methods has historically focused on maximizing the relevance of the suggested tags for a given object, such as a movie or a song. However, relevance by itself may not be enough to guarantee recommendation usefulness. Promoting novelty and diversity in tag recommendation not only increases the chances that the user will select “some” of the recommended tags but also promotes complementary information (i.e., tags), which helps to cover multiple aspects or topics related to the target object. Previous work has addressed the tag recommendation problem by exploiting at most two of the following aspects: (1) relevance, (2) explicit topic diversity, and (3) novelty. In contrast, here we tackle these three aspects conjointly, by introducing two new tag recommendation methods that cover all three aspects of the problem at different levels. Our first method, called Random Forest with topic-related attributes, or RFt, extends a relevance-driven tag recommender based on the Random Forest (RF) learning-to-rank method by including new tag attributes to capture the extent to which a candidate tag is related to the topics of the target object. This solution captures topic diversity as well as novelty at the attribute level while aiming at maximizing relevance in its objective function. Our second method, called Explicit Tag Recommendation Diversifier with Novelty Promotion, or xTReND, reranks the recommendations provided by any tag recommender to jointly promote relevance, novelty, and topic diversity. We use RFt as a basic recommender applied before the reranking, thus building a solution that addresses the problem at both attribute and objective levels. Furthermore, to enable the use of our solutions on applications in which category information is unavailable, we investigate the suitability of using latent Dirichlet allocation (LDA) to automatically generate topics for objects. We evaluate all tag recommendation approaches using real data from five popular Web 2.0 applications. Our results show that RFt greatly outperforms the relevance-driven RF baseline in diversity while producing gains in relevance as well. We also find that our new xTReND reranker obtains considerable gains in both novelty and relevance when compared to that same baseline while keeping the same relevance levels. Furthermore, compared to our previous reranker method, xTReD, which does not consider novelty, xTReND is also quite effective, improving the novelty of the recommended tags while keeping similar relevance and diversity levels in most datasets and scenarios. Comparing our two new proposals, we find that xTReND considerably outperforms RFt in terms of novelty and diversity with only small losses (under 4%) in relevance. Overall, considering the trade-off among relevance, novelty, and diversity, our results demonstrate the superiority of xTReND over the baselines and the proposed alternative, RFt. Finally, the use of automatically generated latent topics as an alternative to manually labeled categories also provides significant improvements, which greatly enhances the applicability of our solutions to applications where the latter is not available.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {26},
numpages = {34},
keywords = {relevance, Tag recommendation, topic diversity, novelty}
}

@article{10.1145/2800794,
author = {Paik, Jiaul H.},
title = {Parameterized Decay Model for Information Retrieval},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2800794},
doi = {10.1145/2800794},
abstract = {This article proposes a term weighting scheme for measuring query-document similarity that attempts to explicitly model the dependency between separate occurrences of a term in a document. The assumption is that, if a term appears once in a document, it is more likely to appear again in the same document. Thus, as the term appears again and again, the information content of the subsequent occurrences decreases gradually, since they are more predictable. We introduce a parameterized decay function to model this assumption, where the initial contribution of the term can be determined using any reasonable term discrimination factor. The effectiveness of the proposed model is evaluated on a number of recent web test collections of varying nature. The experimental results show that the proposed model significantly outperforms a number of well known retrieval models including a recently proposed strong Term Frequency and Inverse Document Frequency (TF-IDF) model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {27},
numpages = {21},
keywords = {term weighting, retrieval model, Document ranking}
}

@article{10.1145/2700496,
author = {Wu, Le and Liu, Qi and Chen, Enhong and Yuan, Nicholas Jing and Guo, Guangming and Xie, Xing},
title = {Relevance Meets Coverage: A Unified Framework to Generate Diversified Recommendations},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700496},
doi = {10.1145/2700496},
abstract = {Collaborative filtering (CF) models offer users personalized recommendations by measuring the relevance between the active user and each individual candidate item. Following this idea, user-based collaborative filtering (UCF) usually selects the local popular items from the like-minded neighbor users. However, these traditional relevance-based models only consider the individuals (i.e., each neighbor user and candidate item) separately during neighbor set selection and recommendation set generation, thus usually incurring highly similar recommendations that lack diversity. While many researchers have recognized the importance of diversified recommendations, the proposed solutions either needed additional semantic information of items or decreased accuracy in this process. In this article, we describe how to generate both accurate and diversified recommendations from a new perspective. Along this line, we first introduce a simple measure of coverage that quantifies the usefulness of the whole set, that is, the neighbor userset and the recommended itemset as a complete entity. Then we propose a recommendation framework named REC that considers both traditional relevance-based scores and the new coverage measure based on UCF. Under REC, we further prove that the goals of maximizing relevance and coverage measures simultaneously in both the neighbor set selection step and the recommendation set generation step are NP-hard. Luckily, we can solve them effectively and efficiently by exploiting the inherent submodular property. Furthermore, we generalize the coverage notion and the REC framework from both a data perspective and an algorithm perspective. Finally, extensive experimental results on three real-world datasets show that the REC-based recommendation models can naturally generate more diversified recommendations without decreasing accuracy compared to some state-of-the-art models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {39},
numpages = {30},
keywords = {diversity, personalized recommendation, Collaborative filtering, coverage}
}

@article{10.1145/2700491,
author = {Moody, Jennifer and Glass, David H.},
title = {A Novel Classification Framework for Evaluating Individual and Aggregate Diversity in Top-N Recommendations},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700491},
doi = {10.1145/2700491},
abstract = {The primary goal of a recommender system is to generate high quality user-centred recommendations. However, the traditional evaluation methods and metrics were developed before researchers understood all the factors that increase user satisfaction. This study is an introduction to a novel user and item classification framework. It is proposed that this framework should be used during user-centred evaluation of recommender systems and the need for this framework is justified through experiments. User profiles are constructed and matched against other users’ profiles to formulate neighbourhoods and generate top-N recommendations. The recommendations are evaluated to measure the success of the process. In conjunction with the framework, a new diversity metric is presented and explained. The accuracy, coverage, and diversity of top-N recommendations is illustrated and discussed for groups of users. It is found that in contradiction to common assumptions, not all users suffer as expected from the data sparsity problem. In fact, the group of users that receive the most accurate recommendations do not belong to the least sparse area of the dataset.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {42},
numpages = {21},
keywords = {performance evaluation metrics, Recommender systems, recommendation diversity, recommendation quality, collaborative filtering, recommendation accuracy}
}

@article{10.1145/2814575,
author = {Yang, Dingqi and Zhang, Daqing and Qu, Bingqing},
title = {Participatory Cultural Mapping Based on Collective Behavior Data in Location-Based Social Networks},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2814575},
doi = {10.1145/2814575},
abstract = {Culture has been recognized as a driving impetus for human development. It co-evolves with both human belief and behavior. When studying culture, Cultural Mapping is a crucial tool to visualize different aspects of culture (e.g., religions and languages) from the perspectives of indigenous and local people. Existing cultural mapping approaches usually rely on large-scale survey data with respect to human beliefs, such as moral values. However, such a data collection method not only incurs a significant cost of both human resources and time, but also fails to capture human behavior, which massively reflects cultural information. In addition, it is practically difficult to collect large-scale human behavior data. Fortunately, with the recent boom in Location-Based Social Networks (LBSNs), a considerable number of users report their activities in LBSNs in a participatory manner, which provides us with an unprecedented opportunity to study large-scale user behavioral data. In this article, we propose a participatory cultural mapping approach based on collective behavior in LBSNs. First, we collect the participatory sensed user behavioral data from LBSNs. Second, since only local users are eligible for cultural mapping, we propose a progressive “home” location identification method to filter out ineligible users. Third, by extracting three key cultural features from daily activity, mobility, and linguistic perspectives, respectively, we propose a cultural clustering method to discover cultural clusters. Finally, we visualize the cultural clusters on the world map. Based on a real-world LBSN dataset, we experimentally validate our approach by conducting both qualitative and quantitative analysis on the generated cultural maps. The results show that our approach can subtly capture cultural features and generate representative cultural maps that correspond well with traditional cultural maps based on survey data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {30},
numpages = {23},
keywords = {collective behavior, participatory sensing, location based social networks, Cultural mapping, cultural difference}
}

@article{10.1145/2840720,
author = {Zhang, Kun and Li, Jiuyong and Bareinboim, Elias and Sch\"{o}lkopf, Bernhard and Pearl, Judea},
title = {Preface to the ACM TIST Special Issue on Causal Discovery and Inference},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2840720},
doi = {10.1145/2840720},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {3}
}

@article{10.1145/2700476,
author = {Zhang, Kun and Wang, Zhikun and Zhang, Jiji and Sch\"{o}lkopf, Bernhard},
title = {On Estimation of Functional Causal Models: General Results and Application to the Post-Nonlinear Causal Model},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700476},
doi = {10.1145/2700476},
abstract = {Compared to constraint-based causal discovery, causal discovery based on functional causal models is able to identify the whole causal model under appropriate assumptions [Shimizu et al. 2006; Hoyer et al. 2009; Zhang and Hyv\"{a}rinen 2009b]. Functional causal models represent the effect as a function of the direct causes together with an independent noise term. Examples include the linear non-Gaussian acyclic model (LiNGAM), nonlinear additive noise model, and post-nonlinear (PNL) model. Currently, there are two ways to estimate the parameters in the models: dependence minimization and maximum likelihood. In this article, we show that for any acyclic functional causal model, minimizing the mutual information between the hypothetical cause and the noise term is equivalent to maximizing the data likelihood with a flexible model for the distribution of the noise term. We then focus on estimation of the PNL causal model and propose to estimate it with the warped Gaussian process with the noise modeled by the mixture of Gaussians. As a Bayesian nonparametric approach, it outperforms the previous one based on mutual information minimization with nonlinear functions represented by multilayer perceptrons; we also show that unlike the ordinary regression, estimation results of the PNL causal model are sensitive to the assumption on the noise distribution. Experimental results on both synthetic and real data support our theoretical claims.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {13},
numpages = {22},
keywords = {statistical independence, maximum likelihood, Causal discovery, post-nonlinear causal model, functional causal model}
}

@article{10.1145/2668134,
author = {Luo, Peng and Geng, Zhi},
title = {Bounds on Direct and Indirect Effects of Treatment on a Continuous Endpoint},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668134},
doi = {10.1145/2668134},
abstract = {Direct effect of a treatment variable on an endpoint variable and indirect effect through a mediate variable are important concepts for understanding a causal mechanism. However, the randomized assignment of treatment is not sufficient for identifying the direct and indirect effects, and extra assumptions and conditions are required, such as the sequential ignorability assumption without unobserved confounders or the sequential potential ignorability assumption. But these assumptions may not be credible in many applications. In this article, we consider the bounds on controlled direct effect, natural direct effect, and natural indirect effect without these extra assumptions. Cai et al. [2008] presented the bounds for the case of a binary endpoint, and we extend their results to the general case for an arbitrary endpoint.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {20},
numpages = {18},
keywords = {mediation analysis, direct and indirect effects, bound, Causal inference}
}

@article{10.1145/2770878,
author = {Hours, Hadrien and Biersack, Ernst and Loiseau, Patrick},
title = {A Causal Approach to the Study of TCP Performance},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2770878},
doi = {10.1145/2770878},
abstract = {Communication networks are complex systems whose operation relies on a large number of components that work together to provide services to end users. As the quality of these services depends on different parameters, understanding how each of them impacts the final performance of a service is a challenging but important problem. However, intervening on individual factors to evaluate the impact of the different parameters is often impractical due to the high cost of intervention in a network. It is, therefore, desirable to adopt a formal approach to understand the role of the different parameters and to predict how a change in any of these parameters will impact performance.The approach of causality pioneered by J. Pearl provides a powerful framework to investigate these questions. Most of the existing theory is non-parametric and does not make any assumption on the nature of the system under study. However, most of the implementations of causal model inference algorithms and most of the examples of usage of a causal model to predict intervention rely on assumptions such linearity, normality, or discrete data.In this article, we present a methodology to overcome the challenges of working with real-world data and extend the application of causality to complex systems in the area of telecommunication networks, for which assumptions of normality, linearity and discrete data do no hold. Specifically, we study the performance of TCP, which is the prevalent protocol for reliable end-to-end transfer in the Internet. Analytical models of the performance of TCP exist, but they take into account the state of network only and disregard the impact of the application at the sender and the receiver, which often influences TCP performance. To address this point, we take as application the file transfer protocol (FTP), which uses TCP for reliable transfer. Studying a well-understood protocol such as TCP allows us to validate our approach and compare its results to previous studies.We first present and evaluate our methodology using TCP traffic obtained via network emulation, which allows us to experimentally validate the prediction of an intervention. We then apply the methodology to real-world TCP traffic sent over the Internet. Throughout the article, we compare the causal approach for studying TCP performance to other approaches such as analytical modeling or simulation and and show how they can complement each other.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {25},
numpages = {25},
keywords = {TCP, Telecommunication networks}
}

@article{10.1145/2801131,
author = {Minkov, Einat},
title = {Event Extraction Using Structured Learning and Rich Domain Knowledge: Application across Domains and Data Sources},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2801131},
doi = {10.1145/2801131},
abstract = {We consider the task of record extraction from text documents, where the goal is to automatically populate the fields of target relations, such as scientific seminars or corporate acquisition events. There are various inferences involved in the record-extraction process, including mention detection, unification, and field assignments. We use structured learning to find the appropriate field-value assignments. Unlike previous works, the proposed approach generates feature-rich models that enable the modeling of domain semantics and structural coherence at all levels and across fields. Given labeled examples, such an approach can, for instance, learn likely event durations and the fact that start times should come before end times. While the inference space is large, effective learning is achieved using a perceptron-style method and simple, greedy beam decoding. A main focus of this article is on practical aspects involved in implementing the proposed framework for real-world applications. We argue and demonstrate that this approach is favorable in conditions of data shift, a real-world setting in which models learned using a limited set of labeled examples are applied to examples drawn from a different data distribution. Much of the framework’s robustness is attributed to the modeling of domain knowledge. We describe design and implementation details for the case study of seminar event extraction from email announcements, and discuss design adaptations across different domains and text genres.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {16},
numpages = {34},
keywords = {beam search, template filling, domain knowledge, Information extraction, structured learning, perceptron}
}

@article{10.1145/2700477,
author = {Liu, Furui and Chan, Laiwan},
title = {Causal Discovery on Discrete Data with Extensions to Mixture Model},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700477},
doi = {10.1145/2700477},
abstract = {In this article, we deal with the causal discovery problem on discrete data. First, we present a causal discovery method for traditional additive noise models that identifies the causal direction by analyzing the supports of the conditional distributions. Then, we present a causal mixture model to address the problem that the function transforming cause to effect varies across the observations. We propose a novel method called Support Analysis (SA) for causal discovery with the mixture model. Experiments using synthetic and real data are presented to demonstrate the performance of our proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {21},
numpages = {19},
keywords = {discrete, mixture, Causal discovery}
}

@article{10.1145/2668135,
author = {Chen, Hua and Ding, Peng and Geng, Zhi and Zhou, Xiao-Hua},
title = {Semiparametric Inference of the Complier Average Causal Effect with Nonignorable Missing Outcomes},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668135},
doi = {10.1145/2668135},
abstract = {Noncompliance and missing data often occur in randomized trials, which complicate the inference of causal effects. When both noncompliance and missing data are present, previous papers proposed moment and maximum likelihood estimators for binary and normally distributed continuous outcomes under the latent ignorable missing data mechanism. However, the latent ignorable missing data mechanism may be violated in practice, because the missing data mechanism may depend directly on the missing outcome itself. Under noncompliance and an outcome-dependent nonignorable missing data mechanism, previous studies showed the identifiability of complier average causal effect for discrete outcomes. In this article, we study the semiparametric identifiability and estimation of complier average causal effect in randomized clinical trials with both all-or-none noncompliance and outcome-dependent nonignorable missing continuous outcomes, and propose a two-step maximum likelihood estimator in order to eliminate the infinite dimensional nuisance parameter. Our method does not need to specify a parametric form for the missing data mechanism. We also evaluate the finite sample property of our method via extensive simulation studies and sensitivity analysis, with an application to a double-blinded psychiatric clinical trial.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {19},
numpages = {15},
keywords = {instrumental variable, principal stratification, noncompliance, missing not at random, Causal inference, outcome-dependent missing}
}

@article{10.1145/2809782,
author = {Fire, Amy and Zhu, Song-Chun},
title = {Learning Perceptual Causality from Video},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2809782},
doi = {10.1145/2809782},
abstract = {Perceptual causality is the perception of causal relationships from observation. Humans, even as infants, form such models from observation of the world around them [Saxe and Carey 2006]. For a deeper understanding, the computer must make similar models through the analogous form of observation: video. In this article, we provide a framework for the unsupervised learning of this perceptual causal structure from video. Our method takes action and object status detections as input and uses heuristics suggested by cognitive science research to produce the causal links perceived between them. We greedily modify an initial distribution featuring independence between potential causes and effects by adding dependencies that maximize information gain. We compile the learned causal relationships into a Causal And-Or Graph, a probabilistic and-or representation of causality that adds a prior to causality. Validated against human perception, experiments show that our method correctly learns causal relations, attributing status changes of objects to causing actions amid irrelevant actions. Our method outperforms Hellinger’s χ2-statistic by considering hierarchical action selection, and outperforms the treatment effect by discounting coincidental relationships.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {23},
numpages = {22},
keywords = {causal induction, Perceptual causality, information projection}
}

@article{10.1145/2806892,
author = {Flaxman, Seth R. and Neill, Daniel B. and Smola, Alexander J.},
title = {Gaussian Processes for Independence Tests with Non-Iid Data in Causal Inference},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2806892},
doi = {10.1145/2806892},
abstract = {In applied fields, practitioners hoping to apply causal structure learning or causal orientation algorithms face an important question: which independence test is appropriate for my data? In the case of real-valued iid data, linear dependencies, and Gaussian error terms, partial correlation is sufficient. But once any of these assumptions is modified, the situation becomes more complex. Kernel-based tests of independence have gained popularity to deal with nonlinear dependencies in recent years, but testing for conditional independence remains a challenging problem. We highlight the important issue of non-iid observations: when data are observed in space, time, or on a network, “nearby” observations are likely to be similar. This fact biases estimates of dependence between variables. Inspired by the success of Gaussian process regression for handling non-iid observations in a wide variety of areas and by the usefulness of the Hilbert-Schmidt Independence Criterion (HSIC), a kernel-based independence test, we propose a simple framework to address all of these issues: first, use Gaussian process regression to control for certain variables and to obtain residuals. Second, use HSIC to test for independence. We illustrate this on two classic datasets, one spatial, the other temporal, that are usually treated as iid. We show how properly accounting for spatial and temporal variation can lead to more reasonable causal graphs. We also show how highly structured data, like images and text, can be used in a causal inference framework using a novel structured input/output Gaussian process formulation. We demonstrate this idea on a dataset of translated sentences, trying to predict the source language.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {22},
numpages = {23},
keywords = {causal inference, Gaussian process, causal structure learning, Reproducing kernel Hilbert space}
}

@article{10.1145/2710025,
author = {Demeshko, Marina and Washio, Takashi and Kawahara, Yoshinobu and Pepyolyshev, Yuriy},
title = {A Novel Continuous and Structural VAR Modeling Approach and Its Application to Reactor Noise Analysis},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2710025},
doi = {10.1145/2710025},
abstract = {A vector autoregressive model in discrete time domain (DVAR) is often used to analyze continuous time, multivariate, linear Markov systems through their observed time series data sampled at discrete timesteps. Based on previous studies, the DVAR model is supposed to be a noncanonical representation of the system, that is, it does not correspond to a unique system bijectively. However, in this article, we characterize the relations of the DVAR model with its corresponding Structural Vector AR (SVAR) and Continuous Time Vector AR (CTVAR) models through a finite difference method across continuous and discrete time domain. We further clarify that the DVAR model of a continuous time, multivariate, linear Markov system is canonical under a highly generic condition. Our analysis shows that we can uniquely reproduce its SVAR and CTVAR models from the DVAR model. Based on these results, we propose a novel Continuous and Structural Vector Autoregressive (CSVAR) modeling approach to derive the SVAR and the CTVAR models from their DVAR model empirically derived from the observed time series of continuous time linear Markov systems. We demonstrate its superior performance through some numerical experiments on both artificial and real-world data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {24},
numpages = {22},
keywords = {CTVAR model, nuclear reactor noise analysis, VAR model, continuous time linear Markov system, SVAR model, canonicality}
}

@article{10.1145/2799648,
author = {Leiva, Luis A. and Mart\'{\i}n-Albo, Daniel and Plamondon, R\'{e}jean},
title = {Gestures \`{a} Go Go: Authoring Synthetic Human-Like Stroke Gestures Using the Kinematic Theory of Rapid Movements},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2799648},
doi = {10.1145/2799648},
abstract = {Training a high-quality gesture recognizer requires providing a large number of examples to enable good performance on unseen, future data. However, recruiting participants, data collection, and labeling, etc., necessary for achieving this goal are usually time consuming and expensive. Thus, it is important to investigate how to empower developers to quickly collect gesture samples for improving UI usage and user experience. In response to this need, we introduce Gestures \`{a} Go Go (g3), a web service plus an accompanying web application for bootstrapping stroke gesture samples based on the kinematic theory of rapid human movements. The user only has to provide a gesture example once, and g3 will create a model of that gesture. Then, by introducing local and global perturbations to the model parameters, g3 generates from tens to thousands of synthetic human-like samples. Through a comprehensive evaluation, we show that synthesized gestures perform equally similar to gestures generated by human users. Ultimately, this work informs our understanding of designing better user interfaces that are driven by gestures.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {15},
numpages = {29},
keywords = {strokes, symbols, kinematics, Gesture synthesis, unistrokes, user interfaces, bootstrapping, gesture recognition, marks, rapid prototyping, multistrokes, multitouch}
}

@article{10.1145/2746410,
author = {Li, Jiuyong and Le, Thuc Duy and Liu, Lin and Liu, Jixue and Jin, Zhou and Sun, Bingyu and Ma, Saisai},
title = {From Observational Studies to Causal Rule Mining},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2746410},
doi = {10.1145/2746410},
abstract = {Randomised controlled trials (RCTs) are the most effective approach to causal discovery, but in many circumstances it is impossible to conduct RCTs. Therefore, observational studies based on passively observed data are widely accepted as an alternative to RCTs. However, in observational studies, prior knowledge is required to generate the hypotheses about the cause-effect relationships to be tested, and hence they can only be applied to problems with available domain knowledge and a handful of variables. In practice, many datasets are of high dimensionality, which leaves observational studies out of the opportunities for causal discovery from such a wealth of data sources. In another direction, many efficient data mining methods have been developed to identify associations among variables in large datasets. The problem is that causal relationships imply associations, but the reverse is not always true. However, we can see the synergy between the two paradigms here. Specifically, association rule mining can be used to deal with the high-dimensionality problem, whereas observational studies can be utilised to eliminate noncausal associations. In this article, we propose the concept of causal rules (CRs) and develop an algorithm for mining CRs in large datasets. We use the idea of retrospective cohort studies to detect CRs based on the results of association rule mining. Experiments with both synthetic and real-world datasets have demonstrated the effectiveness and efficiency of CR mining. In comparison with the commonly used causal discovery methods, the proposed approach generally is faster and has better or competitive performance in finding correct or sensible causes. It is also capable of finding a cause consisting of multiple variables—a feature that other causal discovery methods do not possess.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {14},
numpages = {27},
keywords = {odds ratio, cohort study, association rule, Causal discovery}
}

@article{10.1145/2700498,
author = {Shan, Na and Dong, Xiaogang and Xu, Pingfeng and Guo, Jianhua},
title = {Sharp Bounds on Survivor Average Causal Effects When the Outcome Is Binary and Truncated by Death},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700498},
doi = {10.1145/2700498},
abstract = {In randomized trials with follow-up, outcomes may be undefined for individuals who die before the follow-up is complete. In such settings, Frangakis and Rubin [2002] proposed the “principal stratum effect” or “Survivor Average Causal Effect” (SACE), which is a fair treatment comparison in the subpopulation that would have survived under either treatment arm. Many of the existing results for estimating the SACE are difficult to carry out in practice. In this article, when the outcome is binary, we apply the symbolic Balke-Pearl linear programming method to derive simple formulas for the sharp bounds on the SACE under the monotonicity assumption commonly used by many researchers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {18},
numpages = {11},
keywords = {Linear programming, randomized trials, survivor average causal effect, truncation}
}

@article{10.1145/2766459,
author = {Muntean, Cristina Ioana and Nardini, Franco Maria and Silvestri, Fabrizio and Baraglia, Ranieri},
title = {On Learning Prediction Models for Tourists Paths},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2766459},
doi = {10.1145/2766459},
abstract = {In this article, we tackle the problem of predicting the “next” geographical position of a tourist, given her history (i.e., the prediction is done accordingly to the tourist’s current trail) by means of supervised learning techniques, namely Gradient Boosted Regression Trees and Ranking SVM. The learning is done on the basis of an object space represented by a 68-dimension feature vector specifically designed for tourism-related data. Furthermore, we propose a thorough comparison of several methods that are considered state-of-the-art in recommender and trail prediction systems for tourism, as well as a popularity baseline. Experiments show that the methods we propose consistently outperform the baselines and provide strong evidence of the performance and robustness of our solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {8},
numpages = {34},
keywords = {learning to rank, Geographical PoI prediction}
}

@article{10.1145/2795234,
author = {Guan, Tao and Wang, Yuesong and Duan, Liya and Ji, Rongrong},
title = {On-Device Mobile Landmark Recognition Using Binarized Descriptor with Multifeature Fusion},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2795234},
doi = {10.1145/2795234},
abstract = {Along with the exponential growth of high-performance mobile devices, on-device Mobile Landmark Recognition (MLR) has recently attracted increasing research attention. However, the latency and accuracy of automatic recognition remain as bottlenecks against its real-world usage. In this article, we introduce a novel framework that combines interactive image segmentation with multifeature fusion to achieve improved MLR with high accuracy. First, we propose an effective vector binarization method to reduce the memory usage of image descriptors extracted on-device, which maintains comparable recognition accuracy to the original descriptors. Second, we design a location-aware fusion algorithm that can fuse multiple visual features into a compact yet discriminative image descriptor to improve on-device efficiency. Third, a user-friendly interaction scheme is developed that enables interactive foreground/background segmentation to largely improve recognition accuracy. Experimental results demonstrate the effectiveness of the proposed algorithms for on-device MLR applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {12},
numpages = {29},
keywords = {user interaction, feature fusion, Mobile landmark recognition, binarization, on-device}
}

@article{10.1145/2786761,
author = {Zhang, Jia-Dong and Chow, Chi-Yin},
title = {Spatiotemporal Sequential Influence Modeling for Location Recommendations: A Gravity-Based Approach},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2786761},
doi = {10.1145/2786761},
abstract = {Recommending to users personalized locations is an important feature of Location-Based Social Networks (LBSNs), which benefits users who wish to explore new places and businesses to discover potential customers. In LBSNs, social and geographical influences have been intensively used in location recommendations. However, human movement also exhibits spatiotemporal sequential patterns, but only a few current studies consider the spatiotemporal sequential influence of locations on users’ check-in behaviors. In this article, we propose a new gravity model for location recommendations, called LORE, to exploit the spatiotemporal sequential influence on location recommendations. First, LORE extracts sequential patterns from historical check-in location sequences of all users as a Location-Location Transition Graph (L2TG), and utilizes the L2TG to predict the probability of a user visiting a new location through the developed additive Markov chain that considers the effect of all visited locations in the check-in history of the user on the new location. Furthermore, LORE applies our contrived gravity model to weigh the effect of each visited location on the new location derived from the personalized attractive force (i.e., the weight) between the visited location and the new location. The gravity model effectively integrates the spatiotemporal, social, and popularity influences by estimating a power-law distribution based on (i) the spatial distance and temporal difference between two consecutive check-in locations of the same user, (ii) the check-in frequency of social friends, and (iii) the popularity of locations from all users. Finally, we conduct a comprehensive performance evaluation for LORE using three large-scale real-world datasets collected from Foursquare, Gowalla, and Brightkite. Experimental results show that LORE achieves significantly superior location recommendations compared to other state-of-the-art location recommendation techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {11},
numpages = {25},
keywords = {additive Markov chain, gravity model, social influence, spatiotemporal sequential influence, popularity influence, Location recommendation}
}

@article{10.1145/2770879,
author = {Wang, Yinting and Song, Mingli and Tao, Dacheng and Rui, Yong and Bu, Jiajun and Tsoi, Ah Chung and Zhuo, Shaojie and Tan, Ping},
title = {Where2Stand: A Human Position Recommendation System for Souvenir Photography},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2770879},
doi = {10.1145/2770879},
abstract = {People often take photographs at tourist sites and these pictures usually have two main elements: a person in the foreground and scenery in the background. This type of “souvenir photo” is one of the most common photos clicked by tourists. Although algorithms that aid a user-photographer in taking a well-composed picture of a scene exist [Ni et al. 2013], few studies have addressed the issue of properly positioning human subjects in photographs. In photography, the common guidelines of composing portrait images exist. However, these rules usually do not consider the background scene. Therefore, in this article, we investigate human-scenery positional relationships and construct a photographic assistance system to optimize the position of human subjects in a given background scene, thereby assisting the user in capturing high-quality souvenir photos. We collect thousands of well-composed portrait photographs to learn human-scenery aesthetic composition rules. In addition, we define a set of negative rules to exclude undesirable compositions. Recommendation results are achieved by combining the first learned positive rule with our proposed negative rules. We implement the proposed system on an Android platform in a smartphone. The system demonstrates its efficacy by producing well-composed souvenir photos.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {9},
numpages = {22},
keywords = {photographic composition, Souvenir photography, human-scenery positional relationship, human position recommendation}
}

@article{10.1145/2738050,
author = {Dong, Yongsheng and Tao, Dacheng and Li, Xuelong},
title = {Nonnegative Multiresolution Representation-Based Texture Image Classification},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2738050},
doi = {10.1145/2738050},
abstract = {Effective representation of image texture is important for an image-classification task. Statistical modelling in wavelet domains has been widely used to image texture representation. However, due to the intraclass complexity and interclass diversity of textures, it is hard to use a predefined probability distribution function to fit adaptively all wavelet subband coefficients of different textures. In this article, we propose a novel modelling approach, Heterogeneous and Incrementally Generated Histogram (HIGH), to indirectly model the wavelet coefficients by use of four local features in wavelet subbands. By concatenating all the HIGHs in all wavelet subbands of a texture, we can construct a nonnegative multiresolution vector (NMV) to represent a texture image. Considering the NMV’s high dimensionality and nonnegativity, we further propose a Hessian regularized discriminative nonnegative matrix factorization to compute a low-dimensional basis of the linear subspace of NMVs. Finally, we present a texture classification approach by projecting NMVs on the low-dimensional basis. Experimental results show that our proposed texture classification method outperforms seven representative approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {4},
numpages = {21},
keywords = {histogram, manifold regularization, texture classification, Hessian regularization, Nonnegative matrix factorization}
}

@article{10.1145/2733383,
author = {Liu, Fan and Tang, Jinhui and Song, Yan and Zhang, Liyan and Tang, Zhenmin},
title = {Local Structure-Based Sparse Representation for Face Recognition},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2733383},
doi = {10.1145/2733383},
abstract = {This article presents a simple yet effective face recognition method, called local structure-based sparse representation classification (LS_SRC). Motivated by the “divide-and-conquer” strategy, we first divide the face into local blocks and classify each local block, then integrate all the classification results to make the final decision. To classify each local block, we further divide each block into several overlapped local patches and assume that these local patches lie in a linear subspace. This subspace assumption reflects the local structure relationship of the overlapped patches, making sparse representation-based classification (SRC) feasible even when encountering the single-sample-per-person (SSPP) problem. To lighten the computing burden of LS_SRC, we further propose the local structure-based collaborative representation classification (LS_CRC). Moreover, the performance of LS_SRC and LS_CRC can be further improved by using the confusion matrix of the classifier. Experimental results on four public face databases show that our methods not only generalize well to SSPP problem but also have strong robustness to occlusion; little pose variation; and the variations of expression, illumination, and time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {2},
numpages = {20},
keywords = {sparse representation, collaborative representation, Bayesian inference, single-sample-per-person problem, local structure, Face recognition, confusion matrix}
}

@article{10.1145/2576230,
author = {Ding, Wenkui and Geng, Xiubo and Zhang, Xu-Dong},
title = {Learning to Rank from Noisy Data},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2576230},
doi = {10.1145/2576230},
abstract = {Learning to rank, which learns the ranking function from training data, has become an emerging research area in information retrieval and machine learning. Most existing work on learning to rank assumes that the training data is clean, which is not always true, however. The ambiguity of query intent, the lack of domain knowledge, and the vague definition of relevance levels all make it difficult for common annotators to give reliable relevance labels to some documents. As a result, the relevance labels in the training data of learning to rank usually contain noise. If we ignore this fact, the performance of learning-to-rank algorithms will be damaged.In this article, we propose considering the labeling noise in the process of learning to rank and using a two-step approach to extend existing algorithms to handle noisy training data. In the first step, we estimate the degree of labeling noise for a training document. To this end, we assume that the majority of the relevance labels in the training data are reliable and we use a graphical model to describe the generative process of a training query, the feature vectors of its associated documents, and the relevance labels of these documents. The parameters in the graphical model are learned by means of maximum likelihood estimation. Then the conditional probability of the relevance label given the feature vector of a document is computed. If the probability is large, we regard the degree of labeling noise for this document as small; otherwise, we regard the degree as large. In the second step, we extend existing learning-to-rank algorithms by incorporating the estimated degree of labeling noise into their loss functions. Specifically, we give larger weights to those training documents with smaller degrees of labeling noise and smaller weights to those with larger degrees of labeling noise. As examples, we demonstrate the extensions for McRank, RankSVM, RankBoost, and RankNet. Empirical results on benchmark datasets show that the proposed approach can effectively distinguish noisy documents from clean ones, and the extended learning-to-rank algorithms can achieve better performances than baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {1},
numpages = {21},
keywords = {robust learning, Noisy data}
}

@article{10.1145/2774224,
author = {Hennes, Daniel and Jong, Steven De and Tuyls, Karl and Gal, Ya’akov (Kobi)},
title = {Metastrategies in Large-Scale Bargaining Settings},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2774224},
doi = {10.1145/2774224},
abstract = {This article presents novel methods for representing and analyzing a special class of multiagent bargaining settings that feature multiple players, large action spaces, and a relationship among players’ goals, tasks, and resources. We show how to reduce these interactions to a set of bilateral normal-form games in which the strategy space is significantly smaller than the original settings while still preserving much of their structural relationship. The method is demonstrated using the Colored Trails (CT) framework, which encompasses a broad family of games and has been used in many past studies. We define a set of heuristics (metastrategies) in multiplayer CT games that make varying assumptions about players’ strategies, such as boundedly rational play and social preferences. We show how these CT settings can be decomposed into canonical bilateral games such as the Prisoners’ Dilemma, Stag Hunt, and Ultimatum games in a way that significantly facilitates their analysis. We demonstrate the feasibility of this approach in separate CT settings involving one-shot and repeated bargaining scenarios, which are subsequently analyzed using evolutionary game-theoretic techniques. We provide a set of necessary conditions for CT games for allowing this decomposition. Our results have significance for multiagent systems researchers in mapping large multiplayer CT task settings to smaller, well-known bilateral normal-form games while preserving some of the structure of the original setting.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {10},
numpages = {23},
keywords = {negotiation, decision making, Multiagent systems}
}

@article{10.1145/2746409,
author = {Cheng, Fan-Chieh and Chen, Bo-Hao and Huang, Shih-Chia},
title = {A Hybrid Background Subtraction Method with Background and Foreground Candidates Detection},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2746409},
doi = {10.1145/2746409},
abstract = {Background subtraction for motion detection is often used in video surveillance systems. However, difficulties in bootstrapping restrict its development. This article proposes a novel hybrid background subtraction technique to solve this problem. For performance improvement of background subtraction, the proposed technique not only quickly initializes the background model but also eliminates unnecessary regions containing only background pixels in the object detection process. Furthermore, an embodiment based on the proposed technique is also presented. Experimental results verify that the proposed technique allows for reduced execution time as well as improvement of performance as evaluated by Recall, Precision, F1, and Similarity metrics when used with state-of-the-art background subtraction methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {7},
numpages = {14},
keywords = {foreground candidates, motion detection, background candidates, background subtraction, Video surveillance}
}

@article{10.1145/2743027,
author = {Chen, Bowei and Wang, Jun and Cox, Ingemar J. and Kankanhalli, Mohan S.},
title = {Multi-Keyword Multi-Click Advertisement Option Contracts for Sponsored Search},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2743027},
doi = {10.1145/2743027},
abstract = {In sponsored search, advertisement (abbreviated ad) slots are usually sold by a search engine to an advertiser through an auction mechanism in which advertisers bid on keywords. In theory, auction mechanisms have many desirable economic properties. However, keyword auctions have a number of limitations including: the uncertainty in payment prices for advertisers; the volatility in the search engine’s revenue; and the weak loyalty between advertiser and search engine. In this article, we propose a special ad option that alleviates these problems. In our proposal, an advertiser can purchase an option from a search engine in advance by paying an upfront fee, known as the option price. The advertiser then has the right, but no obligation, to purchase among the prespecified set of keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks in a specified period of time. The proposed option is closely related to a special exotic option in finance that contains multiple underlying assets (multi-keyword) and is also multi-exercisable (multi-click). This novel structure has many benefits: advertisers can have reduced uncertainty in advertising; the search engine can improve the advertisers’ loyalty as well as obtain a stable and increased expected revenue over time. Since the proposed ad option can be implemented in conjunction with the existing keyword auctions, the option price and corresponding fixed CPCs must be set such that there is no arbitrage between the two markets. Option pricing methods are discussed and our experimental results validate the development. Compared to keyword auctions, a search engine can have an increased expected revenue by selling an ad option.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {5},
numpages = {29},
keywords = {revenue analysis, exotic option, pricing model, Sponsored search}
}

@article{10.1145/2733384,
author = {Groves, William and Gini, Maria},
title = {On Optimizing Airline Ticket Purchase Timing},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2733384},
doi = {10.1145/2733384},
abstract = {Proper timing of the purchase of airline tickets is difficult even when historical ticket prices and some domain knowledge are available. To address this problem, we introduce an algorithm that optimizes purchase timing on behalf of customers and provides performance estimates of its computed action policy. Given a desired flight route and travel date, the algorithm uses machine-learning methods on recent ticket price quotes from many competing airlines to predict the future expected minimum price of all available flights. The main novelty of our algorithm lies in using a systematic feature-selection technique, which captures time dependencies in the data by using time-delayed features, and reduces the number of features by imposing a class hierarchy among the raw features and pruning the features based on in-situ performance. Our algorithm achieves much closer to the optimal purchase policy than other existing decision theoretic approaches for this domain, and meets or exceeds the performance of existing feature-selection methods from the literature. Applications of our feature-selection process to other domains are also discussed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {3},
numpages = {28},
keywords = {price prediction, Airline ticket prices, e-commerce, data mining, feature selection}
}

@article{10.1145/2743026,
author = {Font, Frederic and Serr\`{a}, Joan and Serra, Xavier},
title = {Analysis of the Impact of a Tag Recommendation System in a Real-World Folksonomy},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2743026},
doi = {10.1145/2743026},
abstract = {Collaborative tagging systems have emerged as a successful solution for annotating contributed resources to online sharing platforms, facilitating searching, browsing, and organizing their contents. To aid users in the annotation process, several tag recommendation methods have been proposed. It has been repeatedly hypothesized that these methods should contribute to improving annotation quality and reducing the cost of the annotation process. It has been also hypothesized that these methods should contribute to the consolidation of the vocabulary of collaborative tagging systems. However, to date, no empirical and quantitative result supports these hypotheses. In this work, we deeply analyze the impact of a tag recommendation system in the folksonomy of Freesound, a real-world and large-scale online sound sharing platform. Our results suggest that tag recommendation effectively increases vocabulary sharing among users of the platform. In addition, tag recommendation is shown to contribute to the convergence of the vocabulary as well as to a partial increase in the quality of annotations. However, according to our analysis, the cost of the annotation process does not seem to be effectively reduced. Our work is relevant to increase our understanding about the nature of tag recommendation systems and points to future directions for the further development of those systems and their analysis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {6},
numpages = {27},
keywords = {tag recommendation, tagging systems, Freesound, Folksonomy}
}

@article{10.1145/2651445,
author = {Kim, Eunju and Helal, Sumi and Nugent, Chris and Beattie, Mark},
title = {Analyzing Activity Recognition Uncertainties in Smart Home Environments},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2651445},
doi = {10.1145/2651445},
abstract = {In spite of the importance of activity recognition (AR) for intelligent human-computer interaction in emerging smart space applications, state-of-the-art AR technology is not ready or adequate for real-world deployments due to its insufficient accuracy. The accuracy limitation is directly attributed to uncertainties stemming from multiple sources in the AR system. Hence, one of the major goals of AR research is to improve system accuracy by minimizing or managing the uncertainties encountered throughout the AR process. As we cannot manage uncertainties well without measuring them, we must first quantify their impact. Nevertheless, such a quantification process is very challenging given that uncertainties come from diverse and heterogeneous sources. In this article, we propose an approach, which can account for multiple uncertainty sources and assess their impact on AR systems. We introduce several metrics to quantify the various uncertainties and their impact. We then conduct a quantitative impact analysis of uncertainties utilizing data collected from actual smart spaces that we have instrumented. The analysis is intended to serve as groundwork for developing “diagnostic” accuracy measures of AR systems capable of pinpointing the sources of accuracy loss. This is to be contrasted with the currently used accuracy measures.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {52},
numpages = {28},
keywords = {human activity recognition and activity model, Uncertainty analysis}
}

@article{10.1145/2700488,
author = {Jumadinova, Janyl and Dasgupta, Prithviraj},
title = {Automated Pricing in a Multiagent Prediction Market Using a Partially Observable Stochastic Game},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700488},
doi = {10.1145/2700488},
abstract = {Prediction markets offer an efficient market-based mechanism to aggregate large amounts of dispersed or distributed information from different people to predict the possible outcome of future events. Recently, automated prediction markets where software trading agents perform market operations such as trading and updating beliefs on behalf of humans have been proposed. A challenging aspect in automated prediction markets is to develop suitable techniques that can be used by automated trading agents to update the price at which they should trade securities related to an event so that they can increase their profit. This problem is nontrivial, as the decision to trade and the price at which trading should occur depends on several dynamic factors, such as incoming information related to the event for which the security is being traded, the belief-update mechanism and risk attitude of the trading agent, and the trading decision and trading prices of other agents. To address this problem, we have proposed a new behavior model for trading agents based on a game-theoretic framework called partially observable stochastic game with information (POSGI). We propose a correlated equilibrium (CE)-based solution strategy for this game that allows each agent to dynamically choose an action (to buy or sell or hold) in the prediction market. We have also performed extensive simulation experiments using the data obtained from the Intrade prediction market for four different prediction markets. Our results show that our POSGI model and CE strategy produces prices that are strongly correlated with the prices of the real prediction markets. Results comparing our CE strategy with five other strategies commonly used in similar market show that our CE strategy improves price predictions and provides higher utilities to the agents compared to other existing strategies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {48},
numpages = {25},
keywords = {correlated equilibrium, risk-averse traders, stochastic game, Prediction market}
}

@article{10.1145/2668136,
author = {Motai, Yuichi and Ma, Dingkun and Docef, Alen and Yoshida, Hiroyuki},
title = {Smart Colonography for Distributed Medical Databases with Group Kernel Feature Analysis},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668136},
doi = {10.1145/2668136},
abstract = {Computer-Aided Detection (CAD) of polyps in Computed Tomographic (CT) colonography is currently very limited since a single database at each hospital/institution doesn't provide sufficient data for training the CAD system's classification algorithm. To address this limitation, we propose to use multiple databases, (e.g., big data studies) to create multiple institution-wide databases using distributed computing technologies, which we call smart colonography. Smart colonography may be built by a larger colonography database networked through the participation of multiple institutions via distributed computing. The motivation herein is to create a distributed database that increases the detection accuracy of CAD diagnosis by covering many true-positive cases. Colonography data analysis is mutually accessible to increase the availability of resources so that the knowledge of radiologists is enhanced. In this article, we propose a scalable and efficient algorithm called Group Kernel Feature Analysis (GKFA), which can be applied to multiple cancer databases so that the overall performance of CAD is improved. The key idea behind the proposed GKFA method is to allow the feature space to be updated as the training proceeds with more data being fed from other institutions into the algorithm. Experimental results show that GKFA achieves very good classification accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {58},
numpages = {24},
keywords = {kernel feature analysis, Computed tomographic colonography, distributed databases, group learning}
}

@article{10.1145/2736700,
author = {Soto-Mendoza, Valeria and Garc\'{\i}a-Mac\'{\i}as, J. Antonio and Ch\'{a}vez, Edgar and Mart\'{\i}nez-Garc\'{\i}a, Ana I. and Favela, Jes\'{u}s and Serrano-Alvarado, Patricia and Rojas, Mayth\'{e} R. Z\'{u}\~{n}iga},
title = {Design of a Predictive Scheduling System to Improve Assisted Living Services for Elders},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2736700},
doi = {10.1145/2736700},
abstract = {As the number of older adults increases, and with it the demand for dedicated care, geriatric residences face a shortage of caregivers, who themselves experience work overload, stress, and burden. We conducted a long-term field study in three geriatric residences to understand the work conditions of caregivers with the aim of developing technologies to assist them in their work and help them deal with their burdens. From this study, we obtained relevant requirements and insights to design, implement, and evaluate two prototypes for supporting caregivers’ tasks (e.g., electronic recording and automatic notifications) in order to validate the feasibility of their implementation in situ and their technical requirements. The evaluation in situ of the prototypes was conducted for a period of 4 weeks. The results of the evaluation, together with the data collected from 6 months of use, motivated the design of a predictive schedule, which was iteratively improved and evaluated in participative sessions with caregivers. PRESENCE, the predictive schedule we propose, triggers real-time alerts of risky situations (e.g., falls, entering off-limits areas such as the infirmary or the kitchen) and informs caregivers of routine tasks that need to be performed (e.g., medication administration, diaper change, etc.). Moreover, PRESENCE helps caregivers to record caring tasks (such as diaper changes or medication) and well-being assessments (such as the mood) that are difficult to automate. This facilitates caregiver's shift handover and can help to train new caregivers by suggesting routine tasks and by sending reminders and timely information about residents. It can be seen as a tool to reduce the workload of caregivers and medical staff. Instead of trying to substitute the caregiver with an automatic caring system, as proposed by others, we propose our predictive schedule system that blends caregiver assessments and measurements from sensors. We show the feasibility of predicting caregiver tasks and a formative evaluation with caregivers that provides preliminary evidence of its utility.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {53},
numpages = {31},
keywords = {Assistive living systems, activities of daily living (ADL), elderly care}
}

@article{10.1145/2651444,
author = {Kim, Mi-Young and Xu, Ying and Zaiane, Osmar R. and Goebel, Randy},
title = {Recognition of Patient-Related Named Entities in Noisy Tele-Health Texts},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2651444},
doi = {10.1145/2651444},
abstract = {We explore methods for effectively extracting information from clinical narratives that are captured in a public health consulting phone service called HealthLink. Our research investigates the application of state-of-the-art natural language processing and machine learning to clinical narratives to extract information of interest. The currently available data consist of dialogues constructed by nurses while consulting patients by phone. Since the data are interviews transcribed by nurses during phone conversations, they include a significant volume and variety of noise. When we extract the patient-related information from the noisy data, we have to remove or correct at least two kinds of noise: explicit noise, which includes spelling errors, unfinished sentences, omission of sentence delimiters, and variants of terms, and implicit noise, which includes non-patient information and patient's untrustworthy information. To filter explicit noise, we propose our own biomedical term detection/normalization method: it resolves misspelling, term variations, and arbitrary abbreviation of terms by nurses. In detecting temporal terms, temperature, and other types of named entities (which show patients’ personal information such as age and sex), we propose a bootstrapping-based pattern learning process to detect a variety of arbitrary variations of named entities. To address implicit noise, we propose a dependency path-based filtering method. The result of our denoising is the extraction of normalized patient information, and we visualize the named entities by constructing a graph that shows the relations between named entities. The objective of this knowledge discovery task is to identify associations between biomedical terms and to clearly expose the trends of patients’ symptoms and concern; the experimental results show that we achieve reasonable performance with our noise reduction methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {59},
numpages = {23},
keywords = {biomedical text mining, effective information retrieval, named entity recognition, Tele-health mining}
}

@article{10.1145/2717316,
author = {Bai, Aijun and Wu, Feng and Chen, Xiaoping},
title = {Online Planning for Large Markov Decision Processes with Hierarchical Decomposition},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2717316},
doi = {10.1145/2717316},
abstract = {Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the “curse of dimensionality”— the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP—a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain—a common benchmark for MDPs—to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {45},
numpages = {28},
keywords = {MAXQ-OP, online planning, RoboCup, MDP}
}

@article{10.1145/2700497,
author = {Wang, Yi and Zhao, Xuemin and Sun, Zhenlong and Yan, Hao and Wang, Lifeng and Jin, Zhihui and Wang, Liubin and Gao, Yang and Law, Ching and Zeng, Jia},
title = {Peacock: Learning Long-Tail Topic Features for Industrial Applications},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700497},
doi = {10.1145/2700497},
abstract = {Latent Dirichlet allocation (LDA) is a popular topic modeling technique in academia but less so in industry, especially in large-scale applications involving search engine and online advertising systems. A main underlying reason is that the topic models used have been too small in scale to be useful; for example, some of the largest LDA models reported in literature have up to 103 topics, which difficultly cover the long-tail semantic word sets. In this article, we show that the number of topics is a key factor that can significantly boost the utility of topic-modeling systems. In particular, we show that a “big” LDA model with at least 105 topics inferred from 109 search queries can achieve a significant improvement on industrial search engine and online advertising systems, both of which serve hundreds of millions of users. We develop a novel distributed system called Peacock to learn big LDA models from big data. The main features of Peacock include hierarchical distributed architecture, real-time prediction, and topic de-duplication. We empirically demonstrate that the Peacock system is capable of providing significant benefits via highly scalable LDA topic models for several industrial applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {47},
numpages = {23},
keywords = {online advertising systems, big data, Latent Dirichlet allocation, long-tail topic features, search engine, big topic models}
}

@article{10.1145/2700479,
author = {Antonelli, Dario and Baralis, Elena and Bruno, Giulia and Cagliero, Luca and Cerquitelli, Tania and Chiusano, Silvia and Garza, Paolo and Mahoto, Naeem A.},
title = {MeTA: Characterization of Medical Treatments at Different Abstraction Levels},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700479},
doi = {10.1145/2700479},
abstract = {Physicians and health care organizations always collect large amounts of data during patient care. These large and high-dimensional datasets are usually characterized by an inherent sparseness. Hence, analyzing these datasets to figure out interesting and hidden knowledge is a challenging task. This article proposes a new data mining framework based on generalized association rules to discover multiple-level correlations among patient data. Specifically, correlations among prescribed examinations, drugs, and patient profiles are discovered and analyzed at different abstraction levels. The rule extraction process is driven by a taxonomy to generalize examinations and drugs into their corresponding categories. To ease the manual inspection of the result, a worthwhile subset of rules (i.e., nonredundant generalized rules) is considered. Furthermore, rules are classified according to the involved data features (medical treatments or patient profiles) and then explored in a top-down fashion: from the small subset of high-level rules, a drill-down is performed to target more specific rules. The experiments, performed on a real diabetic patient dataset, demonstrate the effectiveness of the proposed approach in discovering interesting rule groups at different abstraction levels.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {57},
numpages = {25},
keywords = {generalized association rule mining, Health care informatics, data mining}
}

@article{10.1145/2710024,
author = {Chen, Bo-Hao and Huang, Shih-Chia and Ye, Jian Hui},
title = {Hazy Image Restoration by Bi-Histogram Modification},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2710024},
doi = {10.1145/2710024},
abstract = {Visibility restoration techniques are widely used for information recovery of hazy images in many computer vision applications. Estimation of haze density is an essential task of visibility restoration techniques. However, conventional visibility restoration techniques often suffer from either the generation of serious artifacts or the loss of object information in the restored images due to uneven haze density, which usually means that the images contain heavy haze formation within their background regions and little haze formation within their foreground regions. This frequently occurs when the images feature real-world scenes with a deep depth of field. How to effectively and accurately estimate the haze density in the transmission map for these images is the most challenging aspect of the traditional state-of-the-art techniques. In response to this problem, this work proposes a novel visibility restoration approach that is based on Bi-Histogram modification, and which integrates a haze density estimation module and a haze formation removal module for effective and accurate estimation of haze density in the transmission map. As our experimental results demonstrate, the proposed approach achieves superior visibility restoration efficacy in comparison with the other state-of-the-art approaches based on both qualitative and quantitative evaluations. The proposed approach proves effective and accurate in terms of both background and foreground restoration of various hazy scenarios.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {17},
keywords = {Visibility restoration, haze density, transmission map}
}

@article{10.1145/2700836,
author = {Fu, Hao and Zhang, Aston and Xie, Xing},
title = {Effective Social Graph Deanonymization Based on Graph Structure and Descriptive Information},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700836},
doi = {10.1145/2700836},
abstract = {The study of online social networks has attracted increasing interest. However, concerns are raised for the privacy risks of user data since they have been frequently shared among researchers, advertisers, and application developers. To solve this problem, a number of anonymization algorithms have been recently developed for protecting the privacy of social graphs. In this article, we proposed a graph node similarity measurement in consideration with both graph structure and descriptive information, and a deanonymization algorithm based on the measurement. Using the proposed algorithm, we evaluated the privacy risks of several typical anonymization algorithms on social graphs with thousands of nodes from Microsoft Academic Search, LiveJournal, and the Enron email dataset, and a social graph with millions of nodes from Tencent Weibo. Our results showed that the proposed algorithm was efficient and effective to deanonymize social graphs without any initial seed mappings. Based on the experiments, we also pointed out suggestions on how to better maintain the data utility while preserving privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {49},
numpages = {29},
keywords = {social network, Deanonymization, privacy protection}
}

@article{10.1145/2700482,
author = {Yang, Haodong and Yang, Christopher C.},
title = {Using Health-Consumer-Contributed Data to Detect Adverse Drug Reactions by Association Mining with Temporal Analysis},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700482},
doi = {10.1145/2700482},
abstract = {Since adverse drug reactions (ADRs) represent a significant health problem all over the world, ADR detection has become an important research topic in drug safety surveillance. As many potential ADRs cannot be detected though premarketing review, drug safety currently depends heavily on postmarketing surveillance. Particularly, current postmarketing surveillance in the United States primarily relies on the FDA Adverse Event Reporting System (FAERS). However, the effectiveness of such spontaneous reporting systems for ADR detection is not as good as expected because of the extremely high underreporting ratio of ADRs. Moreover, it often takes the FDA years to complete the whole process of collecting reports, investigating cases, and releasing alerts. Given the prosperity of social media, many online health communities are publicly available for health consumers to share and discuss any healthcare experience such as ADRs they are suffering. Such health-consumer-contributed content is timely and informative, but this data source still remains untapped for postmarketing drug safety surveillance. In this study, we propose to use (1) association mining to identify the relations between a drug and an ADR and (2) temporal analysis to detect drug safety signals at the early stage. We collect data from MedHelp and use the FDA's alerts and information of drug labeling revision as the gold standard to evaluate the effectiveness of our approach. The experiment results show that health-related social media is a promising source for ADR detection, and our proposed techniques are effective to identify early ADR signals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {55},
numpages = {27},
keywords = {postmarketing surveillance, social media, association mining, temporal analysis, health-consumer-contributed content, Drug safety signal detection, adverse drug reactions}
}

@article{10.1145/2700480,
author = {Champaign, John and Cohen, Robin and Lam, Disney Yan},
title = {Empowering Patients and Caregivers to Manage Healthcare Via Streamlined Presentation of Web Objects Selected by Modeling Learning Benefits Obtained by Similar Peers},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700480},
doi = {10.1145/2700480},
abstract = {In this article, we introduce a framework for selecting web objects (texts, videos, simulations) from a large online repository to present to patients and caregivers, in order to assist in their healthcare. Motivated by the paradigm of peer-based intelligent tutoring, we model the learning gains achieved by users when exposed to specific web objects in order to recommend those objects most likely to deliver benefit to new users. We are able to show that this streamlined presentation leads to effective knowledge gains, both through a process of simulated learning and through a user study, for the specific application of caring for children with autism. The value of our framework for peer-driven content selection of health information is emphasized through two additional roles for peers: attaching commentary to web objects and proposing subdivided objects for presentation, both of which are demonstrated to deliver effective learning gains, in simulations. In all, we are offering an opportunity for patients to navigate the deep waters of excessive online information towards effective management of healthcare, through content selection influenced by previous peer experiences.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {54},
numpages = {41},
keywords = {Shareable health knowledge, e-communities for patients and caregivers, discovery of new knowledge for decision support, effective information retrieval for healthcare applications, cyber-based empowering of patients, computational support for patient-centred care}
}

@article{10.1145/2717317,
author = {Anantharam, Pramod and Barnaghi, Payam and Thirunarayan, Krishnaprasad and Sheth, Amit},
title = {Extracting City Traffic Events from Social Streams},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2717317},
doi = {10.1145/2717317},
abstract = {Cities are composed of complex systems with physical, cyber, and social components. Current works on extracting and understanding city events mainly rely on technology-enabled infrastructure to observe and record events. In this work, we propose an approach to leverage citizen observations of various city systems and services, such as traffic, public transport, water supply, weather, sewage, and public safety, as a source of city events. We investigate the feasibility of using such textual streams for extracting city events from annotated text. We formalize the problem of annotating social streams such as microblogs as a sequence labeling problem. We present a novel training data creation process for training sequence labeling models. Our automatic training data creation process utilizes instance-level domain knowledge (e.g., locations in a city, possible event terms). We compare this automated annotation process to a state-of-the-art tool that needs manually created training data and show that it has comparable performance in annotation tasks. An aggregation algorithm is then presented for event extraction from annotated text. We carry out a comprehensive evaluation of the event annotation and event extraction on a real-world dataset consisting of event reports and tweets collected over 4 months from the San Francisco Bay Area. The evaluation results are promising and provide insights into the utility of social stream for extracting city events.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {27},
keywords = {tweets, event extraction, city events, citizen sensing, physical-cyber-social systems, Smart cities}
}

@article{10.1145/2716328,
author = {Sawant, Anshul and Dickerson, John P. and Hajiaghayi, Mohammad T. and Subrahmanian, V. S.},
title = {Automated Generation of Counterterrorism Policies Using Multiexpert Input},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2716328},
doi = {10.1145/2716328},
abstract = {The use of game theory to model conflict has been studied by several researchers, spearheaded by Schelling. Most of these efforts assume a single payoff matrix that captures players’ utilities under different assumptions about what the players will do. Our experience in counterterrorism applications is that experts disagree on these payoffs. We leverage Shapley’s&nbsp;notion of vector equilibria, which formulates games where there are multiple payoff matrices, but note that they are very hard to compute in practice. To effectively enumerate large numbers of equilibria with payoffs provided by multiple experts, we propose a novel combination of vector payoffs and well-supported ϵ-approximate equilibria. We develop bounds related to computation of these equilibria for some special cases and give a quasipolynomial time approximation scheme (QPTAS) for the general case when the number of players is small (which is true in many real-world applications). Leveraging this QPTAS, we give efficient algorithms to find such equilibria and experimental results showing that they work well on simulated data.We then built a policy recommendation engine based on vector equilibria, called PREVE. We use PREVE to model the terrorist group Lashkar-e-Taiba (LeT), responsible for the 2008 Mumbai attacks, as a five-player game. Specifically, we apply it to three payoff matrices provided by experts in India--Pakistan relations, analyze the equilibria generated by PREVE, and suggest counterterrorism policies that may reduce attacks by LeT. We briefly discuss these results and identify their strengths and weaknesses from a policy point of view.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {44},
numpages = {27},
keywords = {vector equilibria, Counterterrorism, game theory, behavioral modeling, Lashkar-e-Taiba}
}

@article{10.1145/2700483,
author = {Ye, Yanfang and Li, Tao and Shen, Haiyin},
title = {Soter: Smart Bracelets for Children’s Safety},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700483},
doi = {10.1145/2700483},
abstract = {In recent years, crimes against children and cases of missing children have increased at a high rate. Therefore, there is an urgent need for safety support systems to prevent crimes against children or for antiloss, especially when parents are not with their children, such as to and from school. However, existing children’s tracking systems are not smart enough to provide the safety supports, as they simply locate the children’s positions without offering any notification to parents that their children may be in danger. In addition, there is limited research on children’s tracking and their antiloss. In this article, based on location histories, we introduce novel notions of children’s life patterns that capture their general lifestyles and regularities, and develop an intelligent data mining framework to learn the safe regions and safe routes of children on the cloud side. When the children may be in danger, their parents will receive automatic notifications from the cloud. We also propose an effective energy-efficient positioning scheme that leverages the location tracking accuracy of the children while keeping energy overhead low by using a hybrid global positioning system and a global system for mobile communications. To the best of our knowledge, this is the first attempt in applying data mining techniques to applications designed for children’s safety. Our proposed techniques have been incorporated into Soter, a children’s safeguard system that is used to provide cloud service for smart bracelets produced by Qihoo. The case studies on real smart bracelet users of Qihoo demonstrate the effectiveness of our proposed methods and Soter for children’s safety.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {46},
numpages = {20},
keywords = {smart notification, energy-efficient positioning, Smart bracelet, safe region, safe route, children’s safety}
}

@article{10.1145/2791398,
author = {Combi, Carlo and Liu, Jiming},
title = {Introduction to the ACM TIST Special Issue on Intelligent Healthcare Informatics},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2791398},
doi = {10.1145/2791398},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {51},
numpages = {3}
}

@article{10.1145/2700487,
author = {Ullah, Md Zia and Aono, Masaki and Seddiqui, Md Hanif},
title = {Estimating a Ranked List of Human Genetic Diseases by Associating Phenotype-Gene with Gene-Disease Bipartite Graphs},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700487},
doi = {10.1145/2700487},
abstract = {With vast amounts of medical knowledge available on the Internet, it is becoming increasingly practical to help doctors in clinical diagnostics by suggesting plausible diseases predicted by applying data and text mining technologies. Recently, Genome-Wide Association Studies (GWAS) have proved useful as a method for exploring phenotypic associations with diseases. However, since genetic diseases are difficult to diagnose because of their low prevalence, large number, and broad diversity of symptoms, genetic disease patients are often misdiagnosed or experience long diagnostic delays. In this article, we propose a method for ranking genetic diseases for a set of clinical phenotypes. In this regard, we associate a phenotype-gene bipartite graph (PGBG) with a gene-disease bipartite graph (GDBG) by producing a phenotype-disease bipartite graph (PDBG), and we estimate the candidate weights of diseases. In our approach, all paths from a phenotype to a disease are explored by considering causative genes to assign a weight based on path frequency, and the phenotype is linked to the disease in a new PDBG. We introduce the Bidirectionally induced Importance Weight (BIW) prediction method to PDBG for approximating the weights of the edges of diseases with phenotypes by considering link information from both sides of the bipartite graph. The performance of our system is compared to that of other known related systems by estimating Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), and Kendall’s tau metrics. Further experiments are conducted with well-known TF · IDF, BM25, and Jenson-Shannon divergence as baselines. The result shows that our proposed method outperforms the known related tool Phenomizer in terms of NDCG@10, NDCG@20, MAP@10, and MAP@20; however, it performs worse than Phenomizer in terms of Kendall’s tau-b metric at the top-10 ranks. It also turns out that our proposed method has overall better performance than the baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {56},
numpages = {21},
keywords = {Phenotype, Bipartite, Genotype, NDCG, BM25}
}

@article{10.1145/2700478,
author = {Zhang, Wangsheng and Qi, Guande and Pan, Gang and Lu, Hua and Li, Shijian and Wu, Zhaohui},
title = {City-Scale Social Event Detection and Evaluation with Taxi Traces},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700478},
doi = {10.1145/2700478},
abstract = {A social event is an occurrence that involves lots of people and is accompanied by an obvious rise in human flow. Analysis of social events has real-world importance because events bring about impacts on many aspects of city life. Traditionally, detection and impact measurement of social events rely on social investigation, which involves considerable human effort. Recently, by analyzing messages in social networks, researchers can also detect and evaluate country-scale events. Nevertheless, the analysis of city-scale events has not been explored. In this article, we use human flow dynamics, which reflect the social activeness of a region, to detect social events and measure their impacts. We first extract human flow dynamics from taxi traces. Second, we propose a method that can not only discover the happening time and venue of events from abnormal social activeness, but also measure the scale of events through changes in such activeness. Third, we extract traffic congestion information from traces and use its change during social events to measure their impact. The results of experiments validate the effectiveness of both the event detection and impact measurement methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {40},
numpages = {20},
keywords = {social impact, Social event, traffic condition, taxi traces}
}

@article{10.1145/2743025,
author = {Zheng, Yu},
title = {Trajectory Data Mining: An Overview},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2743025},
doi = {10.1145/2743025},
abstract = {The advances in location-acquisition and mobile computing techniques have generated massive spatial trajectory data, which represent the mobility of a diversity of moving objects, such as people, vehicles, and animals. Many techniques have been proposed for processing, managing, and mining trajectory data in the past decade, fostering a broad range of applications. In this article, we conduct a systematic survey on the major research into trajectory data mining, providing a panorama of the field as well as the scope of its research topics. Following a road map from the derivation of trajectory data, to trajectory data preprocessing, to trajectory data management, and to a variety of mining tasks (such as trajectory pattern mining, outlier detection, and trajectory classification), the survey explores the connections, correlations, and differences among these existing techniques. This survey also introduces the methods that transform trajectories into other data formats, such as graphs, matrices, and tensors, to which more data mining and machine learning techniques can be applied. Finally, some public trajectory datasets are presented. This survey can help shape the field of trajectory data mining, providing a quick understanding of this field to the community.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {29},
numpages = {41},
keywords = {trajectory uncertainty, trajectory indexing and retrieval, trajectory classification, trajectory data mining, Spatiotemporal data mining, trajectory pattern mining, trajectory compression, urban computing, trajectory outlier detection}
}

@article{10.1145/2700495,
author = {Elbadrawy, Asmaa and Karypis, George},
title = {User-Specific Feature-Based Similarity Models for Top-<i>n</i> Recommendation of New Items},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700495},
doi = {10.1145/2700495},
abstract = {Recommending new items for suitable users is an important yet challenging problem due to the lack of preference history for the new items. Noncollaborative user modeling techniques that rely on the item features can be used to recommend new items. However, they only use the past preferences of each user to provide recommendations for that user. They do not utilize information from the past preferences of other users, which can potentially be ignoring useful information. More recent factor models transfer knowledge across users using their preference information in order to provide more accurate recommendations. These methods learn a low-rank approximation for the preference matrix, which can lead to loss of information. Moreover, they might not be able to learn useful patterns given very sparse datasets. In this work, we present <scp>UFSM</scp>, a method for top-<i>n</i> recommendation of new items given binary user preferences. <scp>UFSM</scp> learns <b>U</b>ser-specific <b>F</b>eature-based item-<b>S</b>imilarity <b>M</b>odels, and its strength lies in combining two points: (1)&nbsp;exploiting preference information across all users to learn multiple global item similarity functions and (2)&nbsp;learning user-specific weights that determine the contribution of each global similarity function in generating recommendations for each user. <scp>UFSM</scp> can be considered as a sparse high-dimensional factor model where the previous preferences of each user are incorporated within his or her latent representation. This way, <scp>UFSM</scp> combines the merits of item similarity models that capture local relations among items and factor models that learn global preference patterns. A comprehensive set of experiments was conduced to compare <scp>UFSM</scp> against state-of-the-art collaborative factor models and noncollaborative user modeling techniques. Results show that <scp>UFSM</scp> outperforms other techniques in terms of recommendation quality. <scp>UFSM</scp> manages to yield better recommendations even with very sparse datasets. Results also show that <scp>UFSM</scp> can efficiently handle high-dimensional as well as low-dimensional item feature spaces.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {33},
numpages = {20},
keywords = {item content, item similarity, cold start, top-<i>n</i>, item features, Recommender systems}
}

@article{10.1145/2700494,
author = {Zhang, Mingjin and Wang, Huibo and Lu, Yun and Li, Tao and Guang, Yudong and Liu, Chang and Edrosa, Erik and Li, Hongtai and Rishe, Naphtali},
title = {TerraFly GeoCloud: An Online Spatial Data Analysis and Visualization System},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700494},
doi = {10.1145/2700494},
abstract = {With the exponential growth of the usage of web map services, geo-data analysis has become more and more popular. This article develops an online spatial data analysis and visualization system, TerraFly GeoCloud, which helps end-users visualize and analyze spatial data and share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon the TerraFly map and can efficiently support many different visualization functions and spatial data analysis models. Furthermore, users can create unique URLs to visualize and share the analysis results. TerraFly GeoCloud also enables the MapQL technology to customize map visualization using SQL-like statements. The system is available at http://terrafly.fiu.edu/GeoCloud/.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {24},
keywords = {visualization, big data, Geospatial analysis, GIS}
}

@article{10.1145/2700472,
author = {Hirschprung, Ron and Toch, Eran and Maimon, Oded},
title = {Simplifying Data Disclosure Configurations in a Cloud Computing Environment},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700472},
doi = {10.1145/2700472},
abstract = {Cloud computing offers a compelling vision of computation, enabling an unprecedented level of data distribution and sharing. Beyond improving the computing infrastructure, cloud computing enables a higher level of interoperability between information systems, simplifying tasks such as sharing documents between coworkers or enabling collaboration between an organization and its suppliers. While these abilities may result in significant benefits to users and organizations, they also present privacy challenges due to unwanted exposure of sensitive information. As information-sharing processes in cloud computing are complex and domain specific, configuring these processes can be an overwhelming and burdensome task for users. This article investigates the feasibility of configuring sharing processes through a small and representative set of canonical configuration options. For this purpose, we present a generic method, named SCON-UP (Simplified CON-figuration of User Preferences). SCON-UP simplifies configuration interfaces by using a clustering algorithm that analyzes a massive set of sharing preferences and condenses them into a small number of discrete disclosure levels. Thus, the user is provided with a usable configuration model while guaranteeing adequate privacy control. We describe the algorithm and empirically evaluate our model using data collected in two user studies (n = 121 and n = 352). Our results show that when provided with three canonical configuration options, on average, 82% of the population can be covered by at least one option. We exemplify the feasibility of discretizing sharing levels and discuss the tradeoff between coverage and simplicity in discrete configuration options.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {32},
numpages = {26},
keywords = {data protection, information disclosure, preference clustering, human--computer interaction, artificial intelligence (AI), intelligent agents, Privacy, cloud computing}
}

@article{10.1145/2644827,
author = {Xiong, Haoyi and Zhang, Daqing and Wang, Leye and Gibson, J. Paul and Zhu, Jie},
title = {EEMC: Enabling Energy-Efficient Mobile Crowdsensing with Anonymous Participants},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2644827},
doi = {10.1145/2644827},
abstract = {Mobile Crowdsensing (MCS) requires users to be motivated to participate. However, concerns regarding energy consumption and privacy—among other things—may compromise their willingness to join such a crowd. Our preliminary observations and analysis of common MCS applications have shown that the data transfer in MCS applications may incur significant energy consumption due to the 3G connection setup. However, if data are transferred in parallel with a traditional phone call, then such transfer can be done almost “for free”: with only an insignificant additional amount of energy required to piggy-back the data—usually incoming task assignments and outgoing sensor results—on top of the call. Here, we present an <i>Energy-Efficient Mobile Crowdsensing</i> (EEMC) framework where task assignments and sensing results are transferred in parallel with phone calls. The main objective, and the principal contribution of this article, is an MCS task assignment scheme that guarantees that a minimum number of anonymous participants return sensor results within a specified time frame, while also minimizing the waste of energy due to redundant task assignments and considering privacy concerns of participants. Evaluations with a large-scale real-world phone call dataset show that our proposed <i>EEMC</i> framework outperforms the baseline approaches, and it can reduce overall energy consumption in data transfer by 54--66% when compared to the 3G-based solution.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {39},
numpages = {26},
keywords = {Mobile crowdsensing, task assignment decision making, anonymous participants, energy efficiency}
}

@article{10.1145/2700481,
author = {Bouguessa, Mohamed and Romdhane, Lotfi Ben},
title = {Identifying Authorities in Online Communities},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700481},
doi = {10.1145/2700481},
abstract = {Several approaches have been proposed for the problem of identifying authoritative actors in online communities. However, the majority of existing methods suffer from one or more of the following limitations: (1) There is a lack of an automatic mechanism to formally discriminate between authoritative and nonauthoritative users. In fact, a common approach to authoritative user identification is to provide a ranked list of users expecting authorities to come first. A major problem of such an approach is the question of where to stop reading the ranked list of users. How many users should be chosen as authoritative? (2) Supervised learning approaches for authoritative user identification suffer from their dependency on the training data. The problem here is that labeled samples are more difficult, expensive, and time consuming to obtain than unlabeled ones. (3) Several approaches rely on some user parameters to estimate an authority score. Detection accuracy of authoritative users can be seriously affected if incorrect values are used. In this article, we propose a parameterless mixture model-based approach that is capable of addressing the three aforementioned issues in a single framework. In our approach, we first represent each user with a feature vector composed of information related to its social behavior and activity in an online community. Next, we propose a statistical framework, based on the multivariate beta mixtures, in order to model the estimated set of feature vectors. The probability density function is therefore estimated and the beta component that corresponds to the most authoritative users is identified. The suitability of the proposed approach is illustrated on real data extracted from the Stack Exchange question-answering network and Twitter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {23},
keywords = {Online communities, authoritative users, multivariate beta, mixture model, unsupervised learning}
}

@article{10.1145/2700468,
author = {Sang, Jitao and Mei, Tao and Xu, Changsheng},
title = {Activity Sensor: Check-In Usage Mining for Local Recommendation},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700468},
doi = {10.1145/2700468},
abstract = {While on the go, people are using their phones as a personal concierge discovering what is around and deciding what to do. Mobile phone has become a recommendation terminal customized for individuals—capable of recommending activities and simplifying the accomplishment of related tasks. In this article, we conduct usage mining on the check-in data, with summarized statistics identifying the local recommendation challenges of huge solution space, sparse available data, and complicated user intent, and discovered observations to motivate the hierarchical, contextual, and sequential solution. We present a point-of-interest (POI) category-transition--based approach, with a goal of estimating the visiting probability of a series of successive POIs conditioned on current user context and sensor context. A mobile local recommendation demo application is deployed. The objective and subjective evaluations validate the effectiveness in providing mobile users both accurate recommendation and favorable user experience.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {41},
numpages = {24},
keywords = {local recommendation, check-in, usage mining, Location-based service}
}

@article{10.1145/2630074,
author = {Zhang, Bo and Song, Zheng and Liu, Chi Harold and Ma, Jian and Wang, Wendong},
title = {An Event-Driven QoI-Aware Participatory Sensing Framework with Energy and Budget Constraints},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2630074},
doi = {10.1145/2630074},
abstract = {Participatory sensing systems can be used for concurrent event monitoring applications, like noise levels, fire, and pollutant concentrations. However, they are facing new challenges as to how to accurately detect the exact boundaries of these events, and further, to select the most appropriate participants to collect the sensing data. On the one hand, participants’ handheld smart devices are constrained with different energy conditions and sensing capabilities, and they move around with uncontrollable mobility patterns in their daily life. On the other hand, these sensing tasks are within time-varying quality-of-information (QoI) requirements and budget to afford the users’ incentive expectations. Toward this end, this article proposes an event-driven QoI-aware participatory sensing framework with energy and budget constraints. The main method of this framework is event boundary detection. For the former, a two-step heuristic solution is proposed where the coarse-grained detection step finds its approximation and the fine-grained detection step identifies the exact location. Participants are selected by explicitly considering their mobility pattern, required QoI of multiple tasks, and users’ incentive requirements, under the constraint of an aggregated task budget. Extensive experimental results, based on a real trace in Beijing, show the effectiveness and robustness of our approach, while comparing with existing schemes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {42},
numpages = {19},
keywords = {energy efficiency, event boundary detection, Participatory sensing}
}

@article{10.1145/2735959,
author = {Gao, Yue and Yang, You and Zhen, Yi and Dai, Qionghai},
title = {Depth Error Elimination for RGB-D Cameras},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2735959},
doi = {10.1145/2735959},
abstract = {The rapid spreading of RGB-D cameras has led to wide applications of 3D videos in both academia and industry, such as 3D entertainment and 3D visual understanding. Under these circumstances, extensive research efforts have been dedicated to RGB-D camera--oriented topics. In these topics, quality promotion of depth videos with the temporal characteristic is emerging and important. Due to the limited exposure time of RGB-D cameras, object movement can easily lead to motion blurs in intensive images, which can further result in obvious artifacts (holes or fake boundaries) in the corresponding depth frames. With regard to this problem, we propose a depth error elimination method based on time series analysis to remove the artifacts in depth images. In this method, we first locate the regions with erroneous depths in intensive images by using motion blur detection based on a time series analysis model. This is based on the fact that the depth image is calculated by intensive color images that are captured synchronously by RGB-D cameras. Then, the artifacts, such as holes or fake boundaries, are fixed by a depth error elimination method. To evaluate the performance of the proposed method, we conducted experiments on 250 images. Experimental results demonstrate that the proposed method can locate the error regions correctly and eliminate these artifacts effectively. The quality of depth video can be improved significantly by using the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {13},
numpages = {16},
keywords = {depth video, RGB-D cameras, time series analysis, Depth error}
}

@article{10.1145/2745712,
author = {Guo, Bin and Chin, Alvin and Yu, Zhiwen and Huang, Runhe and Zhang, Daqing},
title = {An Introduction to the Special Issue on Participatory Sensing and Crowd Intelligence},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2745712},
doi = {10.1145/2745712},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {36},
numpages = {4}
}

@article{10.1145/2717318,
author = {Tangmunarunkit, H. and Hsieh, C. K. and Longstaff, B. and Nolen, S. and Jenkins, J. and Ketcham, C. and Selsky, J. and Alquaddoomi, F. and George, D. and Kang, J. and Khalapyan, Z. and Ooms, J. and Ramanathan, N. and Estrin, D.},
title = {Ohmage: A General and Extensible End-to-End Participatory Sensing Platform},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2717318},
doi = {10.1145/2717318},
abstract = {Participatory sensing (PS) is a distributed data collection and analysis approach where individuals, acting alone or in groups, use their personal mobile devices to systematically explore interesting aspects of their lives and communities [Burke et al. 2006]. These mobile devices can be used to capture diverse spatiotemporal data through both intermittent self-report and continuous recording from on-board sensors and applications.Ohmage (http://ohmage.org) is a modular and extensible open-source, mobile to Web PS platform that records, stores, analyzes, and visualizes data from both prompted self-report and continuous data streams. These data streams are authorable and can dynamically be deployed in diverse settings. Feedback from hundreds of behavioral and technology researchers, focus group participants, and end users has been integrated into ohmage through an iterative participatory design process. Ohmage has been used as an enabling platform in more than 20 independent projects in many disciplines. We summarize the PS requirements, challenges and key design objectives learned through our design process, and ohmage system architecture to achieve those objectives. The flexibility, modularity, and extensibility of ohmage in supporting diverse deployment settings are presented through three distinct case studies in education, health, and clinical research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {38},
numpages = {21},
keywords = {mobile data collection, experience sampling, educational tool, data sharing, citizen science, Participatory sensing, mobile sensing, clinical research}
}

@article{10.1145/2700484,
author = {Chen, Yi-Cheng and Peng, Wen-Chih and Huang, Jiun-Long and Lee, Wang-Chien},
title = {Significant Correlation Pattern Mining in Smart Homes},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700484},
doi = {10.1145/2700484},
abstract = {Owing to the great advent of sensor technology, the usage data of appliances in a house can be logged and collected easily today. However, it is a challenge for the residents to visualize how these appliances are used. Thus, mining algorithms are much needed to discover appliance usage patterns. Most previous studies on usage pattern discovery are mainly focused on analyzing the patterns of single appliance rather than mining the usage correlation among appliances. In this article, a novel algorithm, namely Correlation Pattern Miner (CoPMiner), is developed to capture the usage patterns and correlations among appliances probabilistically. CoPMiner also employs four pruning techniques and a statistical model to reduce the search space and filter out insignificant patterns, respectively. Furthermore, the proposed algorithm is applied on a real-world dataset to show the practicability of correlation pattern mining.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {35},
numpages = {23},
keywords = {smart home, time interval--based data, Correlation pattern, sequential pattern, usage representation}
}

@article{10.1145/2700466,
author = {Lee, Kyumin and Mahmud, Jalal and Chen, Jilin and Zhou, Michelle and Nichols, Jeffrey},
title = {Who Will Retweet This? Detecting Strangers from Twitter to Retweet Information},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700466},
doi = {10.1145/2700466},
abstract = {There has been much effort on studying how social media sites, such as Twitter, help propagate information in different situations, including spreading alerts and SOS messages in an emergency. However, existing work has not addressed how to actively identify and engage the right strangers at the right time on social media to help effectively propagate intended information within a desired time frame. To address this problem, we have developed three models: (1) a feature-based model that leverages people's exhibited social behavior, including the content of their tweets and social interactions, to characterize their willingness and readiness to propagate information on Twitter via the act of retweeting; (2) a wait-time model based on a user's previous retweeting wait times to predict his or her next retweeting time when asked; and (3) a subset selection model that automatically selects a subset of people from a set of available people using probabilities predicted by the feature-based model and maximizes retweeting rate. Based on these three models, we build a recommender system that predicts the likelihood of a stranger to retweet information when asked, within a specific time window, and recommends the top-N qualified strangers to engage with. Our experiments, including live studies in the real world, demonstrate the effectiveness of our work.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {25},
keywords = {willingness, social media, Twitter, retweet, personality}
}

@article{10.1145/2644828,
author = {Zhang, Fuzheng and Yuan, Nicholas Jing and Wilkie, David and Zheng, Yu and Xie, Xing},
title = {Sensing the Pulse of Urban Refueling Behavior: A Perspective from Taxi Mobility},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2644828},
doi = {10.1145/2644828},
abstract = {Urban transportation is an important factor in energy consumption and pollution, and is of increasing concern due to its complexity and economic significance. Its importance will only increase as urbanization continues around the world. In this article, we explore drivers’ refueling behavior in urban areas. Compared to questionnaire-based methods of the past, we propose a complete data-driven system that pushes towards real-time sensing of individual refueling behavior and citywide petrol consumption. Our system provides the following: detection of individual refueling events (REs) from which refueling preference can be analyzed; estimates of gas station wait times from which recommendations can be made; an indication of overall fuel demand from which macroscale economic decisions can be made, and a spatial, temporal, and economic view of urban refueling characteristics. For individual behavior, we use reported trajectories from a fleet of GPS-equipped taxicabs to detect gas station visits. For time spent estimates, to solve the sparsity issue along time and stations, we propose context-aware tensor factorization (CATF), a factorization model that considers a variety of contextual factors (e.g., price, brand, and weather condition) that affect consumers’ refueling decision. For fuel demand estimates, we apply a queue model to calculate the overall visits based on the time spent inside the station. We evaluated our system on large-scale and real-world datasets, which contain 4-month trajectories of 32,476 taxicabs, 689 gas stations, and the self-reported refueling details of 8,326 online users. The results show that our system can determine REs with an accuracy of more than 90%, estimate time spent with less than 2 minutes of error, and measure overall visits in the same order of magnitude with the records in the field study.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {37},
numpages = {23},
keywords = {arrival rate, expected duration, spatial-temporal unit, Refueling event}
}

@article{10.1145/2629557,
author = {Lian, Defu and Xie, Xing and Zheng, Vincent W. and Yuan, Nicholas Jing and Zhang, Fuzheng and Chen, Enhong},
title = {CEPR: A Collaborative Exploration and Periodically Returning Model for Location Prediction},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629557},
doi = {10.1145/2629557},
abstract = {With the growing popularity of location-based social networks, numerous location visiting records (e.g., check-ins) continue to accumulate over time. The more these records are collected, the better we can understand users’ mobility patterns and the more accurately we can predict their future locations. However, due to the personality trait of neophilia, people also show propensities of novelty seeking in human mobility, that is, exploring unvisited but tailored locations for them to visit. As such, the existing prediction algorithms, mainly relying on regular mobility patterns, face severe challenges because such behavior is beyond the reach of regularity. As a matter of fact, the prediction of this behavior not only relies on the forecast of novelty-seeking tendency but also depends on how to determine unvisited candidate locations. To this end, we put forward a Collaborative Exploration and Periodically Returning model (CEPR), based on a novel problem, Exploration Prediction (EP), which forecasts whether people will seek unvisited locations to visit, in the following. When people are predicted to do exploration, a state-of-the-art recommendation algorithm, armed with collaborative social knowledge and assisted by geographical influence, will be applied for seeking the suitable candidates; otherwise, a traditional prediction algorithm, incorporating both regularity and the Markov model, will be put into use for figuring out the most possible locations to visit. We then perform case studies on check-ins and evaluate them on two large-scale check-in datasets with 6M and 36M records, respectively. The evaluation results show that EP achieves a roughly 20% classification error rate on both datasets, greatly outperforming the baselines, and that CEPR improves performances by as much as 30% compared to the traditional location prediction algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {8},
numpages = {27},
keywords = {location recommendation, location prediction, social network, Exploration Prediction, Location-based services}
}

@article{10.1145/2735952,
author = {Tang, Ao and Lu, Ke and Wang, Yufei and Huang, Jie and Li, Houqiang},
title = {A Real-Time Hand Posture Recognition System Using Deep Neural Networks},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2735952},
doi = {10.1145/2735952},
abstract = {Hand posture recognition (HPR) is quite a challenging task, due to both the difficulty in detecting and tracking hands with normal cameras and the limitations of traditional manually selected features. In this article, we propose a two-stage HPR system for Sign Language Recognition using a Kinect sensor. In the first stage, we propose an effective algorithm to implement hand detection and tracking. The algorithm incorporates both color and depth information, without specific requirements on uniform-colored or stable background. It can handle the situations in which hands are very close to other parts of the body or hands are not the nearest objects to the camera and allows for occlusion of hands caused by faces or other hands. In the second stage, we apply deep neural networks (DNNs) to automatically learn features from hand posture images that are insensitive to movement, scaling, and rotation. Experiments verify that the proposed system works quickly and accurately and achieves a recognition accuracy as high as 98.12%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {21},
numpages = {23},
keywords = {Kinect, deep neural networks, hand tracking, posture recognition}
}

@article{10.1145/2735951,
author = {Kyan, Matthew and Sun, Guoyu and Li, Haiyan and Zhong, Ling and Muneesawang, Paisarn and Dong, Nan and Elder, Bruce and Guan, Ling},
title = {An Approach to Ballet Dance Training through MS Kinect and Visualization in a CAVE Virtual Reality Environment},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2735951},
doi = {10.1145/2735951},
abstract = {This article proposes a novel framework for the real-time capture, assessment, and visualization of ballet dance movements as performed by a student in an instructional, virtual reality (VR) setting. The acquisition of human movement data is facilitated by skeletal joint tracking captured using the popular Microsoft (MS) Kinect camera system, while instruction and performance evaluation are provided in the form of 3D visualizations and feedback through a CAVE virtual environment, in which the student is fully immersed. The proposed framework is based on the unsupervised parsing of ballet dance movement into a structured posture space using the spherical self-organizing map (SSOM). A unique feature descriptor is proposed to more appropriately reflect the subtleties of ballet dance movements, which are represented as gesture trajectories through posture space on the SSOM. This recognition subsystem is used to identify the category of movement the student is attempting when prompted (by a virtual instructor) to perform a particular dance sequence. The dance sequence is then segmented and cross-referenced against a library of gestural components performed by the teacher. This facilitates alignment and score-based assessment of individual movements within the context of the dance sequence. An immersive interface enables the student to review his or her performance from a number of vantage points, each providing a unique perspective and spatial context suggestive of how the student might make improvements in training. An evaluation of the recognition and virtual feedback systems is presented.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {23},
numpages = {37},
keywords = {gesture recognition, ballet, MS Kinect, CAVE, virtual reality, self-organizing maps, immersive training and simulation, human--computer interaction, dance}
}

@article{10.1145/2735521,
author = {Zha, Zheng-Jun and Yang, Yang and Tang, Jinhui and Wang, Meng and Chua, Tat-Seng},
title = {Robust Multiview Feature Learning for RGB-D Image Understanding},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2735521},
doi = {10.1145/2735521},
abstract = {The availability of massive RGB-depth (RGB-D) images poses a compelling need for effective RGB-D content understanding techniques. RGB-D images provide synchronized information from multiple views (e.g., color and depth) of real-world objects and scenes. This work proposes learning compact and discriminative features from the multiple views of RGB-D content toward effective feature representation for RGB-D image understanding. In particular, a robust multiview feature learning approach is developed, which exploits the intrinsic relations among multiple views. The feature learning in multiple views is jointly optimized in an integrated formulation. The joint optimization essentially exploits the intrinsic relations among the views, leading to effective features and making the learning process robust to noises. The feature learning function is formulated as a robust nonnegative graph embedding function over multiple graphs in various views. The graphs characterize the local geometric and discriminating structure of the multiview data. The joint sparsity in ℓ1-norm graph embedding and ℓ21-norm data factorization further enhances the robustness of feature learning. We derive an efficient computational solution for the proposed approach and provide rigorous theoretical proof with regard to its convergence. We apply the proposed approach to two RGB-D image understanding tasks: RGB-D object classification and RGB-D scene categorization. We conduct extensive experiments on two real-world RGB-D image datasets. The experimental results have demonstrated the effectiveness of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {15},
numpages = {19},
keywords = {multiview feature learning, RGB-D content, image understanding}
}

@article{10.1145/2732265,
author = {Hong, Richang and Yan, Shuicheng and Zhang, Zhengyou},
title = {Visual Understanding with RGB-D Sensors: An Introduction to the Special Issue},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2732265},
doi = {10.1145/2732265},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {11},
numpages = {3}
}

@article{10.1145/2700475,
author = {Chen, Chongyu and Cai, Jianfei and Zheng, Jianmin and Cham, Tat Jen and Shi, Guangming},
title = {Kinect Depth Recovery Using a Color-Guided, Region-Adaptive, and Depth-Selective Framework},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700475},
doi = {10.1145/2700475},
abstract = {Considering that the existing depth recovery approaches have different limitations when applied to Kinect depth data, in this article, we propose to integrate their effective features including adaptive support region selection, reliable depth selection, and color guidance together under an optimization framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term, which are designed according to the Kinect characteristics. Our framework inherits and improves the idea of guided filtering by incorporating structure information and prior knowledge of the Kinect noise model. Through analyzing the solution to the optimization framework, we also derive a local filtering version that provides an efficient and effective way of improving the existing filtering techniques. Quantitative evaluations on our developed synthesized dataset and experiments on real Kinect data show that the proposed method achieves superior performance in terms of recovery accuracy and visual quality.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {12},
numpages = {19},
keywords = {variational framework, Kinect, Depth recovery}
}

@article{10.1145/2700465,
author = {Huang, Shanshan and Ma, Jun and Cheng, Peizhe and Wang, Shuaiqiang},
title = {A Hybrid Multigroup Coclustering Recommendation Framework Based on Information Fusion},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700465},
doi = {10.1145/2700465},
abstract = {Collaborative Filtering (CF) is one of the most successful algorithms in recommender systems. However, it suffers from data sparsity and scalability problems. Although many clustering techniques have been incorporated to alleviate these two problems, most of them fail to achieve further significant improvement in recommendation accuracy. First of all, most of them assume each user or item belongs to a single cluster. Since usually users can hold multiple interests and items may belong to multiple categories, it is more reasonable to assume that users and items can join multiple clusters (groups), where each cluster is a subset of like-minded users and items they prefer. Furthermore, most of the clustering-based CF models only utilize historical rating information in the clustering procedure but ignore other data resources in recommender systems such as the social connections of users and the correlations between items. In this article, we propose HMCoC, a Hybrid Multigroup CoClustering recommendation framework, which can cluster users and items into multiple groups simultaneously with different information resources. In our framework, we first integrate information of user--item rating records, user social networks, and item features extracted from the DBpedia knowledge base. We then use an optimization method to mine meaningful user--item groups with all the information. Finally, we apply the conventional CF method in each cluster to make predictions. By merging the predictions from each cluster, we generate the top-n recommendations to the target users for return. Extensive experimental results demonstrate the superior performance of our approach in top-n recommendation in terms of MAP, NDCG, and F1 compared with other clustering-based CF models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {27},
numpages = {22},
keywords = {Recommender systems, data sparsity, information fusion, coclustering, collaborative filtering}
}

@article{10.1145/2700464,
author = {Fire, Michael and Elovici, Yuval},
title = {Data Mining of Online Genealogy Datasets for Revealing Lifespan Patterns in Human Population},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700464},
doi = {10.1145/2700464},
abstract = {Online genealogy datasets contain extensive information about millions of people and their past and present family connections. This vast amount of data can help identify various patterns in the human population. In this study, we present methods and algorithms that can assist in identifying variations in lifespan distributions of the human population in the past centuries, in detecting social and genetic features that correlate with the human lifespan, and in constructing predictive models of human lifespan based on various features that can easily be extracted from genealogy datasets.We have evaluated the presented methods and algorithms on a large online genealogy dataset with over a million profiles and over 9 million connections, all of which were collected from the WikiTree website. Our findings indicate that significant but small positive correlations exist between the parents’ lifespan and their children’s lifespan. Additionally, we found slightly higher and significant correlations between the lifespans of spouses. We also discovered a very small positive and significant correlation between longevity and reproductive success in males, and a small and significant negative correlation between longevity and reproductive success in females. Moreover, our predictive models presented results with a Mean Absolute Error as low as 13.18 in predicting the lifespans of individuals who outlived the age of 10, and our classification models presented better than random classification results in predicting which people who outlive the age of 50 will also outlive the age of 80.We believe that this study will be the first of many studies to utilize the wealth of data on human populations, existing in online genealogy datasets, to better understand factors that influence the human lifespan. Understanding these factors can assist scientists in providing solutions for successful aging.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {28},
numpages = {22},
keywords = {data mining, machine learning, lifespan prediction, human population lifespan, WikiTree, Genealogy data mining}
}

@article{10.1145/2663359,
author = {Hai, Zhen and Chang, Kuiyu and Cong, Gao and Yang, Christopher C.},
title = {An Association-Based Unified Framework for Mining Features and Opinion Words},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2663359},
doi = {10.1145/2663359},
abstract = {Mining features and opinion words is essential for fine-grained opinion analysis of customer reviews. It is observed that semantic dependencies naturally exist between features and opinion words, even among features or opinion words themselves. In this article, we employ a corpus statistics association measure to quantify the pairwise word dependencies and propose a generalized association-based unified framework to identify features, including explicit and implicit features, and opinion words from reviews. We first extract explicit features and opinion words via an association-based bootstrapping method (ABOOT). ABOOT starts with a small list of annotated feature seeds and then iteratively recognizes a large number of domain-specific features and opinion words by discovering the corpus statistics association between each pair of words on a given review domain. Two instances of this ABOOT method are evaluated based on two particular association models, likelihood ratio tests (LRTs) and latent semantic analysis (LSA). Next, we introduce a natural extension to identify implicit features by employing the recognized known semantic correlations between features and opinion words. Experimental results illustrate the benefits of the proposed association-based methods for identifying features and opinion words versus benchmark methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {26},
numpages = {21},
keywords = {Opinion mining, opinion word, feature, implicit feature, association}
}

@article{10.1145/2663358,
author = {Doherty, Jonathan and Curran, Kevin and McKevitt, Paul},
title = {Pattern Matching Techniques for Replacing Missing Sections of Audio Streamed across Wireless Networks},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2663358},
doi = {10.1145/2663358},
abstract = {Streaming media on the Internet can be unreliable. Services such as audio-on-demand drastically increase the loads on networks; therefore, new, robust, and highly efficient coding algorithms are necessary. One method overlooked to date, which can work alongside existing audio compression schemes, is that which takes into account the semantics and natural repetition of music. Similarity detection within polyphonic audio has presented problematic challenges within the field of music information retrieval. One approach to deal with bursty errors is to use self-similarity to replace missing segments. Many existing systems exist based on packet loss and replacement on a network level, but none attempt repairs of large dropouts of 5 seconds or more. Music exhibits standard structures that can be used as a forward error correction (FEC) mechanism. FEC is an area that addresses the issue of packet loss with the onus of repair placed as much as possible on the listener's device. We have developed a server--client-based framework (SoFI) for automatic detection and replacement of large packet losses on wireless networks when receiving time-dependent streamed audio. Whenever dropouts occur, SoFI swaps audio presented to the listener between a live stream and previous sections of the audio stored locally. Objective and subjective evaluations of SoFI where subjects were presented with other simulated approaches to audio repair together with simulations of replacements including varying lengths of time in the repair give positive results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {25},
numpages = {38},
keywords = {data compaction and compression, audio repair, Streaming media, forward error correction}
}

@article{10.1145/2641576,
author = {Shi, Miaojing and Sun, Xinghai and Tao, Dacheng and Xu, Chao and Baciu, George and Liu, Hong},
title = {Exploring Spatial Correlation for Visual Object Retrieval},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2641576},
doi = {10.1145/2641576},
abstract = {Bag-of-visual-words (BOVW)-based image representation has received intense attention in recent years and has improved content-based image retrieval (CBIR) significantly. BOVW does not consider the spatial correlation between visual words in natural images and thus biases the generated visual words toward noise when the corresponding visual features are not stable. This article outlines the construction of a visual word co-occurrence matrix by exploring visual word co-occurrence extracted from small affine-invariant regions in a large collection of natural images. Based on this co-occurrence matrix, we first present a novel high-order predictor to accelerate the generation of spatially correlated visual words and a penalty tree (PTree) to continue generating the words after the prediction. Subsequently, we propose two methods of co-occurrence weighting similarity measure for image ranking: Co-Cosine and Co-TFIDF. These two new schemes down-weight the contributions of the words that are less discriminative because of frequent co-occurrences with other words. We conduct experiments on Oxford and Paris Building datasets, in which the ImageNet dataset is used to implement a large-scale evaluation. Cross-dataset evaluations between the Oxford and Paris datasets and Oxford and Holidays datasets are also provided. Thorough experimental results suggest that our method outperforms the state of the art without adding much additional cost to the BOVW model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {24},
numpages = {21},
keywords = {Co-Cosine, spatial correlation, BOVW, penalty tree, high-order predictor, Co-TFIDF}
}

@article{10.1145/2629701,
author = {Zhang, Quanshi and Song, Xuan and Shao, Xiaowei and Zhao, Huijing and Shibasaki, Ryosuke},
title = {From RGB-D Images to RGB Images: Single Labeling for Mining Visual Models},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629701},
doi = {10.1145/2629701},
abstract = {Mining object-level knowledge, that is, building a comprehensive category model base, from a large set of cluttered scenes presents a considerable challenge to the field of artificial intelligence. How to initiate model learning with the least human supervision (i.e., manual labeling) and how to encode the structural knowledge are two elements of this challenge, as they largely determine the scalability and applicability of any solution. In this article, we propose a model-learning method that starts from a single-labeled object for each category, and mines further model knowledge from a number of informally captured, cluttered scenes. However, in these scenes, target objects are relatively small and have large variations in texture, scale, and rotation. Thus, to reduce the model bias normally associated with less supervised learning methods, we use the robust 3D shape in RGB-D images to guide our model learning, then apply the properly trained category models to both object detection and recognition in more conventional RGB images. In addition to model training for their own categories, the knowledge extracted from the RGB-D images can also be transferred to guide model learning for a new category, in which only RGB images without depth information in the new category are provided for training. Preliminary testing shows that the proposed method performs as well as fully supervised learning methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {16},
numpages = {29},
keywords = {computer vision, visual knowledge base, Data mining, RGB-D sensor, visual mining, big visual data, transfer learning}
}

@article{10.1145/2629673,
author = {Figueroa, Nadia and Dong, Haiwei and Saddik, Abdulmotaleb El},
title = {A Combined Approach Toward Consistent Reconstructions of Indoor Spaces Based on 6D RGB-D Odometry and KinectFusion},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629673},
doi = {10.1145/2629673},
abstract = {We propose a 6D RGB-D odometry approach that finds the relative camera pose between consecutive RGB-D frames by keypoint extraction and feature matching both on the RGB and depth image planes. Furthermore, we feed the estimated pose to the highly accurate KinectFusion algorithm, which uses a fast ICP (Iterative Closest Point) to fine-tune the frame-to-frame relative pose and fuse the depth data into a global implicit surface. We evaluate our method on a publicly available RGB-D SLAM benchmark dataset by Sturm et al. The experimental results show that our proposed reconstruction method solely based on visual odometry and KinectFusion outperforms the state-of-the-art RGB-D SLAM system accuracy. Moreover, our algorithm outputs a ready-to-use polygon mesh (highly suitable for creating 3D virtual worlds) without any postprocessing steps.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {14},
numpages = {10},
keywords = {evaluation, kinect, Indoor mapping, benchmark datasets}
}

@article{10.1145/2629483,
author = {Zhu, Yu and Chen, Wenbin and Guo, Guodong},
title = {Fusing Multiple Features for Depth-Based Action Recognition},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629483},
doi = {10.1145/2629483},
abstract = {Human action recognition is a very active research topic in computer vision and pattern recognition. Recently, it has shown a great potential for human action recognition using the three-dimensional (3D) depth data captured by the emerging RGB-D sensors. Several features and/or algorithms have been proposed for depth-based action recognition. A question is raised: Can we find some complementary features and combine them to improve the accuracy significantly for depth-based action recognition? To address the question and have a better understanding of the problem, we study the fusion of different features for depth-based action recognition. Although data fusion has shown great success in other areas, it has not been well studied yet on 3D action recognition. Some issues need to be addressed, for example, whether the fusion is helpful or not for depth-based action recognition, and how to do the fusion properly. In this article, we study different fusion schemes comprehensively, using diverse features for action characterization in depth videos. Two different levels of fusion schemes are investigated, that is, feature level and decision level. Various methods are explored at each fusion level. Four different features are considered to characterize the depth action patterns from different aspects. The experiments are conducted on four challenging depth action databases, in order to evaluate and find the best fusion methods generally. Our experimental results show that the four different features investigated in the article can complement each other, and appropriate fusion methods can improve the recognition accuracies significantly over each individual feature. More importantly, our fusion-based action recognition outperforms the state-of-the-art approaches on these challenging databases.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {18},
numpages = {20},
keywords = {action recognition, data fusion, depth maps, skeleton, decision level, spatiotemporal features, feature level, feature selection, 4D descriptor, RGB-D sensor}
}

@article{10.1145/2629482,
author = {Zhang, Liyan and Liu, Fan and Tang, Jinhui},
title = {Real-Time System for Driver Fatigue Detection by RGB-D Camera},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629482},
doi = {10.1145/2629482},
abstract = {Drowsy driving is one of the major causes of fatal traffic accidents. In this article, we propose a real-time system that utilizes RGB-D cameras to automatically detect driver fatigue and generate alerts to drivers. By introducing RGB-D cameras, the depth data can be obtained, which provides extra evidence to benefit the task of head detection and head pose estimation. In this system, two important visual cues (head pose and eye state) for driver fatigue detection are extracted and leveraged simultaneously. We first present a real-time 3D head pose estimation method by leveraging RGB and depth data. Then we introduce a novel method to predict eye states employing the WLBP feature, which is a powerful local image descriptor that is robust to noise and illumination variations. Finally, we integrate the results from both head pose and eye states to generate the overall conclusion. The combination and collaboration of the two types of visual cues can reduce the uncertainties and resolve the ambiguity that a single cue may induce. The experiments were performed using an inside-car environment during the day and night, and theyfully demonstrate the effectiveness and robustness of our system as well as the proposed methods of predicting head pose and eye states.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {22},
numpages = {17},
keywords = {head pose, RGB-D cameras, driver fatigue detection system, eye state}
}

@article{10.1145/2629481,
author = {Sun, Chao and Zhang, Tianzhu and Xu, Changsheng},
title = {Latent Support Vector Machine Modeling for Sign Language Recognition with Kinect},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629481},
doi = {10.1145/2629481},
abstract = {Vision-based sign language recognition has attracted more and more interest from researchers in the computer vision field. In this article, we propose a novel algorithm to model and recognize sign language performed in front of a Microsoft Kinect sensor. Under the assumption that some frames are expected to be both discriminative and representative in a sign language video, we first assign a binary latent variable to each frame in training videos for indicating its discriminative capability, then develop a latent support vector machine model to classify the signs, as well as localize the discriminative and representative frames in each video. In addition, we utilize the depth map together with the color image captured by the Kinect sensor to obtain a more effective and accurate feature to enhance the recognition accuracy. To evaluate our approach, we conducted experiments on both word-level sign language and sentence-level sign language. An American Sign Language dataset including approximately 2,000 word-level sign language phrases and 2,000 sentence-level sign language phrases was collected using the Kinect sensor, and each phrase contains color, depth, and skeleton information. Experiments on our dataset demonstrate the effectiveness of the proposed method for sign language recognition.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {20},
numpages = {20},
keywords = {Sign language recognition, Kinect sensor, latent SVM}
}

@article{10.1145/2629480,
author = {Huang, Meiyu and Chen, Yiqiang and Ji, Wen and Miao, Chunyan},
title = {Accurate and Robust Moving-Object Segmentation for Telepresence Systems},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629480},
doi = {10.1145/2629480},
abstract = {Moving-object segmentation is the key issue of Telepresence systems. With monocular camera--based segmentation methods, desirable segmentation results are hard to obtain in challenging scenes with ambiguous color, illumination changes, and shadows. Approaches based on depth sensors often cause holes inside the object and missegmentations on the object boundary due to inaccurate and unstable estimation of depth data. This work proposes an adaptive multi-cue decision fusion method based on Kinect (which integrates a depth sensor with an RGB camera). First, the algorithm obtains an initial foreground mask based on the depth cue. Second, the algorithm introduces a postprocessing framework to refine the segmentation results, which consists of two main steps: (1) automatically adjusting the weight of two weak decisions to identify foreground holes based on the color and contrast cue separately; and (2) refining the object boundary by integrating the motion probability weighted temporal prior, color likelihood, and smoothness constraint. The extensive experiments we conducted demonstrate that our method can segment moving objects accurately and robustly in various situations in real time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {17},
numpages = {28},
keywords = {Foreground segmentation, object boundary refining, foreground hole detection, adaptive multi-cue decision fusion}
}

@article{10.1145/2629465,
author = {Spurlock, Scott and Souvenir, Richard},
title = {An Evaluation of Gamesourced Data for Human Pose Estimation},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629465},
doi = {10.1145/2629465},
abstract = {Gamesourcing has emerged as an approach for rapidly acquiring labeled data for learning-based, computer vision recognition algorithms. In this article, we present an approach for using RGB-D sensors to acquire annotated training data for human pose estimation from 2D images. Unlike other gamesourcing approaches, our method does not require a specific game, but runs alongside any gesture-based game using RGB-D sensors. The automatically generated datasets resulting from this approach contain joint estimates within a few pixel units of manually labeled data, and a gamesourced dataset created using a relatively small number of players, games, and locations performs as well as large-scale, manually annotated datasets when used as training data with recent learning-based human pose estimation methods for 2D images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {19},
numpages = {16},
keywords = {Kinect, Crowdsourcing, datasets, data collection, automatic annotation, dataset generation, depth images}
}

@article{10.1145/2630075,
author = {Sepehri-Rad, Hoda and Barbosa, Denilson},
title = {Identifying Controversial Wikipedia Articles Using Editor Collaboration Networks},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2630075},
doi = {10.1145/2630075},
abstract = {Wikipedia is probably the most commonly used knowledge reference nowadays, and the high quality of its articles is widely acknowledged. Nevertheless, disagreement among editors often causes some articles to become controversial over time. These articles span thousands of popular topics, including religion, history, and politics, to name a few, and are manually tagged as controversial by the editors, which is clearly suboptimal. Moreover, disagreement, bias, and conflict are expressed quite differently in Wikipedia compared to other social media, rendering previous approaches ineffective. On the other hand, the social process of editing Wikipedia is partially captured in the edit history of the articles, opening the door for novel approaches. This article describes a novel controversy model that builds on the interaction history of the editors and not only predicts controversy but also sheds light on the process that leads to controversy. The model considers the collaboration history of pairs of editors to predict their attitude toward one another. This is done in a supervised way, where the votes of Wikipedia administrator elections are used as labels indicating agreement (i.e., support vote) or disagreement (i.e., oppose vote). From each article, a collaboration network is built, capturing the pairwise attitude among editors, allowing the accurate detection of controversy. Extensive experimental results establish the superiority of this approach compared to previous work and very competitive baselines on a wide range of settings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {5},
numpages = {24},
keywords = {Wikipedia, collaboration, classification, controversial articles}
}

@article{10.1145/2597181,
author = {Ji, Rongrong and Gao, Yue and Liu, Wei and Xie, Xing and Tian, Qi and Li, Xuelong},
title = {When Location Meets Social Multimedia: A Survey on Vision-Based Recognition and Mining for Geo-Social Multimedia Analytics},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2597181},
doi = {10.1145/2597181},
abstract = {Coming with the popularity of multimedia sharing platforms such as Facebook and Flickr, recent years have witnessed an explosive growth of geographical tags on social multimedia content. This trend enables a wide variety of emerging applications, for example, mobile location search, landmark recognition, scene reconstruction, and touristic recommendation, which range from purely research prototype to commercial systems. In this article, we give a comprehensive survey on these applications, covering recent advances in recognition and mining of geographical-aware social multimedia. We review related work in the past decade regarding to location recognition, scene summarization, tourism suggestion, 3D building modeling, mobile visual search and city navigation. At the end, we further discuss potential challenges, future topics, as well as open issues related to geo-social multimedia computing, recognition, mining, and analytics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {1},
numpages = {18},
keywords = {image analysis, knowledge representation, Algorithms, multimedia systems, Internet}
}

@article{10.1145/2668133,
author = {Chin, Wei-Sheng and Zhuang, Yong and Juan, Yu-Chin and Lin, Chih-Jen},
title = {A Fast Parallel Stochastic Gradient Method for Matrix Factorization in Shared Memory Systems},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668133},
doi = {10.1145/2668133},
abstract = {Matrix factorization is known to be an effective method for recommender systems that are given only the ratings from users to items. Currently, stochastic gradient (SG) method is one of the most popular algorithms for matrix factorization. However, as a sequential approach, SG is difficult to be parallelized for handling web-scale problems. In this article, we develop a fast parallel SG method, FPSG, for shared memory systems. By dramatically reducing the cache-miss rate and carefully addressing the load balance of threads, FPSG is more efficient than state-of-the-art parallel algorithms for matrix factorization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {2},
numpages = {24},
keywords = {matrix factorization, shared memory algorithm, stochastic gradient descent, parallel computing, Recommender system}
}

@article{10.1145/2631926,
author = {Patel, Dhaval},
title = {On Discovery of Spatiotemporal Influence-Based Moving Clusters},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2631926},
doi = {10.1145/2631926},
abstract = {A moving object cluster is a set of objects that move close to each other for a long time interval. Existing works have utilized object trajectories to discover moving object clusters efficiently. In this article, we define a spatiotemporal influence-based moving cluster that captures spatiotemporal influence spread over a set of spatial objects. A spatiotemporal influence-based moving cluster is a sequence of spatial clusters, where each cluster is a set of nearby objects, such that each object in a cluster influences at least one object in the next immediate cluster and is also influenced by an object from the immediate preceding cluster. Real-life examples of spatiotemporal influence-based moving clusters include diffusion of infectious diseases and spread of innovative ideas. We study the discovery of spatiotemporal influence-based moving clusters in a database of spatiotemporal events. While the search space for discovering all spatiotemporal influence-based moving clusters is prohibitively huge, we design a method, STIMer, to efficiently retrieve the maximal answer. The algorithm STIMer adopts a top-down recursive refinement method to generate the maximal spatiotemporal influence-based moving clusters directly. Empirical studies on the real data as well as large synthetic data demonstrate the effectiveness and efficiency of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {4},
numpages = {23},
keywords = {spatiotemporal influence-based moving clusters, Spatiotemporal events}
}

@article{10.1145/2631925,
author = {Stapleton, Gem and Plimmer, Beryl and Delaney, Aidan and Rodgers, Peter},
title = {Combining Sketching and Traditional Diagram Editing Tools},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2631925},
doi = {10.1145/2631925},
abstract = {The least cognitively demanding way to create a diagram is to draw it with a pen. Yet there is also a need for more formal visualizations, that is, diagrams created using both traditional keyboard and mouse interaction. Our objective is to allow the creation of diagrams using traditional and stylus-based input. Having two diagram creation interfaces requires that changes to a diagram should be automatically rendered in the other visualization. Because sketches are imprecise, there is always the possibility that conversion between visualizations results in a lack of syntactic consistency between the two visualizations. We propose methods for converting diagrams between forms, checking them for equivalence, and rectifying inconsistencies. As a result of our theoretical contributions, we present an intelligent software system allowing users to create and edit diagrams in sketch or formal mode. Our proof-of-concept tool supports diagrams with connected and spatial syntactic elements. Two user studies show that this approach is viable and participants found the software easy to use. We conclude that supporting such diagram creation is now possible in practice.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {10},
numpages = {29},
keywords = {tools for visual languages, Sketching interfaces, diagram editing}
}

@article{10.1145/2629674,
author = {Ghosh, Siddhartha and Reece, Steve and Rogers, Alex and Roberts, Stephen and Malibari, Areej and Jennings, Nicholas R.},
title = {Modeling the Thermal Dynamics of Buildings: A Latent-Force- Model-Based Approach},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629674},
doi = {10.1145/2629674},
abstract = {Minimizing the energy consumed by heating, ventilation, and air conditioning (HVAC) systems of residential buildings without impacting occupants’ comfort has been highlighted as an important artificial intelligence (AI) challenge. Typically, approaches that seek to address this challenge use a model that captures the thermal dynamics within a building, also referred to as a thermal model. Among thermal models, gray-box models are a popular choice for modeling the thermal dynamics of buildings. They combine knowledge of the physical structure of a building with various data-driven inputs and are accurate estimators of the state (internal temperature). However, existing gray-box models require a detailed specification of all the physical elements that can affect the thermal dynamics of a building a priori. This limits their applicability, particularly in residential buildings, where additional dynamics can be induced by human activities such as cooking, which contributes additional heat, or opening of windows, which leads to additional leakage of heat. Since the incidence of these additional dynamics is rarely known, their combined effects cannot readily be accommodated within existing models.To overcome this limitation and improve the general applicability of gray-box models, we introduce a novel model, which we refer to as a latent force thermal model of the thermal dynamics of a building, or LFM-TM. Our model is derived from an existing gray-box thermal model, which is augmented with an extra term referred to as the learned residual. This term is capable of modeling the effect of any a priori unknown additional dynamic, which, if not captured, appears as a structure in a thermal model’s residual (the error induced by the model). More importantly, the learned residual can also capture the effects of physical elements such as a building’s envelope or the lags in a heating system, leading to a significant reduction in complexity compared to existing models.To evaluate the performance of LFM-TM, we apply it to two independent data sources. The first is an established dataset, referred to as the FlexHouse data, which was previously used for evaluating the efficacy of existing gray-box models [Bacher and Madsen 2011]. The second dataset consists of heating data logged within homes located on the University of Southampton campus, which were specifically instrumented to collect data for our thermal modeling experiments. On both datasets, we show that LFM-TM outperforms existing models in its ability to accurately fit the observed data, generate accurate day-ahead internal temperature predictions, and explain a large amount of the variability in the future observations. This, along with the fact that we also use a corresponding efficient sequential inference scheme for LFM-TM, makes it an ideal candidate for model-based predictive control, where having accurate online predictions of internal temperatures is essential for high-quality solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {7},
numpages = {27},
keywords = {HVAC, residential buildings, Artificial intelligence, latent force models, sustainability, thermal models, energy savings, smart homes}
}

@article{10.1145/2629528,
author = {Feuz, Kyle D. and Cook, Diane J.},
title = {Transfer Learning across Feature-Rich Heterogeneous Feature Spaces via Feature-Space Remapping (FSR)},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629528},
doi = {10.1145/2629528},
abstract = {Transfer learning aims to improve performance on a target task by utilizing previous knowledge learned from source tasks. In this paper we introduce a novel heterogeneous transfer learning technique, Feature-Space Remapping (FSR), which transfers knowledge between domains with different feature spaces. This is accomplished without requiring typical feature-feature, feature instance, or instance-instance co-occurrence data. Instead we relate features in different feature-spaces through the construction of metafeatures. We show how these techniques can utilize multiple source datasets to construct an ensemble learner which further improves performance. We apply FSR to an activity recognition problem and a document classification problem. The ensemble technique is able to outperform all other baselines and even performs better than a classifier trained using a large amount of labeled data in the target domain. These problems are especially difficult because, in addition to having different feature-spaces, the marginal probability distributions and the class labels are also different. This work extends the state of the art in transfer learning by considering large transfer across dramatically different spaces.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {3},
numpages = {27},
keywords = {domain adaption, Heterogeneous transfer learning, text classification, activity recognition}
}

@article{10.1145/2601408,
author = {Zhang, Zhao and Liu, Cheng-Lin and Zhao, Ming-Bo},
title = {A Sparse Projection and Low-Rank Recovery Framework for Handwriting Representation and Salient Stroke Feature Extraction},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2601408},
doi = {10.1145/2601408},
abstract = {In this article, we consider the problem of simultaneous low-rank recovery and sparse projection. More specifically, a new Robust Principal Component Analysis (RPCA)-based framework called Sparse Projection and Low-Rank Recovery (SPLRR) is proposed for handwriting representation and salient stroke feature extraction. In addition to achieving a low-rank component encoding principal features and identify errors or missing values from a given data matrix as RPCA, SPLRR also learns a similarity-preserving sparse projection for extracting salient stroke features and embedding new inputs for classification. These properties make SPLRR applicable for handwriting recognition and stroke correction and enable online computation. A cosine-similarity-style regularization term is incorporated into the SPLRR formulation for encoding the similarities of local handwriting features. The sparse projection and low-rank recovery are calculated from a convex minimization problem that can be efficiently solved in polynomial time. Besides, the supervised extension of SPLRR is also elaborated. The effectiveness of our SPLRR is examined by extensive handwritten digital repairing, stroke correction, and recognition based on benchmark problems. Compared with other related techniques, SPLRR delivers strong generalization capability and state-of-the-art performance for handwriting representation and recognition.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {9},
numpages = {26},
keywords = {Sparse projection, similarity preservation, salient stroke feature extraction, handwriting representation and recognition, low-rank recovery}
}

@article{10.1145/2505349,
author = {Changuel, Sahar and Labroche, Nicolas and Bouchon-Meunier, Bernadette},
title = {Resources Sequencing Using Automatic Prerequisite--Outcome Annotation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2505349},
doi = {10.1145/2505349},
abstract = {The objective of any tutoring system is to provide resources to learners that are adapted to their current state of knowledge. With the availability of a large variety of online content and the disjunctive nature of results provided by traditional search engines, it becomes crucial to provide learners with adapted learning paths that propose a sequence of resources that match their learning objectives. In an ideal case, the sequence of documents provided to the learner should be such that each new document relies on concepts that have been already defined in previous documents. Thus, the problem of determining an effective learning path from a corpus of web documents depends on the accurate identification of outcome and prerequisite concepts in these documents and on their ordering according to this information. Until now, only a few works have been proposed to distinguish between prerequisite and outcome concepts, and to the best of our knowledge, no method has been introduced so far to benefit from this information to produce a meaningful learning path. To this aim, this article first describes a concept annotation method that relies on machine-learning techniques to predict the class of each concept—prerequisite or outcome—on the basis of contextual and local features. Then, this categorization is exploited to produce an automatic resource sequencing on the basis of different representations and scoring functions that transcribe the precedence relation between learning resources. Experiments conducted on a real dataset built from online resources show that our concept annotation approach outperforms the baseline method and that the learning paths automatically generated are consistent with the ground truth provided by the author of the online content.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {6},
numpages = {30},
keywords = {Adapted learning path, classification, resource sequencing, scoring function, tutoring system}
}

@article{10.1145/2668123,
author = {Shen, Dou and Agarwal, Deepak},
title = {Introduction to the Special Issue on Online Advertising},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668123},
doi = {10.1145/2668123},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {57},
numpages = {2}
}

@article{10.1145/2668110,
author = {Zhuang, Jinfeng and Mei, Tao and Hoi, Steven C. H. and Hua, Xian-Sheng and Zhang, Yongdong},
title = {Community Discovery from Social Media by Low-Rank Matrix Recovery},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668110},
doi = {10.1145/2668110},
abstract = {The pervasive usage and reach of social media have attracted a surge of attention in the multimedia research community. Community discovery from social media has therefore become an important yet challenging issue. However, due to the subjective generating process, the explicitly observed communities (e.g., group-user and user-user relationship) are often noisy and incomplete in nature. This paper presents a novel approach to discovering communities from social media, including the group membership and user friend structure, by exploring a low-rank matrix recovery technique. In particular, we take Flickr as one exemplary social media platform. We first model the observed indicator matrix of the Flickr community as a summation of a low-rank true matrix and a sparse error matrix. We then formulate an optimization problem by regularizing the true matrix to coincide with the available rich context and content (i.e., photos and their associated tags). An iterative algorithm is developed to recover the true community indicator matrix. The proposed approach leads to a variety of social applications, including community visualization, interest group refinement, friend suggestion, and influential user identification. The evaluations on a large-scale testbed, consisting of 4,919 Flickr users, 1,467 interest groups, and over five million photos, show that our approach opens a new yet effective perspective to solve social network problems with sparse learning technique. Despite being focused on Flickr, our technique can be applied in any other social media community.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {67},
numpages = {19},
keywords = {low-rank matrix, community discovery, Social networks, social media, context information}
}

@article{10.1145/2668108,
author = {Qin, Tao and Chen, Wei and Liu, Tie-Yan},
title = {Sponsored Search Auctions: Recent Advances and Future Directions},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668108},
doi = {10.1145/2668108},
abstract = {Sponsored search has been proven to be a successful business model, and sponsored search auctions have become a hot research direction. There have been many exciting advances in this field, especially in recent years, while at the same time, there are also many open problems waiting for us to resolve. In this article, we provide a comprehensive review of sponsored search auctions in hopes of helping both industry practitioners and academic researchers to become familiar with this field, to know the state of the art, and to identify future research topics. Specifically, we organize the article into two parts. In the first part, we review research works on sponsored search auctions with basic settings, where fully rational advertisers without budget constraints, preknown click-through rates (CTRs) without interdependence, and exact match between queries and keywords are assumed. Under these assumptions, we first introduce the generalized second price (GSP) auction, which is the most popularly used auction mechanism in the industry. Then we give the definitions of several well-studied equilibria and review the latest results on GSP’s efficiency and revenue in these equilibria. In the second part, we introduce some advanced topics on sponsored search auctions. In these advanced topics, one or more assumptions made in the basic settings are relaxed. For example, the CTR of an ad could be unknown and dependent on other ads; keywords could be broadly matched to queries before auctions are executed; and advertisers are not necessarily fully rational, could have budget constraints, and may prefer rich bidding languages. Given that the research on these advanced topics is still immature, in each section of the second part, we provide our opinions on how to make further advances, in addition to describing what has been done by researchers in the corresponding direction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {60},
numpages = {34},
keywords = {efficiency and revenue, mechanism design, Sponsored search auctions, equilibrium concepts, generalized second price auction}
}

@article{10.1145/2629350,
author = {Ribeiro, Marco Tulio and Ziviani, Nivio and Moura, Edleno Silva De and Hata, Itamar and Lacerda, Anisio and Veloso, Adriano},
title = {Multiobjective Pareto-Efficient Approaches for Recommender Systems},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629350},
doi = {10.1145/2629350},
abstract = {Recommender systems are quickly becoming ubiquitous in applications such as e-commerce, social media channels, and content providers, among others, acting as an enabling mechanism designed to overcome the information overload problem by improving browsing and consumption experience. A typical task in many recommender systems is to output a ranked list of items, so that items placed higher in the rank are more likely to be interesting to the users. Interestingness measures include how accurate, novel, and diverse are the suggested items, and the objective is usually to produce ranked lists optimizing one of these measures. Suggesting items that are simultaneously accurate, novel, and diverse is much more challenging, since this may lead to a conflicting-objective problem, in which the attempt to improve a measure further may result in worsening other measures. In this article, we propose new approaches for multiobjective recommender systems based on the concept of Pareto efficiency—a state achieved when the system is devised in the most efficient manner in the sense that there is no way to improve one of the objectives without making any other objective worse off. Given that existing multiobjective recommendation algorithms differ in their level of accuracy, diversity, and novelty, we exploit the Pareto-efficiency concept in two distinct manners: (i) the aggregation of ranked lists produced by existing algorithms into a single one, which we call Pareto-efficient ranking, and (ii) the weighted combination of existing algorithms resulting in a hybrid one, which we call Pareto-efficient hybridization. Our evaluation involves two real application scenarios: music recommendation with implicit feedback (i.e., Last.fm) and movie recommendation with explicit feedback (i.e., MovieLens). We show that the proposed Pareto-efficient approaches are effective in suggesting items that are likely to be simultaneously accurate, diverse, and novel. We discuss scenarios where the system achieves high levels of diversity and novelty without compromising its accuracy. Further, comparison against multiobjective baselines reveals improvements in terms of accuracy (from 10.4% to 10.9%), novelty (from 5.7% to 7.5%), and diversity (from 1.6% to 4.2%).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {53},
numpages = {20},
keywords = {Pareto efficiency, Multiobjective recommender systems}
}

@article{10.1145/2560365,
author = {Liu, Qingzhong and Chen, Zhongxue},
title = {Improved Approaches with Calibrated Neighboring Joint Density to Steganalysis and Seam-Carved Forgery Detection in JPEG Images},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2560365},
doi = {10.1145/2560365},
abstract = {Steganalysis and forgery detection in image forensics are generally investigated separately. We have designed a method targeting the detection of both steganography and seam-carved forgery in JPEG images. We analyze the neighboring joint density of the DCT coefficients and reveal the difference between the untouched image and the modified version. In realistic detection, the untouched image and the modified version may not be obtained at the same time, and different JPEG images may have different neighboring joint density features. By exploring the self-calibration under different shift recompressions, we propose calibrated neighboring joint density-based approaches with a simple feature set to distinguish steganograms and tampered images from untouched ones. Our study shows that this approach has multiple promising applications in image forensics. Compared to the state-of-the-art steganalysis detectors, our approach delivers better or comparable detection performances with a much smaller feature set while detecting several JPEG-based steganographic systems including DCT-embedding-based adaptive steganography and Yet Another Steganographic Scheme (YASS). Our approach is also effective in detecting seam-carved forgery in JPEG images. By integrating calibrated neighboring density with spatial domain rich models that were originally designed for steganalysis, the hybrid approach obtains the best detection accuracy to discriminate seam-carved forgery from an untouched image. Our study also offers a promising manner to explore steganalysis and forgery detection together.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {63},
numpages = {30},
keywords = {steganalysis, seam carving, YASS, calibration, Steganography, image tampering, neighboring joint density, JPEG}
}

@article{10.1145/2558397,
author = {Azaria, Amos and Rabinovich, Zinovi and Goldman, Claudia V. and Kraus, Sarit},
title = {Strategic Information Disclosure to People with Multiple Alternatives},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2558397},
doi = {10.1145/2558397},
abstract = {In this article, we study automated agents that are designed to encourage humans to take some actions over others by strategically disclosing key pieces of information. To this end, we utilize the framework of persuasion games—a branch of game theory that deals with asymmetric interactions where one player (Sender) possesses more information about the world, but it is only the other player (Receiver) who can take an action. In particular, we use an extended persuasion model, where the Sender’s information is imperfect and the Receiver has more than two alternative actions available. We design a computational algorithm that, from the Sender’s standpoint, calculates the optimal information disclosure rule. The algorithm is parameterized by the Receiver’s decision model (i.e., what choice he will make based on the information disclosed by the Sender) and can be retuned accordingly.We then provide an extensive experimental study of the algorithm’s performance in interactions with human Receivers. First, we consider a fully rational (in the Bayesian sense) Receiver decision model and experimentally show the efficacy of the resulting Sender’s solution in a routing domain. Despite the discrepancy in the Sender’s and the Receiver’s utilities from each of the Receiver’s choices, our Sender agent successfully persuaded human Receivers to select an option more beneficial for the agent. Dropping the Receiver’s rationality assumption, we introduce a machine learning procedure that generates a more realistic human Receiver model. We then show its significant benefit to the Sender solution by repeating our routing experiment. To complete our study, we introduce a second (supply--demand) experimental domain and, by contrasting it with the routing domain, obtain general guidelines for a Sender on how to construct a Receiver model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {64},
numpages = {21},
keywords = {persuasion, information disclosure, human modeling, Human-agent interaction}
}

@article{10.1145/2558327,
author = {Reches, Shulamit and Kalech, Meir},
title = {Choosing a Candidate Using Efficient Allocation of Biased Information},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2558327},
doi = {10.1145/2558327},
abstract = {This article deals with a decision-making problem concerning an agent who wants to choose a partner from multiple candidates for long-term collaboration. To choose the best partner, the agent can rely on prior information he knows about the candidates. However, to improve his decision, he can request additional information from information sources. Nonetheless, acquiring information from external information sources about candidates may be biased due to different personalities of the agent searching for a partner and the information source. In addition, information may be costly. Considering the bias and the cost of the information sources, the optimization problem addressed in this article is threefold: (1) determining the necessary amount of additional information, (2) selecting information sources from which to request the information, and (3) choosing the candidates on whom to request the additional information. We propose a heuristic to solve this optimization problem. The results of experiments on simulated and real-world domains demonstrate the efficiency of our algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {66},
numpages = {30},
keywords = {Decision theory, multiagent systems}
}

@article{10.1145/2534398,
author = {Ashkan, Azin and Clarke, Charles L. A.},
title = {Location- and Query-Aware Modeling of Browsing and Click Behavior in Sponsored Search},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2534398},
doi = {10.1145/2534398},
abstract = {An online advertisement’s clickthrough rate provides a fundamental measure of its quality, which is widely used in ad selection strategies. Unfortunately, ads placed in contexts where they are rarely viewed—or where users are unlikely to be interested in commercial results—may receive few clicks regardless of their quality. In this article, we model the variability of a user’s browsing behavior for the purpose of click analysis and prediction in sponsored search. Our model incorporates several important contextual factors that influence ad clickthrough rates, including the user’s query and ad placement on search engine result pages. We formally model these factors with respect to the list of ads displayed on a result page, the probability that the user will initiate browsing of this list, and the persistence of the user in browsing the list. We incorporate these factors into existing click models by augmenting them with appropriate query and location biases. Using expectation maximization, we learn the parameters of these augmented models from click signals recorded in the logs of a commercial search engine.To evaluate the performance of the models and to compare them with state-of-the-art performance, we apply standard evaluation metrics, including log-likelihood and perplexity. Our evaluation results indicate that, through the incorporation of query and location biases, significant improvements can be achieved in predicting browsing and click behavior in sponsored search. In addition, we explore the extent to which these biases actually reflect varying behavioral patterns. Our observations confirm that correlations exist between the biases and user search behavior.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {59},
numpages = {31},
keywords = {contextual factors, query log, clickthrough, Click model, Bayesian inference, sponsored search}
}

@article{10.1145/2532128,
author = {Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
title = {Simple and Scalable Response Prediction for Display Advertising},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532128},
doi = {10.1145/2532128},
abstract = {Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this article a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: It is easy to implement and deploy, it is highly scalable (we have trained it on terabytes of data), and it provides models with state-of-the-art accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {61},
numpages = {34},
keywords = {hashing, machine learning, click prediction, feature selection, Display advertising, distributed learning}
}

@article{10.1145/2559952,
author = {Adamopoulos, Panagiotis and Tuzhilin, Alexander},
title = {On Unexpectedness in Recommender Systems: Or How to Better Expect the Unexpected},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2559952},
doi = {10.1145/2559952},
abstract = {Although the broad social and business success of recommender systems has been achieved across several domains, there is still a long way to go in terms of user satisfaction. One of the key dimensions for significant improvement is the concept of unexpectedness. In this article, we propose a method to improve user satisfaction by generating unexpected recommendations based on the utility theory of economics. In particular, we propose a new concept of unexpectedness as recommending to users those items that depart from what they would expect from the system - the consideration set of each user. We define and formalize the concept of unexpectedness and discuss how it differs from the related notions of novelty, serendipity, and diversity. In addition, we suggest several mechanisms for specifying the users’ expectations and propose specific performance metrics to measure the unexpectedness of recommendation lists. We also take into consideration the quality of recommendations using certain utility functions and present an algorithm for providing users with unexpected recommendations of high quality that are hard to discover but fairly match their interests. Finally, we conduct several experiments on “real-world” datasets and compare our recommendation results with other methods. The proposed approach outperforms these baseline methods in terms of unexpectedness and other important metrics, such as coverage, aggregate diversity and dispersion, while avoiding any accuracy loss.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {54},
numpages = {32},
keywords = {Diversity, novelty, utility theory, recommender systems, recommendations, unexpectedness, serendipity, evaluation}
}

@article{10.1145/2668113,
author = {Castells, Pablo and Wang, Jun and Lara, Rub\'{e}n and Zhang, Dell},
title = {Introduction to the Special Issue on Diversity and Discovery in Recommender Systems},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668113},
doi = {10.1145/2668113},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {52},
numpages = {3}
}

@article{10.1145/2668111,
author = {Yang, Yiyang and Gong, Zhiguo and U, Leong Hou},
title = {Identifying Points of Interest Using Heterogeneous Features},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668111},
doi = {10.1145/2668111},
abstract = {Deducing trip-related information from web-scale datasets has received large amounts of attention recently. Identifying points of interest (POIs) in geo-tagged photos is one of these problems. The problem can be viewed as a standard clustering problem of partitioning two-dimensional objects. In this work, we study spectral clustering, which is the first attempt for the identification of POIs. However, there is no unified approach to assigning the subjective clustering parameters, and these parameters vary immensely in different metropolitans and locations. To address this issue, we study a self-tuning technique that can properly determine the parameters for the clustering needed. Besides geographical information, web photos inherently store other rich information. Such heterogenous information can be used to enhance the identification accuracy. Thereby, we study a novel refinement framework that is based on the tightness and cohesion degree of the additional information. We thoroughly demonstrate our findings by web-scale datasets collected from Flickr.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {68},
numpages = {27},
keywords = {spectral clustering, Web images}
}

@article{10.1145/2668109,
author = {Liu, Si and Chen, Qiang and Yan, Shuicheng and Xu, Changsheng and Lu, Hanqing},
title = {Snap &amp; Play: Auto-Generated Personalized Find-the-Difference Game},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668109},
doi = {10.1145/2668109},
abstract = {In this article, by taking a popular game, the Find-the-Difference (FiDi) game, as a concrete example, we explore how state-of-the-art image processing techniques can assist in developing a personalized, automatic, and dynamic game. Unlike the traditional FiDi game, where image pairs (source image and target image) with five different patches are manually produced by professional game developers, the proposed Personalized FiDi (P-FiDi) electronic game can be played in a fully automatic Snap &amp; Play mode. Snap means that players first take photos with their digital cameras. The newly captured photos are used as source images and fed into the P-FiDi system to autogenerate the counterpart target images for users to play. Four steps are adopted to autogenerate target images: enhancing the visual quality of source images, extracting some changeable patches from the source image, selecting the most suitable combination of changeable patches and difference styles for the image, and generating the differences on the target image with state-of-the-art image processing techniques. In addition, the P-FiDi game can be easily redesigned for the im-game advertising. Extensive experiments show that the P-FiDi electronic game is satisfying in terms of player experience, seamless advertisement, and technical feasibility.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {65},
numpages = {18},
keywords = {image editing, image modeling, human--computer interaction, Automatic personalized system, image analysis}
}

@article{10.1145/2668107,
author = {Javari, Amin and Jalili, Mahdi},
title = {Accurate and Novel Recommendations: An Algorithm Based on Popularity Forecasting},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668107},
doi = {10.1145/2668107},
abstract = {Recommender systems are in the center of network science, and they are becoming increasingly important in individual businesses for providing efficient, personalized services and products to users. Previous research in the field of recommendation systems focused on improving the precision of the system through designing more accurate recommendation lists. Recently, the community has been paying attention to diversity and novelty of recommendation lists as key characteristics of modern recommender systems. In many cases, novelty and precision do not go hand in hand, and the accuracy--novelty dilemma is one of the challenging problems in recommender systems, which needs efforts in making a trade-off between them.In this work, we propose an algorithm for providing novel and accurate recommendation to users. We consider the standard definition of accuracy and an effective self-information--based measure to assess novelty of the recommendation list. The proposed algorithm is based on item popularity, which is defined as the number of votes received in a certain time interval. Wavelet transform is used for analyzing popularity time series and forecasting their trend in future timesteps. We introduce two filtering algorithms based on the information extracted from analyzing popularity time series of the items. The popularity-based filtering algorithm gives a higher chance to items that are predicted to be popular in future timesteps. The other algorithm, denoted as a novelty and population-based filtering algorithm, is to move toward items with low popularity in past timesteps that are predicted to become popular in the future. The introduced filters can be applied as adds-on to any recommendation algorithm. In this article, we use the proposed algorithms to improve the performance of classic recommenders, including item-based collaborative filtering and Markov-based recommender systems. The experiments show that the algorithms could significantly improve both the accuracy and effective novelty of the classic recommenders.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {56},
numpages = {20},
keywords = {time-aware recommendation systems, collaborative filtering, item popularity forecasting, Item popularity time series}
}

@article{10.1145/2668106,
author = {K\"{u}\c{c}\"{u}ktun\c{c}, Onur and Saule, Erik and Kaya, Kamer and \c{C}ataly\"{u}rek, \"{U}mit V.},
title = {Diversifying Citation Recommendations},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2668106},
doi = {10.1145/2668106},
abstract = {Literature search is one of the most important steps of academic research. With more than 100,000 papers published each year just in computer science, performing a complete literature search becomes a Herculean task. Some of the existing approaches and tools for literature search cannot compete with the characteristics of today’s literature, and they suffer from ambiguity and homonymy. Techniques based on citation information are more robust to the mentioned issues. Thus, we recently built a Web service called the advisor, which provides personalized recommendations to researchers based on their papers of interest. Since most recommendation methods may return redundant results, diversifying the results of the search process is necessary to increase the amount of information that one can reach via an automated search. This article targets the problem of result diversification in citation-based bibliographic search, assuming that the citation graph itself is the only information available and no categories or intents are known. The contribution of this work is threefold. We survey various random walk--based diversification methods and enhance them with the direction awareness property to allow users to reach either old, foundational (possibly well-cited and well-known) research papers or recent (most likely less-known) ones. Next, we propose a set of novel algorithms based on vertex selection and query refinement. A set of experiments with various evaluation criteria shows that the proposed γ-RLM algorithm performs better than the existing approaches and is suitable for real-time bibliographic search in practice.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {55},
numpages = {21},
keywords = {diversity, Bibliographic search, direction awareness}
}

@article{10.1145/2532515,
author = {Zhu, Hengshu and Chen, Enhong and Xiong, Hui and Yu, Kuifei and Cao, Huanhuan and Tian, Jilei},
title = {Mining Mobile User Preferences for Personalized Context-Aware Recommendation},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532515},
doi = {10.1145/2532515},
abstract = {Recent advances in mobile devices and their sensing capabilities have enabled the collection of rich contextual information and mobile device usage records through the device logs. These context-rich logs open a venue for mining the personal preferences of mobile users under varying contexts and thus enabling the development of personalized context-aware recommendation and other related services, such as mobile online advertising. In this article, we illustrate how to extract personal context-aware preferences from the context-rich device logs, or context logs for short, and exploit these identified preferences for building personalized context-aware recommender systems. A critical challenge along this line is that the context log of each individual user may not contain sufficient data for mining his or her context-aware preferences. Therefore, we propose to first learn common context-aware preferences from the context logs of many users. Then, the preference of each user can be represented as a distribution of these common context-aware preferences. Specifically, we develop two approaches for mining common context-aware preferences based on two different assumptions, namely, context-independent and context-dependent assumptions, which can fit into different application scenarios. Finally, extensive experiments on a real-world dataset show that both approaches are effective and outperform baselines with respect to mining personal context-aware preferences for mobile users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {58},
numpages = {27},
keywords = {personalized recommendation, Context aware, mobile user}
}

@article{10.1145/2532441,
author = {Balakrishnan, Raju and Bhatt, Rushi P.},
title = {Real-Time Bid Optimization for Group-Buying Ads},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532441},
doi = {10.1145/2532441},
abstract = {Group-buying ads seeking a minimum number of customers before the deal expiry are increasingly used by daily-deal providers. Unlike traditional web ads, the advertiser’s profits for group-buying ads depend on the time to expiry and additional customers needed to satisfy the minimum group size. Since both these quantities are time-dependent, optimal bid amounts to maximize profits change with every impression. Consequently, traditional static bidding strategies are far from optimal. Instead, bid values need to be optimized in real-time to maximize expected bidder profits. This online optimization of deal profits is made possible by the advent of ad exchanges offering real-time (spot) bidding. To this end, we propose a real-time bidding strategy for group-buying deals based on the online optimization of bid values. We derive the expected bidder profit of deals as a function of the bid amounts and dynamically vary the bids to maximize profits. Furthermore, to satisfy time constraints of the online bidding, we present methods of minimizing computation timings. Subsequently, we derive the real-time ad selection, admissibility, and real-time bidding of the traditional ads as the special cases of the proposed method. We evaluate the proposed bidding, selection, and admission strategies on a multimillion click stream of 935 ads. The proposed real-time bidding, selection, and admissibility show significant profit increases over the existing strategies. Further experiments illustrate the robustness of the bidding and acceptable computation timings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {62},
numpages = {21},
keywords = {group-buying, display ads, daily deals, Real-time bidding}
}

@article{10.1145/2629592,
author = {Zheng, Yu and Capra, Licia and Wolfson, Ouri and Yang, Hai},
title = {Urban Computing: Concepts, Methodologies, and Applications},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2629592},
doi = {10.1145/2629592},
abstract = {Urbanization's rapid progress has modernized many people's lives but also engendered big issues, such as traffic congestion, energy consumption, and pollution. Urban computing aims to tackle these issues by using the data that has been generated in cities (e.g., traffic flow, human mobility, and geographical data). Urban computing connects urban sensing, data management, data analytics, and service providing into a recurrent process for an unobtrusive and continuous improvement of people's lives, city operation systems, and the environment. Urban computing is an interdisciplinary field where computer sciences meet conventional city-related fields, like transportation, civil engineering, environment, economy, ecology, and sociology in the context of urban spaces. This article first introduces the concept of urban computing, discussing its general framework and key challenges from the perspective of computer sciences. Second, we classify the applications of urban computing into seven categories, consisting of urban planning, transportation, the environment, energy, social, economy, and public safety and security, presenting representative scenarios in each category. Third, we summarize the typical technologies that are needed in urban computing into four folds, which are about urban sensing, urban data management, knowledge fusion across heterogeneous data, and urban data visualization. Finally, we give an outlook on the future of urban computing, suggesting a few research topics that are somehow missing in the community.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {38},
numpages = {55},
keywords = {knowledge fusion, urban sensing, computing with heterogeneous data, trajectories, big data, human mobility, city dynamics, Urban computing, urban informatics}
}

@article{10.1145/2566617,
author = {Joseph, Kenneth and Carley, Kathleen M. and Hong, Jason I.},
title = {Check-Ins in “Blau Space”: Applying Blau’s Macrosociological Theory to Foursquare Check-Ins from New York City},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2566617},
doi = {10.1145/2566617},
abstract = {Peter Blau was one of the first to define a latent social space and utilize it to provide concrete hypotheses. Blau defines social structure via social “parameters” (constraints). Actors that are closer together (more homogenous) in this social parameter space are more likely to interact. One of Blau’s most important hypotheses resulting from this work was that the consolidation of parameters could lead to isolated social groups. For example, the consolidation of race and income might lead to segregation. In the present work, we use Foursquare data from New York City to explore evidence of homogeneity along certain social parameters and consolidation that breeds social isolation in communities of locations checked in to by similar users.More specifically, we first test the extent to which communities detected via Latent Dirichlet Allocation are homogenous across a set of four social constraints—racial homophily, income homophily, personal interest homophily and physical space. Using a bootstrapping approach, we find that 14 (of 20) communities are statistically, and all but one qualitatively, homogenous along one of these social constraints, showing the relevance of Blau’s latent space model in venue communities determined via user check-in behavior. We then consider the extent to which communities with consolidated parameters, those homogenous on more than one parameter, represent socially isolated populations. We find communities homogenous on multiple parameters, including a homosexual community and a “hipster” community, that show support for Blau’s hypothesis that consolidation breeds social isolation. We consider these results in the context of mediated communication, in particular in the context of self-representation on social media.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {46},
numpages = {22},
keywords = {urban analytics, community structure, latent social space, Foursquare}
}

@article{10.1145/2542666,
author = {Gurung, Sashi and Lin, Dan and Jiang, Wei and Hurson, Ali and Zhang, Rui},
title = {Traffic Information Publication with Privacy Preservation},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542666},
doi = {10.1145/2542666},
abstract = {We are experiencing the expanding use of location-based services such as AT&amp;T’s TeleNav GPS Navigator and Intel’s Thing Finder. Existing location-based services have collected a large amount of location data, which has great potential for statistical usage in applications like traffic flow analysis, infrastructure planning, and advertisement dissemination. The key challenge is how to wisely use the data without violating each user’s location privacy concerns. In this article, we first identify a new privacy problem, namely, the inference-route problem, and then present our anonymization algorithms for privacy-preserving trajectory publishing. The experimental results have demonstrated that our approach outperforms the latest related work in terms of both efficiency and effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {44},
numpages = {26},
keywords = {road-network constraint, Location privacy, trajectory anonymization}
}

@article{10.1145/2542665,
author = {Tan, Chang and Liu, Qi and Chen, Enhong and Xiong, Hui and Wu, Xiang},
title = {Object-Oriented Travel Package Recommendation},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542665},
doi = {10.1145/2542665},
abstract = {Providing better travel services for tourists is one of the important applications in urban computing. Though many recommender systems have been developed for enhancing the quality of travel service, most of them lack a systematic and open framework to dynamically incorporate multiple types of additional context information existing in the tourism domain, such as the travel area, season, and price of travel packages. To that end, in this article, we propose an open framework, the Objected-Oriented Recommender System (ORS), for the developers performing personalized travel package recommendations to tourists. This framework has the ability to import all the available additional context information to the travel package recommendation process in a cost-effective way. Specifically, the different types of additional information are extracted and uniformly represented as feature--value pairs. Then, we define the Object, which is the collection of the feature--value pairs. We propose two models that can be used in the ORS framework for extracting the implicit relationships among Objects. The Objected-Oriented Topic Model (OTM) can extract the topics conditioned on the intrinsic feature--value pairs of the Objects. The Objected-Oriented Bayesian Network (OBN) can effectively infer the cotravel probability of two tourists by calculating the co-occurrence time of feature--value pairs belonging to different kinds of Objects. Based on the relationships mined by OTM or OBN, the recommendation list is generated by the collaborative filtering method. Finally, we evaluate these two models and the ORS framework on real-world travel package data, and the experimental results show that the ORS framework is more flexible in terms of incorporating additional context information, and thus leads to better performances for travel package recommendations. Meanwhile, for feature selection in ORS, we define the feature information entropy, and the experimental results demonstrate that using features with lower entropies usually leads to better recommendation results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {43},
numpages = {26},
keywords = {object-oriented, collaborative filtering, Bayesian, topic model, travel}
}

@article{10.1145/2535912,
author = {Neviarouskaya, Alena and Aono, Masaki and Prendinger, Helmut and Ishizuka, Mitsuru},
title = {Intelligent Interface for Textual Attitude Analysis},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2535912},
doi = {10.1145/2535912},
abstract = {This article describes a novel intelligent interface for attitude sensing in text driven by a robust computational tool for the analysis of fine-grained attitudes (emotions, judgments, and appreciations) expressed in text. The module responsible for textual attitude analysis was developed using a compositional linguistic approach based on the attitude-conveying lexicon, the analysis of syntactic and dependency relations between words in a sentence, the compositionality principle applied at various grammatical levels, the rules elaborated for semantically distinct verb classes, and a method considering the hierarchy of concepts. The performance of this module was evaluated on sentences from personal stories about life experiences. The developed web-based interface supports recognition of nine emotions, positive and negative judgments, and positive and negative appreciations conveyed in text. It allows users to adjust parameters, to enable or disable various functionality components of the algorithm, and to select the format of text annotation and attitude statistics visualization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {48},
numpages = {20},
keywords = {Affective computing, attitude analysis in text, affective user interface}
}

@article{10.1145/2513567,
author = {Momtazpour, Marjan and Butler, Patrick and Ramakrishnan, Naren and Hossain, M. Shahriar and Bozchalui, Mohammad C. and Sharma, Ratnesh},
title = {Charging and Storage Infrastructure Design for Electric Vehicles},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2513567},
doi = {10.1145/2513567},
abstract = {Ushered by recent developments in various areas of science and technology, modern energy systems are going to be an inevitable part of our societies. Smart grids are one of these modern systems that have attracted many research activities in recent years. Before utilizing the next generation of smart grids, we should have a comprehensive understanding of the interdependent energy networks and processes. Next-generation energy systems networks cannot be effectively designed, analyzed, and controlled in isolation from the social, economic, sensing, and control contexts in which they operate. In this article, we present a novel framework to support charging and storage infrastructure design for electric vehicles. We develop coordinated clustering techniques to work with network models of urban environments to aid in placement of charging stations for an electrical vehicle deployment scenario. Furthermore, we evaluate the network before and after the deployment of charging stations, to recommend the installation of appropriate storage units to overcome the extra load imposed on the network by the charging stations. We demonstrate the multiple factors that can be simultaneously leveraged in our framework to achieve practical urban deployment. Our ultimate goal is to help realize sustainable energy system management in urban electrical infrastructure by modeling and analyzing networks of interactions between electric systems and urban populations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {42},
numpages = {27},
keywords = {synthetic populations, Clustering, coordinated clustering, data mining, charging stations, electric vehicles, storage, smart grids}
}

@article{10.1145/2523068,
author = {Ying, Josh Jia-Ching and Kuo, Wen-Ning and Tseng, Vincent S. and Lu, Eric Hsueh-Chan},
title = {Mining User Check-In Behavior with a Random Walk for Urban Point-of-Interest Recommendations},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2523068},
doi = {10.1145/2523068},
abstract = {In recent years, research into the mining of user check-in behavior for point-of-interest (POI) recommendations has attracted a lot of attention. Existing studies on this topic mainly treat such recommendations in a traditional manner—that is, they treat POIs as items and check-ins as ratings. However, users usually visit a place for reasons other than to simply say that they have visited. In this article, we propose an approach referred to as Urban POI-Walk (UPOI-Walk), which takes into account a user's social-triggered intentions (SI), preference-triggered intentions (PreI), and popularity-triggered intentions (PopI), to estimate the probability of a user checking-in to a POI. The core idea of UPOI-Walk involves building a HITS-based random walk on the normalized check-in network, thus supporting the prediction of POI properties related to each user's preferences. To achieve this goal, we define several user--POI graphs to capture the key properties of the check-in behavior motivated by user intentions. In our UPOI-Walk approach, we propose a new kind of random walk model—Dynamic HITS-based Random Walk—which comprehensively considers the relevance between POIs and users from different aspects. On the basis of similitude, we make an online recommendation as to the POI the user intends to visit. To the best of our knowledge, this is the first work on urban POI recommendations that considers user check-in behavior motivated by SI, PreI, and PopI in location-based social network data. Through comprehensive experimental evaluations on two real datasets, the proposed UPOI-Walk is shown to deliver excellent performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {40},
numpages = {26},
keywords = {location-based social network, user preference mining, Point-of-interest recommendation, urban computing, data mining}
}

@article{10.1145/2642650,
author = {Zheng, Yu and Capra, Licia and Wolfson, Ouri and Yang, Hai},
title = {Introduction to the Special Section on Urban Computing},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2642650},
doi = {10.1145/2642650},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {2}
}

@article{10.1145/2560188,
author = {Etienne, C\^{o}me and Latifa, Oukhellou},
title = {Model-Based Count Series Clustering for Bike Sharing System Usage Mining: A Case Study with the V\'{e}Lib’ System of Paris},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2560188},
doi = {10.1145/2560188},
abstract = {Today, more and more bicycle sharing systems (BSSs) are being introduced in big cities. These transportation systems generate sizable transportation data, the mining of which can reveal the underlying urban phenomenon linked to city dynamics. This article presents a statistical model to automatically analyze the trip data of a bike sharing system. The proposed solution partitions (i.e., clusters) the stations according to their usage profiles. To do so, count series describing the stations’s usage through departure/arrival counts per hour throughout the day are built and analyzed. The model for processing these count series is based on Poisson mixtures and introduces a station scaling factor that handles the differences between the stations’s global usage. Differences between weekday and weekend usage are also taken into account. This model identifies the latent factors that shape the geography of trips, and the results may thus offer insights into the relationships between station neighborhood type (its amenities, its demographics, etc.) and the generated mobility patterns. In other words, the proposed method brings to light the different functions in different areas that induce specific patterns in BSS data. These potentials are demonstrated through an in-depth analysis of the results obtained on the Paris V\'{e}lib’ large-scale bike sharing system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {21},
keywords = {Bike sharing systems, clustering, count data, model-based clustering}
}

@article{10.1145/2542668,
author = {Hsieh, Hsun-Ping and Li, Cheng-Te and Lin, Shou-De},
title = {Measuring and Recommending Time-Sensitive Routes from Location-Based Data},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542668},
doi = {10.1145/2542668},
abstract = {Location-based services allow users to perform geospatial recording actions, which facilitates the mining of the moving activities of human beings. This article proposes to recommend time-sensitive trip routes consisting of a sequence of locations with associated timestamps based on knowledge extracted from large-scale timestamped location sequence data (e.g., check-ins and GPS traces). We argue that a good route should consider (a) the popularity of places, (b) the visiting order of places, (c) the proper visiting time of each place, and (d) the proper transit time from one place to another. By devising a statistical model, we integrate these four factors into a route goodness function that aims to measure the quality of a route. Equipped with the route goodness, we recommend time-sensitive routes for two scenarios. The first is about constructing the route based on the user-specified source location with the starting time. The second is about composing the route between the specified source location and the destination location given a starting time. To handle these queries, we propose a search method, Guidance Search, which consists of a novel heuristic satisfaction function that guides the search toward the destination location and a backward checking mechanism to boost the effectiveness of the constructed route. Experiments on the Gowalla check-in datasets demonstrate the effectiveness of our model on detecting real routes and performing cloze test of routes, comparing with other baseline methods. We also develop a system TripRouter as a real-time demo platform.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {45},
numpages = {27},
keywords = {location-based data, Time-sensitive route, trip recommendation}
}

@article{10.1145/2528548,
author = {Mahmud, Jalal and Nichols, Jeffrey and Drews, Clemens},
title = {Home Location Identification of Twitter Users},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2528548},
doi = {10.1145/2528548},
abstract = {We present a new algorithm for inferring the home location of Twitter users at different granularities, including city, state, time zone, or geographic region, using the content of users’ tweets and their tweeting behavior. Unlike existing approaches, our algorithm uses an ensemble of statistical and heuristic classifiers to predict locations and makes use of a geographic gazetteer dictionary to identify place-name entities. We find that a hierarchical classification approach, where time zone, state, or geographic region is predicted first and city is predicted next, can improve prediction accuracy. We have also analyzed movement variations of Twitter users, built a classifier to predict whether a user was travelling in a certain period of time, and use that to further improve the location detection accuracy. Experimental evidence suggests that our algorithm works well in practice and outperforms the best existing algorithms for predicting the home location of Twitter users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {47},
numpages = {21},
keywords = {time zone, tweets, Location}
}

@article{10.1145/2517028,
author = {Mcardle, Gavin and Furey, Eoghan and Lawlor, Aonghus and Pozdnoukhov, Alexei},
title = {Using Digital Footprints for a City-Scale Traffic Simulation},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2517028},
doi = {10.1145/2517028},
abstract = {This article introduces a microsimulation of urban traffic flows within a large-scale scenario implemented for the Greater Dublin region in Ireland. Traditionally, the data available for traffic simulations come from a population census and dedicated road surveys that only partly cover shopping, leisure, or recreational trips. To account for the latter, the presented traffic modeling framework exploits the digital footprints of city inhabitants on services such as Twitter and Foursquare. We enriched the model with findings from our previous studies on geographical layout of communities in a country-wide mobile phone network to account for socially related journeys. These datasets were used to calibrate a variant of a radiation model of spatial choice, which we introduced in order to drive individuals’ decisions on trip destinations within an assigned daily activity plan. We observed that given the distribution of population, the workplace locations, a comprehensive set of urban facilities, and a list of typical activity sequences of city dwellers collected within a national travel survey, the developed microsimulation reproduces not only the journey statistics such as peak travel periods but also the traffic volumes at main road segments with surprising accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {16},
keywords = {traffic simulation, Urban analysis, social networks}
}

@article{10.1145/2542048,
author = {Wang, Shuaiqiang and Sun, Jiankai and Gao, Byron J. and Ma, Jun},
title = {VSRank: A Novel Framework for Ranking-Based Collaborative Filtering},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542048},
doi = {10.1145/2542048},
abstract = {Collaborative filtering (CF) is an effective technique addressing the information overload problem. CF approaches generally fall into two categories: rating based and ranking based. The former makes recommendations based on historical rating scores of items and the latter based on their rankings. Ranking-based CF has demonstrated advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we propose VSRank, a novel framework that seeks accuracy improvement of ranking-based CF through adaptation of the vector space model. In VSRank, we consider each user as a document and his or her pairwise relative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Extensive experiments on benchmarks in comparison with the state-of-the-art approaches demonstrate the promise of our approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {51},
numpages = {24},
keywords = {ranking-based collaborative filtering, Recommender systems, vector space model, term weighting, collaborative filtering}
}

@article{10.1145/2533989,
author = {Song, Yicheng and Zhang, Yongdong and Cao, Juan and Tang, Jinhui and Gao, Xingyu and Li, Jintao},
title = {A Unified Geolocation Framework for Web Videos},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2533989},
doi = {10.1145/2533989},
abstract = {In this article, we propose a unified geolocation framework to automatically determine where on the earth a web video was shot. We analyze different social, visual, and textual relationships from a real-world dataset and find four relationships with apparent geography clues that can be used for web video geolocation. Then, the geolocation process is formulated as an optimization problem that simultaneously takes the social, visual, and textual relationships into consideration. The optimization problem is solved by an iterative procedure, which can be interpreted as a propagation of the geography information among the web video social network. Extensive experiments on a real-world dataset clearly demonstrate the effectiveness of our proposed framework, with the geolocation accuracy higher than state-of-the-art approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {49},
numpages = {22},
keywords = {web video, geotag, Unified geolocation framework}
}

@article{10.1145/2532439,
author = {Zhao, Yi-Liang and Nie, Liqiang and Wang, Xiangyu and Chua, Tat-Seng},
title = {Personalized Recommendations of Locally Interesting Venues to Tourists via Cross-Region Community Matching},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532439},
doi = {10.1145/2532439},
abstract = {You are in a new city. You are not familiar with the places and neighborhoods. You want to know all about the exciting sights, food outlets, and cultural venues that the locals frequent, in particular those that suit your personal interests. Even though there exist many mapping, local search, and travel assistance sites, they mostly provide popular and famous listings such as Statue of Liberty and Eiffel Tower, which are well-known places but may not suit your personal needs or interests. Therefore, there is a gap between what tourists want and what dominant tourism resources are providing. In this work, we seek to provide a solution to bridge this gap by exploiting the rich user-generated location contents in location-based social networks in order to offer tourists the most relevant and personalized local venue recommendations. In particular, we first propose a novel Bayesian approach to extract the social dimensions of people at different geographical regions to capture their latent local interests. We next mine the local interest communities in each geographical region. We then represent each local community using aggregated behaviors of community members. Finally, we correlate communities across different regions and generate venue recommendations to tourists via cross-region community matching. We have sampled a representative subset of check-ins from Foursquare and experimentally verified the effectiveness of our proposed approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {26},
keywords = {Location-based social networks, cross-region community matching, locally interesting venues, social dimensions}
}

@article{10.1145/2594455,
author = {Gong, Neil Zhenqiang and Talwalkar, Ameet and Mackey, Lester and Huang, Ling and Shin, Eui Chul Richard and Stefanov, Emil and Shi, Elaine (Runting) and Song, Dawn},
title = {Joint Link Prediction and Attribute Inference Using a Social-Attribute Network},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2594455},
doi = {10.1145/2594455},
abstract = {The effects of social influence and homophily suggest that both network structure and node-attribute information should inform the tasks of link prediction and node-attribute inference. Recently, Yin et al. [2010a, 2010b] proposed an attribute-augmented social network model, which we call Social-Attribute Network (SAN), to integrate network structure and node attributes to perform both link prediction and attribute inference. They focused on generalizing the random walk with a restart algorithm to the SAN framework and showed improved performance. In this article, we extend the SAN framework with several leading supervised and unsupervised link-prediction algorithms and demonstrate performance improvement for each algorithm on both link prediction and attribute inference. Moreover, we make the novel observation that attribute inference can help inform link prediction, that is, link-prediction accuracy is further improved by first inferring missing attributes. We comprehensively evaluate these algorithms and compare them with other existing algorithms using a novel, large-scale Google+ dataset, which we make publicly available (http://www.cs.berkeley.edu/~stevgong/gplus.html).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {27},
numpages = {20},
keywords = {social-attribute network, Google+, attribute inference, heterogeneous network, Link prediction}
}

@article{10.1145/2594454,
author = {Yang, Jaewon and Leskovec, Jure},
title = {Structure and Overlaps of Ground-Truth Communities in Networks},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2594454},
doi = {10.1145/2594454},
abstract = {One of the main organizing principles in real-world networks is that of network communities, where sets of nodes organize into densely linked clusters. Even though detection of such communities is of great interest, understanding the structure communities in large networks remains relatively limited. In particular, due to the unavailability of labeled ground-truth data, it was traditionally very hard to develop accurate models of network community structure.Here we use six large social, collaboration, and information networks where nodes explicitly state their ground-truth community memberships. For example, nodes in social networks join into explicitly defined interest based groups, and we use such groups as explicitly labeled ground-truth communities. We use such ground-truth communities to study their structural signatures by analyzing how ground-truth communities emerge in networks and how they overlap. We observe some surprising phenomena. First, ground-truth communities contain high-degree hub nodes that reside in community overlaps and link to most of the members of the community. Second, the overlaps of communities are more densely connected than the non-overlapping parts of communities. We show that this in contrast to the conventional wisdom that community overlaps are more sparsely connected than the non-overlapping parts themselves. We then show that many existing models of network communities do not capture dense community overlaps. This in turn means that most present models and community detection methods confuse overlaps as separate communities. In contrast, we present the community-affiliation graph model (AGM), a conceptual model of network community structure. We demonstrate that AGM reliably captures the overall structure of networks as well as the overlapping and hierarchical nature of network communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {26},
numpages = {35},
keywords = {social networks, Network communities, affiliation networks}
}

@article{10.1145/2594452,
author = {He, Qi and Li, Juanzi and Yan, Rong and Yen, John and Zhang, Haizheng},
title = {Introduction to the Special Issue on Linking Social Granularity and Functions},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2594452},
doi = {10.1145/2594452},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {22},
numpages = {3}
}

@article{10.1145/2589483,
author = {Heath, Derrall and Norton, David and Ventura, Dan},
title = {Conveying Semantics through Visual Metaphor},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2589483},
doi = {10.1145/2589483},
abstract = {In the field of visual art, metaphor is a way to communicate meaning to the viewer. We present a computational system for communicating visual metaphor that can identify adjectives for describing an image based on a low-level visual feature representation of the image. We show that the system can use this visual-linguistic association to render source images that convey the meaning of adjectives in a way consistent with human understanding. Our conclusions are based on a detailed analysis of how the system's artifacts cluster, how these clusters correspond to the semantic relationships of adjectives as documented in WordNet, and how these clusters correspond to human opinion.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {17},
keywords = {clustering, Visual metaphor, evolutionary art, neural networks}
}

@article{10.1145/2589482,
author = {Reches, Shulamit and Kalech, Meir and Hendrix, Philip},
title = {A Framework for Effectively Choosing between Alternative Candidate Partners},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2589482},
doi = {10.1145/2589482},
abstract = {Many multi-agent settings require that agents identify appropriate partners or teammates with whom to work on tasks. When selecting potential partners, agents may benefit from obtaining information about the alternatives, for instance, through gossip (i.e., by consulting others) or reputation systems. When information is uncertain and associated with cost, deciding on the amount of information needed is a hard optimization problem. This article defines a statistical model, the Information-Acquisition Source Utility model (IASU), by which agents, operating in an uncertain world, can determine (1) which information sources they should request for information, and (2) the amount of information to collect about potential partners from each source. To maximize the expected gain from the choice, IASU computes the utility of choosing a partner by estimating the benefit of additional information. The article presents empirical studies through a simulation domain as well as a real-world domain of restaurants. We compare the IASU model to other relevant models and show that the use of the IASU model significantly increases agents' overall utility.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {28},
keywords = {AI technologies, multi-agent systems, Decision theory}
}

@article{10.1145/2589481,
author = {Li, Nan and Cushing, William and Kambhampati, Subbarao and Yoon, Sungwook},
title = {Learning Probabilistic Hierarchical Task Networks as Probabilistic Context-Free Grammars to Capture User Preferences},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2589481},
doi = {10.1145/2589481},
abstract = {We introduce an algorithm to automatically learn probabilistic hierarchical task networks (pHTNs) that capture a user's preferences on plans by observing only the user's behavior. HTNs are a common choice of representation for a variety of purposes in planning, including work on learning in planning. Our contributions are twofold. First, in contrast with prior work, which employs HTNs to represent domain physics or search control knowledge, we use HTNs to model user preferences. Second, while most prior work on HTN learning requires additional information (e.g., annotated traces or tasks) to assist the learning process, our system only takes plan traces as input. Initially, we will assume that users carry out preferred plans more frequently, and thus the observed distribution of plans is an accurate representation of user preference. We then generalize to the situation where feasibility constraints frequently prevent the execution of preferred plans. Taking the prevalent perspective of viewing HTNs as grammars over primitive actions, we adapt an expectation-maximization (EM) technique from the discipline of probabilistic grammar induction to acquire probabilistic context-free grammars (pCFG) that capture the distribution on plans. To account for the difference between the distributions of possible and preferred plans, we subsequently modify this core EM technique by rescaling its input. We empirically demonstrate that the proposed approaches are able to learn HTNs representing user preferences better than the inside-outside algorithm. Furthermore, when feasibility constraints are obfuscated, the algorithm with rescaled input performs better than the algorithm with the original input.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {29},
numpages = {32},
keywords = {planning, AI Technology, learning user preferences, Hierarchical task networks}
}

@article{10.1145/2532549,
author = {Chen, Yi-Cheng and Zhu, Wen-Yuan and Peng, Wen-Chih and Lee, Wang-Chien and Lee, Suh-Yin},
title = {CIM: Community-Based Influence Maximization in Social Networks},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532549},
doi = {10.1145/2532549},
abstract = {Given a social graph, the problem of influence maximization is to determine a set of nodes that maximizes the spread of influences. While some recent research has studied the problem of influence maximization, these works are generally too time consuming for practical use in a large-scale social network. In this article, we develop a new framework, community-based influence maximization (CIM), to tackle the influence maximization problem with an emphasis on the time efficiency issue. Our proposed framework, CIM, comprises three phases: (i) community detection, (ii) candidate generation, and (iii) seed selection. Specifically, phase (i) discovers the community structure of the network; phase (ii) uses the information of communities to narrow down the possible seed candidates; and phase (iii) finalizes the seed nodes from the candidate set. By exploiting the properties of the community structures, we are able to avoid overlapped information and thus efficiently select the number of seeds to maximize information spreads. The experimental results on both synthetic and real datasets show that the proposed CIM algorithm significantly outperforms the state-of-the-art algorithms in terms of efficiency and scalability, with almost no compromise of effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {25},
numpages = {31},
keywords = {influence maximization, diffusion models, social network analysis, Community detection}
}

@article{10.1145/2517088,
author = {Pool, Simon and Bonchi, Francesco and Leeuwen, Matthijs van},
title = {Description-Driven Community Detection},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2517088},
doi = {10.1145/2517088},
abstract = {Traditional approaches to community detection, as studied by physicists, sociologists, and more recently computer scientists, aim at simply partitioning the social network graph. However, with the advent of online social networking sites, richer data has become available: beyond the link information, each user in the network is annotated with additional information, for example, demographics, shopping behavior, or interests. In this context, it is therefore important to develop mining methods which can take advantage of all available information. In the case of community detection, this means finding good communities (a set of nodes cohesive in the social graph) which are associated with good descriptions in terms of user information (node attributes).Having good descriptions associated to our models make them understandable by domain experts and thus more useful in real-world applications. Another requirement dictated by real-world applications, is to develop methods that can use, when available, any domain-specific background knowledge. In the case of community detection the background knowledge could be a vague description of the communities sought in a specific application, or some prototypical nodes (e.g., good customers in the past), that represent what the analyst is looking for (a community of similar users).Towards this goal, in this article, we define and study the problem of finding a diverse set of cohesive communities with concise descriptions. We propose an effective algorithm that alternates between two phases: a hill-climbing phase producing (possibly overlapping) communities, and a description induction phase which uses techniques from supervised pattern set mining. Our framework has the nice feature of being able to build well-described cohesive communities starting from any given description or seed set of nodes, which makes it very flexible and easily applicable in real-world applications.Our experimental evaluation confirms that the proposed method discovers cohesive communities with concise descriptions in realistic and large online social networks such as Delicious, Flickr, and LastFM.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {28},
numpages = {28},
keywords = {description, Social networks, domain knowledge, behavioral and demographic information, social patterns, community detection}
}

@article{10.1145/2517085,
author = {Tang, Xuning and Yang, Christopher C.},
title = {Detecting Social Media Hidden Communities Using Dynamic Stochastic Blockmodel with Temporal Dirichlet Process},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2517085},
doi = {10.1145/2517085},
abstract = {Detecting evolving hidden communities within dynamic social networks has attracted significant attention recently due to its broad applications in e-commerce, online social media, security intelligence, public health, and other areas. Many community network detection techniques employ a two-stage approach to identify and detect evolutionary relationships between communities of two adjacent time epochs. These techniques often identify communities with high temporal variation, since the two-stage approach detects communities of each epoch independently without considering the continuity of communities across two time epochs. Other techniques require identification of a predefined number of hidden communities which is not realistic in many applications. To overcome these limitations, we propose the Dynamic Stochastic Blockmodel with Temporal Dirichlet Process, which enables the detection of hidden communities and tracks their evolution simultaneously from a network stream. The number of hidden communities is automatically determined by a temporal Dirichlet process without human intervention. We tested our proposed technique on three different testbeds with results identifying a high performance level when compared to the baseline algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {36},
numpages = {21},
keywords = {Temporal Dirichlet Process, Stochastic Blockmodel, Dynamic Community Detection}
}

@article{10.1145/2505272,
author = {Shi, Chuan and Kong, Xiangnan and Fu, Di and Yu, Philip S. and Wu, Bin},
title = {Multi-Label Classification Based on Multi-Objective Optimization},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2505272},
doi = {10.1145/2505272},
abstract = {Multi-label classification refers to the task of predicting potentially multiple labels for a given instance. Conventional multi-label classification approaches focus on single objective setting, where the learning algorithm optimizes over a single performance criterion (e.g., Ranking Loss) or a heuristic function. The basic assumption is that the optimization over one single objective can improve the overall performance of multi-label classification and meet the requirements of various applications. However, in many real applications, an optimal multi-label classifier may need to consider the trade-offs among multiple inconsistent objectives, such as minimizing Hamming Loss while maximizing Micro F1. In this article, we study the problem of multi-objective multi-label classification and propose a novel solution (called Moml) to optimize over multiple objectives simultaneously. Note that optimization objectives may be inconsistent, even conflicting, thus one cannot identify a single solution that is optimal on all objectives. Our Moml algorithm finds a set of non-dominated solutions which are optimal according to different trade-offs among multiple objectives. So users can flexibly construct various predictive models from the solution set, which provides more meaningful classification results in different application scenarios. Empirical studies on real-world tasks demonstrate that the Moml can effectively boost the overall performance of multi-label classification by optimizing over multiple objectives simultaneously.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {35},
numpages = {22},
keywords = {Classification, pattern analysis, model selection, classifier design and evaluation, multi-objective optimization, multi-label classification}
}

@article{10.1145/2505270,
author = {Hossain, M. Shahriar and Marwah, Manish and Shah, Amip and Watson, Layne T. and Ramakrishnan, Naren},
title = {AutoLCA: A Framework for Sustainable Redesign and Assessment of Products},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2505270},
doi = {10.1145/2505270},
abstract = {With increasing public consciousness regarding sustainability, companies are ever more eager to introduce eco-friendly products and services. Assessing environmental footprints and designing sustainable products are challenging tasks since they require analysis of each component of a product through their life cycle. To achieve sustainable design of products, companies need to evaluate the environmental impact of their system, identify the major contributors to the footprint, and select the design alternative with the lowest environmental footprint. In this article, we formulate sustainable design as a series of clustering and classification problems, and propose a framework called AutoLCA that simplifies the effort of estimating the environmental footprint of a product bill of materials by more than an order of magnitude over current methods, which are mostly labor intensive. We apply AutoLCA to real data from a large computer manufacturer. We conduct a case study on bill of materials of four different products, perform a “hotspot” assessment analysis to identify major contributors to carbon footprint, and determine design alternatives that can reduce the carbon footprint from 1% to 36%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {21},
keywords = {Disparate clustering, life cycle analysis, sustainable redesign}
}

@article{10.1145/2501977,
author = {Javari, Amin and Jalili, Mahdi},
title = {Cluster-Based Collaborative Filtering for Sign Prediction in Social Networks with Positive and Negative Links},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2501977},
doi = {10.1145/2501977},
abstract = {Social network analysis and mining get ever-increasingly important in recent years, which is mainly due to the availability of large datasets and advances in computing systems. A class of social networks is those with positive and negative links. In such networks, a positive link indicates friendship (or trust), whereas links with a negative sign correspond to enmity (or distrust). Predicting the sign of the links in these networks is an important issue and has many applications, such as friendship recommendation and identifying malicious nodes in the network.In this manuscript, we proposed a new method for sign prediction in networks with positive and negative links. Our algorithm is based first on clustering the network into a number of clusters and then applying a collaborative filtering algorithm. The clusters are such that the number of intra-cluster negative links and inter-cluster positive links are minimal, that is, the clusters are socially balanced as much as possible (a signed graph is socially balanced if it can be divided into clusters with all positive links inside the clusters and all negative links between them). We then used similarity between the clusters (based on the links between them) in a collaborative filtering algorithm. Our experiments on a number of real datasets showed that the proposed method outperformed previous methods, including those based on social balance and status theories and one based on a machine learning framework (logistic regression in this work).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {19},
keywords = {social balance theory, Social networks, cluster identification, collaborative filtering, social status theory, signed networks}
}

@article{10.1145/2499380,
author = {Wang, Jinpeng and Zhao, Wayne Xin and He, Yulan and Li, Xiaoming},
title = {Infer User Interests via Link Structure Regularization},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2499380},
doi = {10.1145/2499380},
abstract = {Learning user interests from online social networks helps to better understand user behaviors and provides useful guidance to design user-centric applications. Apart from analyzing users' online content, it is also important to consider users' social connections in the social Web. Graph regularization methods have been widely used in various text mining tasks, which can leverage the graph structure information extracted from data. Previously, graph regularization methods operate under the cluster assumption that nearby nodes are more similar and nodes on the same structure (typically referred to as a cluster or a manifold) are likely to be similar. We argue that learning user interests from complex, sparse, and dynamic social networks should be based on the link structure assumption under which node similarities are evaluated based on the local link structures instead of explicit links between two nodes. We propose a regularization framework based on the relation bipartite graph, which can be constructed from any type of relations. Using Twitter as our case study, we evaluate our proposed framework from social networks built from retweet relations. Both quantitative and qualitative experiments show that our proposed method outperforms a few competitive baselines in learning user interests over a set of predefined topics. It also gives superior results compared to the baselines on retweet prediction and topical authority identification.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {23},
numpages = {22},
keywords = {User interests, link structure, graph regularization}
}

@article{10.1145/2493259,
author = {Bian, Jiang and Long, Bo and Li, Lihong and Moon, Taesup and Dong, Anlei and Chang, Yi},
title = {Exploiting User Preference for Online Learning in Web Content Optimization Systems},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2493259},
doi = {10.1145/2493259},
abstract = {Web portal services have become an important medium to deliver digital content (e.g. news, advertisements, etc.) to Web users in a timely fashion. To attract more users to various content modules on the Web portal, it is necessary to design a recommender system that can effectively achieve Web portal content optimization by automatically estimating content item attractiveness and relevance to user interests. The state-of-the-art online learning methodology adapts dedicated pointwise models to independently estimate the attractiveness score for each candidate content item. Although such pointwise models can be easily adapted for online recommendation, there still remain a few critical problems. First, this pointwise methodology fails to use invaluable user preferences between content items. Moreover, the performance of pointwise models decreases drastically when facing the problem of sparse learning samples. To address these problems, we propose exploring a new dynamic pairwise learning methodology for Web portal content optimization in which we exploit dynamic user preferences extracted based on users' actions on portal services to compute the attractiveness scores of content items. In this article, we introduce two specific pairwise learning algorithms, a straightforward graph-based algorithm and a formalized Bayesian modeling one. Experiments on large-scale data from a commercial Web portal demonstrate the significant improvement of pairwise methodologies over the baseline pointwise models. Further analysis illustrates that our new pairwise learning approaches can benefit personalized recommendation more than pointwise models, since the data sparsity is more critical for personalized content optimization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {33},
numpages = {23},
keywords = {bayesian model, Content optimization, pairwise learning, user preference}
}

@article{10.1145/2490890,
author = {Lian, Defu and Xie, Xing},
title = {Mining Check-In History for Personalized Location Naming},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2490890},
doi = {10.1145/2490890},
abstract = {Many innovative location-based services have been established to offer users greater convenience in their everyday lives. These services usually cannot map user's physical locations into semantic names automatically. The semantic names of locations provide important context for mobile recommendations and advertisements. In this article, we proposed a novel location naming approach which can automatically provide semantic names for users given their locations and time. In particular, when a user opens a GPS device and submits a query with her physical location and time, she will be returned the most appropriate semantic name. In our approach, we drew an analogy between location naming and local search, and designed a local search framework to propose a spatiotemporal and user preference (STUP) model for location naming. STUP combined three components, user preference (UP), spatial preference (SP), and temporal preference (TP), by leveraging learning-to-rank techniques. We evaluated STUP on 466,190 check-ins of 5,805 users from Shanghai and 135,052 check-ins of 1,361 users from Beijing. The results showed that SP was most effective among three components and that UP can provide personalized semantic names, and thus it was a necessity for location naming. Although TP was not as discriminative as the others, it can still be beneficial when integrated with SP and UP. Finally, according to the experimental results, STUP outperformed the proposed baselines and returned accurate semantic names for 23.6% and 26.6% of the testing queries from Beijing and Shanghai, respectively.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {32},
numpages = {25},
keywords = {location-based social network, Location-based services, location naming}
}

@article{10.1145/2542182.2542203,
author = {Singh, Munindar P.},
title = {Norms as a Basis for Governing Sociotechnical Systems},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542203},
doi = {10.1145/2542182.2542203},
abstract = {We understand a sociotechnical system as a multistakeholder cyber-physical system. We introduce governance as the administration of such a system by the stakeholders themselves. In this regard, governance is a peer-to-peer notion and contrasts with traditional management, which is a top-down hierarchical notion. Traditionally, there is no computational support for governance and it is achieved through out-of-band interactions among system administrators. Not surprisingly, traditional approaches simply do not scale up to large sociotechnical systems.We develop an approach for governance based on a computational representation of norms in organizations. Our approach is motivated by the Ocean Observatory Initiative, a thirty-year $400 million project, which supports a variety of resources dealing with monitoring and studying the world's oceans. These resources include autonomous underwater vehicles, ocean gliders, buoys, and other instrumentation as well as more traditional computational resources. Our approach has the benefit of directly reflecting stakeholder needs and assuring stakeholders of the correctness of the resulting governance decisions while yielding adaptive resource allocation in the face of changes in both stakeholder needs and physical circumstances.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {21},
numpages = {23},
keywords = {Governance, sociotechnical systems, adaptation}
}

@article{10.1145/2542182.2542202,
author = {Chen, Tianshi and Chen, Yunji and Guo, Qi and Zhou, Zhi-Hua and Li, Ling and Xu, Zhiwei},
title = {Effective and Efficient Microprocessor Design Space Exploration Using Unlabeled Design Configurations},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542202},
doi = {10.1145/2542182.2542202},
abstract = {Ever-increasing design complexity and advances of technology impose great challenges on the design of modern microprocessors. One such challenge is to determine promising microprocessor configurations to meet specific design constraints, which is called Design Space Exploration (DSE). In the computer architecture community, supervised learning techniques have been applied to DSE to build regression models for predicting the qualities of design configurations. For supervised learning, however, considerable simulation costs are required for attaining the labeled design configurations. Given limited resources, it is difficult to achieve high accuracy. In this article, inspired by recent advances in semisupervised learning and active learning, we propose the COAL approach which can exploit unlabeled design configurations to significantly improve the models. Empirical study demonstrates that COAL significantly outperforms a state-of-the-art DSE technique by reducing mean squared error by 35% to 95%, and thus, promising architectures can be attained more efficiently.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {20},
numpages = {18},
keywords = {active learning, Design space exploration, microprocessor design, simulation, machine learning, semisupervised learning}
}

@article{10.1145/2542182.2542201,
author = {Shieh, Jyh-Ren and Lin, Ching-Yung and Wang, Shun-Xuan and Wu, Ja-Ling},
title = {Relational Term-Suggestion Graphs Incorporating Multipartite Concept and Expertise Networks},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542201},
doi = {10.1145/2542182.2542201},
abstract = {Term suggestions recommend query terms to a user based on his initial query. Suggesting adequate terms is a challenging issue. Most existing commercial search engines suggest search terms based on the frequency of prior used terms that match the leading alphabets the user types. In this article, we present a novel mechanism to construct semantic term-relation graphs to suggest relevant search terms in the semantic level. We built term-relation graphs based on multipartite networks of existing social media, especially from Wikipedia. The multipartite linkage networks of contributor-term, term-category, and term-term are extracted from Wikipedia to eventually form term relation graphs. For fusing these multipartite linkage networks, we propose to incorporate the contributor-category networks to model the expertise of the contributors. Based on our experiments, this step has demonstrated clear enhancement on the accuracy of the inferred relatedness of the term-semantic graphs. Experiments on keyword-expanded search based on 200 TREC-5 ad-hoc topics showed obvious advantage of our algorithms over existing approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {19},
numpages = {25},
keywords = {keyword expansion reranking, Social network}
}

@article{10.1145/2542182.2542200,
author = {Lu, Qiang and Huang, Ruoyun and Chen, Yixin and Xu, You and Zhang, Weixiong and Chen, Guoliang},
title = {A SAT-Based Approach to Cost-Sensitive Temporally Expressive Planning},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542200},
doi = {10.1145/2542182.2542200},
abstract = {Complex features, such as temporal dependencies and numerical cost constraints, are hallmarks of real-world planning problems. In this article, we consider the challenging problem of cost-sensitive temporally expressive (CSTE) planning, which requires concurrency of durative actions and optimization of action costs. We first propose a scheme to translate a CSTE planning problem to a minimum cost (MinCost) satisfiability (SAT) problem and to integrate with a relaxed parallel planning semantics for handling true temporal expressiveness. Our scheme finds solution plans that optimize temporal makespan, and also minimize total action costs at the optimal makespan. We propose two approaches for solving MinCost SAT. The first is based on a transformation of a MinCost SAT problem to a weighted partial Max-SAT (WPMax-SAT), and the second, called BB-CDCL, is an integration of the branch-and-bound technique and the conflict driven clause learning (CDCL) method. We also develop a CSTE customized variable branching scheme for BB-CDCL which can significantly improve the search efficiency. Our experiments on the existing CSTE benchmark domains show that our planner compares favorably to the state-of-the-art temporally expressive planners in both efficiency and quality.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {18},
numpages = {35},
keywords = {temporal expressiveness, satisfiability, numerical cost constraint, Planning}
}

@article{10.1145/2542182.2542199,
author = {Montali, Marco and Maggi, Fabrizio M. and Chesani, Federico and Mello, Paola and Aalst, Wil M. P. van der},
title = {Monitoring Business Constraints with the Event Calculus},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542199},
doi = {10.1145/2542182.2542199},
abstract = {Today, large business processes are composed of smaller, autonomous, interconnected subsystems, achieving modularity and robustness. Quite often, these large processes comprise software components as well as human actors, they face highly dynamic environments and their subsystems are updated and evolve independently of each other. Due to their dynamic nature and complexity, it might be difficult, if not impossible, to ensure at design-time that such systems will always exhibit the desired/expected behaviors. This, in turn, triggers the need for runtime verification and monitoring facilities. These are needed to check whether the actual behavior complies with expected business constraints, internal/external regulations and desired best practices. In this work, we present Mobucon EC, a novel monitoring framework that tracks streams of events and continuously determines the state of business constraints. In Mobucon EC, business constraints are defined using the declarative language Declare. For the purpose of this work, Declare has been suitably extended to support quantitative time constraints and non-atomic, durative activities. The logic-based language Event Calculus (EC) has been adopted to provide a formal specification and semantics to Declare constraints, while a light-weight, logic programming-based EC tool supports dynamically reasoning about partial, evolving execution traces. To demonstrate the applicability of our approach, we describe a case study about maritime safety and security and provide a synthetic benchmark to evaluate its scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {30},
keywords = {operational decision support, Business constraints, declarative process models, runtime verification, process mining, monitoring, event calculus}
}

@article{10.1145/2542182.2542198,
author = {Osman, Nardine and Sierra, Carles and Mcneill, Fiona and Pane, Juan and Debenham, John},
title = {Trust and Matching Algorithms for Selecting Suitable Agents},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542198},
doi = {10.1145/2542182.2542198},
abstract = {This article addresses the problem of finding suitable agents to collaborate with for a given interaction in distributed open systems, such as multiagent and P2P systems. The agent in question is given the chance to describe its confidence in its own capabilities. However, since agents may be malicious, misinformed, suffer from miscommunication, and so on, one also needs to calculate how much trusted is that agent. This article proposes a novel trust model that calculates the expectation about an agent's future performance in a given context by assessing both the agent's willingness and capability through the semantic comparison of the current context in question with the agent's performance in past similar experiences. The proposed mechanism for assessing trust may be applied to any real world application where past commitments are recorded and observations are made that assess these commitments, and the model can then calculate one's trust in another with respect to a future commitment by assessing the other's past performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {16},
numpages = {39},
keywords = {trust and reputation, Semantic matching}
}

@article{10.1145/2542182.2542197,
author = {Shi, Ziqiang and Han, Jiqing and Zheng, Tieran},
title = {Audio Classification with Low-Rank Matrix Representation Features},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542197},
doi = {10.1145/2542182.2542197},
abstract = {In this article, a novel framework based on trace norm minimization for audio classification is proposed. In this framework, both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization. For feature extraction, robust principle component analysis (robust PCA) via minimization a combination of the nuclear norm and the ℓ1-norm is used to extract low-rank matrix features which are robust to white noise and gross corruption for audio signal. These low-rank matrix features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems. For this linear classifier, most methods find the parameters, that is the weight matrix and bias in batch-mode, which makes it inefficient for large scale problems. In this article, we propose a parallel online framework using accelerated proximal gradient method. This framework has advantages in processing speed and memory cost. In addition, as a result of the regularization formulation of matrix classification, the Lipschitz constant was given explicitly, and hence the step size estimation of the general proximal gradient method was omitted, and this part of computing burden is saved in our approach. Extensive experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {15},
numpages = {17},
keywords = {proximal gradient, online learning, low-rank matrix, robust principle component analysis, matrix classification, trace norm minimization, Audio classification}
}

@article{10.1145/2542182.2542195,
author = {Elahi, Mehdi and Ricci, Francesco and Rubens, Neil},
title = {Active Learning Strategies for Rating Elicitation in Collaborative Filtering: A System-Wide Perspective},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542195},
doi = {10.1145/2542182.2542195},
abstract = {The accuracy of collaborative-filtering recommender systems largely depends on three factors: the quality of the rating prediction algorithm, and the quantity and quality of available ratings. While research in the field of recommender systems often concentrates on improving prediction algorithms, even the best algorithms will fail if they are fed poor-quality data during training, that is, garbage in, garbage out. Active learning aims to remedy this problem by focusing on obtaining better-quality data that more aptly reflects a user's preferences. However, traditional evaluation of active learning strategies has two major flaws, which have significant negative ramifications on accurately evaluating the system's performance (prediction error, precision, and quantity of elicited ratings). (1) Performance has been evaluated for each user independently (ignoring system-wide improvements). (2) Active learning strategies have been evaluated in isolation from unsolicited user ratings (natural acquisition).In this article we show that an elicited rating has effects across the system, so a typical user-centric evaluation which ignores any changes of rating prediction of other users also ignores these cumulative effects, which may be more influential on the performance of the system as a whole (system centric). We propose a new evaluation methodology and use it to evaluate some novel and state-of-the-art rating elicitation strategies. We found that the system-wide effectiveness of a rating elicitation strategy depends on the stage of the rating elicitation process, and on the evaluation measures (MAE, NDCG, and Precision). In particular, we show that using some common user-centric strategies may actually degrade the overall performance of a system. Finally, we show that the performance of many common active learning strategies changes significantly when evaluated concurrently with the natural acquisition of ratings in recommender systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {13},
numpages = {33},
keywords = {cold start, active learning, rating elicitation, Recommender systems}
}

@article{10.1145/2542182.2542194,
author = {Cagliero, Luca and Fiori, Alessandro and Grimaudo, Luigi},
title = {Personalized Tag Recommendation Based on Generalized Rules},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542194},
doi = {10.1145/2542182.2542194},
abstract = {Tag recommendation is focused on recommending useful tags to a user who is annotating a Web resource. A relevant research issue is the recommendation of additional tags to partially annotated resources, which may be based on either personalized or collective knowledge. However, since the annotation process is usually not driven by any controlled vocabulary, the collections of user-specific and collective annotations are often very sparse. Indeed, the discovery of the most significant associations among tags becomes a challenging task.This article presents a novel personalized tag recommendation system that discovers and exploits generalized association rules, that is, tag correlations holding at different abstraction levels, to identify additional pertinent tags to suggest. The use of generalized rules relevantly improves the effectiveness of traditional rule-based systems in coping with sparse tag collections, because: (i) correlations hidden at the level of individual tags may be anyhow figured out at higher abstraction levels and (ii) low-level tag associations discovered from collective data may be exploited to specialize high-level associations discovered in the user-specific context.The effectiveness of the proposed system has been validated against other personalized approaches on real-life and benchmark collections retrieved from the popular photo-sharing system Flickr.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {22},
keywords = {Tag recommendation, generalized association rule mining, Flickr}
}

@article{10.1145/2542182.2542193,
author = {Cruz, Juan David and Bothorel, C\'{e}cile and Poulet, Fran\c{c}ois},
title = {Community Detection and Visualization in Social Networks: Integrating Structural and Semantic Information},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542193},
doi = {10.1145/2542182.2542193},
abstract = {Due to the explosion of social networking and the information sharing among their users, the interest in analyzing social networks has increased over the recent years. Two general interests in this kind of studies are community detection and visualization. In the first case, most of the classic algorithms for community detection use only the structural information to identify groups, that is, how clusters are formed according to the topology of the relationships. However, these methods do not take into account any semantic information which could guide the clustering process, and which may add elements to conduct further analyses. In the second case most of the layout algorithms for clustered graphs have been designed to differentiate the groups within the graph, but they are not designed to analyze the interactions between such groups. Identifying these interactions gives an insight into the way different communities exchange messages or information, and allows the social network researcher to identify key actors, roles, and paths from one community to another.This article presents a novel model to use, in a conjoint way, the semantic information from the social network and its structural information to, first, find structurally and semantically related groups of nodes, and second, a layout algorithm for clustered graphs which divides the nodes into two types, one for nodes with edges connecting other communities and another with nodes connecting nodes only within their own community. With this division the visualization tool focuses on the connections between groups facilitating deep studies of augmented social networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {11},
numpages = {26},
keywords = {social network analysis, knowledge extraction, unsupervised learning, graph clustering, clustered graphs layout, community detection, Social networks}
}

@article{10.1145/2542182.2542192,
author = {Fire, Michael and Tenenboim-Chekina, Lena and Puzis, Rami and Lesser, Ofrit and Rokach, Lior and Elovici, Yuval},
title = {Computationally Efficient Link Prediction in a Variety of Social Networks},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542192},
doi = {10.1145/2542182.2542192},
abstract = {Online social networking sites have become increasingly popular over the last few years. As a result, new interdisciplinary research directions have emerged in which social network analysis methods are applied to networks containing hundreds of millions of users. Unfortunately, links between individuals may be missing either due to an imperfect acquirement process or because they are not yet reflected in the online network (i.e., friends in the real world did not form a virtual connection). The primary bottleneck in link prediction techniques is extracting the structural features required for classifying links. In this article, we propose a set of simple, easy-to-compute structural features that can be analyzed to identify missing links. We show that by using simple structural features, a machine learning classifier can successfully identify missing links, even when applied to a predicament of classifying links between individuals with at least one common friend. We also present a method for calculating the amount of data needed in order to build more accurate classifiers. The new Friends measure and Same community features we developed are shown to be good predictors for missing links. An evaluation experiment was performed on ten large social networks datasets: Academia.edu, DBLP, Facebook, Flickr, Flixster, Google+, Gowalla, TheMarker, Twitter, and YouTube. Our methods can provide social network site operators with the capability of helping users to find known, offline contacts and to discover new friends online. They may also be used for exposing hidden links in online social networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {10},
numpages = {25},
keywords = {YouTube, imbalanced dataset, Google+, DBLP, Academia.edu, Flickr, supervised learning, Twitter, social networks, Facebook, TheMarker Cafe, Link prediction, Flixster, training set size, hidden links}
}

@article{10.1145/2542182.2542191,
author = {Lee, Kyumin and Caverlee, James and Cheng, Zhiyuan and Sui, Daniel Z.},
title = {Campaign Extraction from Social Media},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542191},
doi = {10.1145/2542182.2542191},
abstract = {In this manuscript, we study the problem of detecting coordinated free text campaigns in large-scale social media. These campaigns—ranging from coordinated spam messages to promotional and advertising campaigns to political astro-turfing—are growing in significance and reach with the commensurate rise in massive-scale social systems. Specifically, we propose and evaluate a content-driven framework for effectively linking free text posts with common “talking points” and extracting campaigns from large-scale social media. Three of the salient features of the campaign extraction framework are: (i) first, we investigate graph mining techniques for isolating coherent campaigns from large message-based graphs; (ii) second, we conduct a comprehensive comparative study of text-based message correlation in message and user levels; and (iii) finally, we analyze temporal behaviors of various campaign types. Through an experimental study over millions of Twitter messages we identify five major types of campaigns—namely Spam, Promotion, Template, News, and Celebrity campaigns—and we show how these campaigns may be extracted with high precision and recall.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {9},
numpages = {28},
keywords = {campaign detection, Social media}
}

@article{10.1145/2542182.2542190,
author = {Arias, Marta and Arratia, Argimiro and Xuriguera, Ramon},
title = {Forecasting with Twitter Data},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542190},
doi = {10.1145/2542182.2542190},
abstract = {The dramatic rise in the use of social network platforms such as Facebook or Twitter has resulted in the availability of vast and growing user-contributed repositories of data. Exploiting this data by extracting useful information from it has become a great challenge in data mining and knowledge discovery. A recently popular way of extracting useful information from social network platforms is to build indicators, often in the form of a time series, of general public mood by means of sentiment analysis. Such indicators have been shown to correlate with a diverse variety of phenomena.In this article we follow this line of work and set out to assess, in a rigorous manner, whether a public sentiment indicator extracted from daily Twitter messages can indeed improve the forecasting of social, economic, or commercial indicators. To this end we have collected and processed a large amount of Twitter posts from March 2011 to the present date for two very different domains: stock market and movie box office revenue. For each of these domains, we build and evaluate forecasting models for several target time series both using and ignoring the Twitter-related data. If Twitter does help, then this should be reflected in the fact that the predictions of models that use Twitter-related data are better than the models that do not use this data. By systematically varying the models that we use and their parameters, together with other tuning factors such as lag or the way in which we build our Twitter sentiment index, we obtain a large dataset that allows us to test our hypothesis under different experimental conditions. Using a novel decision-tree-based technique that we call summary tree we are able to mine this large dataset and obtain automatically those configurations that lead to an improvement in the prediction power of our forecasting models. As a general result, we have seen that nonlinear models do take advantage of Twitter data when forecasting trends in volatility indices, while linear ones fail systematically when forecasting any kind of financial time series. In the case of predicting box office revenue trend, it is support vector machines that make best use of Twitter data. In addition, we conduct statistical tests to determine the relation between our Twitter time series and the different target time series.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {8},
numpages = {24},
keywords = {forecasting, Box office, sentiment index, Twitter, stock market}
}

@article{10.1145/2542182.2542189,
author = {Cataldi, Mario and Caro, Luigi Di and Schifanella, Claudio},
title = {Personalized Emerging Topic Detection Based on a Term Aging Model},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542189},
doi = {10.1145/2542182.2542189},
abstract = {Twitter is a popular microblogging service that acts as a ground-level information news flashes portal where people with different background, age, and social condition provide information about what is happening in front of their eyes. This characteristic makes Twitter probably the fastest information service in the world. In this article, we recognize this role of Twitter and propose a novel, user-aware topic detection technique that permits to retrieve, in real time, the most emerging topics of discussion expressed by the community within the interests of specific users. First, we analyze the topology of Twitter looking at how the information spreads over the network, taking into account the authority/influence of each active user. Then, we make use of a novel term aging model to compute the burstiness of each term, and provide a graph-based method to retrieve the minimal set of terms that can represent the corresponding topic. Finally, since any user can have topic preferences inferable from the shared content, we leverage such knowledge to highlight the most emerging topics within her foci of interest. As evaluation we then provide several experiments together with a user study proving the validity and reliability of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {7},
numpages = {27},
keywords = {topic detection and tracking, trends, Social network analysis, Twitter, aging theory}
}

@article{10.1145/2542182.2542188,
author = {He, Yulan and Lin, Chenghua and Gao, Wei and Wong, Kam-Fai},
title = {Dynamic Joint Sentiment-Topic Model},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542188},
doi = {10.1145/2542182.2542188},
abstract = {Social media data are produced continuously by a large and uncontrolled number of users. The dynamic nature of such data requires the sentiment and topic analysis model to be also dynamically updated, capturing the most recent language use of sentiments and topics in text. We propose a dynamic Joint Sentiment-Topic model (dJST) which allows the detection and tracking of views of current and recurrent interests and shifts in topic and sentiment. Both topic and sentiment dynamics are captured by assuming that the current sentiment-topic-specific word distributions are generated according to the word distributions at previous epochs. We study three different ways of accounting for such dependency information: (1) sliding window where the current sentiment-topic word distributions are dependent on the previous sentiment-topic-specific word distributions in the last S epochs; (2) skip model where history sentiment topic word distributions are considered by skipping some epochs in between; and (3) multiscale model where previous long- and short- timescale distributions are taken into consideration. We derive efficient online inference procedures to sequentially update the model with newly arrived data and show the effectiveness of our proposed model on the Mozilla add-on reviews crawled between 2007 and 2011.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {6},
numpages = {21},
keywords = {opinion mining, Dynamic joint sentiment-topic model, sentiment analysis, topic model}
}

@article{10.1145/2542182.2542187,
author = {Bonchi, Francesco and Buntine, Wray and Gavald\'{a}, Ricard and Guo, Shengbo},
title = {Introduction to the Special Issue on Social Web Mining},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542187},
doi = {10.1145/2542182.2542187},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {5},
numpages = {2}
}

@article{10.1145/2542182.2542186,
author = {Doo, Myungcheol and Liu, Ling},
title = {Mondrian Tree: A Fast Index for Spatial Alarm Processing},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542186},
doi = {10.1145/2542182.2542186},
abstract = {With ubiquitous wireless connectivity and technological advances in mobile devices, we witness the growing demands and increasing market shares of mobile intelligent systems and technologies for real-time decision making and location-based knowledge discovery. Spatial alarms are considered as one of the fundamental capabilities for intelligent mobile location-based systems. Like time-based alarms that remind us the arrival of a future time point, spatial alarms remind us the arrival of a future spatial point. Existing approaches for scaling spatial alarm processing are focused on computing Alarm-Free Regions (Afr) and Alarm-Free Period (Afp) such that mobile objects traveling within an Afr can safely hibernate the alarm evaluation process for the computed Afp, to save battery power, until approaching the nearest alarm of interest. A key technical challenge in scaling spatial alarm processing is to efficiently compute Afr and Afp such that mobile objects traveling within an Afr can safely hibernate the alarm evaluation process during the computed Afp, while maintaining high accuracy. In this article we argue that on-demand computation of Afr is expensive and may not scale well for dense populations of mobile objects. Instead, we propose to maintain an index for both spatial alarms and empty regions (Afr) such that for a given mobile user's location, we can find relevant spatial alarms and whether it is in an alarm-free region more efficiently. We also show that conventional spatial indexing methods, such as R-tree family, k-d tree, Quadtree, and Grid, are by design not well suited to index empty regions. We present Mondrian Tree – a region partitioning tree for indexing both spatial alarms and alarm-free regions. We first introduce the Mondrian Tree indexing algorithms, including index construction, search, and maintenance. Then we describe a suite of Mondrian Tree optimizations to further enhance the performance of spatial alarm processing. Our experimental evaluation shows that the Mondrian Tree index, as an intelligent technology for mobile systems, outperforms traditional index methods, such as R-tree, Quadtree, and k-d tree, for spatial alarm processing.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {4},
numpages = {25},
keywords = {spatial index, Location-based systems, spatial query processing}
}

@article{10.1145/2542182.2542185,
author = {Tang, Lu-An and Zheng, Yu and Yuan, Jing and Han, Jiawei and Leung, Alice and Peng, Wen-Chih and Porta, Thomas La},
title = {A Framework of Traveling Companion Discovery on Trajectory Data Streams},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542185},
doi = {10.1145/2542182.2542185},
abstract = {The advance of mobile technologies leads to huge volumes of spatio-temporal data collected in the form of trajectory data streams. In this study, we investigate the problem of discovering object groups that travel together (i.e., traveling companions) from trajectory data streams. Such technique has broad applications in the areas of scientific study, transportation management, and military surveillance. To discover traveling companions, the monitoring system should cluster the objects of each snapshot and intersect the clustering results to retrieve moving-together objects. Since both clustering and intersection steps involve high computational overhead, the key issue of companion discovery is to improve the efficiency of algorithms. We propose the models of closed companion candidates and smart intersection to accelerate data processing. A data structure termed traveling buddy is designed to facilitate scalable and flexible companion discovery from trajectory streams. The traveling buddies are microgroups of objects that are tightly bound together. By only storing the object relationships rather than their spatial coordinates, the buddies can be dynamically maintained along the trajectory stream with low cost. Based on traveling buddies, the system can discover companions without accessing the object details. In addition, we extend the proposed framework to discover companions on more complicated scenarios with spatial and temporal constraints, such as on the road network and battlefield. The proposed methods are evaluated with extensive experiments on both real and synthetic datasets. Experimental results show that our proposed buddy-based approach is an order of magnitude faster than the baselines and achieves higher accuracy in companion discovery.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {3},
numpages = {34},
keywords = {data stream, Trajectory, clustering}
}

@article{10.1145/2542182.2542184,
author = {Ying, Josh Jia-Ching and Lee, Wang-Chien and Tseng, Vincent S.},
title = {Mining Geographic-Temporal-Semantic Patterns in Trajectories for Location Prediction},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542184},
doi = {10.1145/2542182.2542184},
abstract = {In recent years, research on location predictions by mining trajectories of users has attracted a lot of attention. Existing studies on this topic mostly treat such predictions as just a type of location recommendation, that is, they predict the next location of a user using location recommenders. However, an user usually visits somewhere for reasons other than interestingness. In this article, we propose a novel mining-based location prediction approach called Geographic-Temporal-Semantic-based Location Prediction (GTS-LP), which takes into account a user's geographic-triggered intentions, temporal-triggered intentions, and semantic-triggered intentions, to estimate the probability of the user in visiting a location. The core idea underlying our proposal is the discovery of trajectory patterns of users, namely GTS patterns, to capture frequent movements triggered by the three kinds of intentions. To achieve this goal, we define a new trajectory pattern to capture the key properties of the behaviors that are motivated by the three kinds of intentions from trajectories of users. In our GTS-LP approach, we propose a series of novel matching strategies to calculate the similarity between the current movement of a user and discovered GTS patterns based on various moving intentions. On the basis of similitude, we make an online prediction as to the location the user intends to visit. To the best of our knowledge, this is the first work on location prediction based on trajectory pattern mining that explores the geographic, temporal, and semantic properties simultaneously. By means of a comprehensive evaluation using various real trajectory datasets, we show that our proposed GTS-LP approach delivers excellent performance and significantly outperforms existing state-of-the-art location prediction methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {2},
numpages = {33},
keywords = {frequent movement patterns, semantic trajectory, Trajectory mining, location prediction}
}

@article{10.1145/2542182.2542183,
author = {Xiong, Dr. Hui and Shekhar, Dr. Shashi and Tuzhilin, Dr. Alexander},
title = {Introduction to Special Section on Intelligent Mobile Knowledge Discovery and Management Systems},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2542182.2542183},
doi = {10.1145/2542182.2542183},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {1},
numpages = {2}
}

@article{10.1145/2535526,
author = {Meo, Pasquale de and Ferrara, Emilio and Abel, Fabian and Aroyo, Lora and Houben, Geert-Jan},
title = {Analyzing User Behavior across Social Sharing Environments},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2535526},
doi = {10.1145/2535526},
abstract = {In this work we present an in-depth analysis of the user behaviors on different Social Sharing systems. We consider three popular platforms, Flickr, Delicious and StumbleUpon, and, by combining techniques from social network analysis with techniques from semantic analysis, we characterize the tagging behavior as well as the tendency to create friendship relationships of the users of these platforms. The aim of our investigation is to see if (and how) the features and goals of a given Social Sharing system reflect on the behavior of its users and, moreover, if there exists a correlation between the social and tagging behavior of the users. We report our findings in terms of the characteristics of user profiles according to three different dimensions: (i) intensity of user activities, (ii) tag-based characteristics of user profiles, and (iii) semantic characteristics of user profiles.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {14},
numpages = {31},
keywords = {User modeling, Social networks, Social systems, Semantic analysis, Folksonomies}
}

@article{10.1145/2508037.2508055,
author = {Chen, Bin and Su, Jian and Tan, Chew Lim},
title = {Random Walks down the Mention Graphs for Event Coreference Resolution},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508055},
doi = {10.1145/2508037.2508055},
abstract = {Event coreference is an important task in event extraction and other natural language processing tasks. Despite its importance, it was merely discussed in previous studies. In this article, we present a global coreference resolution system dedicated to various sophisticated event coreference phenomena. First, seven resolvers are utilized to resolve different event and object coreference mention pairs with a new instance selection strategy and new linguistic features. Second, a global solution—a modified random walk partitioning—is employed for the chain formation. Being the first attempt to apply the random walk model for coreference resolution, the revised model utilizes a sampling method, termination criterion, and stopping probability to greatly improve the effectiveness of random walk model for event coreference resolution. Last but not least, the new model facilitates a convenient way to incorporate sophisticated linguistic constraints and preferences, the related object mention graph, as well as pronoun coreference information not used in previous studies for effective chain formation. In total, these techniques impose more than 20% F-score improvement over the baseline system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {74},
numpages = {20},
keywords = {Coreference resolution, competing classifiers, self-interacting walks, event coreference, instance selection, anaphora resolution, random walks partitioning}
}

@article{10.1145/2508037.2508054,
author = {Subbu, Kalyan Pathapati and Gozick, Brandon and Dantu, Ram},
title = {LocateMe: Magnetic-Fields-Based Indoor Localization Using Smartphones},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508054},
doi = {10.1145/2508037.2508054},
abstract = {Fine-grained localization is extremely important to accurately locate a user indoors. Although innovative solutions have already been proposed, there is no solution that is universally accepted, easily implemented, user centric, and, most importantly, works in the absence of GSM coverage or WiFi availability. The advent of sensor rich smartphones has paved a way to develop a solution that can cater to these requirements.By employing a smartphone's built-in magnetic field sensor, magnetic signatures were collected inside buildings. These signatures displayed a uniqueness in their patterns due to the presence of different kinds of pillars, doors, elevators, etc., that consist of ferromagnetic materials like steel or iron. We theoretically analyze the cause of this uniqueness and then present an indoor localization solution by classifying signatures based on their patterns. However, to account for user walking speed variations so as to provide an application usable to a variety of users, we follow a dynamic time-warping-based approach that is known to work on similar signals irrespective of their variations in the time axis.Our approach resulted in localization distances of approximately 2m--6m with accuracies between 80--100% implying that it is sufficient to walk short distances across hallways to be located by the smartphone. The implementation of the application on different smartphones yielded response times of less than five secs, thereby validating the feasibility of our approach and making it a viable solution.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {73},
numpages = {27},
keywords = {smartphones, Indoor localization, ubiquitous, magnetic fields}
}

@article{10.1145/2508037.2508053,
author = {Bi, Jinbo and Sun, Jiangwen and Wu, Yu and Tennen, Howard and Armeli, Stephen},
title = {A Machine Learning Approach to College Drinking Prediction and Risk Factor Identification},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508053},
doi = {10.1145/2508037.2508053},
abstract = {Alcohol misuse is one of the most serious public health problems facing adolescents and young adults in the United States. National statistics shows that nearly 90% of alcohol consumed by youth under 21 years of age involves binge drinking and 44% of college students engage in high-risk drinking activities. Conventional alcohol intervention programs, which aim at installing either an alcohol reduction norm or prohibition against underage drinking, have yielded little progress in controlling college binge drinking over the years. Existing alcohol studies are deductive where data are collected to investigate a psychological/behavioral hypothesis, and statistical analysis is applied to the data to confirm the hypothesis. Due to this confirmatory manner of analysis, the resulting statistical models are cohort-specific and typically fail to replicate on a different sample. This article presents two machine learning approaches for a secondary analysis of longitudinal data collected in college alcohol studies sponsored by the National Institute on Alcohol Abuse and Alcoholism. Our approach aims to discover knowledge, from multiwave cohort-sequential daily data, which may or may not align with the original hypothesis but quantifies predictive models with higher likelihood to generalize to new samples. We first propose a so-called temporally-correlated support vector machine to construct a classifier as a function of daily moods, stress, and drinking expectancies to distinguish days with nighttime binge drinking from days without for individual students. We then propose a combination of cluster analysis and feature selection, where cluster analysis is used to identify drinking patterns based on averaged daily drinking behavior and feature selection is used to identify risk factors associated with each pattern. We evaluate our methods on two cohorts of 530 total college students recruited during the Spring and Fall semesters, respectively. Cross validation on these two cohorts and further on 100 random partitions of the total students demonstrate that our methods improve the model generalizability in comparison with traditional multilevel logistic regression. The discovered risk factors and the interaction of these factors delineated in our models can set a potential basis and offer insights to a new design of more effective college alcohol interventions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {72},
numpages = {24},
keywords = {longitudinal data analysis, college student alcohol consumption, feature selection, clustering, Machine learning}
}

@article{10.1145/2508037.2508052,
author = {Lee, Yugyung and Krishnamoorthy, Saranya and Dinakarpandian, Deendayal},
title = {A Semantic Framework for Intelligent Matchmaking for Clinical Trial Eligibility Criteria},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508052},
doi = {10.1145/2508037.2508052},
abstract = {An integral step in the discovery of new treatments for medical conditions is the matching of potential subjects with appropriate clinical trials. Eligibility criteria for clinical trials are typically specified as inclusion and exclusion criteria for each study in freetext form. While this is sufficient for a human to guide a recruitment interview, it cannot be reliably and computationally construed to identify potential subjects. Standardization of the representation of eligibility criteria can enhance the efficiency and accuracy of this process. This article presents a semantic framework that facilitates intelligent matchmaking by identifying a minimal set of eligibility criteria with maximal coverage of clinical trials. In contrast to existing top-down manual standardization efforts, a bottom-up data driven approach is presented to find a canonical nonredundant representation of an arbitrary collection of clinical trial criteria. The methodology has been validated with a corpus of 709 clinical trials related to Generalized Anxiety Disorder containing 2,760 inclusion and 4,871 exclusion eligibility criteria. This corpus is well represented by a relatively small number of 126 inclusion clusters and 175 exclusion clusters, each of which corresponds to a semantically distinct criterion. Internal and external validation measures provide an objective evaluation of the method. An eligibility criteria ontology has been constructed based on the clustering. The resulting model has been incorporated into the development of the MindTrial clinical trial recruiting system. The prototype for clinical trial recruitment illustrates the effectiveness of the methodology in characterizing clinical trials and subjects and accurate matching between them.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {32},
keywords = {search engine, Clustering, ontology to DB mapping, eligibility criteria, ontology, clinical trials, intelligent matchmaking}
}

@article{10.1145/2508037.2508051,
author = {Baralis, Elena and Cerquitelli, Tania and Chiusano, Silvia and D'elia, Vincenzo and Molinari, Riccardo and Susta, Davide},
title = {Early Prediction of the Highest Workload in Incremental Cardiopulmonary Tests},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508051},
doi = {10.1145/2508037.2508051},
abstract = {Incremental tests are widely used in cardiopulmonary exercise testing, both in the clinical domain and in sport sciences. The highest workload (denoted Wpeak) reached in the test is key information for assessing the individual body response to the test and for analyzing possible cardiac failures and planning rehabilitation, and training sessions. Being physically very demanding, incremental tests can significantly increase the body stress on monitored individuals and may cause cardiopulmonary overload. This article presents a new approach to cardiopulmonary testing that addresses these drawbacks. During the test, our approach analyzes the individual body response to the exercise and predicts the Wpeak value that will be reached in the test and an evaluation of its accuracy. When the accuracy of the prediction becomes satisfactory, the test can be prematurely stopped, thus avoiding its entire execution. To predict Wpeak, we introduce a new index, the CardioPulmonary Efficiency Index (CPE), summarizing the cardiopulmonary response of the individual to the test. Our approach analyzes the CPE trend during the test, together with the characteristics of the individual, and predicts Wpeak. A K-nearest-neighbor-based classifier and an ANN-based classier are exploited for the prediction. The experimental evaluation showed that the Wpeak value can be predicted with a limited error from the first steps of the test.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {70},
numpages = {20},
keywords = {physiological signals analysis, highest workload prediction, multivariate data, classification techniques, Incremental test}
}

@article{10.1145/2508037.2508050,
author = {Lee, Suk Jin and Motai, Yuichi and Weiss, Elisabeth and Sun, Shumei S.},
title = {Customized Prediction of Respiratory Motion with Clustering from Multiple Patient Interaction},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508050},
doi = {10.1145/2508037.2508050},
abstract = {Information processing of radiotherapy systems has become an important research area for sophisticated radiation treatment methodology. Geometrically precise delivery of radiotherapy in the thorax and upper abdomen is compromised by respiratory motion during treatment. Accurate prediction of the respiratory motion would be beneficial for improving tumor targeting. However, a wide variety of breathing patterns can make it difficult to predict the breathing motion with explicit models. We proposed a respiratory motion predictor, that is, customized prediction with multiple patient interactions using neural network (CNN). For the preprocedure of prediction for individual patient, we construct the clustering based on breathing patterns of multiple patients using the feature selection metrics that are composed of a variety of breathing features. In the intraprocedure, the proposed CNN used neural networks (NN) for a part of the prediction and the extended Kalman filter (EKF) for a part of the correction. The prediction accuracy of the proposed method was investigated with a variety of prediction time horizons using normalized root mean squared error (NRMSE) values in comparison with the alternate recurrent neural network (RNN). We have also evaluated the prediction accuracy using the marginal value that can be used as the reference value to judge how many signals lie outside the confidence level. The experimental results showed that the proposed CNN can outperform RNN with respect to the prediction accuracy with an improvement of 50%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {69},
numpages = {17},
keywords = {respiratory motion, Recurrent neural networks, intelligent systems, multilayer perceptron, medical signal analysis, breathing prediction}
}

@article{10.1145/2508037.2508049,
author = {Khan, Atif and Doucette, John A. and Cohen, Robin},
title = {Validation of an Ontological Medical Decision Support System for Patient Treatment Using a Repository of Patient Data: Insights into the Value of Machine Learning},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508049},
doi = {10.1145/2508037.2508049},
abstract = {In this article, we begin by presenting OMeD, a medical decision support system, and argue for its value over purely probabilistic approaches that reason about patients for time-critical decision scenarios. We then progress to present Holmes, a Hybrid Ontological and Learning MEdical System which supports decision making about patient treatment. This system is introduced in order to cope with the case of missing data. We demonstrate its effectiveness by operating on an extensive set of real-world patient health data from the CDC, applied to the decision-making scenario of administering sleeping pills. In particular, we clarify how the combination of semantic, ontological representations, and probabilistic reasoning together enable the proposal of effective patient treatments. Our focus is thus on presenting an approach for interpreting medical data in the context of real-time decision making. This constitutes a comprehensive framework for the design of medical recommendation systems for potential use by medical professionals and patients both, with the end result being personalized patient treatment. We conclude with a discussion of the value of our particular approach for such diverse considerations as coping with misinformation provided by patients, performing effectively in time-critical environments where real-time decisions are necessary, and potential applications facilitating patient information gathering.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {68},
numpages = {31},
keywords = {machine learning, Medical decision support system, automated knowledge inference, ontology-based knowledge representation}
}

@article{10.1145/2508037.2508048,
author = {Hoens, T. Ryan and Blanton, Marina and Steele, Aaron and Chawla, Nitesh V.},
title = {Reliable Medical Recommendation Systems with Patient Privacy},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508048},
doi = {10.1145/2508037.2508048},
abstract = {One of the concerns patients have when confronted with a medical condition is which physician to trust. Any recommendation system that seeks to answer this question must ensure that any sensitive medical information collected by the system is properly secured. In this article, we codify these privacy concerns in a privacy-friendly framework and present two architectures that realize it: the Secure Processing Architecture (SPA) and the Anonymous Contributions Architecture (ACA). In SPA, patients submit their ratings in a protected form without revealing any information about their data and the computation of recommendations proceeds over the protected data using secure multiparty computation techniques. In ACA, patients submit their ratings in the clear, but no link between a submission and patient data can be made. We discuss various aspects of both architectures, including techniques for ensuring reliability of computed recommendations and system performance, and provide their comparison.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {67},
numpages = {31},
keywords = {framework, Recommendation systems, privacy}
}

@article{10.1145/2508037.2508047,
author = {Park, Yubin and Ghosh, Joydeep},
title = {CUDIA: Probabilistic Cross-Level Imputation Using Individual Auxiliary Information},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508047},
doi = {10.1145/2508037.2508047},
abstract = {In healthcare-related studies, individual patient or hospital data are not often publicly available due to privacy restrictions, legal issues, or reporting norms. However, such measures may be provided at a higher or more aggregated level, such as state-level, county-level summaries or averages over health zones, such as hospital referral regions (HRR) or hospital service areas (HSA). Such levels constitute partitions over the underlying individual level data, which may not match the groupings that would have been obtained if one clustered the data based on individual-level attributes. Moreover, treating aggregated values as representatives for the individuals can result in the ecological fallacy. How can one run data mining procedures on such data where different variables are available at different levels of aggregation or granularity? In this article, we seek a better utilization of variably aggregated datasets, which are possibly assembled from different sources. We propose a novel cross-level imputation technique that models the generative process of such datasets using a Bayesian directed graphical model. The imputation is based on the underlying data distribution and is shown to be unbiased. This imputation can be further utilized in a subsequent predictive modeling, yielding improved accuracies. The experimental results using a simulated dataset and the Behavioral Risk Factor Surveillance System (BRFSS) dataset are provided to illustrate the generality and capabilities of the proposed framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {66},
numpages = {24},
keywords = {Clustering, BRFSS, privacy preserving data mining}
}

@article{10.1145/2508037.2508046,
author = {Wolf, Hannes and Herrmann, Klaus and Rothermel, Kurt},
title = {Dealing with Uncertainty: Robust Workflow Navigation in the Healthcare Domain},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508046},
doi = {10.1145/2508037.2508046},
abstract = {Processes in the healthcare domain are characterized by coarsely predefined recurring procedures that are flexibly adapted by the personnel to suite-specific situations. In this setting, a workflow management system that gives guidance and documents the personnel's actions can lead to a higher quality of care, fewer mistakes, and higher efficiency. However, most existing workflow management systems enforce rigid inflexible workflows and rely on direct manual input. Both are inadequate for healthcare processes. In particular, direct manual input is not possible in most cases since (1) it would distract the personnel even in critical situations and (2) it would violate fundamental hygiene principles by requiring disinfected doctors and nurses to touch input devices. The solution could be activity recognition systems that use sensor data (e.g., audio and acceleration data) to infer the current activities by the personnel and provide input to a workflow (e.g., informing it that a certain activity is finished now). However, state-of-the-art activity recognition technologies have difficulties in providing reliable information.We describe a comprehensive framework tailored for flexible human-centric healthcare processes that improves the reliability of activity recognition data. We present a set of mechanisms that exploit the application knowledge encoded in workflows in order to reduce the uncertainty of this data, thus enabling unobtrusive robust healthcare workflows. We evaluate our work based on a real-world case study and show that the robustness of unobtrusive healthcare workflows can be increased to an absolute value of up to 91% (compared to only 12% with a classical workflow system). This is a major breakthrough that paves the way towards future IT-enabled healthcare systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {65},
numpages = {23},
keywords = {business process management, workflow mining, Activity recognition, subjective logic, particle filters, Bayesian Networks, uncertain real-world context}
}

@article{10.1145/2508037.2508045,
author = {Rashidi, Parisa and Cook, Diane J.},
title = {COM: A Method for Mining and Monitoring Human Activity Patterns in Home-Based Health Monitoring Systems},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508045},
doi = {10.1145/2508037.2508045},
abstract = {The increasing aging population in the coming decades will result in many complications for society and in particular for the healthcare system due to the shortage of healthcare professionals and healthcare facilities. To remedy this problem, researchers have pursued developing remote monitoring systems and assisted living technologies by utilizing recent advances in sensor and networking technology, as well as in the data mining and machine learning fields. In this article, we report on our fully automated approach for discovering and monitoring patterns of daily activities. Discovering and tracking patterns of daily activities can provide unprecedented opportunities for health monitoring and assisted living applications, especially for older adults and individuals with mental disabilities. Previous approaches usually rely on preselected activities or labeled data to track and monitor daily activities. In this article, we present a fully automated approach by discovering natural activity patterns and their variations in real-life data. We will show how our activity discovery component can be integrated with an activity recognition component to track and monitor various daily activity patterns. We also provide an activity visualization component to allow caregivers to visually observe and examine the activity patterns using a user-friendly interface. We validate our algorithms using real-life data obtained from two apartments during a three-month period.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {64},
numpages = {20},
keywords = {smart environments, health monitoring, sequence mining, Assisted living technology}
}

@article{10.1145/2508037.2508044,
author = {Batal, Iyad and Valizadegan, Hamed and Cooper, Gregory F. and Hauskrecht, Milos},
title = {A Temporal Pattern Mining Approach for Classifying Electronic Health Record Data},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508044},
doi = {10.1145/2508037.2508044},
abstract = {We study the problem of learning classification models from complex multivariate temporal data encountered in electronic health record systems. The challenge is to define a good set of features that are able to represent well the temporal aspect of the data. Our method relies on temporal abstractions and temporal pattern mining to extract the classification features. Temporal pattern mining usually returns a large number of temporal patterns, most of which may be irrelevant to the classification task. To address this problem, we present the Minimal Predictive Temporal Patterns framework to generate a small set of predictive and nonspurious patterns. We apply our approach to the real-world clinical task of predicting patients who are at risk of developing heparin-induced thrombocytopenia. The results demonstrate the benefit of our approach in efficiently learning accurate classifiers, which is a key step for developing intelligent clinical monitoring systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {63},
numpages = {22},
keywords = {time-interval patterns, classification, Temporal pattern mining, temporal abstractions, multivariate time series}
}

@article{10.1145/2508037.2508043,
author = {Reddy, Chandan K. and Yang, Cristopher C.},
title = {Introduction to the Special Section on Intelligent Systems for Health Informatics},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508043},
doi = {10.1145/2508037.2508043},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {3}
}

@article{10.1145/2508037.2508042,
author = {Chen, Chao and Zhu, Qiusha and Lin, Lin and Shyu, Mei-Ling},
title = {Web Media Semantic Concept Retrieval via Tag Removal and Model Fusion},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508042},
doi = {10.1145/2508037.2508042},
abstract = {Multimedia data on social websites contain rich semantics and are often accompanied with user-defined tags. To enhance Web media semantic concept retrieval, the fusion of tag-based and content-based models can be used, though it is very challenging. In this article, a novel semantic concept retrieval framework that incorporates tag removal and model fusion is proposed to tackle such a challenge. Tags with useful information can facilitate media search, but they are often imprecise, which makes it important to apply noisy tag removal (by deleting uncorrelated tags) to improve the performance of semantic concept retrieval. Therefore, a multiple correspondence analysis (MCA)-based tag removal algorithm is proposed, which utilizes MCA's ability to capture the relationships among nominal features and identify representative and discriminative tags holding strong correlations with the target semantic concepts. To further improve the retrieval performance, a novel model fusion method is also proposed to combine ranking scores from both tag-based and content-based models, where the adjustment of ranking scores, the reliability of models, and the correlations between the intervals divided on the ranking scores and the semantic concepts are all considered. Comparative results with extensive experiments on the NUS-WIDE-LITE as well as the NUS-WIDE-270K benchmark datasets with 81 semantic concepts show that the proposed framework outperforms baseline results and the other comparison methods with each component being evaluated separately.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {61},
numpages = {22},
keywords = {multimedia semantic concept retrieval, multiple correspondence analysis (MCA), noisy tag removal, Social tags, model fusion}
}

@article{10.1145/2508037.2508041,
author = {Biancalana, Claudio and Gasparetti, Fabio and Micarelli, Alessandro and Sansonetti, Giuseppe},
title = {Social Semantic Query Expansion},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508041},
doi = {10.1145/2508037.2508041},
abstract = {Weak semantic techniques rely on the integration of Semantic Web techniques with social annotations and aim to embrace the strengths of both. In this article, we propose a novel weak semantic technique for query expansion. Traditional query expansion techniques are based on the computation of two-dimensional co-occurrence matrices. Our approach proposes the use of three-dimensional matrices, where the added dimension is represented by semantic classes (i.e., categories comprising all the terms that share a semantic property) related to the folksonomy extracted from social bookmarking services, such as delicious and StumbleUpon. The results of an indepth experimental evaluation performed on both artificial datasets and real users show that our approach outperforms traditional techniques, such as relevance feedback and personalized PageRank, so confirming the validity and usefulness of the categorization of the user needs and preferences in semantic classes. We also present the results of a questionnaire aimed to know the users opinion regarding the system. As one drawback of several query expansion techniques is their high computational costs, we also provide a complexity analysis of our system, in order to show its capability of operating in real time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {60},
numpages = {43},
keywords = {query expansion, Social Semantic Web, information retrieval}
}

@article{10.1145/2508037.2508039,
author = {Li, Xi and Hu, Weiming and Shen, Chunhua and Zhang, Zhongfei and Dick, Anthony and Hengel, Anton Van Den},
title = {A Survey of Appearance Models in Visual Object Tracking},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508039},
doi = {10.1145/2508037.2508039},
abstract = {Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models.To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic.The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {58},
numpages = {48},
keywords = {Visual object tracking, appearance model, features, statistical modeling}
}

@article{10.1145/2508037.2508038,
author = {Jiang, Daxin and Pei, Jian and Li, Hang},
title = {Mining Search and Browse Logs for Web Search: A Survey},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508038},
doi = {10.1145/2508037.2508038},
abstract = {Huge amounts of search log data have been accumulated at Web search engines. Currently, a popular Web search engine may receive billions of queries and collect terabytes of records about user search behavior daily. Beside search log data, huge amounts of browse log data have also been collected through client-side browser plugins. Such massive amounts of search and browse log data provide great opportunities for mining the wisdom of crowds and improving Web search. At the same time, designing effective and efficient methods to clean, process, and model log data also presents great challenges.In this survey, we focus on mining search and browse log data for Web search. We start with an introduction to search and browse log data and an overview of frequently-used data summarizations in log mining. We then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. For each aspect, we survey the major tasks, fundamental principles, and state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {57},
numpages = {37},
keywords = {query understanding, user understanding, browse log, Search logs, feedbacks, survey, monitoring, document understanding, Web search, document ranking, log mining}
}

@article{10.1145/2501603,
author = {Cena, Federica and Dattolo, Antonina and Lops, Pasquale and Vassileva, Julita},
title = {Perspectives in Semantic Adaptive Social Web},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2501603},
doi = {10.1145/2501603},
abstract = {The Social Web is now a successful reality with its quickly growing number of users and applications. Also the Semantic Web, which started with the objective of describing Web resources in a machine-processable way, is now outgrowing the research labs and is being massively exploited in many websites, incorporating high-quality user-generated content and semantic annotations. The primary goal of this special section is to showcase some recent research at the intersection of the Social Web and the Semantic Web that explores the benefits that adaptation and personalization have to offer in the Web of the future, the so-called Social Adaptive Semantic Web. We have selected two articles out of fourteen submissions based on the quality of the articles and we present the main lessons learned from the overall analysis of these submissions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {59},
numpages = {8},
keywords = {Semantic Web, adaptation, Social Web}
}

@article{10.1145/2483669.2483689,
author = {Marathe, Achla and Pan, Zhengzheng and Apolloni, Andrea},
title = {Analysis of Friendship Network and Its Role in Explaining Obesity},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483689},
doi = {10.1145/2483669.2483689},
abstract = {We employ Add Health data to show that friendship networks, constructed from mutual friendship nominations, are important in building weight perception, setting weight goals, and measuring social marginalization among adolescents and young adults. We study the relationship between individuals' perceived weight status, actual weight status, weight status relative to friends' weight status, and weight goals. This analysis helps us understand how individual weight perceptions might be formed, what these perceptions do to the weight goals, and how friends' relative weight affects weight perception and weight goals. Combining this information with individuals' friendship network helps determine the influence of social relationships on weight-related variables. Multinomial logistic regression results indicate that relative status is indeed a significant predictor of perceived status, and perceived status is a significant predictor of weight goals. We also address the issue of causality between actual weight status and social marginalization (as measured by the number of friends) and show that obesity precedes social marginalization in time rather than the other way around. This lends credence to the hypothesis that obesity leads to social marginalization not vice versa. Attributes of the friendship network can provide new insights into effective interventions for combating obesity since adolescent friendships provide an important social context for weight-related behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {56},
numpages = {21},
keywords = {friendship network, relative weight, obesity, Add Health, causality, perceived weight}
}

@article{10.1145/2483669.2483688,
author = {Saito, Kazumi and Kimura, Masahiro and Ohara, Kouzou and Motoda, Hiroshi},
title = {Detecting Changes in Information Diffusion Patterns over Social Networks},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483688},
doi = {10.1145/2483669.2483688},
abstract = {We addressed the problem of detecting the change in behavior of information diffusion over a social network which is caused by an unknown external situation change using a small amount of observation data in a retrospective setting. The unknown change is assumed effectively reflected in changes in the parameter values in the probabilistic information diffusion model, and the problem is reduced to detecting where in time and how long this change persisted and how big this change is. We solved this problem by searching the change pattern that maximizes the likelihood of generating the observed information diffusion sequences, and in doing so we devised a very efficient general iterative search algorithm using the derivative of the likelihood which avoids parameter value optimization during each search step. This is in contrast to the naive learning algorithm in that it has to iteratively update the patten boundaries, each requiring the parameter value optimization and thus is very inefficient. We tested this algorithm for two instances of the probabilistic information diffusion model which has different characteristics. One is of information push style and the other is of information pull style. We chose Asynchronous Independent Cascade (AsIC) model as the former and Value-weighted Voter (VwV) model as the latter. The AsIC is the model for general information diffusion with binary states and the parameter to detect its change is diffusion probability and the VwV is the model for opinion formation with multiple states and the parameter to detect its change is opinion value. The results tested on these two models using four real-world network structures confirm that the algorithm is robust enough and can efficiently identify the correct change pattern of the parameter values. Comparison with the naive method that finds the best combination of change boundaries by an exhaustive search through a set of randomly selected boundary candidates shows that the proposed algorithm far outperforms the native method both in terms of accuracy and computation time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {55},
numpages = {23},
keywords = {information diffusion, Change point detection, social networks, parameter learning}
}

@article{10.1145/2483669.2483687,
author = {Fridman, Natalie and Kaminka, Gal A.},
title = {Using Qualitative Reasoning for Social Simulation of Crowds},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483687},
doi = {10.1145/2483669.2483687},
abstract = {The ability to model and reason about the potential violence level of a demonstration is important to the police decision making process. Unfortunately, existing knowledge regarding demonstrations is composed of partial qualitative descriptions without complete and precise numerical information. In this article we describe a first attempt to use qualitative reasoning techniques to model demonstrations. To our knowledge, such techniques have never been applied to modeling and reasoning regarding crowd behaviors, nor in particular demonstrations. We develop qualitative models consistent with the partial, qualitative social science literature, allowing us to model the interactions between different factors that influence violence in demonstrations. We then utilize qualitative simulation to predict the potential eruption of violence, at various levels, based on a description of the demographics, environmental settings, and police responses. We incrementally present and compare three such qualitative models. The results show that while two of these models fail to predict the outcomes of real-world events reported and analyzed in the literature, one model provides good results. We also examine whether a popular machine learning algorithm (decision tree learning) can be used. While the results show that the decision trees provide improved predictions, we show that the QR models can be more sensitive to changes, and can account for what-if scenarios, in contrast to decision trees. Moreover, we introduce a novel analysis algorithm that analyzes the QR simulations, to automatically determine the factors that are most important in influencing the outcome in specific real-world demonstrations. We show that the algorithm identifies factors that correspond to experts' analysis of these events.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {54},
numpages = {21},
keywords = {Demonstrations, social simulation, qualitative reasoning}
}

@article{10.1145/2483669.2483686,
author = {Gintis, Herbert},
title = {Markov Models of Social Dynamics: Theory and Applications},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483686},
doi = {10.1145/2483669.2483686},
abstract = {This article shows how agent-based models of social dynamics can be treated rigorously and analytically as finite Markov processes, and their long-run properties are then given by an expanded version of the ergodic theorem for Markov processes. A Markov process model of a simplified market economy shows the fruitfulness of this approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {53},
numpages = {19}
}

@article{10.1145/2483669.2483685,
author = {Hung, Benjamin W.K. and Kolitz, Stephan E. and Ozdaglar, Asuman},
title = {Optimization-Based Influencing of Village Social Networks in a Counterinsurgency},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483685},
doi = {10.1145/2483669.2483685},
abstract = {This article considers the nonlethal targeting assignment problem in the counterinsurgency in Afghanistan, the problem of deciding on the people whom U.S. forces should engage through outreach, negotiations, meetings, and other interactions in order to ultimately win the support of the population in their area of operations. We propose two models: (1) the Afghan counterinsurgency (COIN) social influence model, to represent how attitudes of local leaders are affected by repeated interactions with other local leaders, insurgents, and counterinsurgents, and (2) the nonlethal targeting model, a NonLinear Programming (NLP) optimization formulation that identifies a strategy for assigning k U.S. agents to produce the greatest arithmetic mean of the expected long-term attitude of the population. We demonstrate in an experiment the merits of the optimization model in nonlethal targeting, which performs significantly better than both doctrine-based and random methods of assignment in a large network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {22},
keywords = {Social network, agent modeling, opinion dynamics, counterinsurgency, network optimization}
}

@article{10.1145/2483669.2483684,
author = {Yang, Shanchieh Jay and Nau, Dana and Salerno, John},
title = {Introduction to the Special Section on Social Computing, Behavioral-Cultural Modeling, and Prediction},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483684},
doi = {10.1145/2483669.2483684},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {51},
numpages = {2}
}

@article{10.1145/2483669.2483683,
author = {Chin, Alvin and Xu, Bin and Wang, Hao and Chang, Lele and Wang, Hao and Zhu, Lijun},
title = {Connecting People through Physical Proximity and Physical Resources at a Conference},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483683},
doi = {10.1145/2483669.2483683},
abstract = {This work investigates how to bridge the gap between offline and online behaviors at a conference and how the physical resources in the conference (the physical objects used in the conference for gathering attendees together in engaging an activity such as rooms, sessions, and papers) can be used to help facilitate social networking. We build Find and Connect, a system that integrates offline activities and interactions captured in real time with online connections in a conference environment, to provide a list of potential people one should connect to for forming an ephemeral social network. We investigate how social connections can be established and integrated with physical resources through positioning technology, and the relationship between physical proximity encounters and online social connections. Results from our two datasets of two trials, one at the UIC/ATC 2010 conference and GCJK internal marketing event, show that social connections that are reciprocal in relationship, such as friendship and exchanged contacts, have tighter, denser, and highly clustered networks compared to unidirectional relationships such as follow. We discover that there is a positive relationship between physical proximity encounters and online social connections before the social connection is made for friends, but a negative relationship for after the social connection is made. The first indicates social selection is strong, and the second indicates social influence is weak. Even though our dataset is sparse, nonetheless we believe our work is promising and novel which is worthy of future research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {21},
keywords = {Ephemeral social network, physical proximity, mobile social network, resource, social networking}
}

@article{10.1145/2483669.2483682,
author = {Yan, Zhixian and Chakraborty, Dipanjan and Parent, Christine and Spaccapietra, Stefano and Aberer, Karl},
title = {Semantic Trajectories: Mobility Data Computation and Annotation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483682},
doi = {10.1145/2483669.2483682},
abstract = {With the large-scale adoption of GPS equipped mobile sensing devices, positional data generated by moving objects (e.g., vehicles, people, animals) are being easily collected. Such data are typically modeled as streams of spatio-temporal (x,y,t) points, called trajectories. In recent years trajectory management research has progressed significantly towards efficient storage and indexing techniques, as well as suitable knowledge discovery. These works focused on the geometric aspect of the raw mobility data. We are now witnessing a growing demand in several application sectors (e.g., from shipment tracking to geo-social networks) on understanding the semantic behavior of moving objects. Semantic behavior refers to the use of semantic abstractions of the raw mobility data, including not only geometric patterns but also knowledge extracted jointly from the mobility data and the underlying geographic and application domains information. The core contribution of this article lies in a semantic model and a computation and annotation platform for developing a semantic approach that progressively transforms the raw mobility data into semantic trajectories enriched with segmentations and annotations. We also analyze a number of experiments we did with semantic trajectories in different domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {49},
numpages = {38},
keywords = {trajectory computing, trajectory annotation, hidden Markov model, trajectory segmentation, spatial join, Spatio-temporal/structured/semantic trajectory, map matching}
}

@article{10.1145/2483669.2483681,
author = {Wei, Ling-Yin and Peng, Wen-Chih and Lee, Wang-Chien},
title = {Exploring Pattern-Aware Travel Routes for Trajectory Search},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483681},
doi = {10.1145/2483669.2483681},
abstract = {With the popularity of positioning devices, Web 2.0 technology, and trip sharing services, many users are willing to log and share their trips on the Web. Thus, trip planning Web sites are able to provide some new services by inferring Regions-Of-Interest (ROIs) and recommending popular travel routes from trip trajectories. We argue that simply providing some travel routes consisting of popular ROIs to users is not sufficient. To tour around a wide geographical area, for example, a city, some users may prefer a trip to visit as many ROIs as possible, while others may like to stop by only a few ROIs for an in-depth visit. We refer to a trip fitting the former user group as an in-breadth trip and a trip suitable for the latter user group as an in-depth trip. Prior studies on trip planning have focused on mining ROIs and travel routes without considering these different preferences. In this article, given a spatial range and a user preference of depth/breadth specified by a user, we develop a Pattern-Aware Trajectory Search (PATS) framework to retrieve the top K trajectories passing through popular ROIs. PATS is novel because the returned travel trajectories, discovered from travel patterns hidden in trip trajectories, may represent the most valuable travel experiences of other travelers fitting the user's trip preference in terms of depth or breadth. The PATS framework comprises two components: travel behavior exploration and trajectory search. The travel behavior exploration component determines a set of ROIs along with their attractive scores by considering not only the popularity of the ROIs but also the travel sequential relationships among the ROIs. To capture the travel sequential relationships among ROIs and to derive their attractive scores, a user movement graph is constructed. For the trajectory search component of PATS, we formulate two trajectory score functions, the depth-trip score function and the breadth-trip score function, by taking into account the number of ROIs in a trajectory and their attractive scores. Accordingly, we propose an algorithm, namely, Bounded Trajectory Search (BTS), to efficiently retrieve the top K trajectories based on the two trajectory scores. The PATS framework is evaluated by experiments and user studies using a real dataset. The experimental results demonstrate the effectiveness and the efficiency of the proposed PATS framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {48},
numpages = {25},
keywords = {trajectory search, data mining, route planning, Trajectory pattern mining}
}

@article{10.1145/2483669.2483680,
author = {Shi, Yue and Serdyukov, Pavel and Hanjalic, Alan and Larson, Martha},
title = {Nontrivial Landmark Recommendation Using Geotagged Photos},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483680},
doi = {10.1145/2483669.2483680},
abstract = {Online photo-sharing sites provide a wealth of information about user behavior and their potential is increasing as it becomes ever-more common for images to be associated with location information in the form of geotags. In this article, we propose a novel approach that exploits geotagged images from an online community for the purpose of personalized landmark recommendation. Under our formulation of the task, recommended landmarks should be relevant to user interests and additionally they should constitute nontrivial recommendations. In other words, recommendations of landmarks that are highly popular and frequently visited and can be easily discovered through other information sources such as travel guides should be avoided in favor of recommendations that relate to users' personal interests. We propose a collaborative filtering approach to the personalized landmark recommendation task within a matrix factorization framework. Our approach, WMF-CR, combines weighted matrix factorization and category-based regularization. The integrated weights emphasize the contribution of nontrivial landmarks in order to focus the recommendation model specifically on the generation of nontrivial recommendations. They support the judicious elimination of trivial landmarks from consideration without also discarding information valuable for recommendation. Category-based regularization addresses the sparse data problem, which is arguably even greater in the case of our landmark recommendation task than in other recommendation scenarios due to the limited amount of travel experience recorded in the online image set of any given user. We use category information extracted from Wikipedia in order to provide the system with a method to generalize the semantics of landmarks and allow the model to relate them not only on the basis of identity, but also on the basis of topical commonality. The proposed approach is computational scalable, that is, its complexity is linear with the number of observed preferences in the user-landmark preference matrix and the number of nonzero similarities in the category-based landmark similarity matrix. We evaluate the approach on a large collection of geotagged photos gathered from Flickr. Our experimental results demonstrate that WMF-CR outperforms several state-of-the-art baseline approaches in recommending nontrivial landmarks. Additionally, they demonstrate that the approach is well suited for addressing data sparseness and provides particular performance improvement in the case of users who have limited travel experience, that is, have visited only few cities or few landmarks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {47},
numpages = {27},
keywords = {geotag, nontrivial recommendation, Collaborative filtering, location-based recommendation, social media application}
}

@article{10.1145/2483669.2483679,
author = {Schuster, Daniel and Rosi, Alberto and Mamei, Marco and Springer, Thomas and Endler, Markus and Zambonelli, Franco},
title = {Pervasive Social Context: Taxonomy and Survey},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483679},
doi = {10.1145/2483669.2483679},
abstract = {As pervasive computing meets social networks, there is a fast growing research field called pervasive social computing. Applications in this area exploit the richness of information arising out of people using sensor-equipped pervasive devices in their everyday life combined with intense use of different social networking services. We call this set of information pervasive social context. We provide a taxonomy to classify pervasive social context along the dimensions space, time, people, and information source (STiPI) as well as commenting on the type and reason for creating such context. A survey of recent research shows the applicability and usefulness of the taxonomy in classifying and assessing applications and systems in the area of pervasive social computing. Finally, we present some research challenges in this area and illustrate how they affect the systems being surveyed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {46},
numpages = {22},
keywords = {context awareness, survey, taxonomy, Pervasive computing, social networks}
}

@article{10.1145/2483669.2483678,
author = {Yu, Zhiwen and Zhang, Daqing and Eagle, Nathan and Cook, Diane},
title = {Introduction to the Special Section on Intelligent Systems for Socially Aware Computing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483678},
doi = {10.1145/2483669.2483678},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {45},
numpages = {3}
}

@article{10.1145/2483669.2483677,
author = {Bouamor, Houda and Max, Aur\'{e}elien and Vilnat, Anne},
title = {Multitechnique Paraphrase Alignment: A Contribution to Pinpointing Sub-Sentential Paraphrases},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483677},
doi = {10.1145/2483669.2483677},
abstract = {This work uses parallel monolingual corpora for a detailed study of the task of sub-sentential paraphrase acquisition. We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited type for studies on paraphrasing. We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, their combinations, as well as four monolingual corpus types of varying comparability. We report, under all conditions, a significant improvement over all techniques by validating candidate paraphrases using a maximum entropy classifier. An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {44},
numpages = {27},
keywords = {Paraphrase acquisition, paraphrase corpora}
}

@article{10.1145/2483669.2483676,
author = {Burrows, Steven and Potthast, Martin and Stein, Benno},
title = {Paraphrase Acquisition via Crowdsourcing and Machine Learning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483676},
doi = {10.1145/2483669.2483676},
abstract = {To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {plagiarism, cost analysis, corpus, Mechanical Turk, Paraphrase generation}
}

@article{10.1145/2483669.2483675,
author = {Moon, Taesun and Erk, Katrin},
title = {An Inference-Based Model of Word Meaning in Context as a Paraphrase Distribution},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483675},
doi = {10.1145/2483669.2483675},
abstract = {Graded models of word meaning in context characterize the meaning of individual usages (occurrences) without reference to dictionary senses. We introduce a novel approach that frames the task of computing word meaning in context as a probabilistic inference problem. The model represents the meaning of a word as a probability distribution over potential paraphrases, inferred using an undirected graphical model. Evaluated on paraphrasing tasks, the model achieves state-of-the-art performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {28},
keywords = {probabilistic inference, Semantics, probabilistic graphical models, loopy belief propagation, lexical semantics, paraphrases}
}

@article{10.1145/2483669.2483674,
author = {Cohn, Trevor and Lapata, Mirella},
title = {An Abstractive Approach to Sentence Compression},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483674},
doi = {10.1145/2483669.2483674},
abstract = {In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {35},
keywords = {language models, machine translation, transduction, Language generation, paraphrases, synchronous grammars, sentence compression}
}

@article{10.1145/2483669.2483673,
author = {Madnani, Nitin and Dorr, Bonnie J.},
title = {Generating Targeted Paraphrases for Improved Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483673},
doi = {10.1145/2483669.2483673},
abstract = {Today's Statistical Machine Translation (SMT) systems require high-quality human translations for parameter tuning, in addition to large bitexts for learning the translation units. This parameter tuning usually involves generating translations at different points in the parameter space and obtaining feedback against human-authored reference translations as to how good the translations. This feedback then dictates what point in the parameter space should be explored next. To measure this feedback, it is generally considered wise to have multiple (usually 4) reference translations to avoid unfair penalization of translation hypotheses which could easily happen given the large number of ways in which a sentence can be translated from one language to another. However, this reliance on multiple reference translations creates a problem since they are labor intensive and expensive to obtain. Therefore, most current MT datasets only contain a single reference. This leads to the problem of reference sparsity. In our previously published research, we had proposed the first paraphrase-based solution to this problem and evaluated its effect on Chinese-English translation. In this article, we first present extended results for that solution on additional source languages. More importantly, we present a novel way to generate “targeted” paraphrases that yields substantially larger gains (up to 2.7 BLEU points) in translation quality when compared to our previous solution (up to 1.6 BLEU points). In addition, we further validate these improvements by supplementing with human preference judgments obtained via Amazon Mechanical Turk.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {40},
numpages = {25},
keywords = {Natural language processing, paraphrasing, machine translation}
}

@article{10.1145/2483669.2483672,
author = {Marton, Yuval},
title = {Distributional Phrasal Paraphrase Generation for Statistical Machine Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483672},
doi = {10.1145/2483669.2483672},
abstract = {Paraphrase generation has been shown useful for various natural language processing tasks, including statistical machine translation. A commonly used method for paraphrase generation is pivoting [Callison-Burch et al. 2006], which benefits from linguistic knowledge implicit in the sentence alignment of parallel texts, but has limited applicability due to its reliance on parallel texts. Distributional paraphrasing [Marton et al. 2009a] has wider applicability, is more language-independent, but doesn't benefit from any linguistic knowledge. Nevertheless, we show that using distributional paraphrasing can yield greater gains in translation tasks. We report method improvements leading to higher gains than previously published, of almost 2 Bleu points, and provide implementation details, complexity analysis, and further insight into this method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {32},
keywords = {Semantic similarity, SMT, semantic distance, paraphrase generation, statistical machine translation}
}

@article{10.1145/2483669.2483671,
author = {Resnik, Philip and Buzek, Olivia and Kronrod, Yakov and Hu, Chang and Quinn, Alexander J. and Bederson, Benjamin B.},
title = {Using Targeted Paraphrasing and Monolingual Crowdsourcing to Improve Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483671},
doi = {10.1145/2483669.2483671},
abstract = {Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation, which makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e., paraphrases) with only monolingual knowledge of the source language. Formal evaluation demonstrates that this approach can yield substantial improvements in translation quality, and the idea has been integrated into a broader framework for monolingual collaborative translation that produces fully accurate, fully fluent translations for a majority of sentences in a real-world translation task, with no involvement of human bilingual speakers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {38},
numpages = {21},
keywords = {translation, paraphrase, machine translation, wisdom of crowds, human computation, Monolingual, crowdsourcing, translation interface}
}

@article{10.1145/2483669.2483670,
author = {Wang, Haifeng and Dolan, Bill and Szpektor, Idan and Zhao, Shiqi},
title = {Introduction to Special Section on Paraphrasing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483670},
doi = {10.1145/2483669.2483670},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {2}
}

@article{10.1145/2438653.2438671,
author = {Tran, Vien and Nguyen, Khoi and Son, Tran Cao and Pontelli, Enrico},
title = {A Conformant Planner Based on Approximation: C<span class="smallcaps SmallerCapital">p</span>A(H)},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438671},
doi = {10.1145/2438653.2438671},
abstract = {This article describes the planner CpA(H), the recipient of the Best Nonobservable Nondeterministic Planner Award in the “Uncertainty Track” of the 6th International Planning Competition (IPC), 2008. The article presents the various techniques that help CpA(H) to achieve the level of performance and scalability exhibited in the competition. The article also presents experimental results comparing CpA(H) with state-of-the-art conformant planners.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {36},
numpages = {38},
keywords = {international planning competition, approximations, conformant planning, Plan generation}
}

@article{10.1145/2438653.2438670,
author = {Song, Xuan and Shao, Xiaowei and Zhang, Quanshi and Shibasaki, Ryosuke and Zhao, Huijing and Cui, Jinshi and Zha, Hongbin},
title = {A Fully Online and Unsupervised System for Large and High-Density Area Surveillance: Tracking, Semantic Scene Learning and Abnormality Detection},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438670},
doi = {10.1145/2438653.2438670},
abstract = {For reasons of public security, an intelligent surveillance system that can cover a large, crowded public area has become an urgent need. In this article, we propose a novel laser-based system that can simultaneously perform tracking, semantic scene learning, and abnormality detection in a fully online and unsupervised way. Furthermore, these three tasks cooperate with each other in one framework to improve their respective performances. The proposed system has the following key advantages over previous ones: (1) It can cover quite a large area (more than 60\texttimes{}35m), and simultaneously perform robust tracking, semantic scene learning, and abnormality detection in a high-density situation. (2) The overall system can vary with time, incrementally learn the structure of the scene, and perform fully online abnormal activity detection and tracking. This feature makes our system suitable for real-time applications. (3) The surveillance tasks are carried out in a fully unsupervised manner, so that there is no need for manual labeling and the construction of huge training datasets. We successfully apply the proposed system to the JR subway station in Tokyo, and demonstrate that it can cover an area of 60\texttimes{}35m, robustly track more than 150 targets at the same time, and simultaneously perform online semantic scene learning and abnormality detection with no human intervention.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {35},
numpages = {21},
keywords = {Surveillance, abnormality detection, Multitarget tracking, semantic scene learning}
}

@article{10.1145/2438653.2438669,
author = {Wang, Zhengxiang and Hu, Yiqun and Chia, Liang-Tien},
title = {Learning Image-to-Class Distance Metric for Image Classification},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438669},
doi = {10.1145/2438653.2438669},
abstract = {Image-To-Class (I2C) distance is a novel distance used for image classification and has successfully handled datasets with large intra-class variances. However, it uses Euclidean distance for measuring the distance between local features in different classes, which may not be the optimal distance metric in real image classification problems. In this article, we propose a distance metric learning method to improve the performance of I2C distance by learning per-class Mahalanobis metrics in a large margin framework. Our I2C distance is adaptive to different classes by combining with the learned metric for each class. These multiple per-class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distances to other classes by a large margin. A subgradient descent method is applied to efficiently solve this optimization problem. For efficiency and scalability to large-scale problems, we also show how to simplify the method to learn a diagonal matrix for each class. We show in experiments that our learned Mahalanobis I2C distance can significantly outperform the original Euclidean I2C distance as well as other distance metric learning methods in several prevalent image datasets, and our simplified diagonal matrices can preserve the performance but significantly speed up the metric learning procedure for large-scale datasets. We also show in experiment that our method is able to correct the class imbalance problem, which usually leads the NN-based methods toward classes containing more training images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {22},
keywords = {Image-to-class distance, image classification, distance metric learning, nearest-neighbor classification}
}

@article{10.1145/2438653.2438668,
author = {Tabia, Hedi and Daoudi, Mohamed and Vandeborre, Jean-Philippe and Colot, Olivier},
title = {A Parts-Based Approach for Automatic 3D Shape Categorization Using Belief Functions},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438668},
doi = {10.1145/2438653.2438668},
abstract = {Grouping 3D objects into (semantically) meaningful categories is a challenging and important problem in 3D mining and shape processing. Here, we present a novel approach to categorize 3D objects. The method described in this article, is a belief-function-based approach and consists of two stages: the training stage, where 3D objects in the same category are processed and a set of representative parts is constructed, and the labeling stage, where unknown objects are categorized. The experimental results obtained on the Tosca-Sumner and the Shrec07 datasets show that the system efficiently performs in categorizing 3D models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {33},
numpages = {16},
keywords = {Multimedia data, object recognition, classification, belief functions, 3D categorization}
}

@article{10.1145/2438653.2438667,
author = {Wang, Fei-Yue and Wong, Pak Kin},
title = {Intelligent Systems and Technology for Integrative and Predictive Medicine: An ACP Approach},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438667},
doi = {10.1145/2438653.2438667},
abstract = {One of the principal goals in medicine is to determine and implement the best treatment for patients through fastidious estimation of the effects and benefits of therapeutic procedures. The inherent complexities of physiological and pathological networks that span across orders of magnitude in time and length scales, however, represent fundamental hurdles in determining effective treatments for patients. Here we argue for a new approach, called the ACP-based approach, that combines artificial (societies), computational (experiments), and parallel (execution) methods in intelligent systems and technology for integrative and predictive medicine, or more generally, precision medicine and smart health management. The advent of artificial societies that collect the clinically relevant information in prognostics and therapeutics provides a promising platform for organizing and experimenting complex physiological systems toward integrative medicine. The ability of computational experiments to analyze distinct, interactive systems such as the host mechanisms, pathological pathways, and therapeutic strategies, as well as other factors using the artificial systems, will enable control and management through parallel execution of real and arficial systems concurrently within the integrative medicine context. The development of this framework in integrative medicine, fueled by close collaborations between physicians, engineers, and scientists, will result in preventive and predictive practices of a personal, proactive, and precise nature, including rational combinatorial treatments, adaptive therapeutics, and patient-oriented disease management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {32},
numpages = {6}
}

@article{10.1145/2438653.2438666,
author = {Ehara, Yo and Shimizu, Nobuyuki and Ninomiya, Takashi and Nakagawa, Hiroshi},
title = {Personalized Reading Support for Second-Language Web Documents},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438666},
doi = {10.1145/2438653.2438666},
abstract = {A novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by a collective intelligence method and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. The system was evaluated in terms of prediction accuracy and reading simulation. The reading simulation results show that this system can reduce the number of clicks for most readers with insufficient vocabulary to read documents and can significantly reduce the remaining number of unfamiliar words after the prediction and glossing for all users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {19},
keywords = {glossing systems, logistic regression, Web pages, item response theory, Reading support}
}

@article{10.1145/2438653.2438665,
author = {Yen, Neil Y. and Shih, Timothy K. and Jin, Qun},
title = {LONET: An Interactive Search Network for Intelligent Lecture Path Generation},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438665},
doi = {10.1145/2438653.2438665},
abstract = {Sharing resources and information on the Internet has become an important activity for education. In distance learning, instructors can benefit from resources, also known as Learning Objects (LOs), to create plenteous materials for specific learning purposes. Our repository (called the MINE Registry) has been developed for storing and sharing learning objects, around 22,000 in total, in the past few years. To enhance reusability, one significant concept named Reusability Tree was implemented to trace the process of changes. Also, weighting and ranking metrics have been proposed to enhance the searchability in the repository. Following the successful implementation, this study goes further to investigate the relationships between LOs from a perspective of social networks. The LONET (Learning Object Network), as an extension of Reusability Tree, is newly proposed and constructed to clarify the vague reuse scenario in the past, and to summarize collaborative intelligence through past interactive usage experiences. We define a social structure in our repository based on past usage experiences from instructors, by proposing a set of metrics to evaluate the interdependency such as prerequisites and references. The structure identifies usage experiences and can be graphed in terms of implicit and explicit relations among learning objects. As a practical contribution, an adaptive algorithm is proposed to mine the social structure in our repository. The algorithm generates adaptive routes, based on past usage experiences, by computing possible interactive input, such as search criteria and feedback from instructors, and assists them in generating specific lectures.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {27},
keywords = {distance learning, social network analysis, ranking, interactive search, Learning object network, SCORM, repository, lecture path}
}

@article{10.1145/2438653.2438664,
author = {Folsom-Kovarik, Jeremiah T. and Sukthankar, Gita and Schatz, Sae},
title = {Tractable POMDP Representations for Intelligent Tutoring Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438664},
doi = {10.1145/2438653.2438664},
abstract = {With Partially Observable Markov Decision Processes (POMDPs), Intelligent Tutoring Systems (ITSs) can model individual learners from limited evidence and plan ahead despite uncertainty. However, POMDPs need appropriate representations to become tractable in ITSs that model many learner features, such as mastery of individual skills or the presence of specific misconceptions. This article describes two POMDP representations—state queues and observation chains—that take advantage of ITS task properties and let POMDPs scale to represent over 100 independent learner features. A real-world military training problem is given as one example. A human study (n = 14) provides initial validation for the model construction. Finally, evaluating the experimental representations with simulated students helps predict their impact on ITS performance. The compressed representations can model a wide range of simulated problems with instructional efficacy equal to lossless representations. With improved tractability, POMDP ITSs can accommodate more numerous or more detailed learner states and inputs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {29},
numpages = {22},
keywords = {intelligent tutoring systems, computer-based training, Partially observable Markov decision processes}
}

@article{10.1145/2438653.2438663,
author = {Li, Qing and Luo, Xiangfeng and Wenyin, Liu and Conati, Cristina},
title = {Introduction to the Special Section on Intelligent Tutoring and Coaching Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438663},
doi = {10.1145/2438653.2438663},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {28},
numpages = {2}
}

@article{10.1145/2438653.2438662,
author = {Falcone, Rino and Piunti, Michele and Venanzi, Matteo and Castelfranchi, Cristiano},
title = {From Manifesta to Krypta: The Relevance of Categories for Trusting Others},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438662},
doi = {10.1145/2438653.2438662},
abstract = {In this article we consider the special abilities needed by agents for assessing trust based on inference and reasoning. We analyze the case in which it is possible to infer trust towards unknown counterparts by reasoning on abstract classes or categories of agents shaped in a concrete application domain. We present a scenario of interacting agents providing a computational model implementing different strategies to assess trust. Assuming a medical domain, categories, including both competencies and dispositions of possible trustees, are exploited to infer trust towards possibly unknown counterparts. The proposed approach for the cognitive assessment of trust relies on agents' abilities to analyze heterogeneous information sources along different dimensions. Trust is inferred based on specific observable properties (manifesta), namely explicitly readable signals indicating internal features (krypta) regulating agents' behavior and effectiveness on specific tasks. Simulative experiments evaluate the performance of trusting agents adopting different strategies to delegate tasks to possibly unknown trustees, while experimental results show the relevance of this kind of cognitive ability in the case of open multiagent systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {27},
numpages = {24},
keywords = {open systems, Trust by reasoning, cognitive analysis, fuzzy cognitive maps}
}

@article{10.1145/2438653.2438661,
author = {Burnett, Chris and Norman, Timothy J. and Sycara, Katia},
title = {Stereotypical Trust and Bias in Dynamic Multiagent Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438661},
doi = {10.1145/2438653.2438661},
abstract = {Large-scale multiagent systems have the potential to be highly dynamic. Trust and reputation are crucial concepts in these environments, as it may be necessary for agents to rely on their peers to perform as expected, and learn to avoid untrustworthy partners. However, aspects of highly dynamic systems introduce issues which make the formation of trust relationships difficult. For example, they may be short-lived, precluding agents from gaining the necessary experiences to make an accurate trust evaluation. This article describes a new approach, inspired by theories of human organizational behavior, whereby agents generalize their experiences with previously encountered partners as stereotypes, based on the observable features of those partners and their behaviors. Subsequently, these stereotypes are applied when evaluating new and unknown partners. Furthermore, these stereotypical opinions can be communicated within the society, resulting in the notion of stereotypical reputation. We show how this approach can complement existing state-of-the-art trust models, and enhance the confidence in the evaluations that can be made about trustees when direct and reputational information is lacking or limited. Furthermore, we show how a stereotyping approach can help agents detect unwanted biases in the reputational opinions they receive from others in the society.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {26},
numpages = {22},
keywords = {Trust, multiagent systems, stereotypes}
}

@article{10.1145/2438653.2438660,
author = {Erriquez, Elisabetta and Hoek, Wiebe van der and Wooldridge, Michael},
title = {Building and Using Social Structures: A Case Study Using the Agent ART Testbed},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438660},
doi = {10.1145/2438653.2438660},
abstract = {This article investigates the conjecture that agents who make decisions in scenarios where trust is important can benefit from the use of a social structure, representing the social relationships that exist between agents. We propose techniques that can be used by agents to initially build and then progressively update such a structure in the light of experience. We describe an implementation of our techniques in the domain of the Agent ART testbed: we take two existing agents for this domain (“Simplet” and “Connected”) and compare their performance with versions that use our social structure (“SocialSimplet” and “SocialConnected”). We show that SocialSimplet and SocialConnected outperform their counterparts with respect to the quality of the interactions, the number of rounds won in a competition, and the total utility gained.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {25},
numpages = {20},
keywords = {trading competition, trust, Agents}
}

@article{10.1145/2438653.2438659,
author = {Zhang, Jie and Cohen, Robin},
title = {A Framework for Trust Modeling in Multiagent Electronic Marketplaces with Buying Advisors to Consider Varying Seller Behavior and the Limiting of Seller Bids},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438659},
doi = {10.1145/2438653.2438659},
abstract = {In this article, we present a framework of use in electronic marketplaces that allows buying agents to model the trustworthiness of selling agents in an effective way, making use of seller ratings provided by other buying agents known as advisors. The trustworthiness of the advisors is also modeled, using an approach that combines both personal and public knowledge and allows the relative weighting to be adjusted over time. Through a series of experiments that simulate e-marketplaces, including ones where sellers may vary their behavior over time, we are able to demonstrate that our proposed framework delivers effective seller recommendations to buyers, resulting in important buyer profit. We also propose limiting seller bids as a method for promoting seller honesty, thus facilitating successful selection of sellers by buyers, and demonstrate the value of this approach through experimental results. Overall, this research is focused on the technological aspects of electronic commerce and specifically on technology that would be used to manage trust.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {22},
keywords = {trust modeling, Multiagent systems, electronic commerce applications, social networks}
}

@article{10.1145/2438653.2438658,
author = {Falcone, Rino and Singh, Munindar P.},
title = {Introduction to Special Section on Trust in Multiagent Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438658},
doi = {10.1145/2438653.2438658},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {23},
numpages = {2},
keywords = {Trust}
}

@article{10.1145/2438653.2438657,
author = {Baldoni, Matteo and Baroglio, Cristina and Marengo, Elisa and Patti, Viviana},
title = {Constitutive and Regulative Specifications of Commitment Protocols: A Decoupled Approach},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438657},
doi = {10.1145/2438653.2438657},
abstract = {Interaction protocols play a fundamental role in multiagent systems. In this work, after analyzing the trends that are emerging not only from research on multiagent interaction protocols but also from neighboring fields, like research on workflows and business processes, we propose a novel definition of commitment-based interaction protocols, that is characterized by the decoupling of the constitutive and the regulative specifications and that explicitly foresees a representation of the latter based on constraints among commitments. A clear distinction between the two representations has many advantages, mainly residing in a greater openness of multiagent systems, and an easier reuse of protocols and of action definitions. A language, named 2CL, for writing regulative specifications is also given together with a designer-oriented graphical notation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {22},
numpages = {25},
keywords = {commitments, Interaction protocols, constraints among commitments, constitutive and regulative specifications}
}

@article{10.1145/2438653.2438656,
author = {Gerard, Scott N. and Singh, Munindar P.},
title = {Formalizing and Verifying Protocol Refinements},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438656},
doi = {10.1145/2438653.2438656},
abstract = {A (business) protocol describes, in high-level terms, a pattern of communication between two or more participants, specifically via the creation and manipulation of the commitments between them. In this manner, a protocol offers both flexibility and rigor: a participant may communicate in any way it chooses as long as it discharges all of its activated commitments. Protocols thus promise benefits in engineering cross-organizational business processes. However, software engineering using protocols presupposes a formalization of protocols and a notion of the refinement of one protocol by another. Refinement for protocols is both intuitively obvious (e.g., PayViaCheck is clearly a kind of Pay) and technically nontrivial (e.g., compared to Pay, PayViaCheck involves different participants exchanging different messages). This article formalizes protocols and their refinement. It develops Proton, an analysis tool for protocol specifications that overlays a model checker to compute whether one protocol refines another with respect to a stated mapping. Proton and its underlying theory are evaluated by formalizing several protocols from the literature and verifying all and only the expected refinements.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {21},
numpages = {27},
keywords = {agent communication, Commitments, verification of multiagent systems}
}

@article{10.1145/2438653.2438655,
author = {Chopra, Amit K. and Artikis, Alexander and Bentahar, Jamal and Colombetti, Marco and Dignum, Frank and Fornara, Nicoletta and Jones, Andrew J. I. and Singh, Munindar P. and Yolum, Pinar},
title = {Research Directions in Agent Communication},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438655},
doi = {10.1145/2438653.2438655},
abstract = {Increasingly, software engineering involves open systems consisting of autonomous and heterogeneous participants or agents who carry out loosely coupled interactions. Accordingly, understanding and specifying communications among agents is a key concern. A focus on ways to formalize meaning distinguishes agent communication from traditional distributed computing: meaning provides a basis for flexible interactions and compliance checking.Over the years, a number of approaches have emerged with some essential and some irrelevant distinctions drawn among them. As agent abstractions gain increasing traction in the software engineering of open systems, it is important to resolve the irrelevant and highlight the essential distinctions, so that future research can be focused in the most productive directions.This article is an outcome of extensive discussions among agent communication researchers, aimed at taking stock of the field and at developing, criticizing, and refining their positions on specific approaches and future challenges. This article serves some important purposes, including identifying (1) points of broad consensus; (2) points where substantive differences remain; and (3) interesting directions of future work.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {20},
numpages = {23},
keywords = {Communication}
}

@article{10.1145/2438653.2438654,
author = {Chopra, Amit K. and Artikis, Alexander and Bentahar, Jamal and Dignum, Frank},
title = {Introduction to the Special Section on Agent Communication},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438654},
doi = {10.1145/2438653.2438654},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {19},
numpages = {1}
}

@article{10.1145/2414425.2414443,
author = {Song, Xuan and Zhao, Huijing and Cui, Jinshi and Shao, Xiaowei and Shibasaki, Ryosuke and Zha, Hongbin},
title = {An Online System for Multiple Interacting Targets Tracking: Fusion of Laser and Vision, Tracking and Learning},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414443},
doi = {10.1145/2414425.2414443},
abstract = {Multitarget tracking becomes significantly more challenging when the targets are in close proximity or frequently interact with each other. This article presents a promising online system to deal with these problems. The novelty of this system is that laser and vision are integrated with tracking and online learning to complement each other in one framework: when the targets do not interact with each other, the laser-based independent trackers are employed and the visual information is extracted simultaneously to train some classifiers online for “possible interacting targets”. When the targets are in close proximity, the classifiers learned online are used alongside visual information to assist in tracking. Therefore, this mode of cooperation not only deals with various tough problems encountered in tracking, but also ensures that the entire process can be completely online and automatic. Experimental results demonstrate that laser and vision fully display their respective advantages in our system, and it is easy for us to obtain a good trade-off between tracking accuracy and the time-cost factor.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {18},
numpages = {21},
keywords = {sensor fusion, Multiple targets tracking}
}

@article{10.1145/2414425.2414442,
author = {Okada, Isamu and Yamamoto, Hitoshi},
title = {Mathematical Description and Analysis of Adaptive Risk Choice Behavior},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414442},
doi = {10.1145/2414425.2414442},
abstract = {Which risk should one choose when facing alternatives with different levels of risk? We discuss here adaptive processes in such risk choice behavior by generalizing the study of Roos et al. [2010]. We deal with an n-choice game in which every player sequentially chooses n times of lotteries of which there are two types: a safe lottery and a risky lottery. We analyze this model in more detail by elaborating the game. Based on the results of mathematical analysis, replicator dynamics analysis, and numerical simulations, we derived some salient features of risk choice behavior. We show that all the risk strategies can be divided into two groups: persistence and nonpersistence. We also proved that the dynamics with perturbation in which a mutation is installed is globally asymptotically stable to a unique equilibrium point for any initial population. The numerical simulations clarify that the number of persistent strategies seldom increases regardless of the increase in n, and suggest that a rarity of dominant choice strategies is widely observed in many social contexts. These facts not only go hand-in-hand with some well-known insights from prospect theory, but may also provide some theoretical hypotheses for various fields such as behavioral economics, ecology, sociology, and consumer behavioral theory.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {17},
numpages = {21},
keywords = {decision theory, sequentiality, Adaptive process, winner-takes-all, risk, risk choice strategy, replicator dynamics, mutation}
}

@article{10.1145/2414425.2414441,
author = {Shi, Yue and Larson, Martha and Hanjalic, Alan},
title = {Mining Contextual Movie Similarity with Matrix Factorization for Context-Aware Recommendation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414441},
doi = {10.1145/2414425.2414441},
abstract = {Context-aware recommendation seeks to improve recommendation performance by exploiting various information sources in addition to the conventional user-item matrix used by recommender systems. We propose a novel context-aware movie recommendation algorithm based on joint matrix factorization (JMF). We jointly factorize the user-item matrix containing general movie ratings and other contextual movie similarity matrices to integrate contextual information into the recommendation process. The algorithm was developed within the scope of the mood-aware recommendation task that was offered by the Moviepilot mood track of the 2010 context-aware movie recommendation (CAMRa) challenge. Although the algorithm could generalize to other types of contextual information, in this work, we focus on two: movie mood tags and movie plot keywords. Since the objective in this challenge track is to recommend movies for a user given a specified mood, we devise a novel mood-specific movie similarity measure for this purpose. We enhance the recommendation based on this measure by also deploying the second movie similarity measure proposed in this article that takes into account the movie plot keywords. We validate the effectiveness of the proposed JMF algorithm with respect to the recommendation performance by carrying out experiments on the Moviepilot challenge dataset. We demonstrate that exploiting contextual information in JMF leads to significant improvement over several state-of-the-art approaches that generate movie recommendations without using contextual information. We also demonstrate that our proposed mood-specific movie similarity is better suited for the task than the conventional mood-based movie similarity measures. Finally, we show that the enhancement provided by the movie similarity capturing the plot keywords is particularly helpful in improving the recommendation to those users who are significantly more active in rating the movies than other users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {16},
numpages = {19},
keywords = {context-aware recommendation, recommender systems, matrix factorization, mood-specific movie similarity, Collaborative filtering}
}

@article{10.1145/2414425.2414440,
author = {Liu, Nathan N. and He, Luheng and Zhao, Min},
title = {Social Temporal Collaborative Ranking for Context Aware Movie Recommendation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414440},
doi = {10.1145/2414425.2414440},
abstract = {Most existing collaborative filtering models only consider the use of user feedback (e.g., ratings) and meta data (e.g., content, demographics). However, in most real world recommender systems, context information, such as time and social networks, are also very important factors that could be considered in order to produce more accurate recommendations. In this work, we address several challenges for the context aware movie recommendation tasks in CAMRa 2010: (1) how to combine multiple heterogeneous forms of user feedback? (2) how to cope with dynamic user and item characteristics? (3) how to capture and utilize social connections among users? For the first challenge, we propose a novel ranking based matrix factorization model to aggregate explicit and implicit user feedback. For the second challenge, we extend this model to a sequential matrix factorization model to enable time-aware parametrization. Finally, we introduce a network regularization function to constrain user parameters based on social connections. To the best of our knowledge, this is the first study that investigates the collective modeling of social and temporal dynamics. Experiments on the CAMRa 2010 dataset demonstrated clear improvements over many baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {15},
numpages = {26},
keywords = {recommender systems, context awareness, user feedback, Collaborative filtering}
}

@article{10.1145/2414425.2414439,
author = {Bellog\'{\i}n, Alejandro and Cantador, Iv\'{a}n and D\'{\i}ez, Fernando and Castells, Pablo and Chavarriaga, Enrique},
title = {An Empirical Comparison of Social, Collaborative Filtering, and Hybrid Recommenders},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414439},
doi = {10.1145/2414425.2414439},
abstract = {In the Social Web, a number of diverse recommendation approaches have been proposed to exploit the user generated contents available in the Web, such as rating, tagging, and social networking information. In general, these approaches naturally require the availability of a wide amount of these user preferences. This may represent an important limitation for real applications, and may be somewhat unnoticed in studies focusing on overall precision, in which a failure to produce recommendations gets blurred when averaging the obtained results or, even worse, is just not accounted for, as users with no recommendations are typically excluded from the performance calculations. In this article, we propose a coverage metric that uncovers and compensates for the incompleteness of performance evaluations based only on precision. We use this metric together with precision metrics in an empirical comparison of several social, collaborative filtering, and hybrid recommenders. The obtained results show that a better balance between precision and coverage can be achieved by combining social-based filtering (high accuracy, low coverage) and collaborative filtering (low accuracy, high coverage) recommendation techniques. We thus explore several hybrid recommendation approaches to balance this trade-off. In particular, we compare, on the one hand, techniques integrating collaborative and social information into a single model, and on the other, linear combinations of recommenders. For the last approach, we also propose a novel strategy to dynamically adjust the weight of each recommender on a user-basis, utilizing graph measures as indicators of the target user's connectedness and relevance in a social network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {14},
numpages = {29},
keywords = {collaborative filtering, user coverage, social networks, Recommender Systems, random walk, hybrid recommenders, graph theory}
}

@article{10.1145/2414425.2414438,
author = {Said, Alan and Berkovsky, Shlomo and De Luca, Ernesto W.},
title = {Introduction to Special Section on CAMRa2010: Movie Recommendation in Context},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414438},
doi = {10.1145/2414425.2414438},
abstract = {The challenge and workshop on Context-Aware Movie Recommendation (CAMRa2010) were conducted jointly in 2010 with the Recommender Systems conference. The challenge focused on three context-aware recommendation scenarios: time-based, mood-based, and social recommendation. The participants were provided with anonymized datasets from two real-world online movie recommendation communities and competed against each other for obtaining the highest accuracy of recommendations. The datasets contained contextual features, such as tags, annotation, social relationsips, and comments, normally not available in public recommendation datasets. More than 40 teams from 21 countries participated in the challenge. Their participation was summarized by 10 papers published by the workshop, which have been extended and revised for this special section. In this preface we overview the challenge datasets, tasks, evaluation metrics, and the obtained outcomes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {13},
numpages = {9},
keywords = {social network analysis, context-aware recommendations, Recommender systems, datasets, context-awareness, user modeling}
}

@article{10.1145/2414425.2414437,
author = {Chen, Yu-Chih and Lin, Yu-Shi and Shen, Yu-Chun and Lin, Shou-De},
title = {A Modified Random Walk Framework for Handling Negative Ratings and Generating Explanations},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414437},
doi = {10.1145/2414425.2414437},
abstract = {The concept of random walk (RW) has been widely applied in the design of recommendation systems. RW-based approaches are effective in handling locality problem and taking extra information, such as the relationships between items or users, into consideration. However, the traditional RW-based approach has a serious limitation in handling bidirectional opinions. The propagation of positive and negative information simultaneously in a graph is nontrivial using random walk. To address the problem, this article presents a novel and efficient RW-based model that can handle both positive and negative comments with the guarantee of convergence. Furthermore, we argue that a good recommendation system should provide users not only a list of recommended items but also reasonable explanations for the decisions. Therefore, we propose a technique that generates explanations by backtracking the influential paths and subgraphs. The results of experiments on the MovieLens and Netflix datasets show that our model significantly outperforms state-of-the-art RW-based algorithms, and is capable of improving the overall performance in the ensemble with other models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {12},
numpages = {21},
keywords = {Ranking, Explanation, Integration, Random Walk, Collaborative Filtering}
}

@article{10.1145/2414425.2414436,
author = {Gedikli, Fatih and Jannach, Dietmar},
title = {Improving Recommendation Accuracy Based on Item-Specific Tag Preferences},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414436},
doi = {10.1145/2414425.2414436},
abstract = {In recent years, different proposals have been made to exploit Social Web tagging information to build more effective recommender systems. The tagging data, for example, were used to identify similar users or were viewed as additional information about the recommendable items. Recent research has indicated that “attaching feelings to tags” is experienced by users as a valuable means to express which features of an item they particularly like or dislike. When following such an approach, users would therefore not only add tags to an item as in usual Web 2.0 applications, but also attach a preference (affect) to the tag itself, expressing, for example, whether or not they liked a certain actor in a given movie. In this work, we show how this additional preference data can be exploited by a recommender system to make more accurate predictions.In contrast to previous work, which also relied on so-called tag preferences to enhance the predictive accuracy of recommender systems, we argue that tag preferences should be considered in the context of an item. We therefore propose new schemes to infer and exploit context-specific tag preferences in the recommendation process. An evaluation on two different datasets reveals that our approach is capable of providing more accurate recommendations than previous tag-based recommender algorithms and recent tag-agnostic matrix factorization techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {11},
numpages = {19},
keywords = {Recommender systems, algorithms, tagging, social web}
}

@article{10.1145/2414425.2414435,
author = {Biancalana, Claudio and Gasparetti, Fabio and Micarelli, Alessandro and Sansonetti, Giuseppe},
title = {An Approach to Social Recommendation for Context-Aware Mobile Services},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414435},
doi = {10.1145/2414425.2414435},
abstract = {Nowadays, several location-based services (LBSs) allow their users to take advantage of information from the Web about points of interest (POIs) such as cultural events or restaurants. To the best of our knowledge, however, none of these provides information taking into account user preferences, or other elements, in addition to location, that contribute to define the context of use. The provided suggestions do not consider, for example, time, day of week, weather, user activity or means of transport. This article describes a social recommender system able to identify user preferences and information needs, thus suggesting personalized recommendations related to POIs in the surroundings of the user's current location. The proposed approach achieves the following goals: (i) to supply, unlike the current LBSs, a methodology for identifying user preferences and needs to be used in the information filtering process; (ii) to exploit the ever-growing amount of information from social networking, user reviews, and local search Web sites; (iii) to establish procedures for defining the context of use to be employed in the recommendation of POIs with low effort. The flexibility of the architecture is such that our approach can be easily extended to any category of POI. Experimental tests carried out on real users enabled us to quantify the benefits of the proposed approach in terms of performance improvement.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {10},
numpages = {31},
keywords = {ubiquitous computing, user modeling, Social recommender system}
}

@article{10.1145/2414425.2414434,
author = {Zhang, Weishi and Ding, Guiguang and Chen, Li and Li, Chunping and Zhang, Chengbo},
title = {Generating Virtual Ratings from Chinese Reviews to Augment Online Recommendations},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414434},
doi = {10.1145/2414425.2414434},
abstract = {Collaborative filtering (CF) recommenders based on User-Item rating matrix as explicitly obtained from end users have recently appeared promising in recommender systems. However, User-Item rating matrix is not always available or very sparse in some web applications, which has critical impact to the application of CF recommenders. In this article we aim to enhance the online recommender system by fusing virtual ratings as derived from user reviews. Specifically, taking into account of Chinese reviews' characteristics, we propose to fuse the self-supervised emotion-integrated sentiment classification results into CF recommenders, by which the User-Item Rating Matrix can be inferred by decomposing item reviews that users gave to the items. The main advantage of this approach is that it can extend CF recommenders to some web applications without user rating information. In the experiments, we have first identified the self-supervised sentiment classification's higher precision and recall by comparing it with traditional classification methods. Furthermore, the classification results, as behaving as virtual ratings, were incorporated into both user-based and item-based CF algorithms. We have also conducted an experiment to evaluate the proximity between the virtual and real ratings and clarified the effectiveness of the virtual ratings. The experimental results demonstrated the significant impact of virtual ratings on increasing system's recommendation accuracy in different data conditions (i.e., conditions with real ratings and without).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {9},
numpages = {17},
keywords = {Information retrieval, sentiment analysis, online recommendation}
}

@article{10.1145/2414425.2414433,
author = {Quijano-Sanchez, Lara and Recio-Garcia, Juan A. and Diaz-Agudo, Belen and Jimenez-Diaz, Guillermo},
title = {Social Factors in Group Recommender Systems},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414433},
doi = {10.1145/2414425.2414433},
abstract = {In this article we review the existing techniques in group recommender systems and we propose some improvement based on the study of the different individual behaviors when carrying out a decision-making process. Our method includes an analysis of group personality composition and trust between each group member to improve the accuracy of group recommenders. This way we simulate the argumentation process followed by groups of people when agreeing on a common activity in a more realistic way. Moreover, we reflect how they expect the system to behave in a long term recommendation process. This is achieved by including a memory of past recommendations that increases the satisfaction of users whose preferences have not been taken into account in previous recommendations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {8},
numpages = {30},
keywords = {personality, trust, Memory, recommender systems, social networks}
}

@article{10.1145/2414425.2414432,
author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
title = {Introduction to the Special Section on Social Recommender Systems},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414432},
doi = {10.1145/2414425.2414432},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {7},
numpages = {2}
}

@article{10.1145/2414425.2414431,
author = {Shen, Keyi and Wu, Jianmin and Zhang, Ya and Han, Yiping and Yang, Xiaokang and Song, Li and Gu, Xiao},
title = {Reorder User's Tweets},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414431},
doi = {10.1145/2414425.2414431},
abstract = {Twitter displays the tweets a user received in a reversed chronological order, which is not always the best choice. As Twitter is full of messages of very different qualities, many informative or relevant tweets might be flooded or displayed at the bottom while some nonsense buzzes might be ranked higher. In this work, we present a supervised learning method for personalized tweets reordering based on user interests. User activities on Twitter, in terms of tweeting, retweeting, and replying, are leveraged to obtain the training data for reordering models. Through exploring a rich set of social and personalized features, we model the relevance of tweets by minimizing the pairwise loss of relevant and irrelevant tweets. The tweets are then reordered according to the predicted relevance scores. Experimental results with real twitter user activities demonstrated the effectiveness of our method. The new method achieved above 30% accuracy gain compared with the default ordering in twitter based on time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {6},
numpages = {17},
keywords = {twitter, Personalization, retweet, reorder}
}

@article{10.1145/2414425.2414430,
author = {Han, Bo and Cook, Paul and Baldwin, Timothy},
title = {Lexical Normalization for Social Media Text},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414430},
doi = {10.1145/2414425.2414430},
abstract = {Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this article, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalizing lexical variants. Our method uses a classifier to detect lexical variants, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {5},
numpages = {27},
keywords = {short text message, microblog, text preprocessing, lexical normalization, Text analysis}
}

@article{10.1145/2414425.2414429,
author = {Chang, Yi and Dong, Anlei and Kolari, Pranam and Zhang, Ruiqiang and Inagaki, Yoshiyuki and Diaz, Fernanodo and Zha, Hongyuan and Liu, Yan},
title = {Improving Recency Ranking Using Twitter Data},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414429},
doi = {10.1145/2414425.2414429},
abstract = {In Web search and vertical search, recency ranking refers to retrieving and ranking documents by both relevance and freshness. As impoverished in-links and click information is the the biggest challenge for recency ranking, we advocate the use of Twitter data to address the challenge in this article. We propose a method to utilize Twitter TinyURL to detect fresh and high-quality documents, and leverage Twitter data to generate novel and effective features for ranking. The empirical experiments demonstrate that the proposed approach effectively improves a commercial search engine for both Web search ranking and tweet vertical ranking.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {4},
numpages = {24},
keywords = {tweet ranking, Twitter, Recency ranking}
}

@article{10.1145/2414425.2414428,
author = {Liu, Xiaohua and Wei, Furu and Zhang, Shaodian and Zhou, Ming},
title = {Named Entity Recognition for Tweets},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414428},
doi = {10.1145/2414425.2414428},
abstract = {Two main challenges of Named Entity Recognition (NER) for tweets are the insufficient information in a tweet and the lack of training data. We propose a novel method consisting of three core elements: (1) normalization of tweets; (2) combination of a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model; and (3) semisupervised learning framework. The tweet normalization preprocessing corrects common ill-formed words using a global linear model. The KNN-based classifier conducts prelabeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semisupervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of normalization, KNN, and semisupervised learning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {3},
numpages = {15},
keywords = {Semisupervised learning, model combination, tweet normalization}
}

@article{10.1145/2414425.2414427,
author = {Cheng, Zhiyuan and Caverlee, James and Lee, Kyumin},
title = {A Content-Driven Framework for Geolocating Microblog Users},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414427},
doi = {10.1145/2414425.2414427},
abstract = {Highly dynamic real-time microblog systems have already published petabytes of real-time human sensor data in the form of status updates. However, the lack of user adoption of geo-based features per user or per post signals that the promise of microblog services as location-based sensing systems may have only limited reach and impact. Thus, in this article, we propose and evaluate a probabilistic framework for estimating a microblog user's location based purely on the content of the user's posts. Our framework can overcome the sparsity of geo-enabled features in these services and bring augmented scope and breadth to emerging location-based personalized information services. Three of the key features of the proposed approach are: (i) its reliance purely on publicly available content; (ii) a classification component for automatically identifying words in posts with a strong local geo-scope; and (iii) a lattice-based neighborhood smoothing model for refining a user's location estimate. On average we find that the location estimates converge quickly, placing 51% of users within 100 miles of their actual location.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {2},
numpages = {27},
keywords = {Twitter, location-based estimation, spatial data mining, text mining, Microblog}
}

@article{10.1145/2414425.2414426,
author = {King, Irwin and Nejdl, Wolfgang},
title = {Introduction to the Special Section on Twitter and Microblogging Services},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414426},
doi = {10.1145/2414425.2414426},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {1},
numpages = {2}
}

@article{10.1145/2337542.2337562,
author = {Mandrake, Lukas and Rebbapragada, Umaa and Wagstaff, Kiri L. and Thompson, David and Chien, Steve and Tran, Daniel and Pappalardo, Robert T. and Gleeson, Damhnait and Casta\~{n}o, Rebecca},
title = {Surface Sulfur Detection via Remote Sensing and Onboard Classification},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337562},
doi = {10.1145/2337542.2337562},
abstract = {Orbital remote sensing provides a powerful way to efficiently survey targets such as the Earth and other planets and moons for features of interest. One such feature of astrobiological relevance is the presence of surface sulfur deposits. These deposits have been observed to be associated with microbial activity at the Borup Fiord glacial springs in Canada, a location that may provide an analogue to other icy environments such as Europa.This article evaluates automated classifiers for detecting sulfur in remote sensing observations by the hyperion spectrometer on the EO-1 spacecraft. We determined that a data-driven machine learning solution was needed because the sulfur could not be detected by simply matching observations to sulfur lab spectra. We also evaluated several methods (manual and automated) for identifying the most relevant attributes (spectral wavelengths) needed for successful sulfur detection. Our findings include (1) the Borup Fiord sulfur deposits were best modeled as containing two sub-populations: sulfur on ice and sulfur on rock; (2) as expected, classifiers using Gaussian kernels outperformed those based on linear kernels, and should be adopted when onboard computational constraints permit; and (3) Recursive Feature Elimination selected sensible and effective features for use in the computationally constrained environment onboard EO-1. This study helped guide the selection of algorithm parameters and configuration for the classification system currently operational on EO-1. Finally, we discuss implications for a similar onboard classification system for a future Europa orbiter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {77},
numpages = {20},
keywords = {support vector machines, feature selection, Remote sensing}
}

@article{10.1145/2337542.2337561,
author = {Wang, Zhenxing and Chan, Laiwan},
title = {Learning Causal Relations in Multivariate Time Series Data},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337561},
doi = {10.1145/2337542.2337561},
abstract = {Many applications naturally involve time series data and the vector autoregression (VAR), and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series. In the first part of this work, we show that the SVAR method is incapable of identifying contemporaneous causal relations for Gaussian process. In addition, least squares estimators become unreliable when the scales of the problems are large and observations are limited. In the remaining part, we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations, and avoid high-order statistical tests. The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large, and high-order statistical tests are required by Bayesian network learning algorithms in this case. To overcome the difficulty, we show that the search space of conditioning sets d-separating two vertices should be a subset of the Markov blankets. Based on this fact, we propose an algorithm enabling us to learn Bayesian networks locally, and make the largest order of statistical tests independent of the scales of the problems. Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {76},
numpages = {28},
keywords = {Bayesian networks, graphical models, causal modeling, VAR}
}

@article{10.1145/2337542.2337560,
author = {Zhang, Xiaoqin Shelley and Shrestha, Bhavesh and Yoon, Sungwook and Kambhampati, Subbarao and DiBona, Phillip and Guo, Jinhong K. and McFarlane, Daniel and Hofmann, Martin O. and Whitebread, Kenneth and Appling, Darren Scott and Whitaker, Elizabeth T. and Trewhitt, Ethan B. and Ding, Li and Michaelis, James R. and McGuinness, Deborah L. and Hendler, James A. and Doppa, Janardhan Rao and Parker, Charles and Dietterich, Thomas G. and Tadepalli, Prasad and Wong, Weng-Keen and Green, Derek and Rebguns, Anton and Spears, Diana and Kuter, Ugur and Levine, Geoff and DeJong, Gerald and MacTavish, Reid L. and Onta\~{n}\'{o}n, Santiago and Radhakrishnan, Jainarayan and Ram, Ashwin and Mostafa, Hala and Zafar, Huzaifa and Zhang, Chongjie and Corkill, Daniel and Lesser, Victor and Song, Zhexuan},
title = {An Ensemble Architecture for Learning Complex Problem-Solving Techniques from Demonstration},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337560},
doi = {10.1145/2337542.2337560},
abstract = {We present a novel ensemble architecture for learning problem-solving techniques from a very small number of expert solutions and demonstrate its effectiveness in a complex real-world domain. The key feature of our “Generalized Integrated Learning Architecture” (GILA) is a set of heterogeneous independent learning and reasoning (ILR) components, coordinated by a central meta-reasoning executive (MRE). The ILRs are weakly coupled in the sense that all coordination during learning and performance happens through the MRE. Each ILR learns independently from a small number of expert demonstrations of a complex task. During performance, each ILR proposes partial solutions to subproblems posed by the MRE, which are then selected from and pieced together by the MRE to produce a complete solution. The heterogeneity of the learner-reasoners allows both learning and problem solving to be more effective because their abilities and biases are complementary and synergistic. We describe the application of this novel learning and problem solving architecture to the domain of airspace management, where multiple requests for the use of airspaces need to be deconflicted, reconciled, and managed automatically. Formal evaluations show that our system performs as well as or better than humans after learning from the same training data. Furthermore, GILA outperforms any individual ILR run in isolation, thus demonstrating the power of the ensemble architecture for learning and problem solving.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {75},
numpages = {38},
keywords = {Ensemble architecture, learning from demonstration, complex problem-solving}
}

@article{10.1145/2337542.2337559,
author = {Strohmaier, Markus and Helic, Denis and Benz, Dominik and K\"{o}rner, Christian and Kern, Roman},
title = {Evaluation of Folksonomy Induction Algorithms},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337559},
doi = {10.1145/2337542.2337559},
abstract = {Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {74},
numpages = {22},
keywords = {taxonomies, Folksonomies, social tagging systems, evaluation}
}

@article{10.1145/2337542.2337558,
author = {Tang, Xuning and Yang, Christopher C.},
title = {Ranking User Influence in Healthcare Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337558},
doi = {10.1145/2337542.2337558},
abstract = {Due to the revolutionary development of Web 2.0 technology, individual users have become major contributors of Web content in online social media. In light of the growing activities, how to measure a user’s influence to other users in online social media becomes increasingly important. This research need is urgent especially in the online healthcare community since positive influence can be beneficial while negative influence may cause-negative impact on other users of the same community. In this article, a research framework was proposed to study user influence within the online healthcare community. We proposed a new approach to incorporate users’ reply relationship, conversation content and response immediacy which capture both explicit and implicit interaction between users to identify influential users of online healthcare community. A weighted social network is developed to represent the influence between users. We tested our proposed techniques thoroughly on two medical support forums. Two algorithms UserRank and Weighted in-degree are benchmarked with PageRank and in-degree. Experiment results demonstrated the validity and effectiveness of our proposed approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {73},
numpages = {21},
keywords = {social media analytics, User influence, ranking algorithm, social network, social computing, Web mining, online healthcare community}
}

@article{10.1145/2337542.2337557,
author = {Lampos, Vasileios and Cristianini, Nello},
title = {Nowcasting Events from the Social Web with Statistical Learning},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337557},
doi = {10.1145/2337542.2337557},
abstract = {We present a general methodology for inferring the occurrence and magnitude of an event or phenomenon by exploring the rich amount of unstructured textual information on the social part of the Web. Having geo-tagged user posts on the microblogging service of Twitter as our input data, we investigate two case studies. The first consists of a benchmark problem, where actual levels of rainfall in a given location and time are inferred from the content of tweets. The second one is a real-life task, where we infer regional Influenza-like Illness rates in the effort of detecting timely an emerging epidemic disease. Our analysis builds on a statistical learning framework, which performs sparse learning via the bootstrapped version of LASSO to select a consistent subset of textual features from a large amount of candidates. In both case studies, selected features indicate close semantic correlation with the target topics and inference, conducted by regression, has a significant performance, especially given the short length --approximately one year-- of Twitter’s data time series.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {72},
numpages = {22},
keywords = {Event detection, LASSO, social network mining, feature selection, sparse learning, Twitter}
}

@article{10.1145/2337542.2337556,
author = {Wang, Haofen and Fu, Linyun and Jin, Wei and Yu, Yong},
title = {EachWiki: Facilitating Wiki Authoring by Annotation Suggestion},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337556},
doi = {10.1145/2337542.2337556},
abstract = {Wikipedia, one of the best-known wikis and the world’s largest free online encyclopedia, has embraced the power of collaborative editing to harness collective intelligence. However, using such a wiki to create high-quality articles is not as easy as people imagine, given for instance the difficulty of reusing knowledge already available in Wikipedia. As a result, the heavy burden of upbuilding and maintaining the ever-growing online encyclopedia still rests on a small group of people. In this article, we aim at facilitating wiki authoring by providing annotation recommendations, thus lightening the burden of both contributors and administrators. We leverage the collective wisdom of the users by exploiting Semantic Web technologies with Wikipedia data and adopt a unified algorithm to support link, category, and semantic relation recommendation. A prototype system named EachWiki is proposed and evaluated. The experimental results show that it has achieved considerable improvements in terms of effectiveness, efficiency and usability. The proposed approach can also be applied to other wiki-based collaborative editing systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {71},
numpages = {18},
keywords = {semantic relation suggestion, Link suggestion, category suggestion}
}

@article{10.1145/2337542.2337555,
author = {Li, Xiaonan and Li, Chengkai and Yu, Cong},
title = {Entity-Relationship Queries over Wikipedia},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337555},
doi = {10.1145/2337542.2337555},
abstract = {Wikipedia is the largest user-generated knowledge base. We propose a structured query mechanism, entity-relationship query, for searching entities in the Wikipedia corpus by their properties and interrelationships. An entity-relationship query consists of multiple predicates on desired entities. The semantics of each predicate is specified with keywords. Entity-relationship query searches entities directly over text instead of preextracted structured data stores. This characteristic brings two benefits: (1) Query semantics can be intuitively expressed by keywords; (2) It only requires rudimentary entity annotation, which is simpler than explicitly extracting and reasoning about complex semantic information before query-time. We present a ranking framework for general entity-relationship queries and a position-based Bounded Cumulative Model (BCM) for accurate ranking of query answers. We also explore various weighting schemes for further improving the accuracy of BCM. We test our ideas on a 2008 version of Wikipedia using a collection of 45 queries pooled from INEX entity ranking track and our own crafted queries. Experiments show that the ranking and weighting schemes are both effective, particularly on multipredicate queries.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {70},
numpages = {20},
keywords = {Entity search and ranking, Wikipedia, structured entity query}
}

@article{10.1145/2337542.2337554,
author = {Carmel, David and Roitman, Haggai and Yom-Tov, Elad},
title = {On the Relationship between Novelty and Popularity of User-Generated Content},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337554},
doi = {10.1145/2337542.2337554},
abstract = {This work deals with the task of predicting the popularity of user-generated content. We demonstrate how the novelty of newly published content plays an important role in affecting its popularity. More specifically, we study three dimensions of novelty. The first one, termed contemporaneous novelty, models the relative novelty embedded in a new post with respect to contemporary content that was generated by others. The second type of novelty, termed self novelty, models the relative novelty with respect to the user’s own contribution history. The third type of novelty, termed discussion novelty, relates to the novelty of the comments associated by readers with respect to the post content. We demonstrate the contribution of the new novelty measures to estimating blog-post popularity by predicting the number of comments expected for a fresh post. We further demonstrate how novelty based measures can be utilized for predicting the citation volume of academic papers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {69},
numpages = {19},
keywords = {Popularity, novelty, user-generated content}
}

@article{10.1145/2337542.2337553,
author = {Potthast, Martin and Stein, Benno and Loose, Fabian and Becker, Steffen},
title = {Information Retrieval in the Commentsphere},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337553},
doi = {10.1145/2337542.2337553},
abstract = {This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as comment-targeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with comment-exploiting retrieval. Throughout the article, connections to information retrieval research are pointed out.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {68},
numpages = {21},
keywords = {Web comments, commentsphere, comment-based retrieval, survey}
}

@article{10.1145/2337542.2337552,
author = {Trivedi, Anusua and Rai, Piyush and Daum\'{e}, Hal and Duvall, Scott L.},
title = {Leveraging Social Bookmarks from Partially Tagged Corpus for Improved Web Page Clustering},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337552},
doi = {10.1145/2337542.2337552},
abstract = {Automatic clustering of Web pages helps a number of information retrieval tasks, such as improving user interfaces, collection clustering, introducing diversity in search results, etc. Typically, Web page clustering algorithms use only features extracted from the page-text. However, the advent of social-bookmarking Web sites, such as StumbleUpon.com and Delicious.com, has led to a huge amount of user-generated content such as the social tag information that is associated with the Web pages. In this article, we present a subspace based feature extraction approach that leverages the social tag information to complement the page-contents of a Web page for extracting beter features, with the goal of improved clustering performance. In our approach, we consider page-text and tags as two separate views of the data, and learn a shared subspace that maximizes the correlation between the two views. Any clustering algorithm can then be applied in this subspace. We then present an extension that allows our approach to be applicable even if the Web page corpus is only partially tagged, that is, when the social tags are present for not all, but only for a small number of Web pages. We compare our subspace based approach with a number of baselines that use tag information in various other ways, and show that the subspace based approach leads to improved performance on the Web page clustering task. We also discuss some possible future work including an active learning extension that can help in choosing which Web pages to get tags for, if we only can get the social tags for only a small number of Web pages.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {67},
numpages = {18},
keywords = {information retrieval, social bookmarking, Web page clustering, kernel methods, canonical correlation analysis}
}

@article{10.1145/2337542.2337551,
author = {Paltoglou, Georgios and Thelwall, Mike},
title = {Twitter, MySpace, Digg: Unsupervised Sentiment Analysis in Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337551},
doi = {10.1145/2337542.2337551},
abstract = {Sentiment analysis is a growing area of research with significant applications in both industry and academia. Most of the proposed solutions are centered around supervised, machine learning approaches and review-oriented datasets. In this article, we focus on the more common informal textual communication on the Web, such as online discussions, tweets and social network comments and propose an intuitive, less domain-specific, unsupervised, lexicon-based approach that estimates the level of emotional intensity contained in text in order to make a prediction. Our approach can be applied to, and is tested in, two different but complementary contexts: subjectivity detection and polarity classification. Extensive experiments were carried on three real-world datasets, extracted from online social Web sites and annotated by human evaluators, against state-of-the-art supervised approaches. The results demonstrate that the proposed algorithm, even though unsupervised, outperforms machine learning solutions in the majority of cases, overall presenting a very robust and reliable solution for sentiment analysis of informal communication on the Web.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {66},
numpages = {19},
keywords = {social media, Opinion mining, sentiment analysis}
}

@article{10.1145/2337542.2337550,
author = {Cortizo, Jos\'{e} Carlos and Carrero, Francisco and Cantador, Iv\'{a}n and Troyano, Jos\'{e} Antonio and Rosso, Paolo},
title = {Introduction to the Special Section on Search and Mining User-Generated Content},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337550},
doi = {10.1145/2337542.2337550},
abstract = {The primary goal of this special section of ACM Transactions on Intelligent Systems and Technology is to foster research in the interplay between Social Media, Data/Opinion Mining and Search, aiming to reflect the actual developments in technologies that exploit user-generated content.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {65},
numpages = {3},
keywords = {information retrieval, data mining, user-generated contents, text mining, social media, opinion mining, Search}
}

@article{10.1145/2337542.2337549,
author = {Sizov, Sergej},
title = {Latent Geospatial Semantics of Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337549},
doi = {10.1145/2337542.2337549},
abstract = {Multimodal understanding of shared content is an important success factor for many Web 2.0 applications and platforms. This article addresses the fundamental question of geo-spatial awareness in social media applications. In this context, we introduce an approach for improved characterization of social media by combining text features (e.g., tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g., geotags, coordinates of images, and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. We demonstrate in systematic studies the benefits of this combination for a broad spectrum of scenarios related to social media: recommender systems, automatic content organization and filtering, and event detection. Furthermore, we establish a simple and technically sound model that can be seen as a reference baseline for future research in the field of geotagged social media.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {64},
numpages = {20},
keywords = {tagging, geotagging, social media, Web2.0}
}

@article{10.1145/2337542.2337548,
author = {Yin, Zhijun and Cao, Liangliang and Gu, Quanquan and Han, Jiawei},
title = {Latent Community Topic Analysis: Integration of Community Discovery with Topic Modeling},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337548},
doi = {10.1145/2337542.2337548},
abstract = {This article studies the problem of latent community topic analysis in text-associated graphs. With the development of social media, a lot of user-generated content is available with user networks. Along with rich information in networks, user graphs can be extended with text information associated with nodes. Topic modeling is a classic problem in text mining and it is interesting to discover the latent topics in text-associated graphs. Different from traditional topic modeling methods considering links, we incorporate community discovery into topic analysis in text-associated graphs to guarantee the topical coherence in the communities so that users in the same community are closely linked to each other and share common latent topics. We handle topic modeling and community discovery in the same framework. In our model we separate the concepts of community and topic, so one community can correspond to multiple topics and multiple communities can share the same topic. We compare different methods and perform extensive experiments on two real datasets. The results confirm our hypothesis that topics could help understand community structure, while community structure could help model topics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {63},
numpages = {21},
keywords = {community discovery, Topic modeling}
}

@article{10.1145/2337542.2337547,
author = {Lerman, Kristina and Hogg, Tad},
title = {Using Stochastic Models to Describe and Predict Social Dynamics of Web Users},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337547},
doi = {10.1145/2337542.2337547},
abstract = {The popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both the hosts of social media content and its consumers. Accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the content. Predicting the popularity of content in social media is challenging due to the complex interactions between content quality and how the social media site highlights its content. Moreover, most social media sites selectively present content that has been highly rated by similar users, whose similarity is indicated implicitly by their behavior or explicitly by links in a social network. While these factors make it difficult to predict popularity a priori, stochastic models of user behavior on these sites can allow predicting popularity based on early user reactions to new content. By incorporating the various mechanisms through which web sites display content, such models improve on predictions that are based on simply extrapolating from the early votes. Specifically, for one such site, the news aggregator Digg, we show how a stochastic model distinguishes the effect of the increased visibility due to the network from how interested users are in the content. We find a wide range of interest, distinguishing stories primarily of interest to users in the network (“niche interests”) from those of more general interest to the user community. This distinction is useful for predicting a story’s eventual popularity from users’ early reactions to the story.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {62},
numpages = {33},
keywords = {social dynamics, social networks, social news, Social media, modeling, prediction}
}

@article{10.1145/2337542.2337546,
author = {Wang, Guan and Xie, Sihong and Liu, Bing and Yu, Philip S.},
title = {Identify Online Store Review Spammers via Social Review Graph},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337546},
doi = {10.1145/2337542.2337546},
abstract = {Online shopping reviews provide valuable information for customers to compare the quality of products, store services, and many other aspects of future purchases. However, spammers are joining this community trying to mislead consumers by writing fake or unfair reviews to confuse the consumers. Previous attempts have used reviewers’ behaviors such as text similarity and rating patterns, to detect spammers. These studies are able to identify certain types of spammers, for instance, those who post many similar reviews about one target. However, in reality, there are other kinds of spammers who can manipulate their behaviors to act just like normal reviewers, and thus cannot be detected by the available techniques.In this article, we propose a novel concept of review graph to capture the relationships among all reviewers, reviews and stores that the reviewers have reviewed as a heterogeneous graph. We explore how interactions between nodes in this graph could reveal the cause of spam and propose an iterative computation model to identify suspicious reviewers. In the review graph, we have three kinds of nodes, namely, reviewer, review, and store. We capture their relationships by introducing three fundamental concepts, the trustiness of reviewers, the honesty of reviews, and the reliability of stores, and identifying their interrelationships: a reviewer is more trustworthy if the person has written more honesty reviews; a store is more reliable if it has more positive reviews from trustworthy reviewers; and a review is more honest if many other honest reviews support it. This is the first time such intricate relationships have been identified for spam detection and captured in a graph model. We further develop an effective computation method based on the proposed graph model. Different from any existing approaches, we do not use an review text information. Our model is thus complementary to existing approaches and able to find more difficult and subtle spamming activities, which are agreed upon by human judges after they evaluate our results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {61},
numpages = {21},
keywords = {Spammer detection, review graph}
}

@article{10.1145/2337542.2337545,
author = {Carmel, David and Uziel, Erel and Guy, Ido and Mass, Yosi and Roitman, Haggai},
title = {Folksonomy-Based Term Extraction for Word Cloud Generation},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337545},
doi = {10.1145/2337542.2337545},
abstract = {In this work we study the task of term extraction for word cloud generation in sparsely tagged domains, in which manual tags are scarce. We present a folksonomy-based term extraction method, called tag-boost, which boosts terms that are frequently used by the public to tag content. Our experiments with tag-boost based term extraction over different domains demonstrate tremendous improvement in word cloud quality, as reflected by the agreement between manual tags of the testing items and the cloud’s terms extracted from the items’ content. Moreover, our results demonstrate the high robustness of this approach, as compared to alternative cloud generation methods that exhibit a high sensitivity to data sparseness. Additionally, we show that tag-boost can be effectively applied even in nontagged domains, by using an external rich folksonomy borrowed from a well-tagged domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {60},
numpages = {20},
keywords = {keyword extraction, tag-boost, Tag-cloud generation}
}

@article{10.1145/2337542.2337544,
author = {Herda\u{g}delen, Ama\c{c} and Baroni, Marco},
title = {Bootstrapping a Game with a Purpose for Commonsense Collection},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337544},
doi = {10.1145/2337542.2337544},
abstract = {Text mining has been very successful in extracting huge amounts of commonsense knowledge from data, but the extracted knowledge tends to be extremely noisy. Manual construction of knowledge repositories, on the other hand, tends to produce high-quality data in very small amounts. We propose an architecture to combine the best of both worlds: A game with a purpose that induces humans to clean up data automatically extracted by text mining. First, a text miner trained on a set of known commonsense facts harvests many more candidate facts from corpora. Then, a simple slot-machine-with-a-purpose game presents these candidate facts to the players for verification by playing. As a result, a new dataset of high precision commonsense knowledge is created. This combined architecture is able to produce significantly better commonsense facts than the state-of-the-art text miner alone. Furthermore, we report that bootstrapping (i.e., training the text miner on the output of the game) improves the subsequent performance of the text miner.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {59},
numpages = {24},
keywords = {commonsense, Games with a purpose, Facebook, natural language processing, knowledge extraction}
}

@article{10.1145/2337542.2337543,
author = {Gabrilovich, Evgeniy and Su, Zhong and Tang, Jie},
title = {Introduction to the Special Section on Computational Models of Collective Intelligence in the Social Web},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337543},
doi = {10.1145/2337542.2337543},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {58},
numpages = {2}
}

@article{10.1145/2168752.2168771,
author = {Rendle, Steffen},
title = {Factorization Machines with LibFM},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168771},
doi = {10.1145/2168752.2168771},
abstract = {Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {57},
numpages = {22},
keywords = {Factorization model, factorization machine, collaborative filtering, recommender system, tensor factorization, matrix factorization}
}

@article{10.1145/2168752.2168770,
author = {Zheng, Yan-Tao and Zha, Zheng-Jun and Chua, Tat-Seng},
title = {Mining Travel Patterns from Geotagged Photos},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168770},
doi = {10.1145/2168752.2168770},
abstract = {Recently, the phenomenal advent of photo-sharing services, such as Flickr and Panoramio, have led to volumous community-contributed photos with text tags, timestamps, and geographic references on the Internet. The photos, together with their time- and geo-references, become the digital footprints of photo takers and implicitly document their spatiotemporal movements. This study aims to leverage the wealth of these enriched online photos to analyze people’s travel patterns at the local level of a tour destination. Specifically, we focus our analysis on two aspects: (1) tourist movement patterns in relation to the regions of attractions (RoA), and (2) topological characteristics of travel routes by different tourists. To do so, we first build a statistically reliable database of travel paths from a noisy pool of community-contributed geotagged photos on the Internet. We then investigate the tourist traffic flow among different RoAs by exploiting the Markov chain model. Finally, the topological characteristics of travel routes are analyzed by performing a sequence clustering on tour routes. Testings on four major cities demonstrate promising results of the proposed system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {56},
numpages = {18},
keywords = {geotagged photos, Travel pattern mining}
}

@article{10.1145/2168752.2168769,
author = {Xu, Jun-Ming and Zhu, Xiaojin and Rogers, Timothy T.},
title = {Metric Learning for Estimating Psychological Similarities},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168769},
doi = {10.1145/2168752.2168769},
abstract = {An important problem in cognitive psychology is to quantify the perceived similarities between stimuli. Previous work attempted to address this problem with multidimensional scaling (MDS) and its variants. However, there are several shortcomings of the MDS approaches. We propose Yada, a novel general metric-learning procedure based on two-alternative forced-choice behavioral experiments. Our method learns forward and backward nonlinear mappings between an objective space in which the stimuli are defined by the standard feature vector representation and a subjective space in which the distance between a pair of stimuli corresponds to their perceived similarity. We conduct experiments on both synthetic and real human behavioral datasets to assess the effectiveness of Yada. The results show that Yada outperforms several standard embedding and metric-learning algorithms, both in terms of likelihood and recovery error.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {55},
numpages = {22},
keywords = {subjective distance, embedding, Metric learning}
}

@article{10.1145/2168752.2168768,
author = {Zhang, Yu and Yeung, Dit-Yan},
title = {Transfer Metric Learning with Semi-Supervised Extension},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168768},
doi = {10.1145/2168752.2168768},
abstract = {Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce, and hence the metrics learned are often unsatisfactory. In this article, we consider a transfer-learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multitask metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multitask learning and adapt the formulation of multitask metric learning to the transfer-learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Moreover, in many applications, some unlabeled data is also available in the target task, and so we propose a semi-supervised extension of TML called STML to further improve the generalization performance by exploiting the unlabeled data based on the manifold assumption. Experimental results on some commonly used transfer-learning applications demonstrate the effectiveness of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {54},
numpages = {28},
keywords = {multitask learning, transfer learning, Metric learning, semi-supervised learning}
}

@article{10.1145/2168752.2168767,
author = {Zhai, Deming and Chang, Hong and Shan, Shiguang and Chen, Xilin and Gao, Wen},
title = {Multiview Metric Learning with Global Consistency and Local Smoothness},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168767},
doi = {10.1145/2168752.2168767},
abstract = {In many real-world applications, the same object may have different observations (or descriptions) from multiview observation spaces, which are highly related but sometimes look different from each other. Conventional metric-learning methods achieve satisfactory performance on distance metric computation of data in a single-view observation space, but fail to handle well data sampled from multiview observation spaces, especially those with highly nonlinear structure. To tackle this problem, we propose a new method called Multiview Metric Learning with Global consistency and Local smoothness (MVML-GL) under a semisupervised learning setting, which jointly considers global consistency and local smoothness. The basic idea is to reveal the shared latent feature space of the multiview observations by embodying global consistency constraints and preserving local geometric structures. Specifically, this framework is composed of two main steps. In the first step, we seek a global consistent shared latent feature space, which not only preserves the local geometric structure in each space but also makes those labeled corresponding instances as close as possible. In the second step, the explicit mapping functions between the input spaces and the shared latent space are learned via regularized locally linear regression. Furthermore, these two steps both can be solved by convex optimizations in closed form. Experimental results with application to manifold alignment on real-world datasets of pose and facial expression demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {53},
numpages = {22},
keywords = {global consistency, Metric learning, local smoothness, multiview learning}
}

@article{10.1145/2168752.2168766,
author = {Hoi, Steven C. H. and Jin, Rong and Tang, Jinhui and Zhou, Zhi-Hua},
title = {Introduction to the Special Section on Distance Metric Learning in Intelligent Systems},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168766},
doi = {10.1145/2168752.2168766},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {52},
numpages = {2}
}

@article{10.1145/2168752.2168765,
author = {Hayden, David S. and Chien, Steve and Thompson, David R. and Casta\~{n}o, Rebecca},
title = {Using Clustering and Metric Learning to Improve Science Return of Remote Sensed Imagery},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168765},
doi = {10.1145/2168752.2168765},
abstract = {Current and proposed remote space missions, such as the proposed aerial exploration of Titan by an aerobot, often can collect more data than can be communicated back to Earth. Autonomous selective downlink algorithms can choose informative subsets of data to improve the science value of these bandwidth-limited transmissions. This requires statistical descriptors of the data that reflect very abstract and subtle distinctions in science content. We propose a metric learning strategy that teaches algorithms how best to cluster new data based on training examples supplied by domain scientists. We demonstrate that clustering informed by metric learning produces results that more closely match multiple scientists’ labelings of aerial data than do clusterings based on random or periodic sampling. A new metric-learning strategy accommodates training sets produced by multiple scientists with different and potentially inconsistent mission objectives. Our methods are fit for current spacecraft processors (e.g., RAD750) and would further benefit from more advanced spacecraft processor architectures, such as OPERA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {51},
numpages = {19},
keywords = {selective data return, Onboard data analysis, information retrieval, space exploration, clustering}
}

@article{10.1145/2168752.2168764,
author = {Estlin, Tara A. and Bornstein, Benjamin J. and Gaines, Daniel M. and Anderson, Robert C. and Thompson, David R. and Burl, Michael and Casta\~{n}o, Rebecca and Judd, Michele},
title = {AEGIS Automated Science Targeting for the MER Opportunity Rover},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168764},
doi = {10.1145/2168752.2168764},
abstract = {The Autonomous Exploration for Gathering Increased Science (AEGIS) system enables automated data collection by planetary rovers. AEGIS software was uploaded to the Mars Exploration Rover (MER) mission’s Opportunity rover in December 2009 and has successfully demonstrated automated onboard targeting based on scientist-specified objectives. Prior to AEGIS, images were transmitted from the rover to the operations team on Earth; scientists manually analyzed the images, selected geological targets for the rover’s remote-sensing instruments, and then generated a command sequence to execute the new measurements. AEGIS represents a significant paradigm shift---by using onboard data analysis techniques, the AEGIS software uses scientist input to select high-quality science targets with no human in the loop. This approach allows the rover to autonomously select and sequence targeted observations in an opportunistic fashion, which is particularly applicable for narrow field-of-view instruments (such as the MER Mini-TES spectrometer, the MER Panoramic camera, and the 2011 Mars Science Laboratory (MSL) ChemCam spectrometer). This article provides an overview of the AEGIS automated targeting capability and describes how it is currently being used onboard the MER mission Opportunity rover.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {50},
numpages = {19},
keywords = {spacecraft autonomy, Data analysis, autonomous science}
}

@article{10.1145/2168752.2168763,
author = {Wagstaff, Kiri L. and Panetta, Julian and Ansar, Adnan and Greeley, Ronald and Pendleton Hoffer, Mary and Bunte, Melissa and Sch\"{o}rghofer, Norbert},
title = {Dynamic Landmarking for Surface Feature Identification and Change Detection},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168763},
doi = {10.1145/2168752.2168763},
abstract = {Given the large volume of images being sent back from remote spacecraft, there is a need for automated analysis techniques that can quickly identify interesting features in those images. Feature identification in individual images and automated change detection in multiple images of the same target are valuable for scientific studies and can inform subsequent target selection. We introduce a new approach to orbital image analysis called dynamic landmarking. It focuses on the identification and comparison of visually salient features in images. We have evaluated this approach on images collected by five Mars orbiters. These evaluations were motivated by three scientific goals: to study fresh impact craters, dust devil tracks, and dark slope streaks on Mars. In the process we also detected a different kind of surface change that may indicate seasonally exposed bedforms. These experiences also point the way to how this approach could be used in an onboard setting to analyze and prioritize data as it is collected.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {49},
numpages = {22},
keywords = {salience, Change detection, image analysis}
}

@article{10.1145/2168752.2168762,
author = {Chien, Steve and Cesta, Amedeo},
title = {Introduction to the Special Section on Artificial Intelligence in Space},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168762},
doi = {10.1145/2168752.2168762},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {48},
numpages = {2}
}

@article{10.1145/2168752.2168761,
author = {Leung, Clement H. C. and Chan, Alice W. S. and Milani, Alfredo and Liu, Jiming and Li, Yuanxi},
title = {Intelligent Social Media Indexing and Sharing Using an Adaptive Indexing Search Engine},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168761},
doi = {10.1145/2168752.2168761},
abstract = {Effective sharing of diverse social media is often inhibited by limitations in their search and discovery mechanisms, which are particularly restrictive for media that do not lend themselves to automatic processing or indexing. Here, we present the structure and mechanism of an adaptive search engine which is designed to overcome such limitations. The basic framework of the adaptive search engine is to capture human judgment in the course of normal usage from user queries in order to develop semantic indexes which link search terms to media objects semantics. This approach is particularly effective for the retrieval of multimedia objects, such as images, sounds, and videos, where a direct analysis of the object features does not allow them to be linked to search terms, for example, nontextual/icon-based search, deep semantic search, or when search terms are unknown at the time the media repository is built. An adaptive search architecture is presented to enable the index to evolve with respect to user feedback, while a randomized query-processing technique guarantees avoiding local minima and allows the meaningful indexing of new media objects and new terms. The present adaptive search engine allows for the efficient community creation and updating of social media indexes, which is able to instill and propagate deep knowledge into social media concerning the advanced search and usage of media resources. Experiments with various relevance distribution settings have shown efficient convergence of such indexes, which enable intelligent search and sharing of social media resources that are otherwise hard to discover.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {47},
numpages = {27},
keywords = {Adaptive indexing, multimedia semantics, social media, evolutionary computation, relevance feedback, genetic algorithms}
}

@article{10.1145/2168752.2168760,
author = {Zhang, Ning and Duan, Ling-Yu and Li, Lingfang and Huang, Qingming and Du, Jun and Gao, Wen and Guan, Ling},
title = {A Generic Approach for Systematic Analysis of Sports Videos},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168760},
doi = {10.1145/2168752.2168760},
abstract = {Various innovative and original works have been applied and proposed in the field of sports video analysis. However, individual works have focused on sophisticated methodologies with particular sport types and there has been a lack of scalable and holistic frameworks in this field. This article proposes a solution and presents a systematic and generic approach which is experimented on a relatively large-scale sports consortia. The system aims at the event detection scenario of an input video with an orderly sequential process. Initially, domain knowledge-independent local descriptors are extracted homogeneously from the input video sequence. Then the video representation is created by adopting a bag-of-visual-words (BoW) model. The video’s genre is first identified by applying the k-nearest neighbor (k-NN) classifiers on the initially obtained video representation, and various dissimilarity measures are assessed and evaluated analytically. Subsequently, an unsupervised probabilistic latent semantic analysis (PLSA)-based approach is employed at the same histogram-based video representation, characterizing each frame of video sequence into one of four view groups, namely closed-up-view, mid-view, long-view, and outer-field-view. Finally, a hidden conditional random field (HCRF) structured prediction model is utilized for interesting event detection. From experimental results, k-NN classifier using KL-divergence measurement demonstrates the best accuracy at 82.16% for genre categorization. Supervised SVM and unsupervised PLSA have average classification accuracies at 82.86% and 68.13%, respectively. The HCRF model achieves 92.31% accuracy using the unsupervised PLSA based label input, which is comparable with the supervised SVM based input at an accuracy of 93.08%. In general, such a systematic approach can be widely applied in processing massive videos generically.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {46},
numpages = {29},
keywords = {Genre categorization, View classification, Event detection, Generic framework}
}

@article{10.1145/2168752.2168759,
author = {Berretti, Stefano and Del Bimbo, Alberto and Pala, Pietro},
title = {Distinguishing Facial Features for Ethnicity-Based 3D Face Recognition},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168759},
doi = {10.1145/2168752.2168759},
abstract = {Among different approaches for 3D face recognition, solutions based on local facial characteristics are very promising, mainly because they can manage facial expression variations by assigning different weights to different parts of the face. However, so far, a few works have investigated the individual relevance that local features play in 3D face recognition with very simple solutions applied in the practice.In this article, a local approach to 3D face recognition is combined with a feature selection model to study the relative relevance of different regions of the face for the purpose of discriminating between different subjects. The proposed solution is experimented using facial scans of the Face Recognition Grand Challenge dataset. Results of the experimentation are two-fold: they quantitatively demonstrate the assumption that different regions of the face have different relevance for face discrimination and also show that the relevance of facial regions changes for different ethnic groups.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {45},
numpages = {20},
keywords = {Feature selection, iso-geodesic stripes, ethnicity-based learning, 3D face recognition}
}

@article{10.1145/2168752.2168758,
author = {Ji, Rongrong and Yao, Hongxun and Tian, Qi and Xu, Pengfei and Sun, Xiaoshuai and Liu, Xianming},
title = {Context-Aware Semi-Local Feature Detector},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168758},
doi = {10.1145/2168752.2168758},
abstract = {How can interest point detectors benefit from contextual cues? In this articles, we introduce a context-aware semi-local detector (CASL) framework to give a systematic answer with three contributions: (1) We integrate the context of interest points to recurrently refine their detections. (2) This integration boosts interest point detectors from the traditionally local scale to a semi-local scale to discover more discriminative salient regions. (3) Such context-aware structure further enables us to bring forward category learning (usually in the subsequent recognition phase) into interest point detection to locate category-aware, meaningful salient regions.Our CASL detector consists of two phases. The first phase accumulates multiscale spatial correlations of local features into a difference of contextual Gaussians (DoCG) field. DoCG quantizes detector context to highlight contextually salient regions at a semi-local scale, which also reveals visual attentions to a certain extent. The second phase locates contextual peaks by mean shift search over the DoCG field, which subsequently integrates contextual cues into feature description. This phase enables us to integrate category learning into mean shift search kernels. This learning-based CASL mechanism produces more category-aware features, which substantially benefits the subsequent visual categorization process. We conducted experiments in image search, object characterization, and feature detector repeatability evaluations, which reported superior discriminability and comparable repeatability to state-of-the-art works.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {44},
numpages = {27},
keywords = {knowledge representation, learning-based feature extraction, context-aware feature, mean shift, Internet, contextual Gaussian field, multimedia systems, Semi-local feature, image analysis, supervised kernel learning}
}

@article{10.1145/2168752.2168757,
author = {Zhang, Shengping and Yao, Hongxun and Sun, Xin and Liu, Shaohui},
title = {Robust Visual Tracking Using an Effective Appearance Model Based on Sparse Coding},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168757},
doi = {10.1145/2168752.2168757},
abstract = {Intelligent video surveillance is currently one of the most active research topics in computer vision, especially when facing the explosion of video data captured by a large number of surveillance cameras. As a key step of an intelligent surveillance system, robust visual tracking is very challenging for computer vision. However, it is a basic functionality of the human visual system (HVS). Psychophysical findings have shown that the receptive fields of simple cells in the visual cortex can be characterized as being spatially localized, oriented, and bandpass, and it forms a sparse, distributed representation of natural images. In this article, motivated by these findings, we propose an effective appearance model based on sparse coding and apply it in visual tracking. Specifically, we consider the responses of general basis functions extracted by independent component analysis on a large set of natural image patches as features and model the appearance of the tracked target as the probability distribution of these features. In order to make the tracker more robust to partial occlusion, camouflage environments, pose changes, and illumination changes, we further select features that are related to the target based on an entropy-gain criterion and ignore those that are not. The target is finally represented by the probability distribution of those related features. The target search is performed by minimizing the Matusita distance between the distributions of the target model and a candidate using Newton-style iterations. The experimental results validate that the proposed method is more robust and effective than three state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {43},
numpages = {18},
keywords = {Intelligent visual surveillance, appearance model, sparse coding}
}

@article{10.1145/2168752.2168756,
author = {Suk, Myunghoon and Ramadass, Ashok and Jin, Yohan and Prabhakaran, B.},
title = {Video Human Motion Recognition Using a Knowledge-Based Hybrid Method Based on a Hidden Markov Model},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168756},
doi = {10.1145/2168752.2168756},
abstract = {Human motion recognition in video data has several interesting applications in fields such as gaming, senior/assisted-living environments, and surveillance. In these scenarios, we may have to consider adding new motion classes (i.e., new types of human motions to be recognized), as well as new training data (e.g., for handling different type of subjects). Hence, both the accuracy of classification and training time for the machine learning algorithms become important performance parameters in these cases. In this article, we propose a knowledge-based hybrid (KBH) method that can compute the probabilities for hidden Markov models (HMMs) associated with different human motion classes. This computation is facilitated by appropriately mixing features from two different media types (3D motion capture and 2D video). We conducted a variety of experiments comparing the proposed KBH for HMMs and the traditional Baum-Welch algorithms. With the advantage of computing the HMM parameter in a noniterative manner, the KBH method outperforms the Baum-Welch algorithm both in terms of accuracy as well as in reduced training time. Moreover, we show in additional experiments that the KBH method also outperforms the linear support vector machine (SVM).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {42},
numpages = {29},
keywords = {hidden Markov model, video human motion recognition, human-computer interaction, 3D motion capture}
}

@article{10.1145/2168752.2168755,
author = {Ewerth, Ralph and M\"{u}hling, Markus and Freisleben, Bernd},
title = {Robust Video Content Analysis via Transductive Learning},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168755},
doi = {10.1145/2168752.2168755},
abstract = {Reliable video content analysis is an essential prerequisite for effective video search. An important current research question is how to develop robust video content analysis methods that produce satisfactory results for a large variety of video sources, distribution platforms, genres, and content. The work presented in this article exploits the observation that the appearance of objects and events is often related to a particular video sequence, episode, program, or broadcast. This motivates our idea of considering the content analysis task for a single video or episode as a transductive setting: the final classification model must be optimal for the given video only, and not in general, as expected for inductive learning. For this purpose, the unlabeled video test data have to be used in the learning process. In this article, a transductive learning framework for robust video content analysis based on feature selection and ensemble classification is presented. In contrast to related transductive approaches for video analysis (e.g., for concept detection), the framework is designed in a general manner and not only for a single task. The proposed framework is applied to the following video analysis tasks: shot boundary detection, face recognition, semantic video retrieval, and semantic indexing of computer game sequences. Experimental results for diverse video analysis tasks and large test sets demonstrate that the proposed transductive framework improves the robustness of the underlying state-of-the-art approaches, whereas transductive support vector machines do not solve particular tasks in a satisfactory manner.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {41},
numpages = {26},
keywords = {cut detection, concept detection, Transductive learning, face recognition, robustness, ensemble classification, self-supervised learning, robust video content analysis}
}

@article{10.1145/2168752.2168754,
author = {Yang, Yi-Hsuan and Chen, Homer H.},
title = {Machine Recognition of Music Emotion: A Review},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168754},
doi = {10.1145/2168752.2168754},
abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {40},
numpages = {30},
keywords = {Music emotion recognition}
}

@article{10.1145/2168752.2168753,
author = {Hua, Xian-Sheng and Tian, Qi and Del Bimbo, Alberto and Jain, Ramesh},
title = {Introduction to the Special Section on Intelligent Multimedia Systems and Technology Part II},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168753},
doi = {10.1145/2168752.2168753},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {39},
numpages = {2}
}

@article{10.1145/2089094.2089114,
author = {Peng, Wei and Sun, Tong and Revankar, Shriram and Li, Tao},
title = {Mining the “Voice of the Customer” for Business Prioritization},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089114},
doi = {10.1145/2089094.2089114},
abstract = {To gain competitiveness and sustained growth in the 21st century, most businesses are on a mission to become more customer-centric. In order to succeed in this endeavor, it is crucial not only to synthesize and analyze the VOC (the VOice of the Customer) data (i.e., the feedbacks or requirements raised by customers), but also to quickly turn these data into actionable knowledge. Although there are many technologies being developed in this complex problem space, most existing approaches in analyzing customer requests are ad hoc, time-consuming, error-prone, people-based processes which hardly scale well as the quantity of customer information explodes. This often results in the slow response to customer requests. In this article, in order to mine VOC to extract useful knowledge for the best product or service quality, we develop a hybrid framework that integrates domain knowledge with data-driven approaches to analyze the semi-structured customer requests. The framework consists of capturing functional features, discovering the overlap or correlation among the features, and identifying the evolving feature trend by using the knowledge transformation model. In addition, since understanding the relative importance of the individual customer request is very critical and has a direct impact on the effective prioritization in the development process, we develop a novel semantic enhanced link-based ranking (SELRank) algorithm for relatively rating/ranking both customer requests and products. The framework has been successfully applied on Xerox Office Group Feature Enhancement Requirements (XOG FER) datasets to analyze customer requests.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {38},
numpages = {17},
keywords = {business prioritization, text mining, Voice of the customer, ranking}
}

@article{10.1145/2089094.2089113,
author = {Zhou, Ke and Bai, Jing and Zha, Hongyuan and Xue, Gui-Rong},
title = {Leveraging Auxiliary Data for Learning to Rank},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089113},
doi = {10.1145/2089094.2089113},
abstract = {In learning to rank, both the quality and quantity of the training data have significant impacts on the performance of the learned ranking functions. However, in many applications, there are usually not sufficient labeled training data for the construction of an accurate ranking model. It is therefore desirable to leverage existing training data from other tasks when learning the ranking function for a particular task, an important problem which we tackle in this article utilizing a boosting framework with transfer learning. In particular, we propose to adaptively learn transferable representations called super-features from the training data of both the target task and the auxiliary task. Those super-features and the coefficients for combining them are learned in an iterative stage-wise fashion. Unlike previous transfer learning methods, the super-features can be adaptively learned by weak learners from the data. Therefore, the proposed framework is sufficiently flexible to deal with complicated common structures among different learning tasks. We evaluate the performance of the proposed transfer learning method for two datasets from the Letor collection and one dataset collected from a commercial search engine, and we also compare our methods with several existing transfer learning methods. Our results demonstrate that the proposed method can enhance the ranking functions of the target tasks utilizing the training data from the auxiliary tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {37},
numpages = {21},
keywords = {transfer learning, experimental evaluation, super-features, convergence analysis, search engine, learning to rank, Relevance}
}

@article{10.1145/2089094.2089112,
author = {Zhang, Weinan and Wang, Dingquan and Xue, Gui-Rong and Zha, Hongyuan},
title = {Advertising Keywords Recommendation for Short-Text Web Pages Using Wikipedia},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089112},
doi = {10.1145/2089094.2089112},
abstract = {Advertising keywords recommendation is an indispensable component for online advertising with the keywords selected from the target Web pages used for contextual advertising or sponsored search. Several ranking-based algorithms have been proposed for recommending advertising keywords. However, for most of them performance is still lacking, especially when dealing with short-text target Web pages, that is, those containing insufficient textual information for ranking. In some cases, short-text Web pages may not even contain enough keywords for selection. A natural alternative is then to recommend relevant keywords not present in the target Web pages. In this article, we propose a novel algorithm for advertising keywords recommendation for short-text Web pages by leveraging the contents of Wikipedia, a user-contributed online encyclopedia. Wikipedia contains numerous entities with related entities on a topic linked to each other. Given a target Web page, we propose to use a content-biased PageRank on the Wikipedia graph to rank the related entities. Furthermore, in order to recommend high-quality advertising keywords, we also add an advertisement-biased factor into our model. With these two biases, advertising keywords that are both relevant to a target Web page and valuable for advertising are recommended. In our experiments, several state-of-the-art approaches for keyword recommendation are compared. The experimental results demonstrate that our proposed approach produces substantial improvement in the precision of the top 20 recommended keywords on short-text Web pages over existing approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {36},
numpages = {25},
keywords = {topic-sensitive PageRank, advertising keywords recommendation, Wikipedia, Contextual advertising}
}

@article{10.1145/2089094.2089111,
author = {Li, Xueying and Cao, Huanhuan and Chen, Enhong and Tian, Jilei},
title = {Learning to Infer the Status of Heavy-Duty Sensors for Energy-Efficient Context-Sensing},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089111},
doi = {10.1145/2089094.2089111},
abstract = {With the prevalence of smart mobile devices with multiple sensors, the commercial application of intelligent context-aware services becomes more and more attractive. However, limited by the battery capacity, the energy efficiency of context-sensing is the bottleneck for the success of context-aware applications. Though several previous studies for energy-efficient context-sensing have been reported, none of them can be applied to multiple types of high-energy-consuming sensors. Moreover, applying machine learning technologies to energy-efficient context-sensing is underexplored too. In this article, we propose to leverage machine learning technologies for improving the energy efficiency of multiple high-energy-consuming context sensors by trading off the sensing accuracy. To be specific, we try to infer the status of high-energy-consuming sensors according to the outputs of software-based sensors and the physical sensors that are necessary to work all the time for supporting the basic functions of mobile devices. If the inference indicates the high-energy-consuming sensor is in a stable status, we avoid the unnecessary invocation and instead use the latest invoked value as the estimation. The experimental results on real datasets show that the energy efficiency of GPS sensing and audio-level sensing are significantly improved by the proposed approach while the sensing accuracy is over 90%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {23},
keywords = {Context-sensing, energy efficiency, machine learning}
}

@article{10.1145/2089094.2089110,
author = {Shakarian, Paulo and Dickerson, John P. and Subrahmanian, V. S.},
title = {Adversarial Geospatial Abduction Problems},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089110},
doi = {10.1145/2089094.2089110},
abstract = {Geospatial Abduction Problems (GAPs) involve the inference of a set of locations that “best explain” a given set of locations of observations. For example, the observations might include locations where a serial killer committed murders or where insurgents carried out Improvised Explosive Device (IED) attacks. In both these cases, we would like to infer a set of locations that explain the observations, for example, the set of locations where the serial killer lives/works, and the set of locations where insurgents locate weapons caches. However, unlike all past work on abduction, there is a strong adversarial component to this; an adversary actively attempts to prevent us from discovering such locations. We formalize such abduction problems as a two-player game where both players (an “agent” and an “adversary”) use a probabilistic model of their opponent (i.e., a mixed strategy). There is asymmetry as the adversary can choose both the locations of the observations and the locations of the explanation, while the agent (i.e., us) tries to discover these. In this article, we study the problem from the point of view of both players. We define reward functions axiomatically to capture the similarity between two sets of explanations (one corresponding to the locations chosen by the adversary, one guessed by the agent). Many different reward functions can satisfy our axioms. We then formalize the Optimal Adversary Strategy (OAS) problem and the Maximal Counter-Adversary strategy (MCA) and show that both are NP-hard, that their associated counting complexity problems are #P-hard, and that MCA has no fully polynomial approximation scheme unless P=NP. We show that approximation guarantees are possible for MCA when the reward function satisfies two simple properties (zero-starting and monotonicity) which many natural reward functions satisfy. We develop a mixed integer linear programming algorithm to solve OAS and two algorithms to (approximately) compute MCA; the algorithms yield different approximation guarantees and one algorithm assumes a monotonic reward function. Our experiments use real data about IED attacks over a 21-month period in Baghdad. We are able to show that both the MCA algorithms work well in practice; while MCA-GREEDY-MONO is both highly accurate and slightly faster than MCA-LS, MCA-LS (to our surprise) always completely and correctly maximized the expected benefit to the agent while running in an acceptable time period.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {34},
numpages = {35},
keywords = {spatial reasoning, Abduction}
}

@article{10.1145/2089094.2089109,
author = {Shi, Lixin and Zhao, Yuhang and Tang, Jie},
title = {Batch Mode Active Learning for Networked Data},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089109},
doi = {10.1145/2089094.2089109},
abstract = {We study a novel problem of batch mode active learning for networked data. In this problem, data instances are connected with links and their labels are correlated with each other, and the goal of batch mode active learning is to exploit the link-based dependencies and node-specific content information to actively select a batch of instances to query the user for learning an accurate model to label unknown instances in the network. We present three criteria (i.e., minimum redundancy, maximum uncertainty, and maximum impact) to quantify the informativeness of a set of instances, and formalize the batch mode active learning problem as selecting a set of instances by maximizing an objective function which combines both link and content information. As solving the objective function is NP-hard, we present an efficient algorithm to optimize the objective function with a bounded approximation rate. To scale to real large networks, we develop a parallel implementation of the algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {33},
numpages = {25},
keywords = {network classification, combine link and content, Batch mode active learning}
}

@article{10.1145/2089094.2089108,
author = {Kolomvatsos, Kostas and Anagnostopoulos, Christos and Hadjiefthymiades, Stathes},
title = {A Fuzzy Logic System for Bargaining in Information Markets},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089108},
doi = {10.1145/2089094.2089108},
abstract = {Future Web business models involve virtual environments where entities interact in order to sell or buy information goods. Such environments are known as Information Markets (IMs). Intelligent agents are used in IMs for representing buyers or information providers (sellers). We focus on the decisions taken by the buyer in the purchase negotiation process with sellers. We propose a reasoning mechanism on the offers (prices of information goods) issued by sellers based on fuzzy logic. The buyer’s knowledge on the negotiation process is modeled through fuzzy sets. We propose a fuzzy inference engine dealing with the decisions that the buyer takes on each stage of the negotiation process. The outcome of the proposed reasoning method indicates whether the buyer should accept or reject the sellers’ offers. Our findings are very promising for the efficiency of automated transactions undertaken by intelligent agents.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {32},
numpages = {26},
keywords = {negotiation process, Fuzzy systems}
}

@article{10.1145/2089094.2089107,
author = {Ma, Huadong and Zeng, Chengbin and Ling, Charles X.},
title = {A Reliable People Counting System via Multiple Cameras},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089107},
doi = {10.1145/2089094.2089107},
abstract = {Reliable and real-time people counting is crucial in many applications. Most previous works can only count moving people from a single camera, which cannot count still people or can fail badly when there is a crowd (i.e., heavy occlusion occurs). In this article, we build a system for robust and fast people counting under occlusion through multiple cameras. To improve the reliability of human detection from a single camera, we use a dimensionality reduction method on the multilevel edge and texture features to handle the large variations in human appearance and poses. To accelerate the detection speed, we propose a novel two-stage cascade-of-rejectors method. To handle the heavy occlusion in crowded scenes, we present a fusion method with error tolerance to combine human detection from multiple cameras. To improve the speed and accuracy of moving people counting, we combine our multiview fusion detection method with particle tracking to count the number of people moving in/out the camera view (“border control”). Extensive experiments and analyses show that our method outperforms state-of-the-art techniques in single- and multicamera datasets for both speed and reliability. We also design a deployed system for fast and reliable people (still or moving) counting by using multiple cameras.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {31},
numpages = {22},
keywords = {human detection, People counting, video surveillance, multiple cameras}
}

@article{10.1145/2089094.2089106,
author = {Bifet, Albert and Frank, Eibe and Holmes, Geoff and Pfahringer, Bernhard},
title = {Ensembles of Restricted Hoeffding Trees},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089106},
doi = {10.1145/2089094.2089106},
abstract = {The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this article, we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way, each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction, we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important, we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons, with one perceptron per class. We propose a mechanism for setting the perceptrons’ learning rate using the change detection method for data streams, and also use to reset ensemble members (i.e., Hoeffding trees) when they no longer perform well. Our experiments show that the resulting ensemble classifier outperforms bagging for data streams in terms of accuracy when both are used in conjunction with adaptive naive Bayes Hoeffding trees, at the expense of runtime and memory consumption. We also show that our stacking method can improve the performance of a bagged ensemble.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {30},
numpages = {20},
keywords = {Data streams, decision trees, ensemble methods}
}

@article{10.1145/2089094.2089105,
author = {Li, Peipei and Wu, Xindong and Hu, Xuegang},
title = {Mining Recurring Concept Drifts with Limited Labeled Streaming Data},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089105},
doi = {10.1145/2089094.2089105},
abstract = {Tracking recurring concept drifts is a significant issue for machine learning and data mining that frequently appears in real-world stream classification problems. It is a challenge for many streaming classification algorithms to learn recurring concepts in a data stream environment with unlabeled data, and this challenge has received little attention from the research community. Motivated by this challenge, this article focuses on the problem of recurring contexts in streaming environments with limited labeled data. We propose a semi-supervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data, called REDLLA, in which a decision tree is adopted as the classification model. When growing a tree, a clustering algorithm based on k-means is installed to produce concept clusters and unlabeled data are labeled in the method of majority-class at leaves. In view of deviations between history and new concept clusters, potential concept drifts are distinguished and recurring concepts are maintained. Extensive studies on both synthetic and real-world data confirm the advantages of our REDLLA algorithm over three state-of-the-art online classification algorithms of CVFDT, DWCDS, and CDRDT and several known online semi-supervised algorithms, even in the case with more than 90% unlabeled data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {29},
numpages = {32},
keywords = {Data stream, decision tree, clustering, concept drift}
}

@article{10.1145/2089094.2089104,
author = {Hajimirsadeghi, Hossein and Ahmadabadi, Majid Nili and Araabi, Babak Nadjar and Moradi, Hadi},
title = {Conceptual Imitation Learning in a Human-Robot Interaction Paradigm},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089104},
doi = {10.1145/2089094.2089104},
abstract = {In general, imitation is imprecisely used to address different levels of social learning from high-level knowledge transfer to low-level regeneration of motor commands. However, true imitation is based on abstraction and conceptualization. This article presents a model for conceptual imitation through interaction with the teacher to abstract spatio-temporal demonstrations based on their functional meaning. Abstraction, concept acquisition, and self-organization of proto-symbols are performed through an incremental and gradual learning algorithm. In this algorithm, Hidden Markov Models (HMMs) are used to abstract perceptually similar demonstrations. However, abstract (relational) concepts emerge as a collection of HMMs irregularly scattered in the perceptual space but showing the same functionality. Performance of the proposed algorithm is evaluated in two experimental scenarios. The first one is a human-robot interaction task of imitating signs produced by hand movements. The second one is a simulated interactive task of imitating whole body motion patterns of a humanoid model. Experimental results show efficiency of our model for concept extraction, proto-symbol emergence, motion pattern recognition, prediction, and generation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {28},
numpages = {25},
keywords = {Imitation, human-robot interaction, concept learning, hidden Markov model}
}

@article{10.1145/2089094.2089103,
author = {Sugiyama, Masashi and Yang, Qiang},
title = {Introduction to the Special Section on the 2nd Asia Conference on Machine Learning (ACML 2010)},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089103},
doi = {10.1145/2089094.2089103},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {27},
numpages = {1}
}

@article{10.1145/2089094.2089102,
author = {Rohrdantz, Christian and Hao, Ming C. and Dayal, Umeshwar and Haug, Lars-Erik and Keim, Daniel A.},
title = {Feature-Based Visual Sentiment Analysis of Text Document Streams},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089102},
doi = {10.1145/2089094.2089102},
abstract = {This article describes automatic methods and interactive visualizations that are tightly coupled with the goal to enable users to detect interesting portions of text document streams. In this scenario the interestingness is derived from the sentiment, temporal density, and context coherence that comments about features for different targets (e.g., persons, institutions, product attributes, topics, etc.) have. Contributions are made at different stages of the visual analytics pipeline, including novel ways to visualize salient temporal accumulations for further exploration. Moreover, based on the visualization, an automatic algorithm aims to detect and preselect interesting time interval patterns for different features in order to guide analysts. The main target group for the suggested methods are business analysts who want to explore time-stamped customer feedback to detect critical issues. Finally, application case studies on two different datasets and scenarios are conducted and an extensive evaluation is provided for the presented intelligent visual interface for feature-based sentiment exploration over time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {26},
numpages = {25},
keywords = {text mining, Document time series, visual analytics, sentiment analysis}
}

@article{10.1145/2089094.2089101,
author = {Liu, Shixia and Zhou, Michelle X. and Pan, Shimei and Song, Yangqiu and Qian, Weihong and Cai, Weijia and Lian, Xiaoxiao},
title = {TIARA: Interactive, Topic-Based Visual Text Summarization and Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089101},
doi = {10.1145/2089094.2089101},
abstract = {We are building an interactive visual text analysis tool that aids users in analyzing large collections of text. Unlike existing work in visual text analytics, which focuses either on developing sophisticated text analytic techniques or inventing novel text visualization metaphors, ours tightly integrates state-of-the-art text analytics with interactive visualization to maximize the value of both. In this article, we present our work from two aspects. We first introduce an enhanced, LDA-based topic analysis technique that automatically derives a set of topics to summarize a collection of documents and their content evolution over time. To help users understand the complex summarization results produced by our topic analysis technique, we then present the design and development of a time-based visualization of the results. Furthermore, we provide users with a set of rich interaction tools that help them further interpret the visualized results in context and examine the text collection from multiple perspectives. As a result, our work offers three unique contributions. First, we present an enhanced topic modeling technique to provide users with a time-sensitive and more meaningful text summary. Second, we develop an effective visual metaphor to transform abstract and often complex text summarization results into a comprehensible visual representation. Third, we offer users flexible visual interaction tools as alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows promise, especially in support of complex text analyses.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {25},
numpages = {28},
keywords = {Text analytics, topic model, stacked graph, interactive text visualization, text summarization, text trend chart}
}

@article{10.1145/2089094.2089100,
author = {Zhang, Yi and Li, Tao},
title = {DClusterE: A Framework for Evaluating and Understanding Document Clustering Using Visualization},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089100},
doi = {10.1145/2089094.2089100},
abstract = {Over the last decade, document clustering, as one of the key tasks in information organization and navigation, has been widely studied. Many algorithms have been developed for addressing various challenges in document clustering and for improving clustering performance. However, relatively few research efforts have been reported on evaluating and understanding document clustering results. In this article, we present DClusterE, a comprehensive and effective framework for document clustering evaluation and understanding using information visualization. DClusterE integrates cluster validation with user interactions and offers rich visualization tools for users to examine document clustering results from multiple perspectives. In particular, through informative views including force-directed layout view, matrix view, and cluster view, DClusterE provides not only different aspects of document inter/intra-clustering structures, but also the corresponding relationship between clustering results and the ground truth. Additionally, DClusterE supports general user interactions such as zoom in/out, browsing, and interactive access of the documents at different levels. Two new techniques are proposed to implement DClusterE: (1) A novel multiplicative update algorithm (MUA) for matrix reordering to generate narrow-banded (or clustered) nonzero patterns from documents. Combined with coarse seriation, MUA is able to provide better visualization of the cluster structures. (2) A Mallows-distance-based algorithm for establishing the relationship between the clustering results and the ground truth, which serves as the basis for coloring schemes. Experiments and user studies are conducted to demonstrate the effectiveness and efficiency of DClusterE.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {24},
keywords = {performance evaluation, clustering, visualization, Document analysis}
}

@article{10.1145/2089094.2089099,
author = {Gretarsson, Brynjar and O’Donovan, John and Bostandjiev, Svetlin and H\"{o}llerer, Tobias and Asuncion, Arthur and Newman, David and Smyth, Padhraic},
title = {TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089099},
doi = {10.1145/2089094.2089099},
abstract = {We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which TopicNets enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful NSF grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a PhD thesis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {23},
numpages = {26},
keywords = {Topic modeling, text visualization, graph visualization}
}

@article{10.1145/2089094.2089098,
author = {Candan, K. Sel\c{c}uk and Di Caro, Luigi and Sapino, Maria Luisa},
title = {PhC: Multiresolution Visualization and Exploration of Text Corpora with Parallel Hierarchical Coordinates},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089098},
doi = {10.1145/2089094.2089098},
abstract = {The high-dimensional nature of the textual data complicates the design of visualization tools to support exploration of large document corpora. In this article, we first argue that the Parallel Coordinates (PC) technique, which can map multidimensional vectors onto a 2D space in such a way that elements with similar values are represented as similar poly-lines or curves in the visualization space, can be used to help users discern patterns in document collections. The inherent reduction in dimensionality during the mapping from multidimensional points to 2D lines, however, may result in visual complications. For instance, the lines that correspond to clusters of objects that are separate in the multidimensional space may overlap each other in the 2D space; the resulting increase in the number of crossings would make it hard to distinguish the individual document clusters. Such crossings of lines and overly dense regions are significant sources of visual clutter, thus avoiding them may help interpret the visualization. In this article, we note that visual clutter can be significantly reduced by adjusting the resolution of the individual term coordinates by clustering the corresponding values. Such reductions in the resolution of the individual term-coordinates, however, will lead to a certain degree of information loss and thus the appropriate resolution for the term-coordinates has to be selected carefully. Thus, in this article we propose a controlled clutter reduction approach, called Parallel hierarchical Coordinates (or PhC), for reducing the visual clutter in PC-based visualizations of text corpora. We define visual clutter and information loss measures and provide extensive evaluations that show that the proposed PhC provides significant visual gains (i.e., multiple orders of reductions in visual clutter) with small information loss during visualization and exploration of document collections.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {22},
numpages = {36},
keywords = {Document set visualization, clutter reduction, parallel coordinates}
}

@article{10.1145/2089094.2089097,
author = {Thai, Vinhtuan and Rouille, Pierre-Yves and Handschuh, Siegfried},
title = {Visual Abstraction and Ordering in Faceted Browsing of Text Collections},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089097},
doi = {10.1145/2089094.2089097},
abstract = {Faceted navigation is a technique for the exploration and discovery of a collection of resources, which can be of various types including text documents. While being information-rich resources, documents are usually not treated as content-bearing items in faceted browsing interfaces, and yet the required clean metadata is not always available or matches users’ interest. In addition, the existing linear listing paradigm for representing result items from the faceted filtering process makes it difficult for users to traverse or compare across facet values in different orders of importance to them. In this context, we report in this article a visual support toward faceted browsing of a collection of documents based on a set of entities of interest to users. Our proposed approach involves using a multi-dimensional visualization as an alternative to the linear listing of focus items. In this visualization, visual abstraction based on a combination of a conceptual structure and the structural equivalence of documents can be simultaneously used to deal with a large number of items. Furthermore, the approach also enables visual ordering based on the importance of facet values to support prioritized, cross-facet comparisons of focus items. A user study was conducted and the results suggest that interfaces using the proposed approach can support users better in exploratory tasks and were also well-liked by the participants of the study, with the hybrid interface combining the multi-dimensional visualization with the linear listing receiving the most favorable ratings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {21},
numpages = {24},
keywords = {Faceted browsing, visual exploration, text collections}
}

@article{10.1145/2089094.2089096,
author = {Cui, Weiwei and Qu, Huamin and Zhou, Hong and Zhang, Wenbin and Skiena, Steve},
title = {Watch the Story Unfold with TextWheel: Visualization of Large-Scale News Streams},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089096},
doi = {10.1145/2089094.2089096},
abstract = {Keyword-based searching and clustering of news articles have been widely used for news analysis. However, news articles usually have other attributes such as source, author, date and time, length, and sentiment which should be taken into account. In addition, news articles and keywords have complicated macro/micro relations, which include relations between news articles (i.e., macro relation), relations between keywords (i.e., micro relation), and relations between news articles and keywords (i.e., macro-micro relation). These macro/micro relations are time varying and pose special challenges for news analysis.In this article we present a visual analytics system for news streams which can bring multiple attributes of the news articles and the macro/micro relations between news streams and keywords into one coherent analytical context, all the while conveying the dynamic natures of news streams. We introduce a new visualization primitive called TextWheel which consists of one or multiple keyword wheels, a document transportation belt, and a dynamic system which connects the wheels and belt. By observing the TextWheel and its content changes, some interesting patterns can be detected. We use our system to analyze several news corpora related to some major companies and the results demonstrate the high potential of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {17},
keywords = {macro-micro relation, text visualization, Document analysis}
}

@article{10.1145/2089094.2089095,
author = {Liu, Shixia and Zhou, Michelle X. and Carenini, Giuseppe and Qu, Huamin},
title = {Introduction to the Special Section on Intelligent Visual Interfaces for Text Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089095},
doi = {10.1145/2089094.2089095},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {19},
numpages = {3}
}

@article{10.1145/2036264.2036282,
author = {Sukthankar, Gita and Sycara, Katia},
title = {Activity Recognition for Dynamic Multi-Agent Teams},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036282},
doi = {10.1145/2036264.2036282},
abstract = {This article addresses the problem of activity recognition for dynamic, physically embodied agent teams. We define team activity recognition as the process of identifying team behaviors from traces of agent positions over time; for many physical domains, military or athletic, coordinated team behaviors create distinctive spatio-temporal patterns that can be used to identify low-level action sequences. This article focuses on the novel problem of recovering agent-to-team assignments for complex team tasks where team composition, the mapping of agents into teams, changes over time. We suggest two methods for improving the computational efficiency of the multi-agent plan recognition process in these cases of changing team composition; our proposed approach is robust to sensor observation noise and errors in behavior classification.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {18},
numpages = {24},
keywords = {teamwork, plan recognition, Activity recognition, multi-agent systems}
}

@article{10.1145/2036264.2036281,
author = {Liao, Zhen and Jiang, Daxin and Chen, Enhong and Pei, Jian and Cao, Huanhuan and Li, Hang},
title = {Mining Concept Sequences from Large-Scale Search Logs for Context-Aware Query Suggestion},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036281},
doi = {10.1145/2036264.2036281},
abstract = {Query suggestion plays an important role in improving usability of search engines. Although some recently proposed methods provide query suggestions by mining query patterns from search logs, none of them models the immediately preceding queries as context systematically, and uses context information effectively in query suggestions. Context-aware query suggestion is challenging in both modeling context and scaling up query suggestion using context. In this article, we propose a novel context-aware query suggestion approach. To tackle the challenges, our approach consists of two stages. In the first, offline model-learning stage, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. A concept sequence suffix tree is then constructed from session data as a context-aware query suggestion model. In the second, online query suggestion stage, a user’s search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence suffix tree, we suggest to the user context-aware queries. We test our approach on large-scale search logs of a commercial search engine containing 4.0 billion Web queries, 5.9 billion clicks, and 1.87 billion search sessions. The experimental results clearly show that our approach outperforms three baseline methods in both coverage and quality of suggestions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {17},
numpages = {40},
keywords = {context-aware, session data, Query suggestion, click-through data}
}

@article{10.1145/2036264.2036280,
author = {Liu, Zhanyi and Wang, Haifeng and Wu, Hua and Li, Sheng},
title = {Two-Word Collocation Extraction Using Monolingual Word Alignment Method},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036280},
doi = {10.1145/2036264.2036280},
abstract = {Statistical bilingual word alignment has been well studied in the field of machine translation. This article adapts the bilingual word alignment algorithm into a monolingual scenario to extract collocations from monolingual corpus, based on the fact that the words in a collocation tend to co-occur in similar contexts as in bilingual word alignment. First, the monolingual corpus is replicated to generate a parallel corpus, in which each sentence pair consists of two identical sentences. Next, the monolingual word alignment algorithm is employed to align potentially collocated words. Finally, the aligned word pairs are ranked according to the alignment scores and candidates with higher scores are extracted as collocations. We conducted experiments on Chinese and English corpora respectively. Compared to previous approaches that use association measures to extract collocations from co-occurrence word pairs within a given window, our method achieves higher precision and recall. According to human evaluation, our method achieves precisions of 62% on a Chinese corpus and 64% on an English corpus. In particular, we can extract collocations with longer spans, achieving a higher precision of 83% on the long-span (&gt; 6 words) Chinese collocations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {16},
numpages = {29},
keywords = {statistical word alignment, Collocation extraction}
}

@article{10.1145/2036264.2036279,
author = {Tang, Lei and Wang, Xufei and Liu, Huan},
title = {Group Profiling for Understanding Social Structures},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036279},
doi = {10.1145/2036264.2036279},
abstract = {The prolific use of participatory Web and social networking sites is reshaping the ways in which people interact with one another. It has become a vital part of human social life in both the developed and developing world. People sharing certain similarities or affiliates tend to form communities within social media. At the same time, they participate in various online activities: content sharing, tagging, posting status updates, etc. These diverse activities leave behind traces of their social life, providing clues to understand changing social structures. A large body of existing work focuses on extracting cohesive groups based on network topology. But little attention is paid to understanding the changing social structures. In order to help explain the formation of a group, we explore different group-profiling strategies to construct descriptions of a group. This research can assist network navigation, visualization, and analysis, as well as monitoring and tracking the ebbs and tides of different groups in evolving networks. By exploiting information collected from real-world social media sites, extensive experiments are conducted to evaluate group-profiling results. The pros and cons of different group-profiling strategies are analyzed with concrete examples. We also show some potential applications based on group profiling. Interesting findings with discussions are reported.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {15},
numpages = {25},
keywords = {social media, Group profiling, community, social structure, group formation}
}

@article{10.1145/2036264.2036278,
author = {Anagnostopoulos, Aris and Broder, Andrei Z. and Gabrilovich, Evgeniy and Josifovski, Vanja and Riedel, Lance},
title = {Web Page Summarization for Just-in-Time Contextual Advertising},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036278},
doi = {10.1145/2036264.2036278},
abstract = {Contextual advertising is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, often ads need to be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low relevance or high latency or high load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time.Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 6% fraction of the page text can sacrifice only 1%--3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500--600 bytes to the usual request. We also compared our summarization approach, which is based on structural properties of the HTML content of the page, with a more principled one based on one of the standard text summarization tools (MEAD), and found their performance to be comparable.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {14},
numpages = {32},
keywords = {Text classification, text summarization}
}

@article{10.1145/2036264.2036277,
author = {Prettenhofer, Peter and Stein, Benno},
title = {Cross-Lingual Adaptation Using Structural Correspondence Learning},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036277},
doi = {10.1145/2036264.2036277},
abstract = {Cross-lingual adaptation is a special case of domain adaptation and refers to the transfer of classification knowledge between two languages. In this article we describe an extension of Structural Correspondence Learning (SCL), a recently proposed algorithm for domain adaptation, for cross-lingual adaptation in the context of text classification. The proposed method uses unlabeled documents from both languages, along with a word translation oracle, to induce a cross-lingual representation that enables the transfer of classification knowledge from the source to the target language. The main advantages of this method over existing methods are resource efficiency and task specificity.We conduct experiments in the area of cross-language topic and sentiment classification involving English as source language and German, French, and Japanese as target languages. The results show a significant improvement of the proposed method over a machine translation baseline, reducing the relative error due to cross-lingual adaptation by an average of 30% (topic classification) and 59% (sentiment classification). We further report on empirical analyses that reveal insights into the use of unlabeled data, the sensitivity with respect to important hyperparameters, and the nature of the induced cross-lingual word correspondences.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {13},
numpages = {22},
keywords = {cross-lingual adaptation, structural correspondence learning, Cross-language text classification}
}

@article{10.1145/2036264.2036276,
author = {Wang, Jingdong and Hua, Xian-Sheng},
title = {Interactive Image Search by Color Map},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036276},
doi = {10.1145/2036264.2036276},
abstract = {The availability of large-scale images from the Internet has made the research on image search attract a lot of attention. Text-based image search engines, for example, Google/Microsoft Bing/Yahoo! image search engines using the surrounding text, have been developed and widely used. However, they suffer from an inability to search image content. In this article, we present an interactive image search system, image search by color map, which can be applied to, but not limited to, enhance text-based image search. This system enables users to indicate how the colors are spatially distributed in the desired images, by scribbling a few color strokes, or dragging an image and highlighting a few regions of interest in an intuitive way. In contrast to the conventional sketch-based image retrieval techniques, our system searches images based on colors rather than shapes, and we, technically, propose a simple but effective scheme to mine the latent search intention from the user’s input, and exploit the dominant color filter strategy to make our system more efficient. We integrate our system to existing Web image search engines to demonstrate its superior performance over text-based image search. The user study shows that our system can indeed help users conveniently find desired images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {12},
numpages = {23},
keywords = {color map, intention map, Image search}
}

@article{10.1145/2036264.2036275,
author = {Jiang, Yingying and Tian, Feng and Zhang, Xiaolong (Luke) and Dai, Guozhong and Wang, Hongan},
title = {Understanding, Manipulating and Searching Hand-Drawn Concept Maps},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036275},
doi = {10.1145/2036264.2036275},
abstract = {Concept maps are an important tool to organize, represent, and share knowledge. Building a concept map involves creating text-based concepts and specifying their relationships with line-based links. Current concept map tools usually impose specific task structures for text and link construction, and may increase cognitive burden to generate and interact with concept maps. While pen-based devices (e.g., tablet PCs) offer users more freedom in drawing concept maps with a pen or stylus more naturally, the support for hand-drawn concept map creation and manipulation is still limited, largely due to the lack of methods to recognize the components and structures of hand-drawn concept maps. This article proposes a method to understand hand-drawn concept maps. Our algorithm can extract node blocks, or concept blocks, and link blocks of a hand-drawn concept map by combining dynamic programming and graph partitioning, recognize the text content of each concept node, and build a concept-map structure by relating concepts and links. We also design an algorithm for concept map retrieval based on hand-drawn queries. With our algorithms, we introduce structure-based intelligent manipulation techniques and ink-based retrieval techniques to support the management and modification of hand-drawn concept maps. Results from our evaluation study show high structure recognition accuracy in real time of our method, and good usability of intelligent manipulation and retrieval techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {11},
numpages = {21},
keywords = {intelligent manipulation, retrieval, Hand-drawn concept map, recognition}
}

@article{10.1145/2036264.2036274,
author = {Cioffi-Revilla, Claudio and Rogers, J. Daniel and Hailegiorgis, Atesmachew},
title = {Geographic Information Systems and Spatial Agent-Based Model Simulations for Sustainable Development},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036274},
doi = {10.1145/2036264.2036274},
abstract = {In recent years the interdisciplinary field of Computational Social Science has developed theory and methodologies for building spatial Agent-Based Social Simulation (ABSS) models of human societies that are situated in ecosystems with land cover and climate. This article explains the needs and demand for Geographic Information Systems (GIS) in these types of agent-based models, with an emphasis on models applied to Eastern Africa and Inner Asia and relevance for understanding and analyzing development issues. The models are implemented with the MASON (Multi-Agent Simulator Of Networks and Neighborhoods) system, an open-source simulation environment in the Java language and suitable for developing ABSS models with GIS for representing spatial features.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {10},
numpages = {11},
keywords = {Eastern Africa, Inner Asia, Geographic Information Systems (GIS), Spatial Agent-based Modeling (ABM), computational social science, Multi-agent Simulator of Networks and Neighborhoods (MASON), Multi-agent Systems (MAS)}
}

@article{10.1145/2036264.2036273,
author = {Vu, Thuc and Shoham, Yoav},
title = {Fair Seeding in Knockout Tournaments},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036273},
doi = {10.1145/2036264.2036273},
abstract = {We investigated the existence of fair seeding in knockout tournaments. We define two fairness criteria, both adapted from the literature: envy-freeness and order preservation. We show how to achieve the first criterion in tournaments whose structure is unconstrained, and prove an impossibility result for balanced tournaments. For the second criterion we have a similar result for unconstrained tournaments, but not for the balanced case. We provide instead a heuristic algorithm which we show through experiments to be efficient and effective. This suggests that the criterion is achievable also in balanced tournaments. However, we prove that it again becomes impossible to achieve when we add a weak condition guarding against the phenomenon of tournament dropout.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {9},
numpages = {17},
keywords = {heuristic algorithm, Knockout tournament}
}

@article{10.1145/2036264.2036272,
author = {Gal, Ya’akov and Kraus, Sarit and Gelfand, Michele and Khashan, Hilal and Salmon, Elizabeth},
title = {An Adaptive Agent for Negotiating with People in Different Cultures},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036272},
doi = {10.1145/2036264.2036272},
abstract = {The rapid dissemination of technology such as the Internet across geographical and ethnic lines is opening up opportunities for computer agents to negotiate with people of diverse cultural and organizational affiliations. To negotiate proficiently with people in different cultures, agents need to be able to adapt to the way behavioral traits of other participants change over time. This article describes a new agent for repeated bilateral negotiation that was designed to model and adapt its behavior to the individual traits exhibited by its negotiation partner. The agent’s decision-making model combined a social utility function that represented the behavioral traits of the other participant, as well as a rule-based mechanism that used the utility function to make decisions in the negotiation process. The agent was deployed in a strategic setting in which both participants needed to complete their individual tasks by reaching agreements and exchanging resources, the number of negotiation rounds was not fixed in advance and agreements were not binding. The agent negotiated with human subjects in the United States and Lebanon in situations that varied the dependency relationships between participants at the onset of negotiation. There was no prior data available about the way people would respond to different negotiation strategies in these two countries. Results showed that the agent was able to adopt a different negotiation strategy to each country. Its average performance across both countries was equal to that of people. However, the agent outperformed people in the United States, because it learned to make offers that were likely to be accepted by people, while being more beneficial to the agent than to people. In contrast, the agent was outperformed by people in Lebanon, because it adopted a high reliability measure which allowed people to take advantage of it. These results provide insight for human-computer agent designers in the types of multicultural settings that we considered, showing that adaptation is a viable approach towards the design of computer agents to negotiate with people when there is no prior data of their behavior.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {8},
numpages = {24},
keywords = {Human-agent decision making, cultural modeling}
}

@article{10.1145/2036264.2036271,
author = {Shakarian, Paulo and Subrahmanian, V. S. and Sapino, Maria Luisa},
title = {GAPs: Geospatial Abduction Problems},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036271},
doi = {10.1145/2036264.2036271},
abstract = {There are many applications where we observe various phenomena in space (e.g., locations of victims of a serial killer), and where we want to infer “partner” locations (e.g., the location where the killer lives) that are geospatially related to the observed phenomena. In this article, we define geospatial abduction problems (GAPs for short). We analyze the complexity of GAPs, develop exact and approximate algorithms (often with approximation guarantees) for these problems together with analyses of these algorithms, and develop a prototype implementation of our GAP framework. We demonstrate accuracy of our algorithms on a real world data set consisting of insurgent IED (improvised explosive device) attacks against U.S. forces in Iraq (the observations were the locations of the attacks, while the “partner” locations we were trying to infer were the locations of IED weapons caches).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {7},
numpages = {27},
keywords = {complexity analysis, Abduction, heuristic algorithms}
}

@article{10.1145/2036264.2036270,
author = {Liu, Huan and Nau, Dana},
title = {Introduction},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036270},
doi = {10.1145/2036264.2036270},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {6},
numpages = {1}
}

@article{10.1145/2036264.2036269,
author = {Zhao, Shiwan and Zhou, Michelle X. and Zhang, Xiatian and Yuan, Quan and Zheng, Wentao and Fu, Rongyao},
title = {Who is Doing What and When: Social Map-Based Recommendation for Content-Centric Social Web Sites},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036269},
doi = {10.1145/2036264.2036269},
abstract = {Content-centric social Web sites, such as discussion forums and blog sites, have flourished during the past several years. These sites often contain overwhelming amounts of information that are also being updated rapidly. To help users locate their interests at such sites (e.g., interesting blogs to read or discussion forums to join), researchers have developed a number of recommendation technologies. However, it is difficult to make effective recommendations for new users (a.k.a. the cold start problem) due to a lack of user information (e.g., preferences and interests). Furthermore, the complexity of recommendation algorithms often prevents users from comprehending let alone trusting the recommended results. To tackle these above two challenges, we are building a social map-based recommender system called Pharos. A social map summarizes users’ content-related social behavior over time (e.g., reading, writing, and commenting behavior during the past week) as a set of latent communities. For a given time interval, each community is characterized by the theme of the content being discussed and the key people involved. By discovering, ranking, and displaying the most popular latent communities at different time intervals, Pharos creates a time-sensitive, visual social map of a Web site. This enables new users to obtain a quick overview of the site, alleviating the cold start problem. Furthermore, we use the social map as a context to help explain Pharos-recommended content and people. Users can also interactively explore the social map to locate the content in which they are interested or people that are not being explicitly recommended, compensating for the imperfections in the recommendation algorithms. We have developed several Pharos applications, one of which is deployed within our company. Our preliminary evaluation of the deployed application shows the usefulness of Pharos.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {5},
numpages = {23},
keywords = {visual explanation, social map, Recommender systems, trust, cold start}
}

@article{10.1145/2036264.2036268,
author = {McNally, Kevin and O’Mahony, Michael P. and Coyle, Maurice and Briggs, Peter and Smyth, Barry},
title = {A Case Study of Collaboration and Reputation in Social Web Search},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036268},
doi = {10.1145/2036264.2036268},
abstract = {Although collaborative searching is not supported by mainstream search engines, recent research has highlighted the inherently collaborative nature of many Web search tasks. In this article, we describe HeyStaks, a collaborative Web search framework that is designed to complement mainstream search engines. At search time, HeyStaks learns from the search activities of other users and leverages this information to generate recommendations based on results that others have found relevant for similar searches. The key contribution of this article is to extend the HeyStaks social search model by considering the search expertise, or reputation, of HeyStaks users and using this information to enhance the result recommendation process. In particular, we propose a reputation model for HeyStaks users that utilise the implicit collaboration events that take place between users as recommendations are made and selected. We describe a live-user trial of HeyStaks that demonstrates the relevance of its core recommendations and the ability of the reputation model to further improve recommendation quality. Our findings indicate that incorporating reputation into the recommendation process further improves the relevance of HeyStaks recommendations by up to 40%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {reputation, Trust, social search, HeyStaks}
}

@article{10.1145/2036264.2036267,
author = {Vasuki, Vishvas and Natarajan, Nagarajan and Lu, Zhengdong and Savas, Berkant and Dhillon, Inderjit},
title = {Scalable Affiliation Recommendation Using Auxiliary Networks},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036267},
doi = {10.1145/2036264.2036267},
abstract = {Social network analysis has attracted increasing attention in recent years. In many social networks, besides friendship links among users, the phenomenon of users associating themselves with groups or communities is common. Thus, two networks exist simultaneously: the friendship network among users, and the affiliation network between users and groups. In this article, we tackle the affiliation recommendation problem, where the task is to predict or suggest new affiliations between users and communities, given the current state of the friendship and affiliation networks. More generally, affiliations need not be community affiliations---they can be a user’s taste, so affiliation recommendation algorithms have applications beyond community recommendation. In this article, we show that information from the friendship network can indeed be fruitfully exploited in making affiliation recommendations. Using a simple way of combining these networks, we suggest two models of user-community affinity for the purpose of making affiliation recommendations: one based on graph proximity, and another using latent factors to model users and communities. We explore the affiliation recommendation algorithms suggested by these models and evaluate these algorithms on two real-world networks, Orkut and Youtube. In doing so, we motivate and propose a way of evaluating recommenders, by measuring how good the top 50 recommendations are for the average user, and demonstrate the importance of choosing the right evaluation strategy. The algorithms suggested by the graph proximity model turn out to be the most effective. We also introduce scalable versions of these algorithms, and demonstrate their effectiveness. This use of link prediction techniques for the purpose of affiliation recommendation is, to our knowledge, novel.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {3},
numpages = {20},
keywords = {Scalability}
}

@article{10.1145/2036264.2036266,
author = {Lipczak, Marek and Milios, Evangelos},
title = {Efficient Tag Recommendation for Real-Life Data},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036266},
doi = {10.1145/2036264.2036266},
abstract = {Despite all of the advantages of tags as an easy and flexible information management approach, tagging is a cumbersome task. A set of descriptive tags has to be manually entered by users whenever they post a resource. This process can be simplified by the use of tag recommendation systems. Their objective is to suggest potentially useful tags to the user. We present a hybrid tag recommendation system together with a scalable, highly efficient system architecture. The system is able to utilize user feedback to tune its parameters to specific characteristics of the underlying tagging system and adapt the recommendation models to newly added content. The evaluation of the system on six real-life datasets demonstrated the system’s ability to combine tags from various sources (e.g., resource content or tags previously used by the user) to achieve the best quality of recommended tags. It also confirmed the importance of parameter tuning and content adaptation. A series of additional experiments allowed us to better understand the characteristics of the system and tagging datasets and to determine the potential areas for further system development.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {2},
numpages = {21},
keywords = {folksonomies, Tag recommendation, hybrid systems, collaborative tagging, broad folksonomies, narrow folksonomies}
}

@article{10.1145/2036264.2036265,
author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
title = {Introduction},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036265},
doi = {10.1145/2036264.2036265},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {1},
numpages = {2}
}

@article{10.1145/1989734.1989746,
author = {Haigh, Karen Zita and Yaman, Fusun},
title = {RECYCLE: Learning Looping Workflows from Annotated Traces},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989746},
doi = {10.1145/1989734.1989746},
abstract = {A workflow is a model of a process that systematically describes patterns of activity. Workflows capture a sequence of operations, their enablement conditions, and data flow dependencies among them. It is hard to design a complete and correct workflow from scratch, while it is much easier for humans to demonstrate the solution than to state the solution declaratively.This article presents RECYCLE, our approach to learning workflow models from example demonstration traces. RECYCLE captures control flow, data flow, and enablement conditions of an underlying workflow process. Unlike prior work from workflow mining and AI planning literature, (1) RECYCLE can learn from a single demonstration trace with loops, (2) RECYCLE learns both loop and conditional branch structure, and (3) RECYCLE handles data flow among actions.In this article, we describe the phases of RECYCLE's learning algorithm: substructure analysis and node abstraction. To ground the discussion, we present a simplified flight reservation system with some of the important characteristics of the real domains we worked with. We present some results from a patient transport domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {32},
keywords = {learning from demonstration, learning from traces, process mining, workflow learning, Hierarchical Task Network learning}
}

@article{10.1145/1989734.1989745,
author = {Reddy, Sudhakar Y. and Frank, Jeremy D. and Iatauro, Michael J. and Boyce, Matthew E. and K\"{u}rkl\"{u}, Elif and Ai-Chang, Mitchell and J\'{o}nsson, Ari K.},
title = {Planning Solar Array Operations on the International Space Station},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989745},
doi = {10.1145/1989734.1989745},
abstract = {Flight controllers manage the orientation and modes of eight large solar arrays that power the International Space Station (ISS). The task requires generating plans that balance complex constraints and preferences. These considerations include context-dependent constraints on viable solar array configurations, temporal limits on transitions between configurations, and preferences on which considerations have priority. The Solar Array Constraint Engine (SACE) treats this operations planning problem as a sequence of tractable constrained optimization problems. SACE uses constraint management and automated planning capabilities to reason about the constraints, to find optimal array configurations subject to these constraints and solution preferences, and to automatically generate solar array operations plans. SACE further provides flight controllers with real-time situational awareness and what-if analysis capabilities. SACE is built on the Extensible Universal Remote Operations Planning Architecture (EUROPA) model-based planning system. EUROPA facilitated SACE development by providing model-based planning, built-in constraint reasoning capability, and extensibility. This article formulates the planning problem, explains how EUROPA solves the problem, and provides performance statistics from several planning scenarios. SACE reduces a highly manual process that takes weeks to an automated process that takes tens of minutes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {24},
keywords = {optimization, scheduling, constraint satisfaction, Planning, space mission operations}
}

@article{10.1145/1989734.1989744,
author = {Berry, Pauline M. and Gervasio, Melinda and Peintner, Bart and Yorke-Smith, Neil},
title = {PTIME: Personalized Assistance for Calendaring},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989744},
doi = {10.1145/1989734.1989744},
abstract = {In a world of electronic calendars, the prospect of intelligent, personalized time management assistance seems a plausible and desirable application of AI. PTIME (Personalized Time Management) is a learning cognitive assistant agent that helps users handle email meeting requests, reserve venues, and schedule events. PTIME is designed to unobtrusively learn scheduling preferences, adapting to its user over time. The agent allows its user to flexibly express requirements for new meetings, as they would to an assistant. It interfaces with commercial enterprise calendaring platforms, and it operates seamlessly with users who do not have PTIME. This article overviews the system design and describes the models and technical advances required to satisfy the competing needs of preference modeling and elicitation, constraint reasoning, and machine learning. We further report on a multifaceted evaluation of the perceived usefulness of the system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {40},
numpages = {22},
keywords = {machine learning, preference modeling, calendaring, Personal assistant agents}
}

@article{10.1145/1989734.1989743,
author = {Ding, Wei and Stepinski, Tomasz F. and Mu, Yang and Bandeira, Lourenco and Ricardo, Ricardo and Wu, Youxi and Lu, Zhenyu and Cao, Tianyu and Wu, Xindong},
title = {Subkilometer Crater Discovery with Boosting and Transfer Learning},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989743},
doi = {10.1145/1989734.1989743},
abstract = {Counting craters in remotely sensed images is the only tool that provides relative dating of remote planetary surfaces. Surveying craters requires counting a large amount of small subkilometer craters, which calls for highly efficient automatic crater detection. In this article, we present an integrated framework on autodetection of subkilometer craters with boosting and transfer learning. The framework contains three key components. First, we utilize mathematical morphology to efficiently identify crater candidates, the regions of an image that can potentially contain craters. Only those regions occupying relatively small portions of the original image are the subjects of further processing. Second, we extract and select image texture features, in combination with supervised boosting ensemble learning algorithms, to accurately classify crater candidates into craters and noncraters. Third, we integrate transfer learning into boosting, to enhance detection performance in the regions where surface morphology differs from what is characterized by the training set. Our framework is evaluated on a large test image of 37,500 \texttimes{} 56,250 m2 on Mars, which exhibits a heavily cratered Martian terrain characterized by nonuniform surface morphology. Empirical studies demonstrate that the proposed crater detection framework can achieve an F1 score above 0.85, a significant improvement over the other crater detection algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {22},
keywords = {planetary and space science, feature selection, transfer learning, spatial data mining, Classification}
}

@article{10.1145/1989734.1989742,
author = {Toole, Jameson L. and Eagle, Nathan and Plotkin, Joshua B.},
title = {Spatiotemporal Correlations in Criminal Offense Records},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989742},
doi = {10.1145/1989734.1989742},
abstract = {With the increased availability of rich behavioral datasets, we present a novel application of tools to analyze this information. Using criminal offense records as an example, we employ cross-correlation measures, eigenvalue spectrum analysis, and results from random matrix theory to identify spatiotemporal patterns on multiple scales. With these techniques, we show that most significant correlation exists on the time scale of weeks and identify clusters of neighborhoods whose crime rates are affected simultaneously by external forces.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {38},
numpages = {18},
keywords = {engineering social systems, Big data, computational social science, computational sustainability, criminology}
}

@article{10.1145/1989734.1989741,
author = {Li, Zhenhui and Han, Jiawei and Ji, Ming and Tang, Lu-An and Yu, Yintao and Ding, Bolin and Lee, Jae-Gil and Kays, Roland},
title = {MoveMine: Mining Moving Object Data for Discovery of Animal Movement Patterns},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989741},
doi = {10.1145/1989734.1989741},
abstract = {With the maturity and wide availability of GPS, wireless, telecommunication, and Web technologies, massive amounts of object movement data have been collected from various moving object targets, such as animals, mobile devices, vehicles, and climate radars. Analyzing such data has deep implications in many applications, such as, ecological study, traffic control, mobile communication management, and climatological forecast. In this article, we focus our study on animal movement data analysis and examine advanced data mining methods for discovery of various animal movement patterns. In particular, we introduce a moving object data mining system, MoveMine, which integrates multiple data mining functions, including sophisticated pattern mining and trajectory analysis. In this system, two interesting moving object pattern mining functions are newly developed: (1) periodic behavior mining and (2) swarm pattern mining. For mining periodic behaviors, a reference location-based method is developed, which first detects the reference locations, discovers the periods in complex movements, and then finds periodic patterns by hierarchical clustering. For mining swarm patterns, an efficient method is developed to uncover flexible moving object clusters by relaxing the popularly-enforced collective movement constraints.In the MoveMine system, a set of commonly used moving object mining functions are built and a user-friendly interface is provided to facilitate interactive exploration of moving object data mining and flexible tuning of the mining constraints and parameters. MoveMine has been tested on multiple kinds of real datasets, especially for MoveBank applications and other moving object data analysis. The system will benefit scientists and other users to carry out versatile analysis tasks to analyze object movement regularities and anomalies. Moreover, it will benefit researchers to realize the importance and limitations of current techniques and promote future studies on moving object data mining. As expected, a mastery of animal movement patterns and trends will improve our understanding of the interactions between and the changes of the animal world and the ecosystem and therefore help ensure the sustainability of our ecosystem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {32},
keywords = {swarm pattern, pattern mining, computational sustainability, Moving objects, periodic behavior}
}

@article{10.1145/1989734.1989740,
author = {Mithal, Varun and Garg, Ashish and Boriah, Shyam and Steinbach, Michael and Kumar, Vipin and Potter, Christopher and Klooster, Steven and Castilla-Rubio, Juan Carlos},
title = {Monitoring Global Forest Cover Using Data Mining},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989740},
doi = {10.1145/1989734.1989740},
abstract = {Forests are a critical component of the planet's ecosystem. Unfortunately, there has been significant degradation in forest cover over recent decades as a result of logging, conversion to crop, plantation, and pasture land, or disasters (natural or man made) such as forest fires, floods, and hurricanes. As a result, significant attention is being given to the sustainable use of forests. A key to effective forest management is quantifiable knowledge about changes in forest cover. This requires identification and characterization of changes and the discovery of the relationship between these changes and natural and anthropogenic variables. In this article, we present our preliminary efforts and achievements in addressing some of these tasks along with the challenges and opportunities that need to be addressed in the future. At a higher level, our goal is to provide an overview of the exciting opportunities and challenges in developing and applying data mining approaches to provide critical information for forest and land use management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {36},
numpages = {24},
keywords = {forest cover change, Computational sustainability, land change, remote sensing}
}

@article{10.1145/1989734.1989739,
author = {Ramchurn, Sarvapali D. and Vytelingum, Perukrishnen and Rogers, Alex and Jennings, Nicholas R.},
title = {Agent-Based Homeostatic Control for Green Energy in the Smart Grid},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989739},
doi = {10.1145/1989734.1989739},
abstract = {With dwindling nonrenewable energy reserves and the adverse effects of climate change, the development of the smart electricity grid is seen as key to solving global energy security issues and to reducing carbon emissions. In this respect, there is a growing need to integrate renewable (or green) energy sources in the grid. However, the intermittency of these energy sources requires that demand must also be made more responsive to changes in supply, and a number of smart grid technologies are being developed, such as high-capacity batteries and smart meters for the home, to enable consumers to be more responsive to conditions on the grid in real time. Traditional solutions based on these technologies, however, tend to ignore the fact that individual consumers will behave in such a way that best satisfies their own preferences to use or store energy (as opposed to that of the supplier or the grid operator). Hence, in practice, it is unclear how these solutions will cope with large numbers of consumers using their devices in this way. Against this background, in this article, we develop novel control mechanisms based on the use of autonomous agents to better incorporate consumer preferences in managing demand. These agents, residing on consumers' smart meters, can both communicate with the grid and optimize their owner's energy consumption to satisfy their preferences. More specifically, we provide a novel control mechanism that models and controls a system comprising of a green energy supplier operating within the grid and a number of individual homes (each possibly owning a storage device). This control mechanism is based on the concept of homeostasis whereby control signals are sent to individual components of a system, based on their continuous feedback, in order to change their state so that the system may reach a stable equilibrium. Thus, we define a new carbon-based pricing mechanism for this green energy supplier that takes advantage of carbon-intensity signals available on the Internet in order to provide real-time pricing. The pricing scheme is designed in such a way that it can be readily implemented using existing communication technologies and is easily understandable by consumers. Building upon this, we develop new control signals that the supplier can use to incentivize agents to shift demand (using their storage device) to times when green energy is available. Moreover, we show how these signals can be adapted according to changes in supply and to various degrees of penetration of storage in the system. We empirically evaluate our system and show that, when all homes are equipped with storage devices, the supplier can significantly reduce its reliance on other carbon-emitting power sources to cater for its own shortfalls. By so doing, the supplier reduces the carbon emission of the system by up to 25% while the consumer reduces its costs by up to 14.5%. Finally, we demonstrate that our homeostatic control mechanism is not sensitive to small prediction errors and the supplier is incentivized to accurately predict its green production to minimize costs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {28},
keywords = {agentbased control, Agents, electricity, computational sustainability, multiagent systems}
}

@article{10.1145/1989734.1989738,
author = {Patnaik, Debprakash and Marwah, Manish and Sharma, Ratnesh K. and Ramakrishnan, Naren},
title = {Temporal Data Mining Approaches for Sustainable Chiller Management in Data Centers},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989738},
doi = {10.1145/1989734.1989738},
abstract = {Practically every large IT organization hosts data centers---a mix of computing elements, storage systems, networking, power, and cooling infrastructure---operated either in-house or outsourced to major vendors. A significant element of modern data centers is their cooling infrastructure, whose efficient and sustainable operation is a key ingredient to the “always-on” capability of data centers. We describe the design and implementation of CAMAS (Chiller Advisory and MAnagement System), a temporal data mining solution to mine and manage chiller installations. CAMAS embodies a set of algorithms for processing multivariate time-series data and characterizes sustainability measures of the patterns mined. We demonstrate three key ingredients of CAMAS---motif mining, association analysis, and dynamic Bayesian network inference---that help bridge the gap between low-level, raw, sensor streams, and the high-level operating regions and features needed for an operator to efficiently manage the data center. The effectiveness of CAMAS is demonstrated by its application to a real-life production data center managed by HP.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {34},
numpages = {29},
keywords = {chillers, Data centers, frequent episodes, clustering, motifs, sustainability}
}

@article{10.1145/1989734.1989737,
author = {Cattafi, Massimiliano and Gavanelli, Marco and Milano, Michela and Cagnoli, Paolo},
title = {Sustainable Biomass Power Plant Location in the Italian Emilia-Romagna Region},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989737},
doi = {10.1145/1989734.1989737},
abstract = {Biomass power plants are very promising for reducing carbon oxides emissions, because they provide energy with a carbon-neutral process. Biomass comes from trees and vegetables, so they provide a renewable type of energy. However, biomass plants location, along with their provisioning basins, are heavily regulated by economical aspects, often without careful consideration of their environmental footprint. For example, some Italian biomass plants import from overseas palm-tree oil that is economically convenient. However, the energy consumed for the oil transportation is definitely greater than the energy produced by the palm-tree oil burning. In this way biomass power plants turn out to be environmentally inefficient, even if they produce renewable energy.We propose an Integer Linear Programming approach for defining the energy and cost-efficient biomass plant location along with the corresponding provisioning basin. In addition, the model enables to evaluate existing plants and their energy and cost efficiency. Our study is based on real data gathered in the Emilia-Romagna region of Italy.Finally, this optimization tool is just a small part of a wider perspective that is aimed to define decision support tools for the improvement of regional planning and its precise strategic environmental assessment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {33},
numpages = {19},
keywords = {Computational sustainability, facility location}
}

@article{10.1145/1989734.1989736,
author = {Krause, Andreas and Guestrin, Carlos},
title = {Submodularity and Its Applications in Optimized Information Gathering},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989736},
doi = {10.1145/1989734.1989736},
abstract = {Where should we place sensors to efficiently monitor natural drinking water resources for contamination? Which blogs should we read to learn about the biggest stories on the Web? These problems share a fundamental challenge: How can we obtain the most useful information about the state of the world, at minimum cost?Such information gathering, or active learning, problems are typically NP-hard, and were commonly addressed using heuristics without theoretical guarantees about the solution quality. In this article, we describe algorithms which efficiently find provably near-optimal solutions to large, complex information gathering problems. Our algorithms exploit submodularity, an intuitive notion of diminishing returns common to many sensing problems: the more sensors we have already deployed, the less we learn by placing another sensor. In addition to identifying the most informative sensing locations, our algorithms can handle more challenging settings, where sensors need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data, or where solutions need to be robust against adversaries and sensor failures.We also present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, a sensor placement challenge, and deciding which blogs to read on the Web.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {32},
numpages = {20},
keywords = {computational sustainability, blogs, information overload, environmental monitoring, active learning, submodular functions, Information gathering, sensor networks}
}

@article{10.1145/1989734.1989735,
author = {Gomes, Carla and Yang, Qiang},
title = {Introduction to Special Issue on Computational Sustainability},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989735},
doi = {10.1145/1989734.1989735},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {31},
numpages = {2}
}

@article{10.1145/1961189.1961202,
author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
title = {Learning to Detect Malicious URLs},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961202},
doi = {10.1145/1961189.1961202},
abstract = {Malicious Web sites are a cornerstone of Internet criminal activities. The dangers of these sites have created a demand for safeguards that protect end-users from visiting them. This article explores how to detect malicious Web sites from the lexical and host-based features of their URLs. We show that this problem lends itself naturally to modern algorithms for online learning. Online algorithms not only process large numbers of URLs more efficiently than batch algorithms, they also adapt more quickly to new features in the continuously evolving distribution of malicious URLs. We develop a real-time system for gathering URL features and pair it with a real-time feed of labeled URLs from a large Web mail provider. From these features and labels, we are able to train an online classifier that detects malicious Web sites with 99% accuracy over a balanced dataset.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {30},
numpages = {24},
keywords = {malicious Web sites, Online learning}
}

@article{10.1145/1961189.1961201,
author = {Ma, Hao and King, Irwin and Lyu, Michael R.},
title = {Learning to Recommend with Explicit and Implicit Social Relations},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961201},
doi = {10.1145/1961189.1961201},
abstract = {Recommender systems have been well studied and developed, both in academia and in industry recently. However, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real-world observations where we always turn to our trusted friends for recommendations. Aiming at modeling recommender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework which naturally fuses the users' tastes and their trusted friends' favors together. The proposed framework is quite general, and it can also be applied to pure user-item rating matrix even if we do not have explicit social trust information among users. In this framework, we coin the term social trust ensemble to represent the formulation of the social trust restrictions on the recommender systems. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results show that our method outperforms state-of-the-art approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {29},
numpages = {19},
keywords = {social trust ensemble, Recommender systems, social network, matrix factorization}
}

@article{10.1145/1961189.1961200,
author = {Gasso, Gilles and Pappaioannou, Aristidis and Spivak, Marina and Bottou, L\'{e}on},
title = {Batch and Online Learning Algorithms for Nonconvex Neyman-Pearson Classification},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961200},
doi = {10.1145/1961189.1961200},
abstract = {We describe and evaluate two algorithms for Neyman-Pearson (NP) classification problem which has been recently shown to be of a particular importance for bipartite ranking problems. NP classification is a nonconvex problem involving a constraint on false negatives rate. We investigated batch algorithm based on DC programming and stochastic gradient method well suited for large-scale datasets. Empirical evidences illustrate the potential of the proposed methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {28},
numpages = {19},
keywords = {online learning, DC algorithm, nonconvex SVM, Neyman-Pearson}
}

@article{10.1145/1961189.1961199,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
title = {LIBSVM: A Library for Support Vector Machines},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961199},
doi = {10.1145/1961189.1961199},
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {27},
numpages = {27},
keywords = {Classification LIBSVM optimization regression support vector machines SVM}
}

@article{10.1145/1961189.1961198,
author = {Liu, Zhiyuan and Zhang, Yuzhou and Chang, Edward Y. and Sun, Maosong},
title = {PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961198},
doi = {10.1145/1961189.1961198},
abstract = {Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {26},
numpages = {18},
keywords = {latent Dirichlet allocation, Gibbs sampling, Topic models, distributed parallel computations}
}

@article{10.1145/1961189.1961197,
author = {Hsu, Chun-Nan},
title = {Introduction to Special Issue on Large-Scale Machine Learning},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961197},
doi = {10.1145/1961189.1961197},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {25},
numpages = {2}
}

@article{10.1145/1961189.1961196,
author = {Ge, Yong and Xiong, Hui and Zhou, Wenjun and Li, Siming and Sahoo, Ramendra},
title = {Multifocal Learning for Customer Problem Analysis},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961196},
doi = {10.1145/1961189.1961196},
abstract = {In this study, we formalize a multifocal learning problem, where training data are partitioned into several different focal groups and the prediction model will be learned within each focal group. The multifocal learning problem is motivated by numerous real-world learning applications. For instance, for the same type of problems encountered in a customer service center, the problem descriptions from different customers can be quite different. Experienced customers usually give more precise and focused descriptions about the problem. In contrast, inexperienced customers usually provide diverse descriptions. In this case, the examples from the same class in the training data can be naturally in different focal groups. Therefore, it is necessary to identify those natural focal groups and exploit them for learning at different focuses. Along this line, the key development challenge is how to identify those focal groups in the training data. As a case study, we exploit multifocal learning for profiling customer problems. Also, we provide an empirical study about how the performance of multifocal learning is affected by the quality of focal groups. The results on real-world customer problem logs show that multifocal learning can significantly boost the performance of many existing classification algorithms, such as Support Vector Machines (SVMs), for classifying customer problems and there is strong correlation between the quality of focal groups and the learning performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {24},
numpages = {22},
keywords = {customer service support, Multi-focal learning}
}

@article{10.1145/1961189.1961195,
author = {Zhang, Richong and Tran, Thomas},
title = {A Helpfulness Modeling Framework for Electronic Word-of-Mouth on Consumer Opinion Platforms},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961195},
doi = {10.1145/1961189.1961195},
abstract = {Electronic Word-of-Mouth (eWOM) is growing exponentially with the rapid development of electronic commerce. As a result, consumers are increasingly crowded by a huge amount of eWOM contents and therefore there is a need to automatically recommend eWOM contents that are helpful to them. Existing helpfulness assessment approaches that deterministically estimate the helpfulness of eWOM contents lack a generative formulation and are limited to the training set that has been voted by many readers. This article presents a rigorous probabilistic framework for inferring the “helpfulness” of eWOM contents which can build a “helpfulness” model from a low number of votes on eWOM contents. Furthermore, we introduce a measurement, “helpfulness” bias, as the benchmark for the “helpfulness” of eWOM documents. We also propose a model that exploits the graphical model and expectation maximization algorithm, under this probabilistic framework, to demonstrate the versatility of our framework. Our algorithm is compared experimentally to other existing helpfulness discovering algorithms and the experimental results show that our framework can effectively model the helpfulness of eWOM contents better than other approaches, and therefore indicate the capability of our framework to recommend helpful eWOMs to potential consumers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {23},
numpages = {18},
keywords = {recommender systems, Ranking, online product reviews}
}

@article{10.1145/1961189.1961194,
author = {Bonchi, Francesco and Castillo, Carlos and Gionis, Aristides and Jaimes, Alejandro},
title = {Social Network Analysis and Mining for Business Applications},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961194},
doi = {10.1145/1961189.1961194},
abstract = {Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {22},
numpages = {37},
keywords = {networks dynamics and evolution, community structure, influence propagation, viral marketing, Social networks, expert finding}
}

@article{10.1145/1961189.1961193,
author = {Li, Bin and Hoi, Steven C.H. and Gopalkrishnan, Vivekanand},
title = {CORN: Correlation-Driven Nonparametric Learning Approach for Portfolio Selection},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961193},
doi = {10.1145/1961189.1961193},
abstract = {Machine learning techniques have been adopted to select portfolios from financial markets in some emerging intelligent business applications. In this article, we propose a novel learning-to-trade algorithm termed CORrelation-driven Nonparametric learning strategy (CORN) for actively trading stocks. CORN effectively exploits statistical relations between stock market windows via a nonparametric learning approach. We evaluate the empirical performance of our algorithm extensively on several large historical and latest real stock markets, and show that it can easily beat both the market index and the best stock in the market substantially (without or with small transaction costs), and also surpass a variety of state-of-the-art techniques significantly.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {21},
numpages = {29},
keywords = {nonparametric learning, Correlation coefficient, online portfolio selection}
}

@article{10.1145/1961189.1961192,
author = {Huang, Szu-Hao and Lai, Shang-Hong and Tai, Shih-Hsien},
title = {A Learning-Based Contrarian Trading Strategy via a Dual-Classifier Model},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961192},
doi = {10.1145/1961189.1961192},
abstract = {Behavioral finance is a relatively new and developing research field which adopts cognitive psychology and emotional bias to explain the inefficient market phenomenon and some irrational trading decisions. Unlike the experts in this field who tried to reason the price anomaly and applied empirical evidence in many different financial markets, we employ the advanced binary classification algorithms, such as AdaBoost and support vector machines, to precisely model the overreaction and strengthen the portfolio compositions of the contrarian trading strategies. The novelty of this article is to discover the financial time-series patterns through a high-dimensional and nonlinear model which is constructed by integrated knowledge of finance and machine learning techniques. We propose a dual-classifier learning framework to select candidate stocks from the past results of original contrarian trading strategies based on the defined learning targets. Three different feature extraction methods, including wavelet transformation, historical return distribution, and various technical indicators, are employed to represent these learning samples in a 381-dimensional financial time-series feature space. Finally, we construct the classifier models with four different learning kernels and prove that the proposed methods could improve the returns dramatically, such as the 3-year return that improved from 26.79% to 53.75%. The experiments also demonstrate significantly higher portfolio selection accuracy, improved from 57.47% to 66.41%, than the original contrarian trading strategy. To sum up, all these experiments show that the proposed method could be extended to an effective trading system in the historical stock prices of the leading U.S. companies of S&amp;P 100 index.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {20},
numpages = {20},
keywords = {classification, Behavioral finance, machine learning}
}

@article{10.1145/1961189.1961191,
author = {Dhar, Vasant},
title = {Prediction in Financial Markets: The Case for Small Disjuncts},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961191},
doi = {10.1145/1961189.1961191},
abstract = {Predictive models in regression and classification problems typically have a single model that covers most, if not all, cases in the data. At the opposite end of the spectrum is a collection of models, each of which covers a very small subset of the decision space. These are referred to as “small disjuncts.” The trade-offs between the two types of models have been well documented. Single models, especially linear ones, are easy to interpret and explain. In contrast, small disjuncts do not provides as clean or as simple an interpretation of the data, and have been shown by several researchers to be responsible for a disproportionately large number of errors when applied to out-of-sample data. This research provides a counterpoint, demonstrating that a portfolio of “simple” small disjuncts provides a credible model for financial market prediction, a problem with a high degree of noise. A related novel contribution of this article is a simple method for measuring the “yield” of a learning system, which is the percentage of in-sample performance that the learned model can be expected to realize on out-of-sample data. Curiously, such a measure is missing from the literature on regression learning algorithms. Pragmatically, the results suggest that for problems characterized by a high degree of noise and lack of a stable knowledge base it makes sense to reconstruct the portfolio of small rules periodically.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {19},
numpages = {22},
keywords = {time-series prediction, Machine learning, financial markets, predictive modeling}
}

@article{10.1145/1961189.1961190,
author = {Ling, Charles X.},
title = {Introduction to Special Issue on Machine Learning for Business Applications},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961190},
doi = {10.1145/1961189.1961190},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {18},
numpages = {2}
}

@article{10.1145/1899412.1899421,
author = {Bhatt, Chidansh and Kankanhalli, Mohan},
title = {Probabilistic Temporal Multimedia Data Mining},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899421},
doi = {10.1145/1899412.1899421},
abstract = {Existing sequence pattern mining techniques assume that the obtained events from event detectors are accurate. However, in reality, event detectors label the events from different modalities with a certain probability over a time-interval. In this article, we consider for the first time Probabilistic Temporal Multimedia (PTM) Event data to discover accurate sequence patterns. PTM event data considers the start time, end time, event label and associated probability for the sequence pattern discovery. As the existing sequence pattern mining techniques cannot work on such realistic data, we have developed a novel framework for performing sequence pattern mining on probabilistic temporal multimedia event data. We perform probability fusion to resolve the redundancy among detected events from different modalities, considering their cross-modal correlation. We propose a novel sequence pattern mining algorithm called Probabilistic Interval based Event Miner (PIE-Miner) for discovering frequent sequence patterns from interval based events. PIE-Miner has a new support counting mechanism developed for PTM data. Existing sequence pattern mining algorithms have event label level support counting mechanism, whereas we have developed event cluster level support counting mechanism. We discover the complete set of all possible temporal relationships based on Allen's interval algebra. The experimental results showed that the discovered sequence patterns are more useful than the patterns discovered with state-of-the-art sequence pattern mining algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {17},
numpages = {19},
keywords = {Multimedia datamining, sequence pattern mining, multimodal datamining, cross-modal correlation, probabilistic interval based event mining}
}

@article{10.1145/1899412.1899420,
author = {Liu, Qingzhong and Sung, Andrew H. and Qiao, Mengyu},
title = {Neighboring Joint Density-Based JPEG Steganalysis},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899420},
doi = {10.1145/1899412.1899420},
abstract = {The threat posed by hackers, spies, terrorists, and criminals, etc. using steganography for stealthy communications and other illegal purposes is a serious concern of cyber security. Several steganographic systems that have been developed and made readily available utilize JPEG images as carriers. Due to the popularity of JPEG images on the Internet, effective steganalysis techniques are called for to counter the threat of JPEG steganography. In this article, we propose a new approach based on feature mining on the discrete cosine transform (DCT) domain and machine learning for steganalysis of JPEG images. First, neighboring joint density features on both intra-block and inter-block are extracted from the DCT coefficient array and the absolute array, respectively; then a support vector machine (SVM) is applied to the features for detection. An evolving neural-fuzzy inference system is employed to predict the hiding amount in JPEG steganograms. We also adopt a feature selection method of support vector machine recursive feature elimination to reduce the number of features. Experimental results show that, in detecting several JPEG-based steganographic systems, our method prominently outperforms the well-known Markov-process based approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {16},
numpages = {16},
keywords = {JPEG, neighboring joint density, nuero-fuzzy, SVMRFE, classification, steganalysis, SVM, steganography}
}

@article{10.1145/1899412.1899419,
author = {Tong, Xiaofeng and Liu, Jia and Wang, Tao and Zhang, Yimin},
title = {Automatic Player Labeling, Tracking and Field Registration and Trajectory Mapping in Broadcast Soccer Video},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899419},
doi = {10.1145/1899412.1899419},
abstract = {In this article, we present a method to perform automatic player trajectories mapping based on player detection, unsupervised labeling, efficient multi-object tracking, and playfield registration in broadcast soccer videos. Player detector determines the players' positions and scales by combining the ability of dominant color based background subtraction and a boosting detector with Haar features. We first learn the dominant color with accumulate color histogram at the beginning of processing, then use the player detector to collect hundreds of player samples, and learn player appearance codebook by unsupervised clustering. In a soccer game, a player can be labeled as one of four categories: two teams, referee or outlier. The learning capability enables the method to be generalized well to different videos without any manual initialization. With the dominant color and player appearance model, we can locate and label each player. After that, we perform multi-object tracking by using Markov Chain Monte Carlo (MCMC) data association to generate player trajectories. Some data driven dynamics are proposed to improve the Markov chain's efficiency, such as label consistency, motion consistency, and track length, etc. Finally, we extract key-points and find the mapping from an image plane to the standard field model, and then map players' position and trajectories to the field. A large quantity of experimental results on FIFA World Cup 2006 videos demonstrate that this method can reach high detection and labeling precision, reliably tracking in scenes of player occlusion, moderate camera motion and pose variation, and yield promising field registration results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {15},
numpages = {32},
keywords = {Boosting, MCMC, sports video, video analysis, codebook, field registration, multiple player tracking, player labeling}
}

@article{10.1145/1899412.1899418,
author = {Tang, Jinhui and Hong, Richang and Yan, Shuicheng and Chua, Tat-Seng and Qi, Guo-Jun and Jain, Ramesh},
title = {Image Annotation by <i>k</i>NN-Sparse Graph-Based Label Propagation over Noisily Tagged Web Images},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899418},
doi = {10.1145/1899412.1899418},
abstract = {In this article, we exploit the problem of annotating a large-scale image corpus by label propagation over noisily tagged web images. To annotate the images more accurately, we propose a novel kNN-sparse graph-based semi-supervised learning approach for harnessing the labeled and unlabeled data simultaneously. The sparse graph constructed by datum-wise one-vs-kNN sparse reconstructions of all samples can remove most of the semantically unrelated links among the data, and thus it is more robust and discriminative than the conventional graphs. Meanwhile, we apply the approximate k nearest neighbors to accelerate the sparse graph construction without loosing its effectiveness. More importantly, we propose an effective training label refinement strategy within this graph-based learning framework to handle the noise in the training labels, by bringing in a dual regularization for both the quantity and sparsity of the noise. We conduct extensive experiments on a real-world image database consisting of 55,615 Flickr images and noisily tagged training labels. The results demonstrate both the effectiveness and efficiency of the proposed approach and its capability to deal with the noise in the training labels.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {14},
numpages = {15},
keywords = {kNN, label propagation, web image, semi-supervised learning, noisy tags, Sparse graph}
}

@article{10.1145/1899412.1899417,
author = {Wu, Lei and Hoi, Steven C.H. and Jin, Rong and Zhu, Jianke and Yu, Nenghai},
title = {Distance Metric Learning from Uncertain Side Information for Automated Photo Tagging},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899417},
doi = {10.1145/1899412.1899417},
abstract = {Automated photo tagging is an important technique for many intelligent multimedia information systems, for example, smart photo management system and intelligent digital media library. To attack the challenge, several machine learning techniques have been developed and applied for automated photo tagging. For example, supervised learning techniques have been applied to automated photo tagging by training statistical classifiers from a collection of manually labeled examples. Although the existing approaches work well for small testbeds with relatively small number of annotation words, due to the long-standing challenge of object recognition, they often perform poorly in large-scale problems. Another limitation of the existing approaches is that they require a set of high-quality labeled data, which is not only expensive to collect but also time consuming. In this article, we investigate a social image based annotation scheme by exploiting implicit side information that is available for a large number of social photos from the social web sites. The key challenge of our intelligent annotation scheme is how to learn an effective distance metric based on implicit side information (visual or textual) of social photos. To this end, we present a novel “Probabilistic Distance Metric Learning” (PDML) framework, which can learn optimized metrics by effectively exploiting the implicit side information vastly available on the social web. We apply the proposed technique to photo annotation tasks based on a large social image testbed with over 1 million tagged photos crawled from a social photo sharing portal. Encouraging results show that the proposed technique is effective and promising for social photo based annotation tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {13},
numpages = {28},
keywords = {Automated photo tagging, uncertain side information, social images, distance metric learning, content-based image retrieval}
}

@article{10.1145/1899412.1899416,
author = {Yu, Jie and Jin, Xin and Han, Jiawei and Luo, Jiebo},
title = {Collection-Based Sparse Label Propagation and Its Application on Social Group Suggestion from Photos},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899416},
doi = {10.1145/1899412.1899416},
abstract = {Online social network services pose great opportunities and challenges for many research areas. In multimedia content analysis, automatic social group recommendation for images holds the promise to expand one's social network through media sharing. However, most existing techniques cannot generate satisfactory social group suggestions when the images are classified independently. In this article, we present novel methods to produce accurate suggestions of suitable social groups from a user's personal photo collection. First, an automatic clustering process is designed to estimate the group similarities, select the optimal number of clusters and categorize the social groups. Both visual content and textual annotations are integrated to generate initial predictions of the group categories for the images. Next, the relationship among images in a user's collection is modeled as a sparse graph. A collection-based sparse label propagation method is proposed to improve the group suggestions. Furthermore, the sparse graph-based collection model can be readily exploited to select the most influential and informative samples for active relevance feedback, which can be integrated with the label propagation process without the need for classifier retraining. The proposed methods have been tested on group suggestion tasks for real user collections and demonstrated superior performance over the state-of-the-art techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {12},
numpages = {21},
keywords = {Social image, active relevance feedback, collection-based sparse label propagation, group recommendation}
}

@article{10.1145/1899412.1899415,
author = {Shao, Yuanlong and Zhou, Yuan and Cai, Deng},
title = {Variational Inference with Graph Regularization for Image Annotation},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899415},
doi = {10.1145/1899412.1899415},
abstract = {Image annotation is a typical area where there are multiple types of attributes associated with each individual image. In order to achieve better performance, it is important to develop effective modeling by utilizing prior knowledge. In this article, we extend the graph regularization approaches to a more general case where the regularization is imposed on the factorized variational distributions, instead of posterior distributions implicitly involved in EM-like algorithms. In this way, the problem modeling can be more flexible, and we can choose any factor in the problem domain to impose graph regularization wherever there are similarity constraints among the instances. We formulate the problem formally and show its geometrical background in manifold learning. We also design two practically effective algorithms and analyze their properties such as the convergence. Finally, we apply our approach to image annotation and show the performance improvement of our algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {11},
numpages = {21},
keywords = {variational inference, Automatic image annotation, Laplacian regularization, semantic indexing, graph regularization, semi-supervised learning}
}

@article{10.1145/1899412.1899414,
author = {Wang, Meng and Hua, Xian-Sheng},
title = {Active Learning in Multimedia Annotation and Retrieval: A Survey},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899414},
doi = {10.1145/1899412.1899414},
abstract = {Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {10},
numpages = {21},
keywords = {content-based image retrieval, image annotation, model learning, sample selection, Active learning, video annotation}
}

@article{10.1145/1899412.1899413,
author = {Hua, Xian-Sheng and Tian, Qi and Bimbo, Alberto Del and Jain, Ramesh},
title = {Introduction to the Special Issue on Intelligent Multimedia Systems and Technology},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899413},
doi = {10.1145/1899412.1899413},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {9},
numpages = {2}
}

@article{10.1145/1889681.1889689,
author = {Bao, Xinlong and Dietterich, Thomas G.},
title = {FolderPredictor: Reducing the Cost of Reaching the Right Folder},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889689},
doi = {10.1145/1889681.1889689},
abstract = {Helping computer users rapidly locate files in their folder hierarchies is a practical research problem involving both intelligent systems and user interface design. This article reports on FolderPredictor, a software system that can reduce the cost of locating files in hierarchical folders. FolderPredictor applies a cost-sensitive prediction algorithm to the user's previous file access information to predict the next folder that will be accessed. Experimental results show that, on average, FolderPredictor reduces the number of clicks spent on locating a file by 50%. Several variations of the cost-sensitive prediction algorithm are discussed. An experimental study shows that the best algorithm among them is a mixture of the most recently used (MRU) folder and the cost-sensitive predictions. Furthermore, FolderPredictor does not require users to adapt to a new interface, but rather meshes with the existing interface for opening files on the Windows platform.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {8},
numpages = {23},
keywords = {folders, intelligent user interfaces, Activities, user interface, prediction, directories, shortcuts, tasks, intelligent systems, recommendation}
}

@article{10.1145/1889681.1889688,
author = {Wyatt, Danny and Choudhury, Tanzeem and Bilmes, Jeff and Kitts, James A.},
title = {Inferring Colocation and Conversation Networks from Privacy-Sensitive Audio with Implications for Computational Social Science},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889688},
doi = {10.1145/1889681.1889688},
abstract = {New technologies have made it possible to collect information about social networks as they are acted and observed in the wild, instead of as they are reported in retrospective surveys. These technologies offer opportunities to address many new research questions: How can meaningful information about social interaction be extracted from automatically recorded raw data on human behavior? What can we learn about social networks from such fine-grained behavioral data? And how can all of this be done while protecting privacy? With the goal of addressing these questions, this article presents new methods for inferring colocation and conversation networks from privacy-sensitive audio. These methods are applied in a study of face-to-face interactions among 24 students in a graduate school cohort during an academic year. The resulting analysis shows that networks derived from colocation and conversation inferences are quite different. This distinction can inform future research in computational social science, especially work that only measures colocation or employs colocation data as a proxy for conversation networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {7},
numpages = {41},
keywords = {mobile sensing, Social networks}
}

@article{10.1145/1889681.1889687,
author = {Ward, Jamie A. and Lukowicz, Paul and Gellersen, Hans W.},
title = {Performance Metrics for Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889687},
doi = {10.1145/1889681.1889687},
abstract = {In this article, we introduce and evaluate a comprehensive set of performance metrics and visualisations for continuous activity recognition (AR). We demonstrate how standard evaluation methods, often borrowed from related pattern recognition problems, fail to capture common artefacts found in continuous AR—specifically event fragmentation, event merging and timing offsets. We support our assertion with an analysis on a set of recently published AR papers. Building on an earlier initial work on the topic, we develop a frame-based visualisation and corresponding set of class-skew invariant metrics for the one class versus all evaluation. These are complemented by a new complete set of event-based metrics that allow a quick graphical representation of system performance—showing events that are correct, inserted, deleted, fragmented, merged and those which are both fragmented and merged. We evaluate the utility of our approach through comparison with standard metrics on data from three different published experiments. This shows that where event- and frame-based precision and recall lead to an ambiguous interpretation of results in some cases, the proposed metrics provide a consistently unambiguous explanation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {6},
numpages = {23},
keywords = {performance evaluation, Activity recognition, metrics}
}

@article{10.1145/1889681.1889686,
author = {Zhou, Yue and Ni, Bingbing and Yan, Shuicheng and Huang, Thomas S.},
title = {Recognizing Pair-Activities by Causality Analysis},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889686},
doi = {10.1145/1889681.1889686},
abstract = {In this article, beyond solo-activity analysis for single object, we study the more complicated pair-activity recognition problem by exploring the relationship between two active objects based on their trajectory clues obtained from video sensor. Our contributions are three-fold. First, we design two sets of features for representing the pair-activities encoded as length-variable trajectory pairs. One set characterizes the strength of causality between two trajectories, for example, the causality ratio and feedback ratio based on the Granger Causality Test (GCT), and another set describes the style of causality between two trajectories, for example, the sampled frequency responses of the digital filter with these two trajectories as the input and output discrete signals respectively. These features along with conventional velocity and position features of a trajectory-pair are essentially of multi-modalities, and may be greatly different in scales and importance. To make full use of them, we then develop a novel feature fusing procedure to learn the coefficients for weighting these features by maximizing the discriminating power measured by weighted correlation. Finally, we collected a pair-activity database of five popular categories, each of which consists of about 170 instances. The extensive experiments on this database validate the effectiveness of the designed features for pair-activity representation, and also demonstrate that the proposed feature fusing procedure significantly boosts the pair-activity classification accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {5},
numpages = {20},
keywords = {digital filter, causality analysis, Activity analysis, frequency responses}
}

@article{10.1145/1889681.1889685,
author = {Hsu, Jane Yung-Jen and Lian, Chia-Chun and Jih, Wan-Rong},
title = {Probabilistic Models for Concurrent Chatting Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889685},
doi = {10.1145/1889681.1889685},
abstract = {Recognition of chatting activities in social interactions is useful for constructing human social networks. However, the existence of multiple people involved in multiple dialogues presents special challenges. To model the conversational dynamics of concurrent chatting behaviors, this article advocates Factorial Conditional Random Fields (FCRFs) as a model to accommodate co-temporal relationships among multiple activity states. In addition, to avoid the use of inefficient Loopy Belief Propagation (LBP) algorithm, we propose using Iterative Classification Algorithm (ICA) as the inference method for FCRFs. We designed experiments to compare our FCRFs model with two dynamic probabilistic models, Parallel Condition Random Fields (PCRFs) and Hidden Markov Models (HMMs), in learning and decoding based on auditory data. The experimental results show that FCRFs outperform PCRFs and HMMs-like models. We also discover that FCRFs using the ICA inference approach not only improves the recognition accuracy but also takes significantly less time than the LBP inference method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {4},
numpages = {20},
keywords = {factorial conditional random fields, Chatting activity recognition, loopy belief propagation, iterative classification}
}

@article{10.1145/1889681.1889684,
author = {Farrahi, Katayoun and Gatica-Perez, Daniel},
title = {Discovering Routines from Large-Scale Human Locations Using Probabilistic Topic Models},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889684},
doi = {10.1145/1889681.1889684},
abstract = {In this work, we discover the daily location-driven routines that are contained in a massive real-life human dataset collected by mobile phones. Our goal is the discovery and analysis of human routines that characterize both individual and group behaviors in terms of location patterns. We develop an unsupervised methodology based on two differing probabilistic topic models and apply them to the daily life of 97 mobile phone users over a 16-month period to achieve these goals. Topic models are probabilistic generative models for documents that identify the latent structure that underlies a set of words. Routines dominating the entire group's activities, identified with a methodology based on the Latent Dirichlet Allocation topic model, include “going to work late”, “going home early”, “working nonstop” and “having no reception (phone off)” at different times over varying time-intervals. We also detect routines which are characteristic of users, with a methodology based on the Author-Topic model. With the routines discovered, and the two methods of characterizing days and users, we can then perform various tasks. We use the routines discovered to determine behavioral patterns of users and groups of users. For example, we can find individuals that display specific daily routines, such as “going to work early” or “turning off the mobile (or having no reception) in the evenings”. We are also able to characterize daily patterns by determining the topic structure of days in addition to determining whether certain routines occur dominantly on weekends or weekdays. Furthermore, the routines discovered can be used to rank users or find subgroups of users who display certain routines. We can also characterize users based on their entropy. We compare our method to one based on clustering using K-means. Finally, we analyze an individual's routines over time to determine regions with high variations, which may correspond to specific events.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {3},
numpages = {27},
keywords = {reality mining, Human activity modeling, topic models}
}

@article{10.1145/1889681.1889683,
author = {Zheng, Yu and Xie, Xing},
title = {Learning Travel Recommendations from User-Generated GPS Traces},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889683},
doi = {10.1145/1889681.1889683},
abstract = {The advance of GPS-enabled devices allows people to record their location histories with GPS traces, which imply human behaviors and preferences related to travel. In this article, we perform two types of travel recommendations by mining multiple users' GPS traces. The first is a generic one that recommends a user with top interesting locations and travel sequences in a given geospatial region. The second is a personalized recommendation that provides an individual with locations matching her travel preferences. To achieve the first recommendation, we model multiple users' location histories with a tree-based hierarchical graph (TBHG). Based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based model to infer the interest level of a location and a user's travel experience (knowledge). In the personalized recommendation, we first understand the correlation between locations, and then incorporate this correlation into a collaborative filtering (CF)-based model, which predicts a user's interests in an unvisited location based on her locations histories and that of others. We evaluated our system based on a real-world GPS trace dataset collected by 107 users over a period of one year. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, we achieved a better performance in recommending travel sequences beyond baselines like rank-by-count. Regarding the personalized recommendation, our approach is more effective than the weighted Slope One algorithm with a slightly additional computation, and is more efficient than the Pearson correlation-based CF model with the similar effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {2},
numpages = {29},
keywords = {Location recommendation, GPS trace, collaborative filtering, location history, GeoLife}
}

@article{10.1145/1889681.1889682,
author = {Zhang, Daqing and Philipose, Matthai and Yang, Qiang},
title = {Introduction to the Special Issue on Intelligent Systems for Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889682},
doi = {10.1145/1889681.1889682},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {1},
numpages = {4}
}

@article{10.1145/1869397.1869404,
author = {Cirillo, Marcello and Karlsson, Lars and Saffiotti, Alessandro},
title = {Human-Aware Task Planning: An Application to Mobile Robots},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869404},
doi = {10.1145/1869397.1869404},
abstract = {Consider a house cleaning robot planning its activities for the day. Assume that the robot expects the human inhabitant to first dress, then have breakfast, and finally go out. Then, it should plan not to clean the bedroom while the human is dressing, and to clean the kitchen after the human has had breakfast. In general, robots operating in inhabited environments, like households and future factory floors, should plan their behavior taking into account the actions that will be performed by the humans sharing the same environment. This would improve human-robot cohabitation, for example, by avoiding undesired situations for the human. Unfortunately, current task planners only consider the robot's actions and unexpected external events in the planning process, and cannot accommodate expectations about the actions of the humans.In this article, we present a human-aware planner able to address this problem. Our planner supports alternative hypotheses of the human plan, temporal duration for the actions of both the robot and the human, constraints on the interaction between robot and human, partial goal achievement and, most importantly, the possibility to use observations of human actions in the policy generated for the robot. Our planner has been tested both as a stand-alone component and within a full framework for human-robot interaction in a real environment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {15},
numpages = {26},
keywords = {human-aware planning, Human-robot interaction}
}

@article{10.1145/1869397.1869403,
author = {Talamadupula, Kartik and Benton, J. and Kambhampati, Subbarao and Schermerhorn, Paul and Scheutz, Matthias},
title = {Planning for Human-Robot Teaming in Open Worlds},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869403},
doi = {10.1145/1869397.1869403},
abstract = {As the number of applications for human-robot teaming continue to rise, there is an increasing need for planning technologies that can guide robots in such teaming scenarios. In this article, we focus on adapting planning technology to Urban Search And Rescue (USAR) with a human-robot team. We start by showing that several aspects of state-of-the-art planning technology, including temporal planning, partial satisfaction planning, and replanning, can be gainfully adapted to this scenario. We then note that human-robot teaming also throws up an additional critical challenge, namely, enabling existing planners, which work under closed-world assumptions, to cope with the open worlds that are characteristic of teaming problems such as USAR. In response, we discuss the notion of conditional goals, and describe how we represent and handle a specific class of them called open world quantified goals. Finally, we describe how the planner, and its open world extensions, are integrated into a robot control architecture, and provide an empirical evaluation over USAR experimental runs to establish the effectiveness of the planning components.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {14},
numpages = {24},
keywords = {Automated planning, search and rescue, robot, planner}
}

@article{10.1145/1869397.1869402,
author = {Benaskeur, Abder Rezak and Kabanza, Froduald and Beaudry, Eric},
title = {CORALS: A Real-Time Planner for Anti-Air Defense Operations},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869402},
doi = {10.1145/1869397.1869402},
abstract = {Forces involved in modern conflicts may be exposed to a variety of threats, including coordinated raids of advanced ballistic and cruise missiles. To respond to these, a defending force will rely on a set of combat resources. Determining an efficient allocation and coordinated use of these resources, particularly in the case of multiple simultaneous attacks, is a very complex decision-making process in which a huge amount of data must be dealt with under uncertainty and time pressure. This article presents CORALS (COmbat Resource ALlocation Support), a real-time planner developed to support the command team of a naval force defending against multiple simultaneous threats. In response to such multiple threats, CORALS uses a local planner to generate a set of local plans, one for each threat considered apart, and then combines and coordinates them into a single optimized, conflict-free global plan. The coordination is performed through an iterative process of plan merging and conflict detection and resolution, which acts as a plan repair mechanism. Such an incremental plan repair approach also allows adapting previously generated plans to account for dynamic changes in the tactical situation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {13},
numpages = {21},
keywords = {anti-air defense operations, decision support, Planning}
}

@article{10.1145/1869397.1869401,
author = {Refanidis, Ioannis and Yorke-Smith, Neil},
title = {A Constraint-Based Approach to Scheduling an Individual's Activities},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869401},
doi = {10.1145/1869397.1869401},
abstract = {The goal of helping to automate the management of an individual's time is ambitious in terms both of knowledge engineering and of the quality of the plans produced by an AI system. Modeling an individual's activities is itself a challenge, due to the variety of activity, constraint, and preference types involved. Activities might be simple or interruptible; they might have fixed or variable durations, constraints over their temporal domains, and binary constraints between them. Activities might require the individual being at specific locations in order, whereas traveling time should be taken into account. Some activities might require exclusivity, whereas others can be overlapped with compatible concurrent activities. Finally, while scheduled activities generate utility for the individual, extra utility might result from the way activities are scheduled in time, individually and in conjunction.This article presents a rigorous, expressive model to represent an individual's activities, that is, activities whose scheduling is not contingent on any other person. Joint activities such as meetings are outside our remit; it is expected that these are arranged manually or through negotiation mechanisms and they are considered as fixed busy times in the individual's calendar. The model, formulated as a constraint optimization problem, is general enough to accommodate a variety of situations. We present a scheduler that operates on this rich model, based on the general squeaky wheel optimization framework and enhanced with domain-dependent heuristics and forward checking. Our empirical evaluation demonstrates both the efficiency and the effectiveness of the selected approach. Part of the work described has been implemented in the SelfPlanner system, a Web-based intelligent calendar application that utilizes Google Calendar.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {12},
numpages = {32},
keywords = {activity modeling, constraints, preferences, intelligent calendar applications, Greedy algorithms}
}

@article{10.1145/1869397.1869400,
author = {Bryce, Daniel and Verdicchio, Michael and Kim, Seungchan},
title = {Planning Interventions in Biological Networks},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869400},
doi = {10.1145/1869397.1869400},
abstract = {Modeling the dynamics of biological processes has recently become an important research topic in computational biology and systems engineering. One of the most important reasons to model a biological process is to enable high-throughput in-silico experiments that attempt to predict or intervene in the process. These experiments can help accelerate the design of therapies through their rapid and inexpensive replication and alteration. While some techniques exist for reasoning with biological processes, few take advantage of the flexible and scalable algorithms popular in AI research. In reasoning about interventions in biological processes, where scalability is crucial for feasible application, we apply AI planning-based search techniques and demonstrate their advantage over existing enumerative methods. We also present a novel formulation of intervention planning that relies on models that characterize and attempt to change the phenotype of a system. We study three biological systems: the yeast cell cycle, a model of the human aging process, and the Wnt5a network governing the metastasis of melanoma in humans. The contribution of our investigation is in demonstrating that: (i) prior approaches, based on dynamic programming, cannot scale as well as heuristic search, and (ii) the newly found scalability enables us to plan previously unknown sequences of interventions that reveal novel and biologically significant responses in the systems which are consistent with biological knowledge in the literature.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {11},
numpages = {26},
keywords = {Planning, search, systems biology}
}

@article{10.1145/1869397.1869399,
author = {Porteous, Julie and Cavazza, Marc and Charles, Fred},
title = {Applying Planning to Interactive Storytelling: Narrative Control Using State Constraints},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869399},
doi = {10.1145/1869397.1869399},
abstract = {We have seen ten years of the application of AI planning to the problem of narrative generation in Interactive Storytelling (IS). In that time planning has emerged as the dominant technology and has featured in a number of prototype systems. Nevertheless key issues remain, such as how best to control the shape of the narrative that is generated (e.g., by using narrative control knowledge, i.e., knowledge about narrative features that enhance user experience) and also how best to provide support for real-time interactive performance in order to scale up to more realistic sized systems. Recent progress in planning technology has opened up new avenues for IS and we have developed a novel approach to narrative generation that builds on this. Our approach is to specify narrative control knowledge for a given story world using state trajectory constraints and then to treat these state constraints as landmarks and to use them to decompose narrative generation in order to address scalability issues and the goal of real-time performance in larger story domains. This approach to narrative generation is fully implemented in an interactive narrative based on the “Merchant of Venice.” The contribution of the work lies both in our novel use of state constraints to specify narrative control knowledge for interactive storytelling and also our development of an approach to narrative generation that exploits such constraints. In the article we show how the use of state constraints can provide a unified perspective on important problems faced in IS.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {planning, Interactive storytelling, agents in games and virtual environments, narrative modeling}
}

@article{10.1145/1869397.1869398,
author = {Chen, Yixin},
title = {Preface to Special Issue on Applications of Automated Planning},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869398},
doi = {10.1145/1869397.1869398},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {9},
numpages = {3}
}

@article{10.1145/1858948.1858956,
author = {Wang, Meng and Liu, Bo and Hua, Xian-Sheng},
title = {Accessible Image Search for Colorblindness},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858956},
doi = {10.1145/1858948.1858956},
abstract = {This article introduces an intelligent system that accommodates colorblind users in image search. Color plays an important role in the human perception and recognition of images. However, there are about 8% of men and 0.8% of women suffering from colorblindness. We show that the existing image search techniques cannot provide satisfactory results for these users since many images will not be well perceived by them due to the loss of color information. To deal with this difficulty, we introduce a system named Accessible Image Search (AIS) to accommodate these users. Different from the general image search scheme that aims at returning more relevant results, AIS further takes into account the colorblind accessibilities of the returned results, that is, the image qualities in the eyes of colorblind users. The system contains three components: accessibility assessment, accessibility improvement, and color indication. The accessibility assessment component measures the accessibility scores of images, and consequently different reranking methods can be performed to prioritize images with high accessibilities. In the accessibility improvement component, we propose an efficient recoloring algorithm to modify the colors of the images such that they can be better perceived by colorblind users. Color indication aims to indicate the name of the interesting color in an image. We evaluate the introduced system with more than 60 queries and 20 anonymous colorblind users, and the empirical results demonstrate its effectiveness and usefulness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {8},
numpages = {26},
keywords = {Image search, colorblindness}
}

@article{10.1145/1858948.1858955,
author = {Goolsby, Rebecca},
title = {Social Media as Crisis Platform: The Future of Community Maps/Crisis Maps},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858955},
doi = {10.1145/1858948.1858955},
abstract = {Social media provides the means for creating new communities and for reenergizing old communities. Recently, a new kind of quickly formulated, powerful community has formed as existing social media communities, news organizations, and users have converged in social media spaces to respond to sudden tragedies. This article addresses the ad-hoc crisis community, whith uses the social madia as a crisis platform to generate community crisis maps.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {7},
numpages = {11},
keywords = {Social networking, crisis mapping, maps, Haiti, disaster management, Mumbai, visualization, information tools}
}

@article{10.1145/1858948.1858954,
author = {Roos, Patrick and Carr, J. Ryan and Nau, Dana S.},
title = {Evolution of State-Dependent Risk Preferences},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858954},
doi = {10.1145/1858948.1858954},
abstract = {Researchers have invested much effort in constructing models of the state-dependent (sometimes risk-averse and sometimes risk-prone) nature of human decision making. An important open question is how state-dependent risk behavior can arise and remain prominent in populations. We believe that one part of the answer is the interplay between risk-taking and sequentiality of choices in populations subject to evolutionary population dynamics. To support this hypothesis, we provide simulation and analytical results for evolutionary lottery games, including results on evolutionary stability. We consider a parameterized class of imitation dynamics in which the parameter 0 ≤ α ≤ 1 yields the replicator dynamic with α = 1 and the imitate-the-better dynamic with α = 0. Our results demonstrate that for every population dynamic in this class except for the replicator dynamic, the interplay between risk-taking and sequentiality of choices allows state-dependent risk behavior to have an evolutionary advantage over expected-value maximization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {6},
numpages = {21},
keywords = {decision theory, population dynamics, Evolutionary games, risk}
}

@article{10.1145/1858948.1858953,
author = {Wu, Fang and Huberman, Bernardo A.},
title = {Opinion Formation under Costly Expression},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858953},
doi = {10.1145/1858948.1858953},
abstract = {Opinions play an important role in trust building and the creation of consensus about issues and products and a number of studies have focused on the design, evaluation, and utilization of online opinion systems. However, little effort has been spent on the dynamic aspects of online opinion formation. In this article, we study the dynamics of online opinion expression by analyzing the temporal evolution of vey large sets of user views and determine that in the course of time, later opinions tend to show a big difference with earlier opinions, which moderates the average opinion to the less extreme. Online posters also tend to disagree with previous opinions when the cost of expression is high.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {5},
numpages = {13},
keywords = {Opinion formation, costly expression}
}

@article{10.1145/1858948.1858952,
author = {Feldman, Michal and Tennenholtz, Moshe},
title = {Structured Coalitions in Resource Selection Games},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858952},
doi = {10.1145/1858948.1858952},
abstract = {We study stability against coalitional deviations in resource selection games where the coalitions have a certain structure. In particular, the agents are partitioned into coalitions, and only deviations by the prescribed coalitions are considered. This is in contrast to the classical concept of strong equilibrium according to which any subset of the agents may deviate. In resource selection games, each agent selects a resource from a set of resources, and its payoff is an increasing (or nondecreasing) function of the number of agents selecting its resource. While it has been shown that a strong equilibrium always exists in resource selection games, a closer look reveals severe limitations to the applicability of the existence result even in the simplest case of two identical resources with increasing cost functions. First, these games do not possess a super strong equilibrium in which a fruitful deviation benefits at least one deviator without hurting any other deviator. Second, a strong equilibrium may not exist when the game is played repeatedly. We prove that for any given partition, there exists a super strong equilibrium for resource selection games of identical resources with increasing cost functions. In addition, we show similar existence results for a variety of other classes of resource selection games. For the case of repeated games, we characterize partitions that guarantee the existence of a strong equilibrium. Together, our work introduces a natural concept, which turns out to lead to positive and applicable results in one of the basic domains studied in the literature.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {4},
numpages = {21},
keywords = {Coalitions, partition, resource selection games, strong equilibrium, repeated games}
}

@article{10.1145/1858948.1858951,
author = {Bainbridge, William Sims},
title = {Virtual Worlds as Cultural Models},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858951},
doi = {10.1145/1858948.1858951},
abstract = {Thirteen gamelike virtual worlds illustrate issues that overlap social science and information science, because they embody rather clear theories of society and culture: World of Warcraft, Lord of the Rings Online, Dark Age of Camelot, Age of Conan, Pirates of the Burning Sea, A Tale in the Desert, Entropia Universe, Anarchy Online, The Matrix Online, Tabula Rasa, EVE Online, Star Trek Online, and Dungeons and Dragons Online. A fourteenth, Star Wars Galaxies, illustrates the possibility that not all virtual worlds embody clear theories. After describing the thirteen, this essay discusses their economic systems, social systems, communication challenges, and the ways in which autonomous agents and semi-autonomous secondary avatars enrich interactive complexity.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {3},
numpages = {21},
keywords = {game, Culture, virtual world}
}

@article{10.1145/1858948.1858950,
author = {Liu, Huan and Nau, Dana},
title = {Introduction to the ACM TIST Special Issue AI in Social Computing and Cultural Modeling},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858950},
doi = {10.1145/1858948.1858950},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {2},
numpages = {2}
}

@article{10.1145/1858948.1858949,
author = {Yang, Qiang},
title = {Introduction to ACM TIST},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858949},
doi = {10.1145/1858948.1858949},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {1},
numpages = {2}
}

