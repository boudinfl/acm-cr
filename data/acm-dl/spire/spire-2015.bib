@inproceedings{10.1007/978-3-319-23826-5_1,
author = {Dimond, Jonathan and Sanders, Peter},
title = {Faster Exact Search Using Document Clustering},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_1},
doi = {10.1007/978-3-319-23826-5_1},
abstract = {We show how full-text search based on inverted indices can be accelerated by clustering the documents without losing results SeCluD --- Search with Clustered Documents. We develop a fast multilevel clustering algorithm that uses query cost of conjunctive queries as an objective function. Depending on the inputs we get up to four times faster than non-clustered search. The resulting clusters are also useful for data compression and for distributing the work over many machines.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {1–12},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_2,
author = {Policriti, Alberto and Prezza, Nicola},
title = {Fast Online Lempel-Ziv Factorization in Compressed Space},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_2},
doi = {10.1007/978-3-319-23826-5_2},
abstract = {Let T be a text of length n on an alphabet $$Sigma $$ of size $$sigma $$, and let $$H_0$$ be the zero-order empirical entropy of T. We show that the LZ77 factorization of T can be computed in $$nH_0+onlog sigma + mathcal {O}sigma log n$$ bits of working space with an online algorithm running in $$mathcal {O}nlog n$$ time. Previous space-efficient online solutions either work in compact space and $$mathcal {O}nlog n$$ time, or in succinct space and $$mathcal {O}nlog ^3 n$$ time.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {13–20},
numpages = {8},
keywords = {Lempel-Ziv, Compression, BWT},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_3,
author = {Barbay, J\'{e}r\'{e}my and P\'{e}rez-Lantero, Pablo},
title = {Adaptive Computation of the Swap-Insert Correction Distance},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_3},
doi = {10.1007/978-3-319-23826-5_3},
abstract = {The Swap-Insert Correction distance from a string S of length n to another string L of length $$mge n$$ on the alphabet [1..d] is the minimum number of insertions, and swaps of pairs of adjacent symbols, converting S into L. Contrarily to other correction distances, computing it is NP-Hard in the size d of the alphabet. We describe an algorithm computing this distance in time within $$Od^2 nm g^{d-1}$$, where there are $$n_alpha $$ occurrences of $$alpha $$ in S, $$m_alpha $$ occurrences of $$alpha $$ in L, and where $$g=max _{alpha in [1..d]} min {n_alpha ,m_alpha -n_alpha }$$ measures the difficulty of the instance. The difficulty g is bounded by above by various terms, such as the length of the shortest string S, and by the maximum number of occurrences of a single character in S. The latter bound yields a running time within $$Odn+m+d/d-1^{d-2}cdot n^{d}m-n$$ in the worst case over instances of fixed lengths n and m for S and L, which further simplifies to within $$On^dm-n+m$$ when d is fixed, the state of the art for this problem. This illustrates how, in many cases, the correction distance between two strings can be easier to compute than in the worst case scenario.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {21–32},
numpages = {12},
keywords = {Adaptive, Dynamic programming, Edit distance, Insert, Swap},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_4,
author = {Maneth, Sebastian and Ord\'{o}\~{n}ez, Alberto and Seidl, Helmut},
title = {Transforming XML Streams with References},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_4},
doi = {10.1007/978-3-319-23826-5_4},
abstract = {Many useful xml transformations can be formulated through deterministic top-down tree transducers. If transducers process parts of the input repeatedly or in non-document order, then they cannot be realized over the xml stream with constant or even depth-bounded memory. We show that by enriching streams by forward references both in the input and in the output, every such transformation can be compiled into a stream processor with a space consumption depending only on the transducer and the depth of the xml document. References allow to produce DAG-compressed output that is guaranteed to be linear in the size of the input up to the space required for labels. Our model is designed so that without decompression, the output may again serve as the input of a subsequent transducer.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {33–45},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_5,
author = {Melucci, Massimo},
title = {Efficient Term Set Prediction Using the Bell-Wigner Inequality},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_5},
doi = {10.1007/978-3-319-23826-5_5},
abstract = {The task of measuring the dependence between terms is computationally expensive for IR systems which have to deal with large and sparse datasets. The current approaches to mining frequent term sets are based on the enumeration of the term sets found in a set of documents and on monotonicity, the latter being the property that a term set is frequent only if all its subsets are frequent as implemented by Apriori. However, the computational time can be very large. An alternative approach is to store the dataset in a FPT and to visit and prune the tree in a recursive way as implemented by FPGrowth. However, the storage space can still be very large. We introduce the BWI as a conceptual enhancement of monotonicity to predict with certainty when an itemset is frequent and when it is infrequent. We describe the empirical validation that the BWI can significantly reduce both the computational time of Apriori and the storage space of pattern tree-based algorithms such as FPGrowth. The empirical validation has been performed using some runs produced by IR systems from the TIPSTER test collection.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {46–53},
numpages = {8},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_6,
author = {Dumitran, Marius and Manea, Florin and Nowotka, Dirk},
title = {On Prefix/Suffix-Square Free Words},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_6},
doi = {10.1007/978-3-319-23826-5_6},
abstract = {We present a series of algorithms identifying efficiently the factors of a word that neither start nor end with squares called, accordingly, prefix-suffix-square free factors. A series of closely related algorithmic problems are discussed.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {54–66},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_7,
author = {Craveiro, Olga and Macedo, Joaquim and Madeira, Henrique},
title = {Temporal Analysis of CHAVE Collection},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_7},
doi = {10.1007/978-3-319-23826-5_7},
abstract = {The importance of temporal information TI is increasing in several Information Retrieval IR tasks. CHAVE, available from Linguateca's site, is the only ad hoc IR test collection with Portuguese texts. So, the research question of this work is whether this collection is sufficiently rich to be used in Temporal IR evaluation. The obtained answer was yes. By the analysis of the CHAVE collection, we verified that 22% of the topics and 86% of the documents have at least one chronon. 49% of topics are time-sensitive. Analyzing the relation of topics with documents, relevant documents of time-sensitive topics converge to a specific dates, while the non-relevant ones are dispersed along the timeline. Finally, we used a peak dates strategy as a time-aware query expansion QE process. Experiments showed effectiveness improvements for time-sensitive queries.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {67–74},
numpages = {8},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_8,
author = {Ionescu, Radu Tudor and Chifu, Adrian-Gabriel and Mothe, Josiane},
title = {DeShaTo: Describing the Shape of Cumulative Topic Distributions to Rank Retrieval Systems Without Relevance Judgments},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_8},
doi = {10.1007/978-3-319-23826-5_8},
abstract = {This paper investigates an approach for estimating the effectiveness of any IR system. The approach is based on the idea that a set of documents retrieved for a specific query is highly relevant if there are only a small number of predominant topics in the retrieved documents. The proposed approach is to determine the topic probability distribution of each document offline, using Latent Dirichlet Allocation. Then, for a retrieved set of documents, a set of probability distribution shape descriptors, namely the skewness and the kurtosis, are used to compute a score based on the shape of the cumulative topic distribution of the respective set of documents. The proposed model is termed DeShaTo, which is short for Describing the Shape of cumulative Topic distributions. In this work, DeShaTo is used to rank retrieval systems without relevance judgments. In most cases, the empirical results are better than the state of the art approach. Compared to other approaches, DeShaTo works independently for each system. Therefore, it remains reliable even when there are less systems to be ranked by relevance.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {75–82},
numpages = {8},
keywords = {Ranking retrieval systems, Information retrieval, Kurtosis, Skewness, Topic modeling, Document topic distribution, LDA},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_9,
author = {Liu, Wei Jun and Nong, Ge and Chan, Wai Hong and Wu, Yi},
title = {Induced Sorting Suffixes in External Memory with Better Design and Less Space},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_9},
doi = {10.1007/978-3-319-23826-5_9},
abstract = {Recently, several attempts have been made to extend\"{\i} undefinedthe internal memory suffix array SA construction algorithm SA-IS to the external memory model, e.g., eSAIS, EM-SA-DS and DSA-IS. While the developed programs for these algorithms achieve remarkable performance in terms of I/O complexity and speed, their designs are quite complex and their disk requirements remain rather heavy. Currently, the core algorithmic part of each of these programs consists of thousands of lines in C++, and the average peak disk requirement is over 20n bytes for an input string of size $$n&lt;2^{40}$$. We re-investigate the problem of induced sorting suffixes in external memory and propose a new algorithm SAIS-PQ SAIS with Priority Queue and its enhanced alternative SAIS-PQ+. Using the library STXXL, the core algorithmic parts of SAIS-PQ and SAIS-PQ+ are coded in around 800 and 1600 lines in C++, respectively. The time and space performance of these two programs are evaluated in comparison with eSAIS that is also implemented using STXXL. In our experiment, eSAIS runs the fastest for the input strings not larger than 16 GiB, but it is slower than SAIS-PQ+ for the only two input strings of 32 and 48.44 GiB. For the average peak disk requirements, eSAIS and SAIS-PQ+ are around 23n and 15n bytes, respectively.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {83–94},
numpages = {12},
keywords = {Sorting algorithm, External memory, Suffix array},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_10,
author = {Bannai, Hideo and Inenaga, Shunsuke and Kociumaka, Tomasz and Lefebvre, Arnaud and Radoszewski, Jakub and Rytter, Wojciech and Sugimoto, Shiho and Wale\'{n}, Tomasz},
title = {Efficient Algorithms for Longest Closed Factor Array},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_10},
doi = {10.1007/978-3-319-23826-5_10},
abstract = {We consider a family of strings called closed strings and a related array of Longest Closed Factors LCF. We show that the reconstruction of a string from its LCF array is easier than the construction and verification of this array. Moreover, the reconstructed string is unique. We improve also the time of construction/verification, reducing it from $$mathcal {O}n log n/log log n$$ the best previously known to $$mathcal {O}nsqrt{log n}$$. We use connections between the LCF array and the longest previous/next factor arrays.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {95–102},
numpages = {8},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_11,
author = {Brisaboa, Nieves R. and Cerdeira-Pena, Ana and Fari\~{n}a, Antonio and Navarro, Gonzalo},
title = {A Compact RDF Store Using Suffix Arrays},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_11},
doi = {10.1007/978-3-319-23826-5_11},
abstract = {RDF has become a standard format to describe resources in the Semantic Web and other scenarios. RDF data is composed of triples subject,\"{\i} undefinedpredicate,\"{\i} undefinedobject, referring respectively to a resource, a property of that resource, and the value of such property. Compact storage schemes allow fitting larger datasets in main memory for faster processing. On the other hand, supporting efficient SPARQL queries on RDF datasets requires index data structures to accompany the data, which hampers compactness. As done for text collections, we introduce a self-index for RDF data, which combines the data and its index in a single representation that takes less space than the raw triples and efficiently supports basic SPARQL queries. Our storage format, RDFCSA, builds on compressed suffix arrays. Although there exist more compact representations of RDF data, RDFCSA uses about half of the space of the raw data and replaces it and displays much more robust and predictable query times around 1---2 microseconds per retrieved triple. RDFCSA is 3 orders of magnitude faster than representations like MonetDB or RDF-3X, while using the same space as the former and 6 times less space than the latter. It is also faster than the more compact representations on most queries, in some cases by 2 orders of magnitude.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {103–115},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_12,
author = {Allali, Julien and Chauve, Cedric and Bourgeade, Laetitia},
title = {Chaining Fragments in Sequences: To Sweep or Not Extended Abstract},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_12},
doi = {10.1007/978-3-319-23826-5_12},
abstract = {Computing an optimal chain of fragments is a classical problem in string algorithms, with important applications in computational biology. There exist two efficient dynamic programming algorithms solving this problem, based on different principles. In the present note, we show how it is possible to combine the principles of two of these algorithms in order to design a hybrid dynamic programming algorithm that combines the advantages of both algorithms.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {116–123},
numpages = {8},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_13,
author = {Tanimura, Yuka and Fujishige, Yuta and I, Tomohiro and Inenaga, Shunsuke and Bannai, Hideo and Takeda, Masayuki},
title = {A Faster Algorithm for Computing Maximal $$\alpha $$-Gapped Repeats in a String},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_13},
doi = {10.1007/978-3-319-23826-5_13},
abstract = {A string $$x = uvu$$ with both u,\"{\i} undefinedv being non-empty is called a gapped repeat with period$$p = |uv|$$, and is denoted by pair x,\"{\i} undefinedp. If $$p le alpha |x|-p$$ with $$alpha &gt; 1$$, then x,\"{\i} undefinedp is called an $$alpha $$-gapped repeat. An occurrence $$[i, i+|x|-1]$$ of an $$alpha $$-gapped repeat x,\"{\i} undefinedp in a string w is called a maximal$$alpha $$-gapped repeat of w, if it cannot be extended either to the left or to the right in w with the same period p. Kolpakov et al. CPM 2014 showed that, given a string of length n over a constant alphabet, all the occurrences of maximal $$alpha $$-gapped repeats in the string can be computed in $$Oalpha ^2 n + occ $$ time, where $$ occ $$ is the number of occurrences. In this paper, we propose a faster $$Oalpha n + occ $$-time algorithm to solve this problem, improving the result of Kolpakov et al. by a factor of $$alpha $$.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {124–136},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_14,
author = {Hui, Kai and Berberich, Klaus},
title = {Selective Labeling and Incomplete Label Mitigation for Low-Cost Evaluation},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_14},
doi = {10.1007/978-3-319-23826-5_14},
abstract = {Information retrieval evaluation heavily relies on human effort to assess the relevance of result documents. Recent years have seen efforts and good progress to reduce the human effort and thus lower the cost of evaluation. Selective labeling strategies carefully choose a subset of result documents to label, for instance, based on their aggregate rank in results; strategies to mitigate incomplete labels seek to make up for missing labels, for instance, predicting them using machine learning methods. How different strategies interact, though, is unknown.In this work, we study the interaction of several state-of-the-art strategies for selective labeling and incomplete label mitigation on four years of TREC Web Track data 2011---2014. Moreover, we propose and evaluate MaxRep as a novel selective labeling strategy, which has been designed so as to select effective training data for missing label prediction.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {137–148},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_15,
author = {Boucher, Christina and Bowe, Alexander and Gagie, Travis and Manzini, Giovanni and Sir\'{e}n, Jouni},
title = {Relative Select},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_15},
doi = {10.1007/978-3-319-23826-5_15},
abstract = {Motivated by the problem of storing coloured de Bruijn graphs, we show how, if we can already support fast select queries on one string, then we can store a little extra information and support fairly fast select queries on a similar string.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {149–155},
numpages = {7},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_16,
author = {Gupta, Dhruv and Berberich, Klaus},
title = {Temporal Query Classification at Different Granularities},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_16},
doi = {10.1007/978-3-319-23826-5_16},
abstract = {In this work, we consider the problem of classifying time-sensitive queries at different temporal granularities day, month, and year. Our approach involves performing Bayesian analysis on time intervals of interest obtained from pseudo-relevant documents. Based on the Bayesian analysis we derive several effective features which are used to train a supervised machine learning algorithm for classification. We evaluate our method on a large temporal query workload to show that we can determine the temporal class of a query with high precision.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {156–164},
numpages = {9},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_17,
author = {Fertin, Guillaume and Jankowiak, Lo\"{\i}c and Jean, G\'{e}raldine},
title = {Prefix and Suffix Reversals on Strings},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_17},
doi = {10.1007/978-3-319-23826-5_17},
abstract = {The Sorting by Prefix Reversals problem consists in sorting the elements of a given permutation $$pi $$ with a minimum number of prefix reversals, i.e. reversals that always imply the leftmost element of $$pi $$. A natural extension of this problem is to consider strings in which any letter may appear several times rather than permutations. In strings, three different types of problems arise: grouping starting from a string S, transform it so that all identical letters are consecutive, sorting a constrained version of grouping, in which the target string must be lexicographically ordered and rearranging given two strings S and T, transform S into T. In this paper, we study these three problems, under an algorithmic viewpoint, in the setting where two operations rather than one are allowed: namely, prefix and suffix reversals - where a suffix reversal must always imply the rightmost element of the string. We first give elements of comparison between the "prefix reversals only" case and our case. The algorithmic results we obtain on these three problems depend on the size k of the alphabet on which the strings are built. In particular, we show that the grouping problem is in P for $$kin [2;4]$$ and when $$n-k=O1$$, where n is the length of the string. We also show that the grouping problem admits a PTAS for any constant k, and is 2-approximable for any k. Concerning sorting, it is in P for $$kin [2;3]$$, admits a PTAS for constant k, and is NP-hard for $$k=n$$. Finally, concerning the rearranging problem, we show that it is NP-hard, both for $$k=O1$$ and $$k=n$$. We also show that the three problems are FPT when the parameter is the maximum number of blocks over the source and target strings.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {165–176},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_18,
author = {Chhabra, Tamanna and Giaquinta, Emanuele and Tarhio, Jorma},
title = {Filtration Algorithms for Approximate Order-Preserving Matching},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_18},
doi = {10.1007/978-3-319-23826-5_18},
abstract = {The exact order-preserving matching problem is to find all the substrings of a text T which have the same length and relative order as a pattern P. Like string maching, order-preserving matching can be generalized by allowing the match to be approximate. In approximate order-preserving matching two strings match if they have the same relative order after removing up to k elements in the same positions in both strings. In this paper we present practical solutions for this problem. The methods are based on filtration, and one of them is the first sublinear solution on average. We show by practical experiments that the new solutions are fast and efficient.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {177–187},
numpages = {11},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_19,
author = {Bo\v{z}a, Vladim\'{\i}r and Jursa, Jakub and Brejov\'{a}, Bro\v{n}a and Vina\v{r}, Tom\'{a}\v{s}},
title = {Fishing in Read Collections: Memory Efficient Indexing for Sequence Assembly},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_19},
doi = {10.1007/978-3-319-23826-5_19},
abstract = {In this paper, we present a memory efficient index for storing a large set of DNA sequencing reads. The index allows us to quickly retrieve the set of reads containing a certain query k-mer. Instead of the usual approach of treating each read as a separate string, we take an advantage of significant overlap between reads and compress the data by aligning the reads to an approximate superstring constructed specifically for this purpose in combination with several succint data structures.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {188–198},
numpages = {11},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_20,
author = {Hozza, Michal and Vina\v{r}, Tom\'{a}\v{s} and Brejov\'{a}, Bro\v{n}a},
title = {How Big is That Genome? Estimating Genome Size and Coverage from k-Mer Abundance Spectra},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_20},
doi = {10.1007/978-3-319-23826-5_20},
abstract = {Many practical algorithms for sequence alignment, genome assembly and other tasks represent a sequence as a set of k-mers. Here, we address the problems of estimating genome size and sequencing coverage from sequencing reads, without the need for sequence assembly. Our estimates are based on a histogram of k-mer abundance in the input set of sequencing reads and on probabilistic modeling of distribution of k-mer abundance based on parameters related to the coverage, error rate and repeat structure of the genome. Our method provides reliable estimates even at coverage as low as 0.5 or at error rates as high as 10%.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {199–209},
numpages = {11},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_21,
author = {Gomes Ferreira, Wadson and Ant\^{o}nio Dos Santos, Willian and Macena Pereira De Souza, Breno and Matta Machado Zaidan, Tiago and Cardoso Brand\~{a}o, Wladmir},
title = {Assessing the Efficiency of Suffix Stripping Approaches for Portuguese Stemming},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_21},
doi = {10.1007/978-3-319-23826-5_21},
abstract = {Stemming is the process of reducing inflected words to their root form, the stem. Search engines use stemming algorithms to conflate words in the same stem, reducing index size and improving recall. Suffix stripping is a strategy used by stemming algorithms to reduce words to stems by processing suffix rules suitable to address the constraints of each language. For Portuguese stemming, the RSLP was the first suffix stripping algorithm proposed in literature, and it is still widely used in commercial and open source search engines. Typically, the RSLP algorithm uses a list-based approach to process rules for suffix stripping. In this article, we introduce two suffix stripping approaches for Portuguese stemming. Particularly, we propose the hash-based and the automata-based approach, and we assess their efficiency by contrasting them with the state-of-the-art list-based approach. Complexity analysis shows that the automata-based approach is more efficient in time. In addition, experiments on two datasets attest the efficiency of our approaches. In particular, the hash-based and the automata-based approaches outperform the list-based approach, with reduction of up to 65.28% and 86.48% in stemming time, respectively.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {210–221},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_22,
author = {Belazzougui, Djamal and Cunial, Fabio},
title = {Space-Efficient Detection of Unusual Words},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_22},
doi = {10.1007/978-3-319-23826-5_22},
abstract = {Detecting all the strings that occur in a text more frequently or less frequently than expected according to an IID or a Markov model is a basic problem in string mining, yet current algorithms are based on data structures that are either space-inefficient or incur large slowdowns, and current implementations cannot scale to genomes or metagenomes in practice. In this paper we engineer an algorithm based on the suffix tree of a string to use just a small data structure built on the Burrows-Wheeler transform, and a stack of $$Osigma ^2log ^2 n$$ bits, where n is the length of the string and $$sigma $$ is the size of the alphabet. The size of the stack is on except for very large values of $$sigma $$. We further improve the algorithm by removing its time dependency on $$sigma $$, by reporting only a subset of the maximal repeats and of the minimal rare words of the string, and by detecting and scoring candidate under-represented strings that do not occur in the string. Our algorithms are practical and work directly on the BWT, thus they can be immediately applied to a number of existing datasets that are available in this form, returning this string mining problem to a manageable scale.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {222–233},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_23,
author = {Baier, Uwe and Beller, Timo and Ohlebusch, Enno},
title = {Parallel Construction of Succinct Representations of Suffix Tree Topologies},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_23},
doi = {10.1007/978-3-319-23826-5_23},
abstract = {A compressed suffix tree usually consists of three components: a compressed suffix array, a compressed $$mathsf {LCP}$$-array, and a succinct representation of the suffix tree topology. There are parallel algorithms that construct the suffix array and the $$mathsf {LCP}$$-array, but none for the third component. In this paper, we present parallel algorithms on shared memory architectures that construct the enhanced balanced parentheses representation $$mathsf {BPR}$$. The enhanced $$mathsf {BPR}$$ is an implicit succinct representation of the suffix tree topology, which supports all navigational operations on the suffix tree. It can also be used to efficiently construct the $$mathsf {BPS}$$, an explicit succinct representation of the suffix tree topology.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {234–245},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_24,
author = {Gawrychowski, Pawe\l{} and Kucherov, Gregory and Sach, Benjamin and Starikovskaya, Tatiana},
title = {Computing the Longest Unbordered Substring},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_24},
doi = {10.1007/978-3-319-23826-5_24},
abstract = {A substring of a string is unbordered if its only border is the empty string. The study of unbordered substrings goes back to the paper of Ehrenfeucht and Silberger [Discr. Math 26 1979]. The main focus of their and subsequent papers was to elucidate the relationship between the longest unbordered substring and the minimal period of strings. In this paper, we consider the algorithmic problem of computing the longest unbordered substring of a string. The problem was introduced recently by G. Kucherov et al.\"{\i} undefined[CPM 2015], where the authors showed that the average-case running time of the simple, border-array based algorithm can be bounded by $$mathcal {O}max {n, n^2/sigma ^4}$$ for $$sigma $$ being the size of the alphabet. The worst-case running time remained $$mathcal {O}n^2$$. Here we propose two algorithms, both presenting substantial theoretical improvements to the result of [11]. The first algorithm has $$mathcal {O}n log n$$ average-case running time and $$mathcal {O}n^2$$ worst-case running time, and the second algorithm has $$mathcal {O}n^{1.5}$$ worst-case running time.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {246–257},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_25,
author = {Takabatake, Yoshimasa and Tabei, Yasuo and Sakamoto, Hiroshi},
title = {Online Self-Indexed Grammar Compression},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_25},
doi = {10.1007/978-3-319-23826-5_25},
abstract = {Although several grammar-based self-indexes have been proposed thus far, their applicability is limited to offline settings where whole input texts are prepared, thus requiring to rebuild index structures for given additional inputs, which is often the case in the big data era. In this paper, we present the first online self-indexed grammar compression named OESP-index that can gradually build the index structure by reading input characters one-by-one. Such a property is another advantage which enables saving a working space for construction, because we do not need to store input texts in memory. We experimentally test OESP-index on the ability to build index structures and search query texts, and we show OESP-index's efficiency, especially space-efficiency for building index structures.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {258–269},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_26,
author = {Gawrychowski, Pawe\l{} and Kociumaka, Tomasz and Rytter, Wojciech and Wale\'{n}, Tomasz},
title = {Tight Bound for the Number of Distinct Palindromes in a Tree},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_26},
doi = {10.1007/978-3-319-23826-5_26},
abstract = {For an undirected tree with n edges labelled by single letters, we consider its substrings, which are labels of the simple paths between pairs of nodes. We prove that there are $$mathcal {O}n^{1.5}$$ different palindromic substrings. This solves an open problem of Brlek, Lafreni\`{e}re and Proven\c{c}al DLT 2015, who gave a matching lower-bound construction. Hence, we settle the tight bound of $$Theta n^{1.5}$$ for the maximum palindromic complexity of trees. For standard strings, i.e., for paths, the palindromic complexity is $$n+1$$.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {270–276},
numpages = {7},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_27,
author = {Fischer, Johannes and Holub, \v{S}tundefinedp\'{a}n and I, Tomohiro and Lewenstein, Moshe},
title = {Beyond the Runs Theorem},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_27},
doi = {10.1007/978-3-319-23826-5_27},
abstract = {In [3], a short and elegant proof was presented showing that a word of length n contains at most $$n-3$$ runs. Here we show, using the same technique and a computer search, that the number of runs in a binary word of length n is at most $$frac{22}{23}n&lt;0.957n$$.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {277–286},
numpages = {10},
keywords = {Runs, Lyndon words, Combinatorics on words},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_28,
author = {Grabowski, Szymon and Raniszewski, Marcin},
title = {Sampling the Suffix Array with Minimizers},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_28},
doi = {10.1007/978-3-319-23826-5_28},
abstract = {Sampling evenly the suffixes from the suffix array is an old idea trading the pattern search time for reduced index space. A few years ago Claude et al. showed an alphabet sampling scheme allowing for more efficient pattern searches compared to the sparse suffix array, for long enough patterns. A drawback of their approach is the requirement that sought patterns need to contain at least one character from the chosen subalphabet. In this work we propose an alternative suffix sampling approach with only a minimum pattern length as a requirement, which seems more convenient in practice. Experiments show that our algorithm achieves competitive time-space tradeoffs on most standard benchmark data. As a side result, we show that $$n'$$ arbitrarily selected suffixes from a text of length n, where $$n' &lt; n$$, over an integer alphabet, can be sorted in On time using $$On'$$ words of space.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {287–298},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_29,
author = {Manzini, Giovanni},
title = {Longest Common Prefix with Mismatches},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_29},
doi = {10.1007/978-3-319-23826-5_29},
abstract = {The Longest Common Prefix LCP array is a data structure commonly used in combination with the Suffix Array. However, in some settings we are interested in the LCP values per se since they provide useful information on the repetitiveness of the underlying sequence.Since sequences can contain alterations, which can be either malicious plagiarism attempts or pseudo-random as in sequencing experiments, when using LCP values to measure repetitiveness it makes sense to allow for a small number of errors. In this paper we formalize this notion by considering the longest common prefix in the presence of mismatches. In particular, we propose an algorithm that computes, for each text suffix, the length of its longest prefix that occurs elsewhere in the text with at most one mismatch. For a sequence of length n our algorithm uses $$Theta nlog n$$ bits and runs in $$mathcal {O}n text{L}_{ave}log n/log log n$$ time where $$text{L}_{ave}$$ is the average LCP of the input sequence. Although $$text{L}_{ave}$$ is $$Theta n$$ in the worst case, recent analyses of real world data show that it usually grows logarithmically with the input size. We then describe and analyse a second algorithm that uses a greedy strategy to reduce the amount of computation and that can be turned into an even faster algorithm if allow an additive one-sided error.Finally, we consider the related problem of computing the 1-mappability of a sequence. In this problem we are asked to compute, for each length-m substring of the input sequence, the number of other substrings which are at Hamming distance one. For this problem we propose an algorithm that takes $$mathcal {O}m n log n/log log n$$ time using $$Theta n log n$$ bits of space.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {299–310},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_30,
author = {Ferr\'{e}s, Daniel and Rodr\'{\i}guez, Horacio},
title = {Evaluating Geographical Knowledge Re-Ranking, Linguistic Processing and Query Expansion Techniques for Geographical Information Retrieval},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_30},
doi = {10.1007/978-3-319-23826-5_30},
abstract = {This paper describes and evaluates the use of Geographical Knowledge Re-Ranking, Linguistic Processing, and Query Expansion techniques to improve Geographical Information Retrieval effectiveness. Geographical Knowledge Re-Ranking is performed with Geographical Gazetteers and conservative Toponym Disambiguation techniques that boost the ranking of the geographically relevant documents retrieved by standard state-of-the-art Information Retrieval algorithms. Linguistic Processing is performed in two ways: 1 Part-of-Speech tagging and Named Entity Recognition and Classification are applied to analyze the text collections and topics to detect toponyms, 2 Stemming Porter's algorithm and Lemmatization are also applied in combination with default stopwords filtering. The Query Expansion methods tested are the Bose-Einstein Bo1 and Kullback-Leibler term weighting models. The experiments have been performed with the English Monolingual test collections of the GeoCLEF evaluations from years 2005, 2006, 2007, and 2008 using the TF-IDF, BM25, and InL2 Information Retrieval algorithms over unprocessed texts as baselines. The experiments have been performed with each GeoCLEF test collection 25 topics per evaluation separately and with the fusion of all these collections 100 topics. The results of evaluating separately Geographical Knowledge Re-Ranking, Linguistic Processing lemmatization, stemming, and the combination of both, and Query Expansion with the fusion of all the topics show that all these processes improve the Mean Average Precision MAP and RPrecision effectiveness measures in all the experiments and show statistical significance over the baselines in most of them. The best results in MAP and RPrecision are obtained with the InL2 algorithm using the following techniques: Geographical Knowledge Re-Ranking, Lemmatization with Stemming, and Kullback-Leibler Query Expansion. Some configurations with Geographical Knowledge Re-Ranking, Linguistic Processing and Query Expansion have improved the MAP of the best official results at GeoCLEF evaluations of 2005, 2006, and 2007.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {311–323},
numpages = {13},
keywords = {Geographical gazetteers, Query expansion, Efectiveness measures, Natural language processing, Toponym disambiguation, Information retrieval},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_31,
author = {Poyias, Andreas and Raman, Rajeev},
title = {Improved Practical Compact Dynamic Tries},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_31},
doi = {10.1007/978-3-319-23826-5_31},
abstract = {We consider the problem of implementing a dynamic trie with an emphasis on good practical performance. For a trie with n nodes with an alphabet of size $$sigma $$, the information-theoretic lower bound is $$n log sigma + On$$ bits. The Bonsai data structure [1] supports trie operations in O1 expected time based on assumptions about the behaviour of hash functions. While its practical speed performance is excellent, its space usage of $$1+epsilon n log sigma + Olog log n$$ bits, where $$epsilon $$ is any constant $$&gt; 0$$, is not asymptotically optimal. We propose an alternative, m-Bonsai, that uses $$1 + epsilon n log sigma + O1$$ bits in expectation, and supports operations in O1 expected time again based on assumptions about the behaviour of hash functions. We give a heuristic implementation of m-Bonsai which uses considerably less memory and is slightly faster than the original Bonsai.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {324–336},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_32,
author = {Kulkarni, Anagha},
title = {ShRkC: Shard Rank Cutoff Prediction for Selective Search},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_32},
doi = {10.1007/978-3-319-23826-5_32},
abstract = {In search environments where large document collections are partitioned into smaller subsets shards, processing the query against only the relevant shards improves search efficiency. The problem of ranking the shards based on their estimated relevance to the query has been studied extensively. However, a related important task of identifying how many of the top ranked relevant shards should be searched for the query, so as to balance the competing objectives of effectiveness and efficiency, has not received much attention. This task of shard rank cutoff estimation is the focus of the presented work. The central premise for the proposed solution is that the number of top shards searched should be dependent on --- 1. the query, 2. the given ranking of shards, and 3. on the type of search need being served precision-oriented versus recall-oriented task. An array of features that capture these three factors are defined, and a regression model is induced based on these features to learn a query-specific shard rank cutoff estimator. An empirical evaluation using two large datasets demonstrates that the learned shard rank cutoff estimator provides substantial improvement in search efficiency as compared to strong baselines without degrading search effectiveness.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {337–349},
numpages = {13},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_33,
author = {Amir, Amihood and Lewenstein, Moshe and Thankachan, Sharma V.},
title = {Range LCP Queries Revisited},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_33},
doi = {10.1007/978-3-319-23826-5_33},
abstract = {The Range LCP problem is to preprocess a string $$S[1dots n]$$, to enable efficient solutions of the following query: given a range [l,\"{\i} undefinedr] as the input, report $$max _{i, j in {l,ldots ,r}} |mathsf {LCP}S_{i}, S_j|$$. Here $$mathsf {LCP}S_i, S_j$$ is the longest common prefix of the suffixes of S starting at locations i and j and $$|mathsf {LCP}S_i,S_j|$$ is its length. We study a natural extension of this problem, where the query consists of two ranges. Additionally, we allow a bounded number say $$kge 0$$ of mismatches in the $$mathsf {LCP}$$ computation. Specifically, our task is to report the following when two ranges $$[ell _1, r_1]$$ and $$[ell _2,r_2]$$ comes as input: $$max _{{ell _1le ile r_1, ell _2le jle r_2}}|mathsf {LCP}_kS_i,S_j|$$Here $$mathsf {LCP}_kS_i,S_j$$ is the longest prefix of $$S_i$$ and $$S_j$$ with at most k mismatches allowed. We show that the queries can be answered in Ok time using an $$On^2/w$$ space data structure, where w is the word size. We also present space efficient data structures for $$k=0$$ and $$k=1$$. For $$k=0$$, we obtain a linear space data structure with query time $$Osqrt{n/w}log ^{epsilon } n$$, where w is the word size and $$epsilon &gt;0$$ is an arbitrarily small constant.For the case $$k=1$$ we obtain an $$Onlog n$$ space data structure with query time $$Osqrt{n}log n$$.Finally, we give a reduction from Set Intersection to Range LCP queries, suggesting that it will be very difficult to improve our upper bound by more than a factor of $$Olog ^{epsilon }n$$.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {350–361},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

@inproceedings{10.1007/978-3-319-23826-5_34,
author = {Baeza-Yates, Ricardo and Mayo-Casademont, Mart\'{\i} and Rello, Luz},
title = {Feasibility of Word Difficulty Prediction},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_34},
doi = {10.1007/978-3-319-23826-5_34},
abstract = {We present a machine learning algorithm to predict how difficult is a word for a person with dyslexia. To train the algorithm we used a data set of words labeled as easy or difficult. The algorithm predicts correctly slightly above 72% of our instances, showing the feasibility of building such a predictive solution for this problem. The main purpose of our work is to be able to weight words in order to perform lexical simplification in texts read by people with dyslexia. Since the main feature used by the classifier, and the only that is not computed in constant time, is the number of similar words in a dictionary, we did a study on the different methods that exist to compute efficiently this feature. This algorithmic comparison is interesting on its own sake and shows that two algorithms can solve the problem in less than a second.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {362–373},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

