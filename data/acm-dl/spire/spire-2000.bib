@inproceedings{10.5555/829519.830838,
title = {Preface},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {.07},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830839,
title = {Program Committee},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {.08},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830837,
title = {Reviewers},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {.09},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830842,
author = {de Agostino, S.},
title = {Speeding up Parallel Decoding of LZ Compressed Text on the PRAM EREW},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {While sliding window (LZ1) compression can be parallelized efficiently, the LZ2 compression method seems hardly parallelizable since some related heuristics are known to be P-complete. In spite of such negative result, there are parallel decoders which run in O(log/sup 2/ n) time with O(n/log n) processors on the PRAM EREW where n is the length of the output string, as for LZ1 decompression. We show a faster parallel decoding algorithm which runs on the PRAM EREW in O(log n) time with O(n) processors for text compressed by a standard implementation of the LZ2 algorithm (next character heuristic). We observe that LZ1 parallel decoders also can have such speed up. Moreover, we address a different implementation of LZ2 compression called identity heuristic. In this case, decoding on the PRAM EREW takes O(log n log log n) time with O(n/log n) processors with the realistic assumption that the length of the dictionary elements is logarithmic.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {2},
keywords = {dictionary, data compression, LZ1 compression, LZ compressed text, text compression, sliding window compression, parallel decoding, PRAM EREW, output string, P-complete, next character heuristic, LZ2 compression method, identity heuristic},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830843,
author = {Alonso, O. and Baeza-Yates, R.},
title = {A Model and Software Architecture for Search Results Visualization on the WWW},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We analyze the dependency problem of the user interface with the information retrieval software. Our approach allows the separation of the user interface from the retrieval component. This is useful when the user wants to select an interface or visualization metaphor that could not always be available for different information retrieval systems. We present a model for visualizing large collections of documents in World Wide Web retrieval, independently of the retrieval system. We describe a software architecture that could be used to implement a solution in an intranet or Internet environment. Our proposal allows to ease the use of visualization tools which partially solve the problem of data overload on the Internet.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {8},
keywords = {user interface, information retrieval software, intranet, information retrieval systems, search result visualization, World Wide Web, data overload, Internet, software architecture},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830833,
author = {Baeza-Yates, R. A. and Jones, T. and Rawlins, G. J.},
title = {New Approaches to Information Management: Attribute-Centric Data Systems},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Trying to find information on the Web is like trying to find something at a jumble sale: it is fun, and you can make serendipitous discoveries, but for directed search it is better to go to a department store; there, someone has already done most of the arranging for you. Unfortunately, the Web's continuing explosion in size, its enormous diversity of topics, and its great volatility, make unaided human indexing impossible. This problem is just a special case of the general problem of organizing information to create knowledge. A similar problem arises on the desktop when dealing with file systems, where users must search by name. When searching for a particular file, however, users often do not remember the file's name or location. File names are artifacts of current operating systems, but human understanding neither requires objects to be named, nor does it have problems with multiple objects sharing properties, names, for instance. That more general approach is not developed in current file systems or user interfaces. We argue for an approach to information representation based on the use of attributes and search. This representation is organization-neutral, thereby giving a flexible substrate for anyone to build multiple simultaneous organizations. We argue the approach from three perspectives: Attribute Value System (AVS), a networked storage system where objects are composed solely of attribute-value pairs; DomainView (DV), a desktop metaphor where objects do not have explicit names and retrieval is done by content; and KnownSpace (KS), a personalized desktop data manager.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {17},
keywords = {networked storage system, file systems, content based retrieval, personalized desktop data manager, information representation, Attribute Value System, World Wide Web, operating systems, indexing, desktop metaphor, user interfaces, DomainView, attribute-centric data systems, information resources, directed search, information management, KnownSpace},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830830,
author = {Baeza-Yates, R. and Valiente, G.},
title = {An Image Similarity Measure Based on Graph Matching},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The problem of computing the similarity between two images is transformed to that of approximating the distance between two extended region adjacency graphs, which are extracted from the images in time and space linear in the number of pixels. Invariance to translation and rotation is thus achieved. Invariance to scaling is also achieved by taking the relative size of regions into account. Furthermore, the method provides a trade-off between pixel similarity threshold and approximation of the distance measure, which can be used to bound the error in image recognition as well as the time complexity of the computation.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {28},
keywords = {invariance to rotation, time complexity, invariance to translation, pixels, image similarity measure, pixel similarity threshold, image matching, error, image recognition, graph matching, distance measure approximation, extended region adjacency graphs, directed graph, invariance to scaling},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830817,
author = {Bergroth, L. and Hakonen, H. and Raita, T.},
title = {A Survey of Longest Common Subsequence Algorithms},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The aim of this paper is to give a comprehensive comparison of well-known longest common subsequence algorithms (for two input strings) and study their behaviour in various application environments. The performance of the methods depends heavily on the properties of the problem instance as well as the supporting data structures used in the implementation. We want to make also a clear distinction between methods that determine the actual lcs and those calculating only its length, since the execution time and more importantly, the space demand depends crucially on the type of the task. To our knowledge, this is the first time this kind of survey has been done. Due to the page limits, the paper gives only a coarse overview of the performance of the algorithms; more detailed studies are reported elsewhere.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {39},
keywords = {string matching, execution time, string comparison, algorithm performance, data structures, longest common subsequence algorithms},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830823,
author = {de Brevern, A. G. and Hazout, S. A.},
title = {Hybrid Protein Model (HPM): A Method to Compact Protein 3D-Structure Information and Physicochemical Properties},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The transformation of protein 1D-sequence to protein 3D-structure is one of the main difficulties of structural biology. A structural alphabet has been previously defined from dihedral angles describing the protein backbone as structural information by using an unsupervised classifier. The 16 protein blocks (PBs), basis element of the structural alphabet, allows a correct 3D structure approximation. Local prediction had been estimated by a Bayesian approach and shown that sequence information induces strongly the local fold, but stays coarse (prediction rate of 40.7% with one PB, 75,8% with the four most probable PBs). The Hybrid Protein Model presented in this study learns both the sequence and structure of the proteins. The analysis made along the hybrid protein has permitted to appreciate more precisely the spatial location of some types of amino acid residues in the secondary structures and their flanking regions. This study leads to a fuzzy model of dependence between sequence and structure.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {49},
keywords = {amino acid residues, biology computing, fuzzy model, pattern matching, sequence information, structural alphabet, structural biology, Bayesian approach, unsupervised classifier, physicochemical properties, Hybrid Protein Model, protein 1D-sequence, protein 3D-structure, protein blocks},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830849,
author = {Brun, A. and Smaili, K. and Haton, J.-P.},
title = {Experiment Analysis in Newspaper Topic Detection},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present several methods for topic detection on newspaper articles, using either a general vocabulary or topic-specific vocabularies. Specific vocabularies are determined manually or statistically. In both cases, we aim at finding the most representative words of a topic. Several methods have been experimented, the first one is based on perplexity, this method achieves a 100% topic identification rate, on large test corpora, when the two first propositions are taken into account. Other methods are based on statistical counts and achieve 94% of identification on smaller test corpora. The major challenge of this work is to identify topics with only few words in order to be able, during speech recognition, to determine the best adequate language model.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {55},
keywords = {large test corpora, representative words, experiment analysis, perplexity, newspaper topic detection, speech recognition, language model, vocabulary, statistical counts},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830828,
author = {Castellanos, J. and Paun, G. and Rodriguez-Paton, A.},
title = {Computing with Membranes: P Systems with Worm-Objects},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We consider a combination of P systems with objects described by symbols with P systems with objects described by strings. Namely, we work with multisets of strings and consider as the result of a computation the number of strings in a given output membrane. The strings (also called worms) are processed by replication, splitting, mutation, and recombination; no priority among rules and no other ingredient is used. In these circumstances, it is proved that: (1) P systems of this type can generate all recursively enumerable sets of numbers; and moreover, (2) the Hamiltonian Path Problem in a directed graph can be solved in quadratic time, while the SAT problem can be solved in linear time. The interest of the latter result comes from the fact that it is the first time that a polynomial solution to an NP-complete problem has been obtained in the P system framework without making use of the (non-realistic) operation of membrane division.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {65},
keywords = {string multisets, recursively enumerable sets, output membrane, directed graph, membrane division, computational complexity, SAT problem, P systems, polynomial solution, worm-objects, NP-complete problem, Hamiltonian Path Problem, quadratic time, linear time, symbols},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830819,
author = {Chavez, E. and Navarro, G.},
title = {An Effective Clustering Algorithm to Index High Dimensional Metric Spaces},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A metric space consists of a collection of objects and a distance function defined among them, which satisfies the triangular inequality. The goal is to preprocess the set so that, given a set of objects and a query, one can retrieve those objects close enough to the query. The number of distances computed to achieve this goal is the complexity measure. The problem is very difficult in the so-called high dimensional metric spaces, where the histogram of distances has a large mean and a small variance. A recent survey on methods to index metric spaces has shown that the so-called clustering algorithms are better suited than their competitors, pivot based algorithms, to cope with high dimensional metric spaces. The authors present a new clustering method that achieves much better performance than all the existing data structures. We present analytical and experimental results that support our claims and that give the users the tuning parameters to make optimal use of this data structure.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {75},
keywords = {clustering method, clustering algorithms, set preprocessing, pivot based algorithms, object retrieval, high dimensional metric space indexing, information retrieval, distance histogram, distance function, data structures, tuning parameters, complexity measure, triangular inequality, clustering algorithm},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830851,
author = {Fernandez-Iglesias, M. J. and Pavon-Marino, P. and Rodriguez-Estevez, J. and Rifon, L. A. and Llamas-Nistal, M.},
title = {DelfosnetX: A Workbench for XML-Based Information Retrieval Systems},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The authors present DelfosnetX, an information retrieval (IR) system intended to evaluate different relevance analysis and ranking techniques for metadata-enabled IR, and more specifically, XML based IR. The theoretical background that supports the proposed model is also discussed.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {87},
keywords = {ranking techniques, DelfosnetX, relevance analysis, metadata-enabled IR, theoretical background, XML based IR, workbench, XML based information retrieval systems, information retrieval},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830844,
author = {Fredriksson, K. and Ukkonen, E.},
title = {Combinatorial Methods for Approximate Pattern Matching under Rotations and Translations in 3D Arrays},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We consider the problem of defining and evaluating the distance between a three-dimensional pattern P[1..m, 1..m, 1..m] of voxels and a three-dimensional volume V[1..n, 1..n, 1..n] of voxels when rotations of P are also allowed. In particular we are interested in finding the orientation and location of P with respect of V that gives the minimum distance. We consider several distance measures. Our basic method works for all distance measures such that the voxels affect the distance between P and V only locally, that is, the distance between two voxels can be completed in unit time. The number of different orientations that P can have is analyzed. We give incremental algorithms to compute the distance, and several filtering algorithms to compute the upper and lower bounds for the distance. We conclude with experimental results on real data (three dimensional reconstruction of a biological virus).},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {96},
keywords = {distance measures, pattern matching, filtering algorithms, translations, real data, voxels, incremental algorithms, combinatorial methods, biological virus, three-dimensional pattern, minimum distance, three-dimensional volume, rotations, 3D arrays, approximate pattern matching, three dimensional reconstruction},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830840,
author = {Fredriksson, K.},
title = {Rotation Invariant Histogram Filters for Similarity and Distance Measures between Digital Images},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Fast rotation invariant filtering algorithms are presented for speeding-up the search for the position of a given pattern template from a large image. The algorithms work for several distance/similarity measures. The algorithms give useful correlation information as such, and can be used as filtering algorithms for more sophisticated methods. The results are presented in the 2D case, but extensions to 3D are straightforward. The algorithms are based on color histograms computed from the pattern. A technique is described for pruning the filtered positions incrementally, using a simple, yet effective boundary that divides the search space in two.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {105},
keywords = {pattern template, 2D case, large image, distance measures, image recognition, search space, rotation invariant histogram filters, color histograms, distance/similarity measures, as filtering algorithms, correlation information, digital images, filtered positions, fast rotation invariant filtering algorithms},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830847,
author = {Garzon, M. and Drumwright, E. and Deaton, R. J. and Renault, D.},
title = {Virtual Test Tubes: A New Methodology for Computing},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Biomolecular computing (BMC) aims to capture the innumerable advantages that biological molecules have gained in the course of millions of years of evolution to perform computation unfeasible on conventional electronic computers. While biomolecules have resolved fundamental problems as a parallel computer system that we are just beginning to decipher, BMC still suffers from our inability to harness these properties to bring biomolecular computations to levels of reliability, efficiency and scalability that are now taken for granted with conventional solid-state based computers. The authors explore an alternative approach to exploiting these properties by building virtual test tubes in software that would capture the fundamental advantages of biomolecules, in the same way that evolutionary algorithms capture in silico the key properties of Darwinian evolution. We use a previously built tool, Edna, to explore the capabilities of the new paradigm.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {116},
keywords = {evolutionary algorithms, reliability, biomolecules, virtual test tubes, biological molecules, biomolecular computing, solid-state based computers, electronic computers, computing methodology, biocomputers, Edna},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830848,
author = {Hakli, R. and Nykanen, M. and Tamm, H.},
title = {Adding String Processing Capabilities to Data Management Systems},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current data management and information retrieval systems lack advanced string processing capabilities needed in string-oriented application areas like computational molecular biology. Several theoretical models for string processing and querying have been proposed but they either have not been implemented in practice or the implementations are too restricted or platform-dependent to be generally useful. We propose a design for a string processing method that could be used to extend different kinds of data management systems by trying to minimise the interconnections between the database management system and the string handling part. The work continues the development of the Alignment Declaration language, our proposal for specifying string relations, by presenting a new and improved method for evaluating the queries and analysing their finiteness.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {122},
keywords = {Alignment Declaration language, finiteness analysis, string theory, string handling, computational molecular biology, information retrieval systems, string relations, database management system, string processing capabilities, string-oriented application areas, data management systems, query evaluation, string processing method},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830821,
author = {Hirao, M. and Shinohara, A. and Takeda, M. and Arikawa, S.},
title = {Fully Compressed Pattern Matching Algorithm for Balanced Straight-Line Programs},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We consider a fully compressed pattern matching problem, where both text T and pattern P are given by its succinct representation, in terms of straight-line programs and its variant. The length of the text T and pattern P may grow exponentially with respect to its description size n and m, respectively. The best known algorithm for the problems runs in O(n/sup 2/m/sup 2/) time using O(nm) space. The authors introduce a variant of straight-line programs, called balanced straight-line programs so that we establish a faster fully compressed pattern matching algorithm. Although the compression ratio of balanced straight-line programs may be worse than the original straight-line programs, they can still express exponentially long strings. Our algorithm runs in O(nm) time using O(nm) space.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {132},
keywords = {faster fully compressed pattern matching algorithm, fully compressed pattern matching algorithm, succinct representation, description size, fully compressed pattern matching problem, exponentially long strings, pattern matching, balanced straight-line programs, compression ratio},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830835,
author = {Honrado, A. and Leon, R. and O'Donnel, R. and Sinclair, D.},
title = {A Word Stemming Algorithm for the Spanish Language},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The paper describes a word stemming algorithm for the Spanish language. Experiments in document retrieval regarding English text suggest that word stemming based on morphological analysis does not generally or consistently outperform ad-hoc hand tuned algorithms such as that proposed by M. Porter (1980). It is difficult to produce a Porter style algorithm for a romantic language such as Spanish, however due to the greater grammatical complexity and due to the fact that inflection often causes changes to the root of words, not just to their endings (as is mostly the case with English). In general terms, the difficulty consists of producing an algorithm which can cope with the additional complexity of Spanish morphology whilst preserving the simplicity of a Porter style algorithm. One such algorithm is presented. The algorithm combines dictionary look-ups with some 300 stemming and intermediate reduction rules.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {139},
keywords = {Porter style algorithm, English text, inflection, grammatical complexity, document retrieval, dictionary look-ups, word roots, natural languages, ad-hoc hand tuned algorithms, morphological analysis, word stemming algorithm, Spanish morphology, Spanish language, intermediate reduction rules},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830841,
author = {Hoshino, H. and Shinohara, A. and Takeda, M. and Arikawa, S.},
title = {Online Construction of Subsequence Automata for Multiple Texts},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We consider a deterministic finite automaton which accepts all subsequences of a set of texts, called subsequence automaton. We show an online algorithm for constructing a subsequence automaton for a set of texts. It runs in O(|/spl Sigma/|(m+k)+N) time using O(|/spl Sigma/|m) space, where |/spl Sigma/| is the size of alphabet, m is the size of the resulting subsequence automaton, k is the number of texts, and N is the total length of texts. It can be used to preprocess a given set S of texts in such a way that for any query /spl omega/ /spl isin/ /spl Sigma/*, returns in O(|/spl omega/|) time the number of texts in S which contain /spl omega/ as a subsequence. We also show an upper bound of the size of automaton compared to the minimum automaton.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {146},
keywords = {deterministic finite automaton, online algorithm, online construction, alphabet, subsequence automaton, minimum automaton, multiple texts, preprocessing, subsequence automata, deterministic automata},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830825,
author = {Kim, Minkoo and Lu, Fenghua and Raghavan, V. V.},
title = {Automatic Construction of Rule-Based Trees for Conceptual Retrieval},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many intelligent retrieval approaches have been studied to bridge the terminological gap existing between the way in which users specify their information needs and the way in which queries are expressed. One of the approaches, called RUBRIC (RUle-Based Retrieval of Information by Computer), uses production rules to capture user query concepts (or topics). A set of related production rules is represented as an AND/OR tree, called a rule-based tree. One of the main problems in this approach is how to construct such rules that can capture user query concepts. This paper provides a logical framework that is semantically essential to defining the rules for the user query concepts, and proposes a way to automatically construct rule-based trees from typical thesauri. Experiments performed on small collections with a domain-specific thesaurus show that the automatically constructed rules are more effective than hand-made rules in terms of precision.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {153},
keywords = {intelligent information retrieval, production rules, conceptual information retrieval, terminology, precision, rule-based tree construction, RUBRIC, AND/OR tree, rule definition, domain-specific thesaurus, information retrieval, query expression, user query concepts, topic capture, computerized rule-based information retrieval, logical framework, automatically constructed rules, information needs specification},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830846,
author = {Kolpakov, R. and Kucherov, G.},
title = {Finding Repeats with Fixed Gap},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We propose an algorithm for finding, within a word, all pairs of occurrences of the same subword within a given distance r. The obtained complexity is O(n log r + S), where S is the size of the output. We also show how the algorithm can be modified in order to find all such pairs of occurrences separated by a given word. The solution uses an algorithm for finding all quasi-squares in two strings, a problem that generalizes the well-known problem of searching for squares.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {162},
keywords = {output size, strings, complexity, subword occurrence repeats, occurrence pairs, fixed gap, string matching, searching, quasi-squares},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830832,
author = {Kuri, J. and Navarro, G.},
title = {Fast Multipattern Search Algorithms for Intrusion Detection},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Presents new search algorithms to detect the occurrences of any pattern from a given pattern set in a text, allowing in the occurrences a limited number of spurious text characters among those of the pattern. This is a common requirement in intrusion detection applications. Our algorithms exploit the ability to represent the search state of one or more patterns in the bits of a single machine word and to update all the search states in a single operation. We show analytically and experimentally that the algorithms are able of rapidly searching large sets of patterns, allowing a large number of spurious characters, yielding about a 75-fold improvement over the classical algorithm.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {169},
keywords = {fast multi-pattern search algorithms, spurious characters, pattern occurrence detection, spurious text characters, search state updating, search problems, machine word, search state representation, intrusion detection},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830818,
author = {Laurikari, V.},
title = {NFAs with Tagged Transitions, Their Conversion to Deterministic Automata and Application to Regular Expressions},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A conservative extension to traditional nondeterministic finite automata (NFAs) is proposed to keep track of the positions in the input string for the last uses of selected transitions, by adding "tags" to transitions. The resulting automata are reminiscent of nondeterministic Mealy machines. A formal semantics of automata with tagged transitions is given. An algorithm is given to convert these augmented automata to the corresponding deterministic automata, which can be used to process strings efficiently. The application to regular expressions is discussed, explaining how the algorithms can be used to implement, for example, substring addressing and a lookahead operator, and an informal comparison to other widely-used algorithms is made.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {181},
keywords = {automata conversion, tagged transitions, input string position tracking, substring addressing, finite automata, deterministic automata, nondeterministic Mealy machines, regular expressions, lookahead operator, formal semantics, string processing, nondeterministic finite automata, last-use tracking},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830826,
author = {Losada, D. E. and Barreiro, A.},
title = {Implementing Document Ranking within a Logical Framework},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Deals with the implementation of a logical model of information retrieval. Specifically, we present algorithms for document ranking within the belief revision framework. Therefore, the logical model that stands on the basis of our proposal can be efficiently implemented within realistic systems. Besides the inherent advantages introduced by logic, the expressiveness is extended with respect to classical systems because documents are represented as unrestricted propositional formulas. As well as representing classical vectors, the model can deal with partial descriptions of documents. Scenarios that can benefit from these more expressive representations are discussed.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {188},
keywords = {vector representation, belief revision, document ranking, expressive representations, information retrieval, belief maintenance, partial document descriptions, logical framework, unrestricted propositional formulas, logical model},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830850,
author = {Walter, M. E. M. T. and Dias, Z. and Meidanis, J.},
title = {A New Approach for Approximating the Transposition Distance},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One of the proposed ways to compare genomes or other large DNA molecules is by computing a rearrangement distance, defined as the minimum number of rearrangement events necessary to transform one molecule into another taking into account only the relative order of similar genes. In this work, we study the problem of computing the transposition distance between two linear gene orders, represented by permutations. To help solve it, we present a very simple structure, the breakpoint diagram, and a 2.25-approximation algorithm for the problem based on this structure. While there are better approximation algorithms, they are based on more complex data structures. Our algorithm was implemented in the C programming language and we show experimental results obtained with it on all permutations of up to 11 genes, plus selected permutations of higher size.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {199},
keywords = {relative gene order, data structures, similar genes, genome comparison, transposition distance approximation, C programming language, linear gene orders, DNA, DNA molecules, approximation algorithm, permutations, rearrangement distance, rearrangement events, breakpoint diagram},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830829,
author = {MacFarlane, A. and McCann, J. A. and Robertson, S. E.},
title = {Parallel Search Using Partitioned Inverted Files},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Examines the searching of partitioned inverted files with particular emphasis on issues that arise from different types of partitioning methods. Two types of index partitions are investigated, namely term identifier (TermId) partitioning and document identifier (DocId) partitioning. We describe the search operations implemented in order to support parallelism in probabilistic searching. We also describe higher-level features, such as search topologies, in parallel search methods. The results from runs on the two types of partitioning are compared and contrasted. We conclude that, within our framework, the DocId method is the best.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {209},
keywords = {partitioned inverted files, high-level features, search topologies, document identifier partitioning, parallel algorithms, index partitions, parallel search methods, search operations, probabilistic searching, term identifier partitioning},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830827,
author = {Matsumoto, T. and Kida, T. and Takeda, M. and Shinohara, A. and Arikawa, S.},
title = {Bit-Parallel Approach to Approximate String Matching in Compressed Texts},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Addresses the problem of approximate string matching on compressed text. We consider this problem for a text string described in terms of a collage system, which is a formal system proposed by T. Kida et al. (1999) that captures various dictionary-based compression methods. We present an algorithm that exploits bit-parallelism, assuming that our problem fits in a single machine word, e.g. (m-k)(k+1)/spl les/L, where m is the pattern length, k is the number of allowed errors and L is the length, in bits, of the machine word. For a class of simple collage systems, the algorithm runs in O(k/sup 2/(/spl par//spl Dscr//spl par/+|/spl Sscr/|)+km) time using O(k/sup 2//spl par//spl Dscr//spl par/) space, where /spl par//spl Dscr//spl par/ is the size of dictionary /spl Dscr/ and |/spl Sscr/| is the number of tokens in the sequence /spl Sscr/. The LZ78 (Lempel-Ziv, 1978) and the LZW (Lempel-Ziv-Welch, 1984) compression methods are covered by this class. Since we can regard n=/spl par//spl Dscr//spl par/+|/spl Sscr/| as the compressed length, the time and space complexities are O(k/sup 2/n+km) and O(k/sup 2/n), respectively. For general k and m, they become O(k/sup 3/mn/L+km) and O(k/sup 3/mn/L). Thus, our algorithm is competitive to the algorithm proposed by J. Ka/spl uml/rkka/spl uml/inen, et al. (2000), which runs in O(km) time using O(kmn) space, when k=O(/spl radic/L).},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {221},
keywords = {approximate string matching, compressed texts, allowed errors, compressed length, dictionary size, space complexity, pattern length, data compression, machine word, Ziv-Lempel compression methods, token sequence, dictionary-based compression methods, collage system, time complexity, bit-parallelism},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830836,
author = {Milidiu, R. L. and Pessoa, A. A. and Laber, E. S. and Renteria, R. P.},
title = {Fast Calculation of Optimal Strategies for Searching with Non-Uniform Costs},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Proposes an algorithm for finding a binary search tree that minimizes the worst-case cost when the access costs are non-uniform and depend on the last accessed key. For this kind of problem, which is commonly found when accessing data stored on magnetic or optical disks, we present an algorithm that finds an optimal search strategy with an expected running time of O(n/sup 2/log n), under some reasonable assumptions on the cost matrix. It is worth mentioning that the best previous algorithm for this problem runs in /spl Theta/(n/sup 3/) time.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {229},
keywords = {optical disks, optimal search strategies, tree searching, magnetic disks, cost matrix, nonuniform access costs, last accessed key, expected running time, NUMA, worst-case cost minimization, binary search tree},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830831,
author = {Proux, D. and Rechenmann, F. and Julliard, L.},
title = {Muninn: A Pragmatic Information Extraction System},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Gathering data from scientific corpora to feed specialized databases has motivated the development of a computer system to help with extracting pertinent information from texts, relying on advanced linguistic tools, complete with object-oriented knowledge modeling capabilities. The methodology and algorithms proposed are under evaluation in genomics, where the goal is to automatically detect and extract information on gene interactions.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {236},
keywords = {Muninn, data mining, object-oriented knowledge modeling capabilities, specialized databases, information extraction system, data gathering, pertinent information, gene interactions, linguistic tools, genomics, scientific text corpora},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830824,
author = {Rozenberg, G.},
title = {DNA Processing in Ciliates - A Computational Point of View (Invited Abstract)},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {242},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830834,
author = {Silipo, R. and Crestani, F.},
title = {Prosodic Stress and Topic Detection in Spoken Sentences},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The relationship between acoustic stress and the information content of words is investigated. On the one hand, the average acoustic stress is measured for each word throughout each utterance. On the other hand, an information retrieval (IR) index is calculated, based on the word frequency throughout the particular spoken sentence and throughout the collection of analysed spoken sentences. The scatter plots of the two measures (average acoustic stress on the y-axis and IR index on the x-axis) show higher values of average acoustic stress with increasing IR index of the word in the majority of the analysed utterances. A statistically more valid proof of such a relationship is derived from a histogram of the words with high average acoustic stress vs. the IR index. This confirms that a word with high average acoustic stress also has a high value of the IR index and, if we trust IR indexes, also a high information content.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {243},
keywords = {word information content, utterances, scatter plots, word frequency, topic detection, statistically valid proof, information retrieval index, prosodic stress, acoustic stress, spoken sentences, histogram, speech processing},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830845,
author = {Tmar, M. and Boughanem, M.},
title = {Learning Profile in Routing: Comparison between Relevance and Gradient Back-Propagation},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Compares two learning profile strategies in an information routing task: relevance backpropagation and gradient backpropagation. Gradient backpropagation is a learning approach used in multilayered neural networks in general. The convergence of the gradient backpropagation algorithm is still to be discussed, but we show its convergence in the majority of cases. Relevance backpropagation is a relevance feedback method used in our connectionist model called Mercure. Experiments carried out on Amaryllis documents showed the effectiveness of both methods, with a slight benefit for the relevance backpropagation strategy.},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {253},
keywords = {information routing task, Mercure connectionist model, relevance feedback, learning profile strategies, convergence, multilayered neural networks, Amaryllis documents, relevance feedback method, gradient backpropagation, relevance backpropagation},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830820,
author = {Vishkin, U.},
title = {A PRAM-on-Chip Vision (Invited Abstract)},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {260},
series = {SPIRE '00}
}

@inproceedings{10.5555/829519.830822,
title = {Author Index},
year = {2000},
isbn = {0769507468},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Symposium on String Processing Information Retrieval (SPIRE'00)},
pages = {261},
series = {SPIRE '00}
}

