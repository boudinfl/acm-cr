@inproceedings{10.1007/11880561_1,
author = {Esuli, Andrea and Fagni, Tiziano and Sebastiani, Fabrizio},
title = {MP-Boost: A Multiple-Pivot Boosting Algorithm and Its Application to Text Categorization},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_1},
doi = {10.1007/11880561_1},
abstract = {AdaBoost.MH is a popular supervised learning algorithm for building multi-label (aka n-of-m) text classifiers. AdaBoost.MH belongs to the family of “boosting” algorithms, and works by iteratively building a committee of “decision stump” classifiers, where each such classifier is trained to especially concentrate on the document-class pairs that previously generated classifiers have found harder to correctly classify. Each decision stump hinges on a specific “pivot term”, checking its presence or absence in the test document in order to take its classification decision. In this paper we propose an improved version of AdaBoost.MH, called MP-Boost, obtained by selecting, at each iteration of the boosting process, not one but several pivot terms, one for each category. The rationale behind this choice is that this provides highly individualized treatment for each category, since each iteration thus generates, for each category, the best possible decision stump. We present the results of experiments showing that MP-Boost is much more effective than AdaBoost.MH. In particular, the improvement in effectiveness is spectacular when few boosting iterations are performed, and (only) high for many such iterations. The improvement is especially significant in the case of macroaveraged effectiveness, which shows that MP-Boost is especially good at working with hard, infrequent categories.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {1–12},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_2,
author = {Esuli, Andrea and Fagni, Tiziano and Sebastiani, Fabrizio},
title = {TreeBoost.MH: A Boosting Algorithm for Multi-Label Hierarchical Text Categorization},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_2},
doi = {10.1007/11880561_2},
abstract = {In this paper we propose TreeBoost.MH, an algorithm for multi-label Hierarchical Text Categorization (HTC) consisting of a hierarchical variant of AdaBoost.MH. TreeBoost.MH embodies several intuitions that had arisen before within HTC: e.g. the intuitions that both feature selection and the selection of negative training examples should be performed “locally”, i.e. by paying attention to the topology of the classification scheme. It also embodies the novel intuition that the weight distribution that boosting algorithms update at every boosting round should likewise be updated “locally”. We present the results of experimenting TreeBoost.MH on two HTC benchmarks, and discuss analytically its computational cost.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {13–24},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_3,
author = {Geraci, Filippo and Pellegrini, Marco and Maggini, Marco and Sebastiani, Fabrizio},
title = {Cluster Generation and Cluster Labelling for Web Snippets: A Fast and Accurate Hierarchical Solution},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_3},
doi = {10.1007/11880561_3},
abstract = {This paper describes Armil, a meta-search engine that groups into disjoint labelled clusters the Web snippets returned by auxiliary search engines. The cluster labels generated by Armil provide the user with a compact guide to assessing the relevance of each cluster to her information need. Striking the right balance between running time and cluster well-formedness was a key point in the design of our system. Both the clustering and the labelling tasks are performed on the fly by processing only the snippets provided by the auxiliary search engines, and use no external sources of knowledge. Clustering is performed by means of a fast version of the furthest-point-first algorithm for metric k-center clustering. Cluster labelling is achieved by combining intra-cluster and inter-cluster term extraction based on a variant of the information gain measure. We have tested the clustering effectiveness of Armil against Vivisimo, the de facto industrial standard in Web snippet clustering, using as benchmark a comprehensive set of snippets obtained from the Open Directory Project hierarchy. According to two widely accepted “external” metrics of clustering quality, Armil achieves better performance levels by 10%. We also report the results of a thorough user evaluation of both the clustering and the cluster labelling algorithms.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {25–36},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_4,
author = {Dupret, Georges and Piwowarski, Benjamin},
title = {Principal Components for Automatic Term Hierarchy Building},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_4},
doi = {10.1007/11880561_4},
abstract = {We show that the singular value decomposition of a term similarity matrix induces a term hierarchy. This decomposition, used in Latent Semantic Analysis and Principal Component Analysis for text, aims at identifying “concepts” that can be used in place of the terms appearing in the documents. Unlike terms, concepts are by construction uncorrelated and hence are less sensitive to the particular vocabulary used in documents. In this work, we explore the relation between terms and concepts and show that for each term there exists a latent subspace dimension for which the term coincides with a concept. By varying the number of dimensions, terms similar but more specific than the concept can be identified, leading to a term hierarchy.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {37–48},
numpages = {12},
keywords = {information retrieval, principal component analysis, term hierarchy, latent semantic analysis},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_5,
author = {Guo, Qing and Zhang, Hui and Iliopoulos, Costas S.},
title = {Computing the Minimum Approximate λ-Cover of a String},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_5},
doi = {10.1007/11880561_5},
abstract = {This paper studies the minimum approximate λ-cover problem of a string. Given a string x of length n and an integer λ, the minimum approximate λ-cover problem is to find a set of λ substrings of equal length that covers x with the minimum error, under a variety of distance models including the Hamming distance, the edit distance and the weighted edit distance. We present an algorithm that can solve this problem in polynomial time.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {49–60},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_6,
author = {Inenaga, Shunsuke and Takeda, Masayuki},
title = {Sparse Directed Acyclic Word Graphs},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_6},
doi = {10.1007/11880561_6},
abstract = {The suffix tree of string w is a text indexing structure that represents all suffixes of w. A sparse suffix tree of w represents only a subset of suffixes of w. An application to sparse suffix trees is composite pattern discovery from biological sequences. In this paper, we introduce a new data structure named sparse directed acyclic word graphs (SDAWGs), which are a sparse text indexing version of directed acyclic word graphs (DAWGs) of Blumer et al. We show that the size of SDAWGs is linear in the length of w, and present an on-line linear-time construction algorithm for SDAWGs.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {61–73},
numpages = {13},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_7,
author = {Hong, Jin-Ju and Chen, Gen-Huey},
title = {On-Line Repetition Detection},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_7},
doi = {10.1007/11880561_7},
abstract = {A q-repetition is the concatenation of q copies of a primitive string, where q≥2. Given a string S character by character, the on-line repetition detection problem is to determine whether S contains a q-repetition in an on-line manner. For q=2, the problem can be solved in O(mlogβ) time, where m is the ending position of the first 2-repetition and β is the number of distinct characters in the m-th prefix of S. In this paper, we present an on-line algorithm that can detect a q-repetition for q≥3 with the same time complexity.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {74–85},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_8,
author = {Chirita, Paul-Alexandru and Nejdl, Wolfgang},
title = {Analyzing User Behavior to Rank Desktop Items},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_8},
doi = {10.1007/11880561_8},
abstract = {Existing desktop search applications, trying to keep up with the rapidly increasing storage capacities of our hard disks, are an important step towards more efficient personal information management, yet they offer an incomplete solution. While their indexing functionalities in terms of different file types they are able to cope with are impressive, their ranking capabilities are basic, and rely only on textual retrieval measures, comparable to the first generation of web search engines. In this paper we propose to connect semantically related desktop items by exploiting usage analysis information about sequences of accesses to local resources, as well as about each user’s local resource organization structures. We investigate and evaluate in detail the possibilities to translate this information into a desktop linkage structure, and we propose several algorithms that exploit these newly created links in order to efficiently rank desktop items. Finally, we empirically show that the access based links lead to ranking results comparable with TFxIDF ranking, and significantly surpass TFxIDF when used in combination with it, making them a very valuable source of input to desktop search ranking algorithms.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {86–97},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_9,
author = {Baeza-Yates, Ricardo and Calder\'{o}n-Benavides, Liliana and Gonz\'{a}lez-Caro, Cristina},
title = {The Intention behind Web Queries},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_9},
doi = {10.1007/11880561_9},
abstract = {The identification of the user’s intention or interest through queries that they submit to a search engine can be very useful to offer them more adequate results. In this work we present a framework for the identification of user’s interest in an automatic way, based on the analysis of query logs. This identification is made from two perspectives, the objectives or goals of a user and the categories in which these aims are situated. A manual classification of the queries was made in order to have a reference point and then we applied supervised and unsupervised learning techniques. The results obtained show that for a considerable amount of cases supervised learning is a good option, however through unsupervised learning we found relationships between users and behaviors that are not easy to detect just taking the query words. Also, through unsupervised learning we established that there are categories that we are not able to determine in contrast with other classes that were not considered but naturally appear after the clustering process. This allowed us to establish that the combination of supervised and unsupervised learning is a good alternative to find user’s goals. From supervised learning we can identify the user interest given certain established goals and categories; on the other hand, with unsupervised learning we can validate the goals and categories used, refine them and select the most appropriate to the user’s needs.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {98–109},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_10,
author = {Bernstein, Yaniv and Shokouhi, Milad and Zobel, Justin},
title = {Compact Features for Detection of Near-Duplicates in Distributed Retrieval},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_10},
doi = {10.1007/11880561_10},
abstract = {In distributed information retrieval, answers from separate collections are combined into a single result set. However, the collections may overlap. The fact that the collections are distributed means that it is not in general feasible to prune duplicate and near-duplicate documents at index time. In this paper we introduce and analyze the grainy hash vector, a compact document representation that can be used to efficiently prune duplicate and near-duplicate documents from result lists. We demonstrate that, for a modest bandwidth and computational cost, many near-duplicates can be accurately removed from result lists produced by a cooperative distributed information retrieval system.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {110–121},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_11,
author = {Puglisi, Simon J. and Smyth, W. F. and Turpin, Andrew},
title = {Inverted Files versus Suffix Arrays for Locating Patterns in Primary Memory},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_11},
doi = {10.1007/11880561_11},
abstract = {Recent advances in the asymptotic resource costs of pattern matching with compressed suffix arrays are attractive, but a key rival structure, the compressed inverted file, has been dismissed or ignored in papers presenting the new structures. In this paper we examine the resource requirements of compressed suffix array algorithms against compressed inverted file data structures for general pattern matching in genomic and English texts. In both cases, the inverted file indexes q-grams, thus allowing full pattern matching capabilities, rather than simple word based search, making their functionality equivalent to the compressed suffix array structures. When using equivalent memory for the two structures, inverted files are faster at reporting the location of patterns when the number of occurrences of the patterns is high.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {122–133},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_12,
author = {Boldi, Paolo and Vigna, Sebastiano},
title = {Efficient Lazy Algorithms for Minimal-Interval Semantics},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_12},
doi = {10.1007/11880561_12},
abstract = {Minimal-interval semantics [3] associates with each query over a document a set of intervals, called witnesses, that are incomparable with respect to inclusion (i.e., they form an antichain): witnesses define the minimal regions of the document satisfying the query. Minimal-interval semantics makes it easy to define and compute several sophisticated proximity operators, provides snippets for user presentation, and can be used to rank documents: thus, computing efficiently the antichains obtained by operations such as logic conjunction and disjunction is a basic issue. In this paper we provide the first algorithms for computing such operators that are linear in the number of intervals and logarithmic in the number of input antichains. The space used is linear in the number of antichains. Moreover, the algorithms are lazy — they do not assume random access to the input antichains. These properties make the usage of our algorithms feasible in large-scale web search engines.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {134–149},
numpages = {16},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_13,
author = {Bast, Holger and Mortensen, Christian W. and Weber, Ingmar},
title = {Output-Sensitive Autocompletion Search},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_13},
doi = {10.1007/11880561_13},
abstract = {We consider the following autocompletion search scenario: imagine a user of a search engine typing a query; then with every keystroke display those completions of the last query word that would lead to the best hits, and also display the best such hits. The following problem is at the core of this feature: for a fixed document collection, given a set D of documents, and an alphabetical range W of words, compute the set of all word-in-document pairs (w,d) from the collection such that w ∈W and d∈D. We present a new data structure with the help of which such autocompletion queries can be processed, on the average, in time linear in the input plus output size, independent of the size of the underlying document collection. At the same time, our data structure uses no more space than an inverted index. Actual query processing times on a large test collection correlate almost perfectly with our theoretical bound.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {150–162},
numpages = {13},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_14,
author = {Russo, Lu\'{\i}s M. S. and Oliveira, Arlindo L.},
title = {A Compressed Self-Index Using a Ziv-Lempel Dictionary},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_14},
doi = {10.1007/11880561_14},
abstract = {A compressed full-text self-index for a text T, of size u, is a data structure used to search patterns P, of size m, in T that requires reduced space, i.e. that depends on the empirical entropy (Hk, H0) of T, and is, furthermore, able to reproduce any substring of T. In this paper we present a new compressed self-index able to locate the occurrences of P in O((m+occ)logn) time, where occ is the number of occurrences and σ the size of the alphabet of T. The fundamental improvement over previous LZ78 based indexes is the reduction of the search time dependency on m from O(m2) to O(m). To achieve this result we point out the main obstacle to linear time algorithms based on LZ78 data compression and expose and explore the nature of a recurrent structure in LZ-indexes, the $mathcal{T}_{78}$ suffix tree. We show that our method is very competitive in practice by comparing it against the LZ-Index, the FM-index and a compressed suffix array.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {163–180},
numpages = {18},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_15,
author = {Adiego, Joaqu\'{\i}n and de la Fuente, Pablo},
title = {Mapping Words into Codewords on PPM},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_15},
doi = {10.1007/11880561_15},
abstract = {We describe a simple and efficient scheme which allows words to be managed in PPM modelling when a natural language text file is being compressed. The main idea for managing words is to assign them codes to make them easier to manipulate. A general technique is used to obtain this objective: a dictionary mapping on PPM modelling. In order to test our idea, we are implementing three prototypes: one implements the basic dictionary mapping on PPM, another implements the dictionary mapping with the separate alphabets model and the last one implements the dictionary with the spaceless words model. This technique can be applied directly or it can be combined with some word compression model. The results for files of 1 Mb. and over are better than those achieved by the character PPM which was taken as a base. The comparison between different prototypes shows that the best option is to use a word based PPM in conjunction with the spaceless word concept.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {181–192},
numpages = {12},
keywords = {natural language processing, PPM, dictionary algorithms, text compression},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_16,
author = {Mehler, Andrew and Skiena, Steven},
title = {Improving Usability through Password-Corrective Hashing},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_16},
doi = {10.1007/11880561_16},
abstract = {We propose a way to increase the usability of password authentication systems by compensating for transposition and substitution errors. We show how to correct for these errors with low false positive rates (i.e., low probability that an arbitrary string will be accepted as the password for authentication). Thus our techniques increase usability with provably little loss of security.In particular, we propose applying a single password-corrective hash function to each entered password attempt. The key property of the hash function is that two strings differing by a single data entry error be likely to be hashed to the same key, while more substantially differing strings are hashed to different keys.We develop precise analytical formulae for the precision/recall tradeoffs for a variety of corrective hash functions. We evaluate these methods at parameter values reflecting common classes of keys/passwords. Finally, we evaluate these schemes using a popular crack-list (dictionary) of 680,000 common words. We show that we can correct for all user transposition errors while reducing the computational cost of a crack attack by only 13%.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {193–204},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_17,
author = {Magdy, Walid and Darwish, Kareem},
title = {Word-Based Correction for Retrieval of Arabic OCR Degraded Documents},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_17},
doi = {10.1007/11880561_17},
abstract = {Arabic documents that are available only in print continue to be ubiquitous and they can be scanned and subsequently OCR’ed to ease their retrieval. This paper explores the effect of word-based OCR correction on the effectiveness of retrieving Arabic OCR documents using different index terms. The OCR correction uses an improved character segment based noisy channel model and is tested on real and synthetic OCR degradation. Results show that the effect of OCR correction depends on the length of the index term used and that indexing using short n-grams is perhaps superior to word-based error correction. The results are potentially applicable to other languages.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {205–216},
numpages = {12},
keywords = {OCR, error correction, retrieval},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_18,
author = {Dupret, Georges and Piwowarski, Benjamin and Hurtado, Carlos and Mendoza, Marcelo},
title = {A Statistical Model of Query Log Generation},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_18},
doi = {10.1007/11880561_18},
abstract = {Query logs record past query sessions across a time span. A statistical model is proposed to explain the log generation process. Within a search engine list of results, the model explains the document selection – a user’s click – by taking into account both a document position and its popularity. We show that it is possible to quantify this influence and consequently estimate document “un-biased” popularities. Among other applications, this allows to re-order the result list to match more closely user preferences and to use the logs as a feedback to improve search engines.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {217–228},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_19,
author = {Lam-Adesina, Adenike M. and Jones, Gareth J. F.},
title = {Using String Comparison in Context for Improved Relevance Feedback in Different Text Media},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_19},
doi = {10.1007/11880561_19},
abstract = {Query expansion is a long standing relevance feedback technique for improving the effectiveness of information retrieval systems. Previous investigations have shown it to be generally effective for electronic text, to give proportionally better improvement for automatic transcriptions of spoken documents, and to be at best of questionable utility for optical character recognized scanned text documents. We introduce two corpus-based methods based on using a string-edit distance measure in context to automatically detect and correct transcription errors. One method operates at query-time and requires no modification of the document index file, and the other at index-time and operates using the standard query-time expansion process. Experimental investigations show these methods to produce improvements in relevance feedback for all three media types, but most significantly mean that relevance feedback can now successfully be applied to scanned text documents.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {229–241},
numpages = {13},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_20,
author = {Farah, Mohamed and Vanderpooten, Daniel},
title = {A Multiple Criteria Approach for Information Retrieval},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_20},
doi = {10.1007/11880561_20},
abstract = {Research in Information Retrieval shows performance improvement when many sources of evidence are combined to produce a ranking of documents. Most current approaches assess document relevance by computing a single score which aggregates values of some attributes or criteria. We propose a multiple criteria framework using an aggregation mechanism based on decision rules identifying positive and negative reasons for judging whether a document should get a better ranking than another. The resulting procedure also handles imprecision in criteria design. Experimental results are reported.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {242–254},
numpages = {13},
keywords = {information retrieval, multiple criteria, relevance},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_21,
author = {Karimi, Sarvnaz and Turpin, Andrew and Scholer, Falk},
title = {English to Persian Transliteration},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_21},
doi = {10.1007/11880561_21},
abstract = {Persian is an Indo-European language written using Arabic script, and is an official language of Iran, Afghanistan, and Tajikistan. Transliteration of Persian to English—that is, the character-by-character mapping of a Persian word that is not readily available in a bilingual dictionary—is an unstudied problem. In this paper we make three novel contributions. First, we present performance comparisons of existing grapheme-based transliteration methods on English to Persian. Second, we discuss the difficulties in establishing a corpus for studying transliteration. Finally, we introduce a new model of Persian that takes into account the habit of shortening, or even omitting, runs of English vowels. This trait makes transliteration of Persian particularly difficult for phonetic based methods. This new model outperforms the existing grapheme based methods on Persian, exhibiting a 24% relative increase in transliteration accuracy measured using the top-5 criteria.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {255–266},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_22,
author = {Fredriksson, Kimmo and Grabowski, Szymon},
title = {Efficient Algorithms for Pattern Matching with General Gaps and Character Classes},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_22},
doi = {10.1007/11880561_22},
abstract = {We develop efficient dynamic programming algorithms for a pattern matching with general gaps and character classes. We consider patterns of the form p0g(a0,b0) p1g(a1,b1) ...pm−−1, where pi ⊂Σ, where Σ is some finite alphabet, and g(ai,bi) denotes a gap of length ai ...bi between symbols pi and pi+1. The text symbol tj matches pi iff tj ∈pi. Moreover, we require that if pi matches tj, then pi+1 should match one of the text symbols $t_{j+a_{i}+1} ldots t_{j+b_i+1}$. Either or both of ai and bi can be negative. We give algorithms that have efficient average and worst case running times. The algorithms have important applications in music information retrieval and computational biology. We give experimental results showing that the algorithms work well in practice.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {267–278},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_23,
author = {Hartman, Tzvika and Verbin, Elad},
title = {Matrix Tightness: A Linear-Algebraic Framework for Sorting by Transpositions},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_23},
doi = {10.1007/11880561_23},
abstract = {We study the problems of sorting signed permutations by reversals (SBR) and sorting unsigned permutations by transpositions (SBT), which are central problems in computational molecular biology. While a polynomial-time solution for SBR is known, the computational complexity of SBT has been open for more than a decade and is considered a major open problem.In the first efficient solution of SBR, Hannenhalli and Pevzner [HP99] used a graph-theoretic model for representing permutations, called the interleaving graph. This model was crucial to their solution. Here, we define a new model for SBT, which is analogous to the interleaving graph. Our model has some desirable properties that were lacking in earlier models for SBT. These properties make it extremely useful for studying SBT.Using this model, we give a linear-algebraic framework in which SBT can be studied. Specifically, for matrices over any algebraic ring, we define a class of matrices called tight matrices. We show that an efficient algorithm which recognizes tight matrices over a certain ring, $mathbb{M}$, implies an efficient algorithm that solves SBT on an important class of permutations, called simple permutations. Such an algorithm is likely to lead to an efficient algorithm for SBT that works on all permutations.The problem of recognizing tight matrices is also a generalization of SBR and of a large class of other “sorting by rearrangements” problems, and seems interesting in its own right as. We give an efficient algorithm for recognizing tight symmetric matrices over any field of characteristic 2. We leave as an open problem to find an efficient algorithm for recognizing tight matrices over the ring $mathbb{M}$.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {279–290},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_24,
author = {Blin, Guillaume and Touzet, H\'{e}l\`{e}ne},
title = {How to Compare Arc-Annotated Sequences: The Alignment Hierarchy},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_24},
doi = {10.1007/11880561_24},
abstract = {We describe a new unifying framework to express comparison of arc-annotated sequences, which we call alignment of arc-annotated sequences. We first prove that this framework encompasses main existing models, which allows us to deduce complexity results for several cases from the literature. We also show that this framework gives rise to new relevant problems that have not been studied yet. We provide a thorough analysis of these novel cases by proposing two polynomial time algorithms and an NP-completeness proof. This leads to an almost exhaustive study of alignment of arc-annotated sequences.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {291–303},
numpages = {13},
keywords = {computational biology, edit distance, arc-annotated sequences, algorithm, NP-hardness, RNA structures},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.5555/2111648.2111682,
author = {Anh, Vo Ngoc and Moffat, Alistair},
title = {Structured Index Organizations for High-Throughput Text Querying},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Inverted indexes are the preferred mechanism for supporting content-based queries in text retrieval systems, with the various data items usually stored compressed in some way. But different query modalities require that different information be held in the index. For example, phrase querying requires that word offsets be held as well as document numbers. In this study we describe an inverted index organization that provides efficient support for all of conjunctive Boolean queries, ranked queries, and phrase queries. Experimental results on a 426 GB document collection show that the methods we describe provide fast evaluation of all three querying modes.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {304–315},
numpages = {12},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_26,
author = {Baillie, Mark and Azzopardi, Leif and Crestani, Fabio},
title = {Adaptive Query-Based Sampling of Distributed Collections},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_26},
doi = {10.1007/11880561_26},
abstract = {As part of a Distributed Information Retrieval system a description of each remote information resource, archive or repository is usually stored centrally in order to facilitate resource selection. The acquisition of precise resource descriptions is therefore an important phase in Distributed Information Retrieval, as the quality of such representations will impact on selection accuracy, and ultimately retrieval performance. While Query-Based Sampling is currently used for content discovery of uncooperative resources, the application of this technique is dependent upon heuristic guidelines to determine when a sufficiently accurate representation of each remote resource has been obtained. In this paper we address this shortcoming by using the Predictive Likelihood to provide both an indication of the quality of an acquired resource description estimate, and when a sufficiently good representation of a resource has been obtained during Query-Based Sampling.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {316–328},
numpages = {13},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_27,
author = {Coelho, Lu\'{\i}s Pedro and Oliveira, Arlindo L.},
title = {Dotted Suffix Trees a Structure for Approximate Text Indexing},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_27},
doi = {10.1007/11880561_27},
abstract = {In this work, the problem we address is text indexing for approximate matching. Given a text $mathcal{T}$ which undergoes some preprocessing to generate an index, we can later query this index to identify the places where a string occurs up to a certain number of errors k (edition distance). The indexing structure occupies space $mathcal{O}(nlog^kn)$ in the average case, independent of alphabet size. This structure can be used to report the existence of a match with k errors in $mathcal{O}(3^k m^{k+1})$ and to report the occurrences in $mathcal{O}(3^k m^{k+1} + mbox{it ed})$ time, where m is the length of the pattern and ed and the number of matching edit scripts. The construction of the structure has time bound by $mathcal{O}(kN|Sigma|)$, where N is the number of nodes in the index and |Σ| the alphabet size.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {329–336},
numpages = {8},
keywords = {approximate text matching, string algorithms, suffix trees, text indexing},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.5555/2111648.2111686,
author = {Culpepper, J. Shane and Moffat, Alistair},
title = {Phrase-Based Pattern Matching in Compressed Text},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Byte codes are a practical alternative to the traditional bit-oriented compression approaches when large alphabets are being used, and trade away a small amount of compression effectiveness for a relatively large gain in decoding efficiency. Byte codes also have the advantage of being searchable using standard string matching techniques. Here we describe methods for searching in byte-coded compressed text and investigate the impact of large alphabets on traditional string matching techniques. We also describe techniques for phrase-based searching in a restricted type of byte code, and present experimental results that compare our adapted methods with previous approaches.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {337–345},
numpages = {9},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_29,
author = {Hurtado, Carlos A. and Levene, Mark},
title = {Discovering Context-Topic Rules in Search Engine Logs},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_29},
doi = {10.1007/11880561_29},
abstract = {In this paper, we present a class of rules, called context-topic rules, for discovering associations between topics and contexts, where a context is defined as a set of features that can be extracted from the log file of a Web search engine. We introduce a notion of rule interestingness that measures the level of the interest of the topic within a context, and provide an algorithm to compute concise representations of interesting context-topic rules. Finally, we present the results of applying the methodology proposed to a large data log of a search engine.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {346–353},
numpages = {8},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_30,
author = {Rachakonda, Aditya Ramana and Srinivasa, Srinath},
title = {Incremental Aggregation of Latent Semantics Using a Graph-Based Energy Model},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_30},
doi = {10.1007/11880561_30},
abstract = {A graph-theoretic model for incrementally detecting latent associations among terms in a document corpus is presented. The algorithm is based on an energy model that quantifies similarity in context between pairs of terms. Latent associations that are established in turn contribute to the energy of their respective contexts. The proposed model avoids the polysemy problem where spurious associations across terms in different contexts are established due to the presence of one or more common polysemic terms. The algorithm works in an incremental fashion where energy values are adjusted after each document is added to the corpus. This has the advantage that computation is localized around the set of terms contained in the new document, thus making the algorithm run much faster than conventional matrix computations used for singular value decompositions.},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {354–359},
numpages = {6},
location = {Glasgow, UK},
series = {SPIRE'06}
}

@inproceedings{10.1007/11880561_31,
author = {Barsky, Marina and Stege, Ulrike and Thomo, Alex and Upton, Chris},
title = {A New Algorithm for Fast All-against-All Substring Matching},
year = {2006},
isbn = {3540457747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11880561_31},
doi = {10.1007/11880561_31},
abstract = {We present a new and efficient algorithm to solve the ’threshold all vs. all’ problem, which involves searching of two strings (with length N and M respectively) for finding all maximal approximate matches of length at least S and with up to K differences. The algorithm is based on a novel graph model, and it solves the problem in time O(NMK2).},
booktitle = {Proceedings of the 13th International Conference on String Processing and Information Retrieval},
pages = {360–366},
numpages = {7},
location = {Glasgow, UK},
series = {SPIRE'06}
}

