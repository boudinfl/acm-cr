@inproceedings{10.1007/978-3-642-34109-0_1,
author = {Amir, Amihood and Levy, Avivit},
title = {Approximate Period Detection and Correction},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_1},
doi = {10.1007/978-3-642-34109-0_1},
abstract = {Periodicity has been historically well studied and has numerous applications. In nature, however, few cyclic phenomena have an exact period.This paper surveys some recent results in approximate periodicity: concept definition, discovery or recovery, techniques and efficient algorithms. We will also show some interesting connections between error correction codes and periodicity.We will try to pinpoint the issues involved, the context in the literature, and possible future research directions.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {1–15},
numpages = {15},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_2,
author = {Baeza-Yates, Ricardo and Maarek, Yoelle},
title = {Usage Data in Web Search: Benefits and Limitations},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_2},
doi = {10.1007/978-3-642-34109-0_2},
abstract = {Web Search, which takes its root in the mature field of information retrieval, evolved tremendously over the last 20 years. The field encountered its first revolution when it started to deal with huge amounts of Web pages. Then, a major step was accomplished when engines started to consider the structure of the Web graph and link analysis became a differentiator in both crawling and ranking. Finally, a more discrete, but not less critical step, was made when search engines started to monitor and mine the numerous (mostly implicit) signals provided by users while interacting with the search engine. We focus here on this third "revolution" of large scale usage data. We detail the different shapes it takes, illustrating its benefits through a review of some winning search features that could not have been possible without it. We also discuss its limitations and how in some cases it even conflicts with some natural users' aspirations such as personalization and privacy. We conclude by discussing how some of these conflicts can be circumvented by using adequate aggregation principles to create "ad hoc" crowds.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {16},
numpages = {1},
keywords = {long tail, large scale data mining, wisdom of crowds, personalization, privacy, web search, big data, usage data},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_3,
author = {Witten, Ian},
title = {Semantic Document Representation: Do It with Wikification},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_3},
doi = {10.1007/978-3-642-34109-0_3},
abstract = {Wikipedia is a goldmine of information. Each article describes a single concept, and together they constitute a vast investment of manual effort and judgment.Wikification is the process of automatically augmenting a plain-text document with hyperlinks to Wikipedia articles. This involves associating phrases in the document with concepts, disambiguating them, and selecting the most pertinent. All three processes can be addressed by exploiting Wikipedia as a source of data. For the first, link anchor text illustrates how concepts are described in running text. For the second and third, Wikipedia provides millions of examples that can be used to prime machine-learned algorithms for disambiguation and selection respectively.Wikification produces a semantic representation of any document in terms of concepts. We apply this to (a) select index terms for scientific documents, and (b) determine the similarity of two documents, in both cases outperforming humans in terms of agreement with human judgment. I will show how it can be applied to document clustering and classification algorithms, and to produce back of the book indexes, improving on the state of the art in each case.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {17},
numpages = {1},
keywords = {large scale data mining, Semantic representation, wikipeda, document representation},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_4,
author = {Abdullin, Artur and Nasraoui, Olfa},
title = {Clustering Heterogeneous Data with Mutual Semi-Supervision},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_4},
doi = {10.1007/978-3-642-34109-0_4},
abstract = {We propose a new methodology for clustering data comprising multiple domains or parts, in such a way that the separate domains mutually supervise each other within a semi-supervised learning framework. Unlike existing uses of semi-supervised learning, our methodology does not assume the presence of labels from part of the data, but rather, each of the different domains of the data separately undergoes an unsupervised learning process, while sending and receiving supervised information in the form of data constraints to/from the other domains. The entire process is an alternation of semi-supervised learning stages on the different data domains, based on Basu et al.'s Hidden Markov Random Fields (HMRF) variation of the K-means algorithm for semi-supervised clustering that combines the constraint-based and distance-based approaches in a unified model. Our experiments demonstrate a successful mutual semi-supervision between the different domains during clustering, that is superior to the traditional heterogeneous domain clustering baselines consisting of converting the domains to a single domain or clustering each of the domains separately.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {18–29},
numpages = {12},
keywords = {mixed data type clustering, heterogeneous data clustering},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_5,
author = {Abeliuk, Andr\'{e}s and Navarro, Gonzalo},
title = {Compressed Suffix Trees for Repetitive Texts},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_5},
doi = {10.1007/978-3-642-34109-0_5},
abstract = {We design a new compressed suffix tree specifically tailored to highly repetitive text collections. This is particularly useful for sequence analysis on large collections of genomes of the close species. We build on an existing compressed suffix tree that applies statistical compression, and modify it so that it works on the grammar-compressed version of the longest common prefix array, whose differential version inherits much of the repetitiveness of the text.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {30–41},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_6,
author = {Amir, Amihood and Paryenty, Haim and Roditty, Liam},
title = {Configurations and Minority in the String Consensus Problem},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_6},
doi = {10.1007/978-3-642-34109-0_6},
abstract = {The Closest String Problem is defined as follows. Let S be a set of k strings {s1,…sk}, each of length ℓ, find a string $hat{s}$, such that the maximum Hamming distance of $hat{s}$ from each of the strings is minimized. We denote this distance with d. The string $hat{s}$ is called a consensus string. In this paper we present two main algorithms, the Configuration algorithm with O(k2 ℓ k) running time for this problem, and the Minority algorithm.The problem was introduced by Lanctot, Li, Ma, Wang and Zhang [13]. They showed that the problem is $cal{NP}$-hard and provided an IP approximation algorithm. Since then the closest string problem has been studied extensively. This research can be roughly divided into three categories: Approximate, exact and practical solutions. This paper falls under the exact solutions category. Despite the great effort to obtain efficient algorithms for this problem an algorithm with the natural running time of O(ℓ k) was not known. In this paper we close this gap.Our result means that algorithms solving the closest string problem in times O(ℓ2), O(ℓ3), O(ℓ4) and O(ℓ5) exist for the cases of k=2,3,4 and 5, respectively. It is known that, in fact, the cases of k=2,3, and 4 can be solved in linear time. No efficient algorithm is currently known for the case of k=5. We prove the minority lemma that exploit surprising properties of the closest string problem and enable constructing the closest string in a sequential fashion. This lemma with some additional ideas give an O(ℓ2) time algorithm for computing a closest string of 5 binary strings.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {42–53},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_7,
author = {An, Xiangdong and Cercone, Nick and Wang, Hai and Ye, Zheng},
title = {A Study on Novelty Evaluation in Biomedical Information Retrieval},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_7},
doi = {10.1007/978-3-642-34109-0_7},
abstract = {In novelty information retrieval, we expect that novel passages are ranked higher than redundant ones and relevant ones higher than irrelevant ones. Accordingly, we desire an evaluation algorithm that would respect such expectations. In TREC 2006 &amp; 2007, a novelty performance measure, called the aspect-based mean average precision (MAP), was introduced to the Genomics Track to rank the novelty of the medical passages. In this paper, we demonstrate that this measure may not necessarily yeild a higher score for the rankings that honor above expectations better. We propose an improved measure to reflect such expectations more precisely, and present some supporting evidences.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {54–60},
numpages = {7},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_8,
author = {Badkobeh, Golnaz and Crochemore, Maxime and Toopsuwan, Chalita},
title = {Computing the Maximal-Exponent Repeats of an Overlap-Free String in Linear Time},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_8},
doi = {10.1007/978-3-642-34109-0_8},
abstract = {The exponent of a string is the quotient of the string's length over the string's smallest period. The exponent and the period of a string can be computed in time proportional to the string's length. We design an algorithm to compute the maximal exponent of factors of an overlap-free string. Our algorithm runs in linear-time on a fixed-size alphabet, while a naive solution of the question would run in cubic time. The solution for non overlap-free strings derives from algorithms to compute all maximal repetitions, also called runs, occurring in the string. We show there is a linear number of maximal-exponent repeats in an overlap-free string. The algorithm can locate all of them in linear time.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {61–72},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_9,
author = {Balog, Krisztian and Neumayer, Robert and N\o{}rv\r{a}g, Kjetil},
title = {Collection Ranking and Selection for Federated Entity Search},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_9},
doi = {10.1007/978-3-642-34109-0_9},
abstract = {Entity search has emerged as an important research topic over the past years, but so far has only been addressed in a centralized setting. In this paper we present an attempt to solve the task of ad-hoc entity retrieval in a cooperative distributed environment. We propose a new collection ranking and selection method for entity search, called AENN. The key underlying idea is that a lean, name-based representation of entities can efficiently be stored at the central broker, which, therefore, does not have to rely on sampling. This representation can then be utilized for collection ranking and selection in a way that the number of collections selected and the number of results requested from each collection is dynamically adjusted on a per-query basis. Using a collection of structured datasets in RDF and a sample of real web search queries targeting entities, we demonstrate that our approach outperforms state-of-the-art distributed document retrieval methods in terms of both effectiveness and efficiency.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {73–85},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_10,
author = {Bannai, Hideo and Inenaga, Shunsuke and Takeda, Masayuki},
title = {Efficient LZ78 Factorization of Grammar Compressed Text},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_10},
doi = {10.1007/978-3-642-34109-0_10},
abstract = {We present an efficient algorithm for computing the LZ78 factorization of a text, where the text is represented as a straight line program (SLP), which is a context free grammar in the Chomsky normal form that generates a single string. Given an SLP of size n representing a text S of length N, our algorithm computes the LZ78 factorization of T in $O(nsqrt{N}+mlog N)$ time and $O(nsqrt{N}+m)$ space, where m is the number of resulting LZ78 factors. We also show how to improve the algorithm so that the $nsqrt{N}$ term in the time and space complexities becomes either nL, where L is the length of the longest LZ78 factor, or (N−α) where α≥0 is a quantity which depends on the amount of redundancy that the SLP captures with respect to substrings of S of a certain length. Since m=O(N/logσN) where σ is the alphabet size, the latter is asymptotically at least as fast as a linear time algorithm which runs on the uncompressed string when σ is constant, and can be more efficient when the text is compressible, i.e. when m and n are small.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {86–98},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_11,
author = {Beller, Timo and Berger, Katharina and Ohlebusch, Enno},
title = {Space-Efficient Computation of Maximal and Supermaximal Repeats in Genome Sequences},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_11},
doi = {10.1007/978-3-642-34109-0_11},
abstract = {The identification of repetitive sequences (repeats) is an essential component of genome sequence analysis, and the notions of maximal and supermaximal repeats capture all exact repeats in a genome in a compact way. Very recently, K\"{u}lekci et al. (Computational Biology and Bioinformatics, 2012) developed an algorithm for finding all maximal repeats that is very space-efficient because it uses the Burrows-Wheeler transform and wavelet trees. In this paper, we present a new space-efficient algorithm for finding maximal repeats in massive data that outperforms their algorithm both in theory and practice. The algorithm is not confined to this task, it can also be used to find all supermaximal repeats or to solve other problems space-efficiently.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {99–110},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_12,
author = {Jabeur, Lamjed Ben and Tamine, Lynda and Boughanem, Mohand},
title = {Active Microbloggers: Identifying Influencers, Leaders and Discussers in Microblogging Networks},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_12},
doi = {10.1007/978-3-642-34109-0_12},
abstract = {This paper presents a social approach for identifying key actors in microblogging social network. In particular, we propose three specific link analysis algorithms called InfRank, LeadRank and DiscussRank that identify influencers, leaders and discussers, respectively. Conducted experiments on TREC 2011 Microblog dataset, show that the proposed algorithms outperform close microblogger ranking approaches.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {111–117},
numpages = {7},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_13,
author = {Birmel\'{e}, Etienne and Crescenzi, Pierluigi and Ferreira, Rui and Grossi, Roberto and Lacroix, Vincent and Marino, Andrea and Pisanti, Nadia and Sacomoto, Gustavo and Sagot, Marie-France},
title = {Efficient Bubble Enumeration in Directed Graphs},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_13},
doi = {10.1007/978-3-642-34109-0_13},
abstract = {Polymorphisms in DNA- or RNA-seq data lead to recognisable patterns in a de Bruijn graph representation of the reads obtained by sequencing. Such patterns have been called mouths, or bubbles in the literature. They correspond to two vertex-disjoint directed paths between a source s and a target t. Due to the high number of such bubbles that may be present in real data, their enumeration is a major issue concerning the efficiency of dedicated algorithms. We propose in this paper the first linear delay algorithm to enumerate all bubbles with a given source.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {118–129},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_14,
author = {Blin, Guillaume and Jiang, Minghui and Vialette, St\'{e}phane},
title = {The Longest Common Subsequence Problem with Crossing-Free Arc-Annotated Sequences},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_14},
doi = {10.1007/978-3-642-34109-0_14},
abstract = {An arc-annotated sequence is a sequence, over a given alphabet, with additional structure described by a set of arcs, each arc joining a pair of positions in the sequence. As a natural extension of the longest common subsequence problem, Evans introduced the Longest Arc-Preserving Common Subsequence (LAPCS) problem as a framework for studying the similarity of arc-annotated sequences. This problem has been studied extensively in the literature due to its potential application for RNA structure comparison, but also because it has a compact definition. In this paper, we focus on the nested case where no two arcs are allowed to cross because it is widely considered the most important variant in practice. Our contributions are three folds: (i) we revisit the nice NP-hardness proof of Lin et al. for LAPCS(Nested, Nested), (ii) we improve the running time of the FPT algorithm of Alber et al. from $O(3.31^{k_1 + k_2} n)$ to $O(3^{k_1 + k_2} n)$, where resp. k1 and k2 deletions from resp. the first and second sequence are needed to obtain an arc-preserving common subsequence, and (iii) we show that LAPCS(Stem, Stem) is NP-complete for constant alphabet size.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {130–142},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_15,
author = {Bravo-Marquez, Felipe and Manriquez, Manuel},
title = {A Zipf-like Distant Supervision Approach for Multi-Document Summarization Using Wikinews Articles},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_15},
doi = {10.1007/978-3-642-34109-0_15},
abstract = {This work presents a sentence ranking strategy based on distant supervision for the multi-document summarization problem. Due to the difficulty of obtaining large training datasets formed by document clusters and their respective human-made summaries, we propose building a training and a testing corpus from Wikinews. Wikinews articles are modeled as "distant" summaries of their cited sources, considering that first sentences of Wikinews articles tend to summarize the event covered in the news story. Sentences from cited sources are represented as tuples of numerical features and labeled according to a relationship with the given distant summary that is based on the Zipf law. Ranking functions are trained using linear regressions and ranking SVMs, which are also combined using Borda count. Top ranked sentences are concatenated and used to build summaries, which are compared with the first sentences of the distant summary using ROUGE evaluation measures. Experimental results obtained show the effectiveness of the proposed method and that the combination of different ranking techniques outperforms the quality of the generated summary.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {143–154},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_16,
author = {Brisaboa, Nieves R. and Cerdeira-Pena, Ana and Navarro, Gonzalo and Pedreira, \'{O}scar},
title = {Ranked Document Retrieval in (Almost) No Space},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_16},
doi = {10.1007/978-3-642-34109-0_16},
abstract = {Ranked document retrieval is a fundamental task in search engines. Such queries are solved with inverted indexes that require additional 45%-80% of the compressed text space, and take tens to hundreds of microseconds per query. In this paper we show how ranked document retrieval queries can be solved within tens of milliseconds using essentially no extra space over an in-memory compressed representation of the document collection. More precisely, we enhance wavelet trees on bytecodes (WTBCs), a data structure that rearranges the bytes of the compressed collection, so that they support ranked conjunctive and disjunctive queries, using just 6%---18% of the compressed text space.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {155–160},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_17,
author = {Cambazoglu, B. Barla and Altingovde, Ismail Sengor},
title = {Impact of Regionalization on Performance of Web Search Engine Result Caches},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_17},
doi = {10.1007/978-3-642-34109-0_17},
abstract = {Large-scale web search engines are known to maintain caches that store the results of previously issued queries. They are also known to customize their search results in different forms to improve the relevance of their results to a particular group of users. In this paper, we show that the regionalization of search results decreases the hit rates attained by a result cache. As a remedy, we investigate result prefetching strategies that aim to recover the hit rate sacrificed to search result regionalization. Our results indicate that prefetching achieves a reasonable increase in the result cache hit rate under regionalization of search results.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {161–166},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_18,
author = {Claude, Francisco and Navarro, Gonzalo},
title = {The Wavelet Matrix},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_18},
doi = {10.1007/978-3-642-34109-0_18},
abstract = {The wavelet tree (Grossi et al., SODA 2003) is nowadays a popular succinct data structure for text indexes, discrete grids, and many other applications. When it has many nodes, a levelwise representation proposed by M\"{a}kinen and Navarro (LATIN 2006) is preferable. We propose a different arrangement of the levelwise data, so that the bitmaps are shuffled in a different way. The result can no more be called a wavelet tree, and we dub it wavelet matrix. We demonstrate that the wavelet matrix is simpler to build, simpler to query, and faster in practice than the levelwise wavelet tree. This has a direct impact on many applications that use the levelwise wavelet tree for different purposes.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {167–179},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_19,
author = {Claude, Francisco and Navarro, Gonzalo},
title = {Improved Grammar-Based Compressed Indexes},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_19},
doi = {10.1007/978-3-642-34109-0_19},
abstract = {We introduce the first grammar-compressed representation of a sequence that supports searches in time that depends only logarithmically on the size of the grammar. Given a text T[1..u] that is represented by a (context-free) grammar of n (terminal and nonterminal) symbols and size N (measured as the sum of the lengths of the right hands of the rules), a basic grammar-based representation of T takes $Nlg n$ bits of space. Our representation requires $2Nlg n + Nlg u + epsilon, nlg n + o(Nlg n)$ bits of space, for any 0&lt;ε≤1. It can find the positions of the occ occurrences of a pattern of length m in T in $Oleft((m^2/epsilon)lg left(frac{lg u}{lg n}right) + (m+occ)lg nright)$ time, and extract any substring of length ℓ of T in time $O(ell+hlg(N/h))$, where h is the height of the grammar tree.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {180–192},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_20,
author = {de Groc, Cl\'{e}ment and Tannier, Xavier},
title = {Experiments on Pseudo Relevance Feedback Using Graph Random Walks},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_20},
doi = {10.1007/978-3-642-34109-0_20},
abstract = {In this article, we apply a graph-based approach for pseudo-relevance feedback. We model term co-occurrences in a fixed window or at the document level as a graph and apply a random walk algorithm to select expansion terms. Evaluation of the proposed approach on several standard TREC and CLEF collections including the recent TREC-Microblog dataset show that this approach is in line with state-of-the-art pseudo-relevance feedback models.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {193–198},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_21,
author = {Dias, Ga\"{e}l and Moreno, Jos\'{e} G. and Jatowt, Adam and Campos, Ricardo},
title = {Temporal Web Image Retrieval},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_21},
doi = {10.1007/978-3-642-34109-0_21},
abstract = {Temporal Web Image Retrieval can be defined as the process that retrieves sets of Web images with their temporal dimension from explicit or implicit temporal text queries. Supposing that (a) the temporal dimension is included in image indexing and (b) the query is explicitly expressed with a time tag (e.g. "Fukushima 2011"), the retrieval task can be straightforward as image retrieval has been studied for several years with success. However, text queries are usually implicit in time (e.g. "Second World War") and automatically capturing the time dimension included in Web images is a challenge that has not been studied so far to the best of our knowledge. In this paper, we will discuss different research issues about Temporal Web Image Retrieval and the current progresses of our research in temporal ephemeral clustering and temporal image filtering.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {199–204},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_22,
author = {Elmasry, Amr and Katajainen, Jyrki and Teuhola, Jukka},
title = {Improved Address-Calculation Coding of Integer Arrays},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_22},
doi = {10.1007/978-3-642-34109-0_22},
abstract = {In this paper we deal with compressed integer arrays that are equipped with fast random access. Our treatment improves over an earlier approach that used address-calculation coding to locate the elements and supported access and search operations in $O(lg (n+s))$ time for a sequence of n non-negative integers summing up to s. The idea is to complement the address-calculation method with index structures that considerably decrease access times and also enable updates. For all our structures the memory usage is $n lg(1 + s/n) + O(n)$ bits. First a read-only version is introduced that supports rank-based accesses to elements and retrievals of prefix sums in $O(lg lg (n+s)$) time, as well as prefix-sum searches in $O(lg n+ lg lg s)$ time, using the word RAM as the model of computation. The second version of the data structure supports accesses in $O(lglg U)$ time and changes of element values in $O(lg^2 U)$ time, where U is the universe size. Both versions performed quite well in practical experiments. A third extension to dynamic arrays is also described, supporting accesses and prefix-sum searches in $O(lg n + lglg U)$ time, and insertions and deletions in $O(lg^2 U)$ time.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {205–216},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_23,
author = {Faro, Simone and K\"{u}lekci, M. O\u{g}uzhan},
title = {Fast Multiple String Matching Using Streaming SIMD Extensions Technology},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_23},
doi = {10.1007/978-3-642-34109-0_23},
abstract = {Searching for all occurrences of a given set of patterns in a text is a fundamental problem in computer science with applications in many fields, like computational biology and intrusion detection systems.In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. This study introduces a filter based exact multiple string matching algorithm, which benefits from Intel's SSE (streaming SIMD extensions) technology for searching long strings. Our experimental results on various conditions show that the proposed algorithm outperforms other solutions, which are known to be among the fastest in practice.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {217–228},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_24,
author = {Gawrychowski, Pawe\l{}},
title = {Faster Algorithm for Computing the Edit Distance between SLP-Compressed Strings},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_24},
doi = {10.1007/978-3-642-34109-0_24},
abstract = {Given two strings described by SLPs of total size n, we show how to compute their edit distance in $mathcal{O}(nNsqrt{logfrac{N}{n}})$ time, where N is the sum of the strings length. The result can be generalized to any rational scoring function, hence we improve the existing $mathcal{O}(nNlog N)$ [10] and $mathcal{O}(nNlogfrac{N}{n})$ [4] time solutions. This gets us even closer to the $mathcal{O}(nN)$ complexity conjectured by Lifshits [7]. The basic tool in our solution is a linear time procedure for computing the max-product of a vector and a unit-Monge matrix, which might be of independent interest.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {229–236},
numpages = {8},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_25,
author = {HaCohen-Kerner, Yaakov and Greenfield, Izek},
title = {Basic Word Completion and Prediction for Hebrew},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_25},
doi = {10.1007/978-3-642-34109-0_25},
abstract = {This research aims to improve keystroke savings for completion and prediction of Hebrew words. This task is very important to augmentative and alternative communication systems as well as to search engines, short messages services, and mobile phones. The proposed model is composed of Hebrew corpora containing 177M words, a morphological analyzer, various n-gram Hebrew language models and other tools. The achieved keystroke savings rate is higher than those reported in a previous Hebrew word prediction system and previous word prediction systems in other languages. Two main findings have been found: the larger the corpus that the language model is trained on, the better predictions that are achieved and a morphological analyzer helps only when the language model is based on only one corpus.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {237–244},
numpages = {8},
keywords = {word completion, language models, corpora, word prediction, augmentative and alternative communication, hebrew, keystroke savings},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_26,
author = {Hagio, Kazuhito and Ohgami, Takashi and Bannai, Hideo and Takeda, Masayuki},
title = {Eager XPath Evaluation over XML Streams},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_26},
doi = {10.1007/978-3-642-34109-0_26},
abstract = {We address the XPath evaluation problem over XML streams. We consider a fragment of XPath called Conjunctive XPath (CXP). We present an algorithm that eagerly evaluates a CXP query Q against a streaming XML data D in O((|Q|+n2)|D|) time and O(|Q|height(D)+n·maxcands(Q,D)) space, where n is the number of location steps in Q and maxcands(Q,D) is the maximum number of nodes of D that can be candidates for answer nodes, at any one instant. The result improves the previous work of Ramanan (2009) which lazily evaluates Q against D in O((|Q|+n·height(D))|D|) time using the same space.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {245–250},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_27,
author = {He, Jing and Nie, Jian-Yun and Lu, Yang and Zhao, Wayne Xin},
title = {Position-Aligned Translation Model for Citation Recommendation},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_27},
doi = {10.1007/978-3-642-34109-0_27},
abstract = {The goal of a citation recommendation system is to suggest some references for a snippet in an article or a book, and this is very useful for both authors and the readers. The citation recommendation problem can be cast as an information retrieval problem, in which the query is the snippet from an article, and the relevant documents are the cited articles. In reality, the citation snippet and the cited articles may be described in different terms, and this makes the citation recommendation task difficult. Translation model is very useful in bridging the vocabulary gap between queries and documents in information retrieval. It can be trained on a collection of query and document pairs, which are assumed to be parallel. However, such training data contains much noise: a relevant document usually contains some relevant parts along with irrelevant ones. In particular, the citation snippet may only mention only some parts of the cited article's content. To cope with this problem, in this paper, we propose a method to train translation models on such noisy data, called position-aligned translation model. This model tries to align the query to the most relevant parts of the document, so that the estimated translation probabilities could rely more on them. We test this model in a citation recommendation task for scientific papers. Our experiments show that the proposed method can significantly improve the previous retrieval methods based on translation models.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {251–263},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_28,
author = {Hern\'{a}ndez, Cecilia and Navarro, Gonzalo},
title = {Compressed Representation of Web and Social Networks via Dense Subgraphs},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_28},
doi = {10.1007/978-3-642-34109-0_28},
abstract = {Mining and analyzing large web and social networks are challenging tasks in terms of storage and information access. In order to address this problem, several works have proposed compressing large graphs allowing neighbor access over their compressed representations. In this paper, we propose a novel compressed structure aiming to reduce storage and support efficient navigation over web and social graph compressed representations. Our approach uses clustering and mining for finding dense subgraphs and represents them using compact data structures. We perform experiments using a wide range of web and social networks and compare our results with the best known techniques. Our results show that we improve the state of the art space/time tradeoffs for supporting neighbor queries. Our compressed structure also enables mining queries based on dense subgraphs, such as cliques and bicliques.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {264–276},
numpages = {13},
keywords = {web graphs, social networks, compressed data structures},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_29,
author = {Kim, Se-Jong and Lee, Jong-Hyeok},
title = {Method of Mining Subtopics Using Dependency Structure and Anchor Texts},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_29},
doi = {10.1007/978-3-642-34109-0_29},
abstract = {This paper proposes a method that mines subtopics using the co-occurrence of words based on the dependency structure, and anchor texts from web documents in Japanese. We extracted subtopics using simple patterns which reflected the dependency structure, and evaluated subtopics by the proposed score equation. Our method achieved good performance than previous methods which used related or suggested queries from major web search engines. The results of our method will be useful in various search scenarios, such as query suggestion and result diversification.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {277–283},
numpages = {7},
keywords = {anchor text, subtopic mining, search intent, dependency structure},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_30,
author = {Kociumaka, Tomasz and Radoszewski, Jakub and Rytter, Wojciech and Wale\'{n}, Tomasz},
title = {Efficient Data Structures for the Factor Periodicity Problem},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_30},
doi = {10.1007/978-3-642-34109-0_30},
abstract = {We present several efficient data structures for answering queries related to periods in words. For a given word w of length n the Period Query given a factor of w (represented by an interval) returns its shortest period and a compact representation of all periods. Several algorithmic solutions are proposed that balance the data structure space (ranging from O(n) to O(nlogn)), and the query time complexity (ranging from O(log1+εn) to O(logn)).},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {284–294},
numpages = {11},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_31,
author = {Konow, Roberto and Navarro, Gonzalo},
title = {Dual-Sorted Inverted Lists in Practice},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_31},
doi = {10.1007/978-3-642-34109-0_31},
abstract = {We implement a recent theoretical proposal to represent inverted lists in memory, in a way that docid-sorted and weight-sorted lists are simultaneously represented in a single wavelet tree data structure. We compare our implementation with classical representations, where the ordering favors either bag-of-word queries or Boolean and weighted conjunctive queries, and demonstrate that the new data structure is faster than the state of the art for conjunctive queries, while it offers an attractive space/time tradeoff when both kinds of queries are of interest.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {295–306},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_32,
author = {Kucherov, Gregory and Nekrich, Yakov and Starikovskaya, Tatiana},
title = {Computing Discriminating and Generic Words},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_32},
doi = {10.1007/978-3-642-34109-0_32},
abstract = {We study the following three problems of computing generic or discriminating words for a given collection of documents. Given a pattern P and a threshold d, we want to report (i) all longest extensions of P which occur in at least d documents, (ii) all shortest extensions of P which occur in less than d documents, and (iii) all shortest extensions of P which occur only in d selected documents. For these problems, we propose efficient algorithms based on suffix trees and using advanced data structure techniques. For problem (i), we propose an optimal solution with constant running time per output word.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {307–317},
numpages = {11},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_33,
author = {Kusano, Kazuhiko and Narisawa, Kazuyuki and Shinohara, Ayumi},
title = {Computing Maximum Number of Runs in Strings},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_33},
doi = {10.1007/978-3-642-34109-0_33},
abstract = {A run (also called maximal repetition) in a word is a non-extendable repetition. Finding the maximum number ρ(n) of runs in a string of length n is a challenging problem. Although it is known that ρ(n)≤1.029n for any n and there exists large n such that ρ(n)≥0.945n, the exact value of ρ(n) is still unknown. Several algorithms have been proposed to count runs in a string efficiently, and ρ(n) can be obtained for small n by these algorithms. In this paper, we focus on computing ρ(n) for given length parameter n, instead of exhaustively counting all runs for every string of length n. We report exact values of ρ(n) for binary strings for n≤66, together with the strings which contain ρ(n) runs.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {318–329},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_34,
author = {K\"{a}rkk\"{a}inen, Juha and Mikkola, Pekka and Kempa, Dominik},
title = {Grammar Precompression Speeds up Burrows---Wheeler Compression},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_34},
doi = {10.1007/978-3-642-34109-0_34},
abstract = {Text compression algorithms based on the Burrows---Wheeler transform (BWT) typically achieve a good compression ratio but are slow compared to Lempel---Ziv type compression algorithms. The main culprit is the time needed to compute the BWT during compression and its inverse during decompression. We propose to speed up BWT-based compression by performing a grammar-based precompression before the transform. The idea is to reduce the amount of data that BWT and its inverse have to process. We have developed a very fast grammar precompressor using pair replacement. Experiments show a substantial speed up in practice without a significant effect on compression ratio.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {330–335},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_35,
author = {Lee, Lap-Kei and Lewenstein, Moshe and Zhang, Qin},
title = {Parikh Matching in the Streaming Model},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_35},
doi = {10.1007/978-3-642-34109-0_35},
abstract = {Let S be a string over an alphabet Σ={σ1, σ2, …}. A Parikh-mapping maps a substring S′ of S to a |Σ|-length vector that contains, in location i of the vector, the count of σi in S′. Parikh matching refers to the problem of finding all substrings of a text T which match to a given input |Σ|-length count vector.In the streaming model one seeks space-efficient algorithms for problems in which there is one pass over the data. We consider Parikh matching in the streaming model. To make this viable we search for substrings whose Parikh-mappings approximately match the input vector. In this paper we present upper and lower bounds on the problem of approximate Parikh matching in the streaming model.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {336–341},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_36,
author = {Mbarek, Rabeb and Tmar, Mohamed},
title = {Relevance Feedback Method Based on Vector Space Basis Change},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_36},
doi = {10.1007/978-3-642-34109-0_36},
abstract = {The idea of relevance feedback (RF) is to involve the user in the retrieval process to improve the final result set by reformulating the query. The most commonly used methods in RF aim to rewrite the user query. In the vector space model, RF is usually undertaken by re-weighting the query terms without any modification in the vector space basis. In this paper we propose a RF method based on vector space basis change without any modification on the query term weights. The aim of our method is to find a basis which gives a better representation of the documents such that the relevant documents are gathered and the irrelevant ones are kept away from the relevant documents.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {342–347},
numpages = {6},
keywords = {basis change, query, relevance feedback},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_37,
author = {Mendivelso, Juan and Lee, Inbok and Pinz\'{o}n, Yoan J.},
title = {Approximate Function Matching under δ- and γ- Distances},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_37},
doi = {10.1007/978-3-642-34109-0_37},
abstract = {This paper defines a new string matching problem by combining two paradigms: function matching and δγ-matching. The result is an approximate variant of function matching where two equal-length strings X and Y match if there exists a function that maps X to a string X′ such that X′ and Y are δγ- similar. We propose an O(nm) algorithm for finding all the matches of a pattern P1 …m in a text T1 …n.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {348–359},
numpages = {12},
keywords = {function matching, δγ-matching, γ-matching, δ-matching, combinatorial algorithms},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_38,
author = {Nakashima, Yuto and I, Tomohiro and Inenaga, Shunsuke and Bannai, Hideo and Takeda, Masayuki},
title = {The Position Heap of a Trie},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_38},
doi = {10.1007/978-3-642-34109-0_38},
abstract = {The position heap is a text indexing structure for a single text string, recently proposed by Ehrenfeucht et al. [Position heaps: A simple and dynamic text indexing data structure, Journal of Discrete Algorithms, 9(1):100-121, 2011]. In this paper we introduce the position heap for a set of strings, and propose an efficient algorithm to construct the position heap for a set of strings which is given as a trie. For a fixed alphabet our algorithm runs in time linear in the size of the trie. We also show that the position heap can be efficiently updated after addition/removal of a leaf of the input trie.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {360–371},
numpages = {12},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_39,
author = {Brisaboa, Nieves R. and Navarro, Gonzalo and Ord\'{o}\~{n}ez, Alberto},
title = {Smaller Self-Indexes for Natural Language},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_39},
doi = {10.1007/978-3-642-34109-0_39},
abstract = {Self-indexes for natural-language texts, where these are regarded as token (word or separator) sequences, achieve very attractive space and search time. However, they suffer from a space penalty due to their large vocabulary. In this paper we show that by replacing the Huffman encoding they implicitly use by the slightly weaker Hu-Tucker encoding, which respects the lexical order of the vocabulary, both their space and time are improved.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {372–378},
numpages = {7},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_40,
author = {Osipov, Vitaly},
title = {Parallel Suffix Array Construction for Shared Memory Architectures},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_40},
doi = {10.1007/978-3-642-34109-0_40},
abstract = {We present the design of the algorithm for constructing the suffix array of a string using manycore GPUs. Despite of the wide usage in text processing and extensive research over two decades there was a lack of efficient algorithms that were able to exploit shared memory parallelism (as multicore CPUs as manycore GPUs) in practice. To the best of our knowledge we developed the first approach exposing shared memory parallelism that significantly outperforms the state-of-the-art existing implementations for sufficiently large inputs. We reduced the suffix array construction problem to a number of parallel primitives such as prefix-sum, radix sorting, random gather and scatter from/to the memory. Thus, the performance of the algorithm merely depends on the performance of these primitives on the particular shared memory architecture. We demonstrate its performance on manycore GPUs, but the method can also be applied for other parallel architectures, such as multicores, CELL or Intel MIC.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {379–384},
numpages = {6},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_41,
author = {Parida, Laxmi and Pizzi, Cinzia and Rombo, Simona E.},
title = {Characterization and Extraction of Irredundant Tandem Motifs},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_41},
doi = {10.1007/978-3-642-34109-0_41},
abstract = {We address the problem of extracting pairs of subwords (m1,m2) from a text string s of length n, such that, given also an integer constant d in input, m1 and m2 occur in tandem within a maximum distance of d symbols in s.The main effort of this work is to eliminate the possible redundancy from the candidate set of the so found tandem motifs. To this aim, we first introduce the concept of maximality, characterized by four specific conditions, that we show to be not deducible by the corresponding notion of maximality already defined for "simple" (i.e., non tandem) motifs. Then, we further eliminate the remaining redundancy by defining the concept of irredundancy for tandem motifs.We prove that the number of non-overlapping irredundant tandems is O(d2n) which, considering d as a constant, leads to a linear number of tandems in the length of the input string. This is an order of magnitude less than previously developed compact indexes for tandem extraction. As a further contribution we show an algorithm to extract this compact irredundant index.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {385–397},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

@inproceedings{10.1007/978-3-642-34109-0_42,
author = {Takabatake, Yoshimasa and Tabei, Yasuo and Sakamoto, Hiroshi},
title = {Variable-Length Codes for Space-Efficient Grammar-Based Compression},
year = {2012},
isbn = {9783642341083},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34109-0_42},
doi = {10.1007/978-3-642-34109-0_42},
abstract = {Dictionary is a crucial data structure to implement grammar-based compression algorithms. Such a dictionary should access any codes in O(1) time for an efficient compression. A standard dictionary consisting of fixed-length codes consumes a large amount of memory of 2n logn bits for n variables. We present novel dictionaries consisting of variable-length codes for offline and online grammar-based compression algorithms. In an offline setting, we present a dictionary of at most min {nlogn+2n+o(n), 3nlogσ(1+o(1))} bits of space where σ &lt; 2 √n. In an online setting, we present a dictionary of at most $frac{7}{4}nlog n + 4n + o(n)$ bits of space for a constant alphabet and unknown n. Experiments revealed that memory usage in our dictionary was much smaller than that of state-of-the-art dictionaries.},
booktitle = {Proceedings of the 19th International Conference on String Processing and Information Retrieval},
pages = {398–410},
numpages = {13},
location = {Cartagena de Indias, Colombia},
series = {SPIRE'12}
}

