@inproceedings{10.1007/978-3-540-89097-3_1,
author = {Hawking, David},
title = {"Search Is a Solved Problem" and Other Annoying Fallacies},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_1},
doi = {10.1007/978-3-540-89097-3_1},
abstract = {Since Google became a celebrity in the early noughties, many people with the power to control and direct research resources have taken the view that there is no more research to be done on the problem of information retrieval. In reality, there are so many variants of "the search problem" that not all have been catalogued, and few have been solved to the point where we can rely absolutely on the quality of results. Apparently no-one told the Web search companies that the problem was solved as, since that time, they have researched and developed a range of new search facilities and invested heavily in improving their basic products. Google, Yahoo! and Microsoft all maintain search research and development teams much larger than the biggest University computer science departments!Through my involvement with the Funnelback internet and enterprise search company I have worked on many twists on the information retrieval problem which are not modelled in well-known test collections, and not encountered in basic Web search. In my talk I will try to outline some of the issues in trying to apply information retrieval and string processing theory into commercial practice.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {1},
numpages = {1},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_2,
author = {Landau, Gad M.},
title = {Approximate Runs - Revisited},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_2},
doi = {10.1007/978-3-540-89097-3_2},
abstract = {The problem of finding repeats within a string is an importantcomputational problem with applications in data compression and inthe field of molecular biology. Both exact and inexact repeatsoccur frequently in the genome, and certain repeats are known to berelated to human diseases.A multiple tandem repeat in a sequence <em>S</em> is a(periodic) substring <em>r</em> of <em>S</em> of the form<em>r</em> = <em>u</em> <em>a</em> <em>u</em> ',where <em>u</em> (the period) is a prefix of <em>r</em> ,<em>u</em> ' is a prefix of <em>u</em> and <em>a</em> ≥ 2. A run is a maximal (non-extendable) multiple tandem repeat.An <em>approximate</em> run is a run with errors (i.e. the repeatedsubsequences are similar but not identical).Many measures have been proposed that capture the similarityamong all periods. We may measure the number of errors betweenconsecutive periods, between all periods, or between each periodand a consensus string. Another possible measure is the number ofpositions in the periods that may differ.In this talk I will survey a range of our results in this area.Various parts of this work are joint work with Maxime Crochemore,Gene Myers, Jeanette Schmidt and Dina Sokol.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {2},
numpages = {1},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_3,
author = {K\"{a}rkk\"{a}inen, Juha and Rantala, Tommi},
title = {Engineering Radix Sort for Strings},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_3},
doi = {10.1007/978-3-540-89097-3_3},
abstract = {We describe new implementations of MSD radix sort for efficiently sorting large collections of strings. Our implementations are significantly faster than previous MSD radix sort implementations, and in fact faster than any other string sorting algorithm on several data sets. We also describe a new variant that achieves high space-efficiency at a small additional cost on runtime.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {3–14},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_4,
author = {Kolpakov, Roman and Raffinot, Mathieu},
title = {Faster Text Fingerprinting},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_4},
doi = {10.1007/978-3-540-89097-3_4},
abstract = {Let <em>s</em> = <em>s</em> <subscript>1</subscript> .. <em>s</em> <subscript> <em>n</em> </subscript> be a text (or sequence) on a finite alphabet <em>Σ</em> . A fingerprint in <em>s</em> is the set of distinct characters contained in one of its substrings. Fingerprinting a text consists in computing the set ${cal F}$ of all fingerprints of all its substrings. A fingerprint, $f in {cal F}$, admits a number of maximal locations ***<em>i</em> ,<em>j</em> *** in <em>S</em> , that is the alphabet of <em>s</em> <subscript> <em>i</em> </subscript> .. <em>s</em> <subscript> <em>j</em> </subscript> is <em>f</em> and <em>s</em> <subscript> <em>i</em> *** 1</subscript> , <em>s</em> <subscript> <em>j</em> + 1</subscript> , if defined, are not in <em>f</em> . The set of maximal locations is ${cal L}, ; |{cal L}| leq n |Sigma|.$ Two maximal locations ***<em>i</em> ,<em>j</em> *** and ***<em>k</em> ,<em>l</em> *** such that <em>s</em> <subscript> <em>i</em> </subscript> ..<em>s</em> <subscript> <em>j</em> </subscript> = <em>s</em> <subscript> <em>k</em> </subscript> ..<em>s</em> <subscript> <em>l</em> </subscript> are named <em>copies</em> and the quotient of ${cal L}$ according to the copy relation is named ${cal L}_C$. The faster algorithm to compute all fingerprints in <em>s</em> runs in $O(n+|{cal L}|log |Sigma|)$ time. We present an $O((n+|{cal L}_C|)log |Sigma|)$ worst case time algorithm.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {15–26},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_5,
author = {Maruyama, Shirou and Tanaka, Yohei and Sakamoto, Hiroshi and Takeda, Masayuki},
title = {Context-Sensitive Grammar Transform: Compression and Pattern Matching},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_5},
doi = {10.1007/978-3-540-89097-3_5},
abstract = {A framework of context-sensitive grammar transform is proposed. A greedy compression algorithm with the transform model is presented as well as a Knuth-Morris-Pratt (KMP)-type compressed pattern matching (CPM) algorithm. The compression performance is a match for gzip and Re-Pair. The search speed of our CPM algorithm is almost twice faster than the KMP type CPM algorithm on Byte-Pair-Encoding by Shibata et al. (2000), and in the case of short patterns, faster than the Boyer-Moore-Horspool algorithm with the stopper encoding by Rautio et al. (2002), which is regarded as one of the best combinations that allows a practically fast search.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {27–38},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_6,
author = {Klein, Shmuel T. and Shapira, Dana},
title = {Improved Variable-to-Fixed Length Codes},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_6},
doi = {10.1007/978-3-540-89097-3_6},
abstract = {Though many compression methods are based on the use of variable length codes, there has recently been a trend to search for alternatives in which the lengths of the codewords are more restricted, which can be useful for fast decoding and compressed searches. This paper explores the construction of variable-to-fixed length codes, which have been suggested long ago by Tunstall. Using a new heuristic based on suffix trees, the performance of Tunstall codes could be improved by more than 30%.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {39–50},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_7,
author = {Anh, Vo Ngoc and Wan, Raymond and Moffat, Alistair},
title = {Term Impacts as Normalized Term Frequencies for BM25 Similarity Scoring},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_7},
doi = {10.1007/978-3-540-89097-3_7},
abstract = {The <literal>BM25</literal> similarity computation has been shown to provide effective document retrieval. In operational terms, the formulae which form the basis for <literal>BM25</literal> employ both term frequency and document length normalization. This paper considers an alternative form of normalization using document-centric impacts, and shows that the new normalization simplifies <literal>BM25</literal> and reduces the number of tuning parameters. Motivation is provided by a preliminary analysis of a document collection that shows that impacts are more likely to identify documents whose lengths resemble those of the relevant judgments.Experiments on TREC data demonstrate that impact-based <literal>BM25</literal> is as good as or better than the original term frequency-based <literal>BM25</literal> in terms of retrieval effectiveness.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {51–62},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_8,
author = {Park, Laurence A. and Ramamohanarao, Kotagiri},
title = {The Effect of Weighted Term Frequencies on Probabilistic Latent Semantic Term Relationships},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_8},
doi = {10.1007/978-3-540-89097-3_8},
abstract = {Probabilistic latent semantic analysis (PLSA) is a method of calculating term relationships within a document set using term frequencies. It is well known within the information retrieval community that raw term frequencies contain various biases that affect the precision of the retrieval system. Weighting schemes, such as BM25, have been developed in order to remove such biases and hence improve the overall quality of results from the retrieval system. We hypothesised that the biases found within raw term frequencies also affect the calculation of term relationships performed during PLSA. By using portions of the BM25 probabilistic weighting scheme, we have shown that applying weights to the raw term frequencies before performing PLSA leads to a significant increase in precision at 10 documents and average reciprocal rank. When using the BM25 weighted PLSA information in the form of a thesaurus, we achieved an average 8% increase in precision. Our thesaurus method was also compared to pseudo-relevance feedback and a co-occurrence thesaurus, both using BM25 weights. Precision results showed that the probabilistic latent semantic thesaurus using BM25 weights outperformed each method in terms of precision at 10 documents and average reciprocal rank.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {63–74},
numpages = {12},
keywords = {probabilistic latent semantic analysis, information retrieval, probabilistic model},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_9,
author = {J\"{a}rvelin, Anni and J\"{a}rvelin, Antti},
title = {Comparison of S-Gram Proximity Measures in Out-of-Vocabulary Word Translation},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_9},
doi = {10.1007/978-3-540-89097-3_9},
abstract = {Classified <em>s</em> -grams have been successfully used in cross-language information retrieval (CLIR) as an approximate string matching technique for translating out-of-vocabulary (OOV) words. For example, <em>s</em> -grams have consistently outperformed other approximate string matching techniques, like edit distance or <em>n</em> -grams. The Jaccard coefficient has traditionally been used as an <em>s</em> -gram based string proximity measure. However, other proximity measures for <em>s</em> -gram matching have not been tested. In the current study the performance of seven proximity measures for classified <em>s</em> -grams in CLIR context was evaluated using eleven language pairs. The binary proximity measures performed generally better than their non-binary counterparts, but the difference depended mainly on the padding used with <em>s</em> -grams. When no padding was used, the binary and non-binary proximity measures were nearly equal, though the performance at large deteriorated.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {75–86},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_10,
author = {Claude, Francisco and Navarro, Gonzalo and Peltola, Hannu and Salmela, Leena and Tarhio, Jorma},
title = {Speeding Up Pattern Matching by Text Sampling},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_10},
doi = {10.1007/978-3-540-89097-3_10},
abstract = {We introduce a novel alphabet sampling technique for speeding up both online and indexed string matching. We choose a subset of the alphabet and select the corresponding subsequence of the text. Online or indexed searching is then carried out on that subsequence, and candidate matches are verified in the full text. We show that this speeds up online searching, especially for moderate to long patterns, by a factor of up to 5. For indexed searching we achieve indexes that are as fast as the classical suffix array, yet occupy space less than 0.5 times the text size (instead of 4) plus text. Our experiments show no competitive alternatives in a wide space/time range.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {87–98},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_11,
author = {Clifford, Rapha\"{e}l and Efremenko, Klim and Porat, Benny and Porat, Ely and Rothschild, Amir},
title = {Mismatch Sampling},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_11},
doi = {10.1007/978-3-540-89097-3_11},
abstract = {We consider the well known problem of pattern matching under the Hamming distance. Previous approaches have shown how to count the number of mismatches efficiently, especially when a bound is known for the maximum Hamming distance. Our interest is different in that we wish collect a random sample of mismatches of fixed size at each position in the text. Given a pattern <em>p</em> of length <em>m</em> and a text <em>t</em> of length <em>n</em> , we show how to sample with high probability <em>c</em> mismatches where possible from every alignment of <em>p</em> and <em>t</em> in <em>O</em> ((<em>c</em> + log<em>n</em> )(<em>n</em> + <em>m</em> log<em>m</em> )log<em>m</em> ) time. Further, we guarantee that the mismatches are sampled uniformly and can therefore be seen as representative of the types of mismatches that occur.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {99–108},
numpages = {10},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_12,
author = {Senft, Martin and Dvo\v{r}\'{a}k, Tom\'{a}\v{s}},
title = {Sliding CDAWG Perfection},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_12},
doi = {10.1007/978-3-540-89097-3_12},
abstract = {The Compact Directed Acyclic Word Graph (CDAWG) is a well-known suffix data structure designed for an efficient solution to problems on strings. Some applications, especially those from the data compression field, require maintaining a CDAWG over a sliding window. The fastest known solution to this problem is an approximation algorithm that slides a CDAWG in an amortized constant time. However, the existence of an exact algorithm performing within the same complexity bounds has been an open question so far. We show that the answer to this question is negative and present an on-line algorithm with the best asymptotic complexity possible.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {109–120},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_13,
author = {Brisaboa, Nieves R. and Fari\~{n}a, Antonio and Navarro, Gonzalo and Places, Angeles S. and Rodr\'{\i}guez, Eduardo},
title = {Self-Indexing Natural Language},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_13},
doi = {10.1007/978-3-540-89097-3_13},
abstract = {Self-indexing is a concept developed for indexing arbitrary strings. It has been enormously successful to reduce the size of the large indexes typically used on strings, namely suffix trees and arrays. Self-indexes represent a string in a space close to its compressed size and provide indexed searching on it. On natural language, a compressed inverted index over the compressed text already provides a reasonable alternative, in space and time, for indexed searching of words and phrases. In this paper we explore the possibility of regarding natural language text as a string of words and applying a self-index to it. There are several challenges involved, such as dealing with a very large alphabet and detaching searchable content from non-searchable presentation aspects in the text. As a result, we show that the self-index requires space very close to that of the best word-based compressors, and that it obtains better search time than inverted indexes (using the same overall space) when searching for phrases.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {121–132},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_14,
author = {Smyth, W. F. and Wang, Shu},
title = {New Perspectives on the Prefix Array},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_14},
doi = {10.1007/978-3-540-89097-3_14},
abstract = {In this paper we consider the prefix array π =π[1..n] of a string x =x[1..n] in which π[1]=0 and, for <em>i</em> &gt;1, π[i = k iff <em>k</em> is the largest integersuch that x[i..i+k-1]. The prefix array πis closely related to the border array β: an integerarray [1..<em>n</em> ] such that β[i = kiff the length of the longest border of x[1..i] is<em>k</em> . Border arrays or their variants are used in many stringalgorithms and prefix arrays can be used directly forpattern-matching. It is well known that for regular strings πprovides all the information that β does; we showhowever that for indeterminate strings (those containing entriesthat match a subset of the alphabet) π actually provides moreinformation, in fact still enabling all the borders of every prefixof x to be specified. Since a lot of the entries of π areexpected to be zeros, it is natural to represent π in compressedform using integer arrays POS[1..m] and LEN[1..m],where <em>m</em> is the number of nonzero entries in π andπ[POS[j]] = LEN [j] iff the $j^{mbox{th}}$nonzero entry in π occurs in position POS[j] and takesthe value LEN [j]. The expected value of <em>m</em> is<em>n</em> /<em>σ</em> - 1, where <em>σ</em> is thealphabet size. The straightforward way of computing POS/LENrequires computing π first, therefore requires<em>O</em> (<em>n</em> ) extra space. We describe two<em>θ</em> (<em>n</em> )-time algorithms PL1 &amp; PL2 tocompute POS/LEN for regular strings using only 8<em>m</em> bytes ofstorage in addition to the <em>n</em> bytes required for x.PL1 requires about one-third the time of the standard border arrayalgorithm MP on English-language strings; PL2 executes faster thanMP on both English-language and highly periodic strings on{<em>a</em> ,<em>b</em> }. For indeterminate strings, we describe anextension IPL of PL1 that computes POS/LEN in <em>O</em> (<em>n</em> 2) worst-case time (though generally much faster), stillusing only 8<em>m</em> bytes of additional storage. For bothregular and indeterminate strings, the compressed form of π canbe used for efficient pattern-matching.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {133–143},
numpages = {11},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_15,
author = {Russo, Lu\'{\i}s M. and Navarro, Gonzalo and Oliveira, Arlindo L.},
title = {Indexed Hierarchical Approximate String Matching},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_15},
doi = {10.1007/978-3-540-89097-3_15},
abstract = {We present a new search procedure for approximate string matching over suffix trees. We show that hierarchical verification, which is a well-established technique for on-line searching, can also be used with an indexed approach. For this, we need that the index supports bidirectionality, meaning that the search for a pattern can be updated by adding a letter at the right or at the left. This turns out to be easily supported by most compressed text self-indexes, which represent the index and the text essentially in the same space of the compressed text alone. To complete the symbiotic exchange, our hierarchical verification largely reduces the need to access the text, which is expensive in compressed text self-indexes. The resulting algorithm can, in particular, run over an existing fully compressed suffix tree, which makes it very appealing for applications in computational biology. We compare our algorithm with related approaches, showing that our method offers an interesting space/time tradeoff, and in particular does not need of any parameterization, which is necessary in the most successful competing approaches.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {144–154},
numpages = {11},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_16,
author = {Hyyr\"{o}, Heikki},
title = {An Efficient Linear Space Algorithm for Consecutive Suffix Alignment under Edit Distance (Short Preliminary Paper)},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_16},
doi = {10.1007/978-3-540-89097-3_16},
abstract = {We discuss the following variant of incremental edit distance computation: Given strings <em>A</em> and <em>B</em> with lengths <em>m</em> and <em>n</em> , respectively, the task is to compute, in <em>n</em> successive iterations <em>j</em> = <em>n</em> ...1, an encoding of the edit distances between <em>A</em> and all prefixes of <em>B</em> <subscript> <em>j</em> ..<em>n</em> </subscript> . Here <em>B</em> <subscript> <em>j</em> ..<em>n</em> </subscript> is the suffix of <em>B</em> that begins at its <em>j</em> th character. This type of <em>consecutive suffix alignment</em> [3] is powerful e.g. in solving the cyclic string comparison problem [3]. There are two previous efficient algorithms that are capable of consecutive suffix alignment under edit distance: the algorithm of Landau et al. [2] that runs in <em>O</em> (<em>kn</em> ) time and uses <em>O</em> (<em>m</em> + <em>n</em> + <em>k</em> 2) space, and the algorithm of Kim and Park [1] that runs in <em>O</em> ((<em>m</em> + <em>n</em> )<em>n</em> ) time and uses <em>O</em> (<em>mn</em> ) space. Here <em>k</em> is a user-defined upper limit for the computed distances (0 ≤ <em>k</em> ≤ max {<em>m</em> ,<em>n</em> }). In this paper we propose the first efficient linear space algorithm for consecutive suffix alignment under edit distance. Our algorithm uses <em>O</em> ((<em>m</em> + <em>n</em> )<em>n</em> ) time and <em>O</em> (<em>m</em> + <em>n</em> ) space.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {155–163},
numpages = {9},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_17,
author = {Sir\'{e}n, Jouni and V\"{a}lim\"{a}ki, Niko and M\"{a}kinen, Veli and Navarro, Gonzalo},
title = {Run-Length Compressed Indexes Are Superior for Highly Repetitive Sequence Collections},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_17},
doi = {10.1007/978-3-540-89097-3_17},
abstract = {A repetitive sequence collection is one where portions of a <em>base sequence</em> of length <em>n</em> are repeated many times with small variations, forming a collection of total length <em>N</em> . Examples of such collections are version control data and genome sequences of individuals, where the differences can be expressed by lists of basic edit operations. This paper is devoted to studying ways to store massive sets of highly repetitive sequence collections in space-efficient manner so that retrieval of the content as well as queries on the content of the sequences can be provided time-efficiently. We show that the state-of-the-art entropy-bound full-text <em>self-indexes</em> do not yet provide satisfactory space bounds for this specific task. We engineer some new structures that use run-length encoding and give empirical evidence that these structures are superior to the current structures.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {164–175},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_18,
author = {Claude, Francisco and Navarro, Gonzalo},
title = {Practical Rank/Select Queries over Arbitrary Sequences},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_18},
doi = {10.1007/978-3-540-89097-3_18},
abstract = {We present a practical study on the compact representation of sequences supporting <em>rank</em> , <em>select</em> , and <em>access</em> queries. While there are several theoretical solutions to the problem, only a few have been tried out, and there is little idea on how the others would perform, especially in the case of sequences with very large alphabets. We first present a new practical implementation of the compressed representation for bit sequences proposed by Raman, Raman, and Rao [SODA 2002], that is competitive with the existing ones when the sequences are not too compressible. It also has nice local compression properties, and we show that this makes it an excellent tool for compressed text indexing in combination with the Burrows-Wheeler transform. This shows the practicality of a recent theoretical proposal [M\"{a}kinen and Navarro, SPIRE 2007], achieving spaces never seen before. Second, for general sequences, we tune wavelet trees for the case of very large alphabets, by removing their pointer information. We show that this gives an excellent solution for representing a sequence within zero-order entropy space, in cases where the large alphabet poses a serious challenge to typical encoding methods. We also present the first implementation of Golynski et al.'s representation [SODA 2006], which offers another interesting time/space trade-off.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {176–187},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_19,
author = {Francisco, Alexandre P. and Baeza-Yates, Ricardo and Oliveira, Arlindo L.},
title = {Clique Analysis of Query Log Graphs},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_19},
doi = {10.1007/978-3-540-89097-3_19},
abstract = {In this paper we propose a method for the analysis of very large graphs obtained from query logs, using query coverage inspection. The goal is to extract semantic relations between queries and their terms. We take a new approach to successfully and efficiently cluster these large graphs by analyzing clique overlap and <em>a priori</em> induced cliques. The clustering quality is evaluated with an extension of the modularity score. Results obtained with real data show that the identified clusters can be used to infer properties of the queries and interesting semantic relations between them and their terms. The quality of the semantic relations is evaluated both using a tf-idf based score and data from the Open Directory Project. The proposed approach is also able to identify and filter out multitopical URLs, a feature that is interesting in itself.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {188–199},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_20,
author = {Transier, Frederik and Sanders, Peter},
title = {Out of the Box Phrase Indexing},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_20},
doi = {10.1007/978-3-540-89097-3_20},
abstract = {We present a method for optimizing phrase search based on inverted indexes. Our approach adds selected (two-term) phrases to an existing index. Whereas competing approaches are often based on the analysis of query logs, our approach works out of the box and uses only the information contained in the index. Also, our method is competitive in terms of query performance and can even improve on other approaches for difficult queries. Moreover, our approach gives performance guarantees for arbitrary queries. Further, we propose using a phrase index as a substitute for the positional index of an in-memory search engine working with short documents. We support our conclusions with experiments using a high-performance main-memory search engine. We also give evidence that classical disk based systems can profit from our approach.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {200–211},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_21,
author = {Lipsky, Ohad and Porat, Ely},
title = {Approximated Pattern Matching with the L<sub>1</sub> , L<sub>2</sub> and L<sub>∞</sub> Metrics},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_21},
doi = {10.1007/978-3-540-89097-3_21},
abstract = {Given an alphabet <em>Σ</em> ={1,2,...,|<em>Σ</em> |} text string <em>T</em> ε<em>Σ</em> <em>n</em>  and a pattern string<em>P</em> ε <em>Σ</em> <em>m</em>  , foreach <em>i</em> = 1,2,...,<em>n</em> - <em>m</em> + 1 define<em>L</em> <em>d</em>  (<em>i</em> ) as the d-normdistance when the pattern is aligned below the text and starts atposition <em>i</em> of the text. The problem of pattern matchingwith <em>L</em> <em>p</em>  distance is to compute<em>L</em> <em>p</em>  (<em>i</em> ) for every <em>i</em> = 1,2,...,<em>n</em> - <em>m</em> + 1. We discuss the problem for<em>d</em> = 1, ∞. First, in the case of <em>L</em> 1 matching (pattern matching with an <em>L</em> 1 distance) we present an algorithm that approximatesthe <em>L</em> 1 matching up to a factor of 1 +<em>ε</em> , which has an $O(frac{1}{varepsilon^2} nlogmlog |Sigma|)$ run time. Second, we provide an algorithm thatapproximates the <em>L</em> ∞ matching up to afactor of 1 + <em>ε</em> with a run time of$O(frac{1}{varepsilon} nlog mlog |Sigma|)$. We also generalizethe problem of String Matching with mismatches to have weightedmismatches and present an <em>O</em> (<em>n</em> log4<em>m</em> ) algorithm that approximates the results of this problemup to a factor of <em>O</em> (log<em>m</em> ) in the case that theweight function is a metric.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {212–223},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_22,
author = {Kapah, Oren and Landau, Gad M. and Levy, Avivit and Oz, Nitsan},
title = {Interchange Rearrangement: The Element-Cost Model},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_22},
doi = {10.1007/978-3-540-89097-3_22},
abstract = {Given an input string <em>S</em> and a target string <em>T</em> when <em>S</em> is a permutation of <em>T</em> , the <em>interchange rearrangement problem</em> is to apply on <em>S</em> a sequence of interchanges, such that <em>S</em> is transformed into <em>T</em> . The <em>interchange</em> operation exchanges the position of the two elements on which it is applied. The goal is to transform <em>S</em> into <em>T</em> at the minimum cost possible, referred to as the distance between <em>S</em> and <em>T</em> . The distance can be defined by several cost models that determine the cost of every operation. There are two known models: The <em>Unit-cost model</em> and the <em>Length-cost model</em> . In this paper, we suggest a natural cost model: The <em>Element-cost model</em> . In this model, the cost of an operation is determined by the elements that participate in it. Though this model has been studied in other fields, it has never been considered in the context of rearrangement problems. We consider both the special case where all elements in <em>S</em> and <em>T</em> are distinct, referred to as a <em>permutation string</em> , and the general case, referred to as a <em>general string</em> . An efficient optimal algorithm for the permutation string case and efficient approximation algorithms for the general string case, which is $cal{NP}$-hard, are presented.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {224–235},
numpages = {12},
keywords = {Interchange rearrangement, Cost models},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_23,
author = {Lee, Inbok and Mendivelso, Juan and Pinz\'{o}n, Yoan J.},
title = {<em>δ</em> <em>γ</em> --- Parameterized Matching},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_23},
doi = {10.1007/978-3-540-89097-3_23},
abstract = {This paper defines a new pattern matching problem by combiningtwo paradigms: <em>Δ</em> <em>γ</em> ---matching andparameterized matching. The solution is essentially obtained by acombination of bitparallel techniques and a reduction to a graphmatching problem. The time complexity of the algorithm is<em>O</em> (<em>nm</em> ), assuming text size <em>n</em> , patternsize <em>m</em> and a constant size alphabet.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {236–248},
numpages = {13},
keywords = {parameterized matching, <em>δ</em> <em>γ</em> ---matching, <em>δ</em> ---matching, combinatorial algorithms, bipartite matching},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_24,
author = {Porat, Benny and Porat, Ely and Zur, Asaf},
title = {Pattern Matching with Pair Correlation Distance},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_24},
doi = {10.1007/978-3-540-89097-3_24},
abstract = {In pattern matching with <em>pair correlation</em> distanceproblem, the goal is to find all occurrences of a pattern<em>P</em> of length <em>m</em> , in a text <em>T</em> of length<em>n</em> , where the distance between them is less than athreshold <em>k</em> . For each text location <em>i</em> , thedistance is defined as the number of different kinds of mismatchedpairs (<em>α</em> ,<em>β</em> ), between <em>P</em> and<em>T</em> [<em>i</em> ...<em>i</em> + <em>m</em> ]. We present analgorithm with running time of $Oleft(min{left|Sigma_Pright|^2n log m,n !left({m log m}right)^frac{2}{3}}right)!$ forthis problem. Another interesting problem is the <em>one-side paircorrelation</em> distance where it is desired to find alloccurrences of <em>P</em> where the number of mismatched charactersin <em>P</em> is less than <em>k</em> . For this problem, we presentan algorithm with running time of$Oleft(min{left|Sigma_Pright| n log m,nright.left.sqrt{mlog m}}right)$.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {249–256},
numpages = {8},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_25,
author = {Timkovsky, Vadim G.},
title = {Some Approximations for Shortest Common Nonsubsequences and Supersequences},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_25},
doi = {10.1007/978-3-540-89097-3_25},
abstract = {This paper is devoted to polynomial-time approximations for the problems of finding a shortest common nonsubsequence and a shortest common supersequence of given strings. The main attention is paid to the special case of the latter problem where all given strings are of length two. We show strong connections of this case to the feedback vertex set problem, the maximal network flow problem and the maximal multi-commodity network flow problem.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {257–268},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_26,
author = {Boucher, Christina and Brown, Daniel G. and Durocher, Stephane},
title = {On the Structure of Small Motif Recognition Instances},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_26},
doi = {10.1007/978-3-540-89097-3_26},
abstract = {Given a set of sequences, <em>S</em> , and degeneracy parameter, <em>d</em> , the <emphasis type="SmallCaps">Consensus Sequence</emphasis> problem asks whether there exists a sequence that has Hamming distance at most <em>d</em> from each sequence in <em>S</em> . A <em>valid motif set</em> is a set of sequences for which such a consensus sequence exists, while a <em>decoy set</em> is a set of sequences that does not have a consensus sequence but whose pairwise Hamming distances are all at most 2<em>d</em> . At present, no efficient solution is known to the <emphasis type="SmallCaps">Consensus Sequence</emphasis> problem when the number of sequences is greater than three. For instances of <emphasis type="SmallCaps">Consensus Sequence</emphasis> with binary sequences and cardinality four, we present a combinatorial characterization of decoy sets and a linear-time exact algorithm, resolving an open problem posed by Gramm <em>et al.</em> [7].},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {269–281},
numpages = {13},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

@inproceedings{10.1007/978-3-540-89097-3_27,
author = {Benson, Gary and Mak, Denise Y.},
title = {Exact Distribution of a Spaced Seed Statistic for DNA Homology Detection},
year = {2008},
isbn = {9783540890966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-89097-3_27},
doi = {10.1007/978-3-540-89097-3_27},
abstract = {Let a <em>seed</em> , <em>S</em> , be a string from the alphabet {1,*}, of arbitrary length <em>k</em> , which starts and ends with a 1. For example, <em>S</em> = 11*1. <em>S</em> <em>occurs</em> in a binary string <em>T</em> at position <em>h</em> if the length <em>k</em> substring of <em>T</em> ending at position <em>h</em> contains a 1 in every position where there is a 1 in <em>S</em> . We say that the 1s at the corresponding positions in <em>T</em> are <em>covered</em> . We are interested in calculating the probability distribution for the <em>number</em> of 1s covered by a seed <em>S</em> in an iid Bernoulli string of length <em>n</em> with probability of 1 equal to <em>p</em> . We refer to this new probability distribution as <em>C</em> <subscript> <em>nSp</em> </subscript> , for <em>covered</em> , with <em>S</em> being the seed. We present an efficient method to calculate this distribution <em>exactly</em> . Covered 1s represent matching positions detected in DNA sequences when using multiple hits of a spaced seed. Knowledge of the distribution provides a statistical threshold for distinguishing true homologies from randomly matching sequences.},
booktitle = {Proceedings of the 15th International Symposium on String Processing and Information Retrieval},
pages = {282–293},
numpages = {12},
location = {Melbourne, Australia},
series = {SPIRE '08}
}

