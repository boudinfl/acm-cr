@inproceedings{10.1007/11575832_1,
author = {Culpepper, J. Shane and Moffat, Alistair},
title = {Enhanced Byte Codes with Restricted Prefix Properties},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_1},
doi = {10.1007/11575832_1},
abstract = {Byte codes have a number of properties that make them attractive for practical compression systems: they are relatively easy to construct; they decode quickly; and they can be searched using standard byte-aligned string matching techniques. In this paper we describe a new type of byte code in which the first byte of each codeword completely specifies the number of bytes that comprise the suffix of the codeword. Our mechanism gives more flexible coding than previous constrained byte codes, and hence better compression. The structure of the code also suggests a heuristic approximation that allows savings to be made in the prelude that describes the code. We present experimental results that compare our new method with previous approaches to byte coding, in terms of both compression effectiveness and decoding throughput speeds.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {1–12},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_2,
author = {Baeza-Yates, Ricardo and Salinger, Alejandro},
title = {Experimental Analysis of a Fast Intersection Algorithm for Sorted Sequences},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_2},
doi = {10.1007/11575832_2},
abstract = {This work presents an experimental comparison of intersection algorithms for sorted sequences, including the recent algorithm of Baeza-Yates. This algorithm performs on average less comparisons than the total number of elements of both inputs (n and m respectively) when n=αm (α &gt; 1). We can find applications of this algorithm on query processing in Web search engines, where large intersections, or differences, must be performed fast. In this work we concentrate in studying the behavior of the algorithm in practice, using for the experiments test data that is close to the actual conditions of its applications. We compare the efficiency of the algorithm with other intersection algorithm and we study different optimizations, showing that the algorithm is more efficient than the alternatives in most cases, especially when one of the sequences is much larger than the other.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {13–24},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_3,
author = {Boldi, Paolo and Vigna, Sebastiano},
title = {Compressed Perfect Embedded Skip Lists for Quick Inverted-Index Lookups},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_3},
doi = {10.1007/11575832_3},
abstract = {Large inverted indices are by now common in the construction of web-scale search engines. For faster access, inverted indices are indexed internally so that it is possible to skip quickly over unnecessary documents. To this purpose, we describe how to embed efficiently a compressed perfect skip list in an inverted list.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {25–28},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_4,
author = {Tannier, Xavier and Geva, Shlomo},
title = {XML Retrieval with a Natural Language Interface},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_4},
doi = {10.1007/11575832_4},
abstract = {Effective information retrieval in XML documents requires the user to have good knowledge of document structure and of some formal query language. XML query languages like XPath and XQuery are too complex to be considered for use by end users. We present an approach to XML query processing that supports the specification of both textual and structural constraints in natural language. We implemented a system that supports the evaluation of both formal XPath-like queries and natural language XML queries. We present comparative test results that were performed with the INEX 2004 topics and XML collection. Our results quantify the trade-off in performance of natural language XML queries vs formal queries with favourable results.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {29–40},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_5,
author = {Dupret, Georges and Mendoza, Marcelo},
title = {Recommending Better Queries from Click-through Data},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_5},
doi = {10.1007/11575832_5},
abstract = {We present a method to help a user redefine a query based on past users experience, namely the click-through data as recorded by a search engine. Unlike most previous works, the method we propose attempts to recommend better queries rather than related queries. It is effective at identifying query specialization or sub-topics because it take into account the co-occurrence of documents in individual query sessions. It is also particularly simple to implement.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {41–44},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_6,
author = {Macedo, Alessandra Alaniz and Camacho-Guerrero, Jos\'{e} Antonio and da Gra\c{c}a Campos Pimentel, Maria},
title = {A Bilingual Linking Service for the Web},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_6},
doi = {10.1007/11575832_6},
abstract = {The aim of Cross-Language Information Retrieval (CLIR) area is to address situations where a query is made in one language and the application is able to return documents in another. Many CLIR techniques attempt to translate the user's query to the language of the target documents using translation dictionaries. However, these techniques have limitations in terms of lexical coverage of the dictionary adopted. For some applications, the dictionaries are manually edited towards improving the results — but this may require much effort to represent a large collection of information.In this article we propose an infrastructure for defining automatically relationships between Web documents written in different languages. Our approach is based on the Latent Semantic Indexing Technique, which tries to overcome the problems common to the lexical approach due to words with multiple meanings and multiple words with the same meaning. LSI automatically organizes text objects into a semantic structure appropriate for matching [3]. To support the identification of relationships among documents in different languages, the proposed infrastructure manipulates the stem portion of each word in order to index the corresponding Web documents when building the information space manipulated by LSI. To experiment this proposal, we studied the creation of links among news documents in English and Spanish in three different categories: entertainment, technology and world. The results were positive.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {45–48},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_7,
author = {Cigarran, Juan M. and Pen̈as, Anselmo and Gonzalo, Julio and Verdejo, Felisa},
title = {Evaluating Hierarchical Clustering of Search Results},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_7},
doi = {10.1007/11575832_7},
abstract = {We propose a goal-oriented evaluation measure, Hierarchy Quality, for hierarchical clustering algorithms applied to the task of organizing search results -such as the clusters generated by Vivisimo search engine-. Our metric considers the content of the clusters, their hierarchical arrangement, and the effort required to find relevant information by traversing the hierarchy starting from the top node. It compares the effort required to browse documents in a baseline ranked list with the minimum effort required to find the same amount of relevant information by browsing the hierarchy (which involves examining both documents and node descriptors).},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {49–54},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_8,
author = {Sch\"{u}rmann, Klaus-Bernd and Stoye, Jens},
title = {Counting Suffix Arrays and Strings},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_8},
doi = {10.1007/11575832_8},
abstract = {Suffix arrays are used in various application and research areas like data compression or computational biology. In this work, our goal is to characterize the combinatorial properties of suffix arrays and their enumeration. For fixed alphabet size and string length we count the number of strings sharing the same suffix array and the number of such suffix arrays. Our methods have applications to succinct suffix arrays and build the foundation for the efficient generation of appropriate test data sets for suffix array based algorithms.We also show that summing up the strings for all suffix arrays builds a particular instance for some summation identities of Eulerian numbers.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {55–66},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_9,
author = {Amir, Amihood and Kopelowitz, Tsvi and Lewenstein, Moshe and Lewenstein, Noa},
title = {Towards Real-Time Suffix Tree Construction},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_9},
doi = {10.1007/11575832_9},
abstract = {The quest for a real-time suffix tree construction algorithm is over three decades old. To date there is no convincing understandable solution to this problem. This paper makes a step in this direction by constructing a suffix tree online in time O(log n) per every single input symbol. Clearly, it is impossible to achieve better than O(log n) time per symbol in the comparison model, therefore no true real time algorithm can exist for infinite alphabets. Nevertheless, the best that can be hoped for is that the construction time for every symbol does not exceed O(log n) (as opposed to an amortized O(log n) time per symbol, achieved by current known algorithms). To our knowledge, our algorithm is the first that spends in the worst caseO(log n) per every single input symbol.We also provide a simple algorithm that constructs online an indexing structure (the BIS) in time O(log n) per input symbol, where n is the number of text symbols input thus far. This structure and fast LCP (Longest Common Prefix) queries on it, provide the backbone for the suffix tree construction. Together, our two data structures provide a searching algorithm for a pattern of length m whose time is $O(min(m {rm log} |{it Sigma}|,m + {rm log} n) + tocc)$, where tocc is the number of occurrences of the pattern.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {67–78},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_10,
author = {Bialynicka-Birula, Iwona and Grossi, Roberto},
title = {Rank-Sensitive Data Structures},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_10},
doi = {10.1007/11575832_10},
abstract = {Output-sensitive data structures result from preprocessing n items and are capable of reporting the items satisfying an on-line query in O(t(n) + ℓ) time, where t(n) is the cost of traversing the structure and ℓ ≤ n is the number of reported items satisfying the query. In this paper we focus on rank-sensitive data structures, which are additionally given a ranking of the n items, so that just the top k best-ranking items should be reported at query time, sorted in rank order, at a cost of O(t(n) + k) time. Note that k is part of the query as a parameter under the control of the user (as opposed to ℓ which is query-dependent). We explore the problem of adding rank-sensitivity to data structures such as suffix trees or range trees, where the ℓ items satisfying the query form O(polylog(n)) intervals of consecutive entries from which we choose the top k best-ranking ones. Letting s(n) be the number of items (including their copies) stored in the original data structures, we increase the space by an additional term of O(s(n) lgεn) memory words of space, each of O(lg n) bits, for any positive constant ε &lt; 1. We allow for changing the ranking on the fly during the lifetime of the data structures, with ranking values in 0 ... O(n). In this case, query time becomes O(t(n) + k) plus O(lg n/ lg lg n) per interval; each change in the ranking and each insertion/deletion of an item takes O(lg n); the additional term in space occupancy increases to O(s(n) lg n/ lg lg n).},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {79–90},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_11,
author = {Askitis, Nikolas and Zobel, Justin},
title = {Cache-Conscious Collision Resolution in String Hash Tables},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_11},
doi = {10.1007/11575832_11},
abstract = {In-memory hash tables provide fast access to large numbers of strings, with less space overhead than sorted structures such as tries and binary trees. If chains are used for collision resolution, hash tables scale well, particularly if the pattern of access to the stored strings is skew. However, typical implementations of string hash tables, with lists of nodes, are not cache-efficient. In this paper we explore two alternatives to the standard representation: the simple expedient of including the string in its node, and the more drastic step of replacing each list of nodes by a contiguous array of characters. Our experiments show that, for large sets of strings, the improvement is dramatic. In all cases, the new structures give substantial savings in space at no cost in time. In the best case, the overhead space required for pointers is reduced by a factor of around 50, to less than two bits per string (with total space required, including 5.68 megabytes of strings, falling from 20.42 megabytes to 5.81 megabytes), while access times are also reduced.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {91–102},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_12,
author = {Skala, Matthew},
title = {Measuring the Difficulty of Distance-Based Indexing},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_12},
doi = {10.1007/11575832_12},
abstract = {Data structures for similarity search are commonly evaluated on data in vector spaces, but distance-based data structures are also applicable to non-vector spaces with no natural concept of dimensionality. The intrinsic dimensionality statistic of Ch\'{a}vez and Navarro provides a way to compare the performance of similarity indexing and search algorithms across different spaces, and predict the performance of index data structures on non-vector spaces by relating them to equivalent vector spaces. We characterise its asymptotic behaviour, and give experimental results to calibrate these comparisons.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {103–114},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_13,
author = {Kondrak, Grzegorz},
title = {<i>N</i>-Gram Similarity and Distance},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_13},
doi = {10.1007/11575832_13},
abstract = {In many applications, it is necessary to algorithmically quantify the similarity exhibited by two strings composed of symbols from a finite alphabet. Numerous string similarity measures have been proposed. Particularly well-known measures are based are edit distance and the length of the longest common subsequence. We develop a notion of n-gram similarity and distance. We show that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. We provide formal, recursive definitions of n-gram similarity and distance, together with efficient algorithms for computing them. We formulate a family of word similarity measures based on n-grams, and report the results of experiments that suggest that the new measures outperform their unigram equivalents.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {115–126},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_14,
author = {Paredes, Rodrigo and Ch\'{a}vez, Edgar},
title = {Using the <i>k</i>-Nearest Neighbor Graph for Proximity Searching in Metric Spaces},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_14},
doi = {10.1007/11575832_14},
abstract = {Proximity searching consists in retrieving from a database, objects that are close to a query. For this type of searching problem, the most general model is the metric space, where proximity is defined in terms of a distance function. A solution for this problem consists in building an offline index to quickly satisfy online queries. The ultimate goal is to use as few distance computations as possible to satisfy queries, since the distance is considered expensive to compute. Proximity searching is central to several applications, ranging from multimedia indexing and querying to data compression and clustering.In this paper we present a new approach to solve the proximity searching problem. Our solution is based on indexing the database with the k-nearest neighbor graph (knng), which is a directed graph connecting each element to its k closest neighbors.We present two search algorithms for both range and nearest neighbor queries which use navigational and metrical features of the knng graph. We show that our approach is competitive against current ones. For instance, in the document metric space our nearest neighbor search algorithms perform 30% more distance evaluations than AESA using only a 0.25% of its space requirement. In the same space, the pivot-based technique is completely useless.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {127–138},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_15,
author = {van Zaanen, Menno and Pizzato, Luiz Augusto and Moll\'{a}, Diego},
title = {Classifying Sentences Using Induced Structure},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_15},
doi = {10.1007/11575832_15},
abstract = {In this article we will introduce a new approach (and several implementations) to the task of sentence classification, where pre-defined classes are assigned to sentences. This approach concentrates on structural information that is present in the sentences. This information is extracted using machine learning techniques and the patterns found are used to classify the sentences. The approach fits in between the existing machine learning and hand-crafting of regular expressions approaches, and it combines the best of both. The sequential information present in the sentences is used directly, classifiers can be generated automatically and the output and intermediate representations can be investigated and manually optimised if needed.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {139–150},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_16,
author = {Sahlgren, Magnus and Karlgren, Jussi},
title = {Counting Lumps in Word Space: Density as a Measure of Corpus Homogeneity},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_16},
doi = {10.1007/11575832_16},
abstract = {This paper introduces a measure of corpus homogeneity that indicates the amount of topical dispersion in a corpus. The measure is based on the density of neighborhoods in semantic word spaces. We evaluate the measure by comparing the results for five different corpora. Our initial results indicate that the proposed density measure can indeed identify differences in topical dispersion.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {151–154},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_17,
author = {Feng, Yi and Wu, Zhaohui and Zhou, Zhongmei},
title = {Multi-Label Text Categorization Using k-Nearest Neighbor Approach with m-Similarity},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_17},
doi = {10.1007/11575832_17},
abstract = {Due to the ubiquity of textual information nowadays and the multi-topic nature of text, it is of great necessity to explore multi-label text categorization problem. Traditional methods based on vector-space-model text representation suffer the losing of word order information. In this paper, texts are considered as symbol sequences. A multi-label lazy learning approach named kNN-M is proposed, which is derived from traditional k-nearest neighbor (kNN) method. The flexible order-semisensitive measure, M-Similarity, which enables the usage of sequence information in text by swap-allowed dynamic block matching, is applied to evaluate the closeness of texts on finding k-nearest neighbors in kNN-M. Experiments on real-world OHSUMED datasets illustrate that our approach outperforms existing ones considerably, showing the power of considering both term co-occurrence and order on text categorization tasks.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {155–160},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_18,
author = {Lloyd, Levon and Kechagias, Dimitrios and Skiena, Steven},
title = {Lydia: A System for Large-Scale News Analysis},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_18},
doi = {10.1007/11575832_18},
abstract = {Periodical publications represent a rich and recurrent source of knowledge on both current and historical events. The Lydia project seeks to build a relational model of people, places, and things through natural language processing of news sources and the statistical analysis of entity frequencies and co-locations. Lydia is still at a relatively early stage of development, but it is already producing interesting analysis of significant volumes of text. Indeed, we encourage the reader to visit our website (http://www.textmap.com) to see our analysis of recent news obtained from over 500 daily online news sources.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {161–166},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_19,
author = {Angelov, Stanislav and Inenaga, Shunsuke},
title = {Composite Pattern Discovery for PCR Application},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_19},
doi = {10.1007/11575832_19},
abstract = {We consider the problem of finding pairs of short patterns such that, in a given input sequence of length n, the distance between each pair's patterns is at least α. The problem was introduced in [1]and is motivated by the optimization of multiplexed nested PCR.We study algorithms for the following two cases; the special case when the two patterns in the pair are required to have the same length, and the more general case when the patterns can have different lengths. For the first case we present an O(αn log log n) time and O(n) space algorithm, and for the general case we give an O(αn log n) time and O(n) space algorithm. The algorithms work for any alphabet size and use asymptotically less space than the algorithms presented in [1]. For alphabets of constant size we also give an $O(nsqrt{n} {rm log}^{2} n)$ time algorithm for the general case. We demonstrate that the algorithms perform well in practice and present our findings for the human genome.In addition, we study an extended version of the problem where patterns in the pair occur at certain positions at a distance at most α, but do not occur α-close anywhere else, in the input sequence.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {167–178},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_20,
author = {Peterlongo, Pierre and Pisanti, Nadia and Boyer, Frederic and Sagot, Marie-France},
title = {Lossless Filter for Finding Long Multiple Approximate Repetitions Using a New Data Structure, the Bi-Factor Array},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_20},
doi = {10.1007/11575832_20},
abstract = {Similarity search in texts, notably biological sequences, has received substantial attention in the last few years. Numerous filtration and indexing techniques have been created in order to speed up the resolution of the problem. However, previous filters were made for speeding up pattern matching, or for finding repetitions between two sequences or occurring twice in the same sequence. In this paper, we present an algorithm called NIMBUS for filtering sequences prior to finding repetitions occurring more than twice in a sequence or in more than two sequences. NIMBUS uses gapped seeds that are indexed with a new data structure, called a bi-factor array, that is also presented in this paper. Experimental results show that the filter can be very efficient: preprocessing with NIMBUS a data set where one wants to find functional elements using a multiple local alignment tool such as GLAM ([7]), the overall execution time can be reduced from 10 hours to 6 minutes while obtaining exactly the same results.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {179–190},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_21,
author = {Lee, Inbok and Ardila, Yoan Jos\'{e} Pinz\'{o}n},
title = {Linear Time Algorithm for the Generalised Longest Common Repeat Problem},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_21},
doi = {10.1007/11575832_21},
abstract = {Given a set of strings $mathcal{U} = {T_{1}, T_{2}, . . . , T_{ell}}$, the longest common repeat problem is to find the longest common substring that appears at least twice in each string of $mathcal{U}$, considering direct, inverted, mirror as well as everted repeats. In this paper we define the generalised longest common repeat problem, where we can set the number of times that a repeat should appear in each string. We present a linear time algorithm for this problem using the suffix array. We also show an application of our algorithm for finding a longest common substring which appears only in a subset $mathcal{U}^{prime}$ of $mathcal{U}$ but not in $mathcal{U}$-$mathcal{U}^{prime}$.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {191–201},
numpages = {11},
keywords = {DNA satellites, inverted repeats, pattern discovery, suffix arrays, DNA repeats},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_22,
author = {Peres, Patr\'{\i}cia Silva and de Moura, Edleno Silva},
title = {Application of Clustering Technique in Multiple Sequence Alignment},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_22},
doi = {10.1007/11575832_22},
abstract = {This article presents a new approach using clustering technique for creating multiple sequence alignments. Currently, the most widely used strategy is the progressive alignment. However, each step of this strategy might generate an error which will be low for closely related sequences but will increase as sequences diverge. For that reason, determining the order in which the sequences will be aligned is very important. Following this idea, we propose the application of a clustering technique as an alternative way to determine this order. To assess the reliability of this new strategy, two methods were modified in order to apply a clustering technique. The accuracy of their new versions was tested using a reference alignment collection. Besides, the modified methods were also compared with their original versions, obtaining better alignments.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {202–205},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_23,
author = {Nwesri, Abdusalam F. A. and Tahaghoghi, S. M. M. and Scholer, Falk},
title = {Stemming Arabic Conjunctions and Prepositions},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_23},
doi = {10.1007/11575832_23},
abstract = {Arabic is the fourth most widely spoken language in the world, and is characterised by a high rate of inflection. To cater for this, most Arabic information retrieval systems incorporate a stemming stage. Most existing Arabic stemmers are derived from English equivalents; however, unlike English, most affixes in Arabic are difficult to discriminate from the core word. Removing incorrectly identified affixes sometimes results in a valid but incorrect stem, and in most cases reduces retrieval precision. Conjunctions and prepositions form an interesting class of these affixes. In this work, we present novel approaches for dealing with these affixes. Unlike previous approaches, our approaches focus on retaining valid Arabic core words, while maintaining high retrieval performance.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {206–217},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_24,
author = {Kong, Zhigang and Lalmas, Mounia},
title = {XML Multimedia Retrieval},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_24},
doi = {10.1007/11575832_24},
abstract = {Multimedia XML documents can be viewed as a tree, whose nodes correspond to XML elements, and where multimedia objects are referenced in attributes as external entities. This paper investigates the use of textual XML elements for retrieving multimedia objects.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {218–223},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_25,
author = {Imafouo, Am\'{e}lie and Tannier, Xavier},
title = {Retrieval Status Values in Information Retrieval Evaluation},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_25},
doi = {10.1007/11575832_25},
abstract = {Retrieval systems rank documents according to their retrieval status values (RSV) if these are monotonously increasing with the probability of relevance of documents. In this work, we investigate the links between RSVs and IR system evaluation.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {224–227},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_26,
author = {de Madariaga, Ricardo S\'{a}nchez and del Castillo, Jos\'{e} Ra\'{u}l Fern\'{a}ndez and Hilera, Jos\'{e} Ram\'{o}n},
title = {A Generalization of the Method for Evaluation of Stemming Algorithms Based on Error Counting},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_26},
doi = {10.1007/11575832_26},
abstract = {Until the introduction of the method for evaluation of stemming algorithms based on error counting, the effectiveness of these algorithms was compared by determining their retrieval performance for various experimental test collections. With this method, the performance of a stemmer is computed by counting the number of identifiable errors during the stemming of words from various text samples, thus making the evaluation independent of Information Retrieval. In order to implement the method it is necessary to group manually the words in each sample into disjoint sets of words holding the same semantic concept. One single word can belong to only one concept. In order to do this grouping automatically, in the present work this constraint has been generalized, allowing one word to belong to several different concepts. Results with the generalized method confirm those obtained by the non-generalized method, but show considerable less differences between three affix removal stemmers. For first time evaluated four letter successor variety stemmers, these appear to be slightly inferior with respect to the other three in terms of general accuracy (ERRT, error rate relative to truncation), but they are weight adjustable and, most important, need no linguistic knowledge about the language they are applied to.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {228–233},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_27,
author = {Ardila, Yoan Jos\'{e} Pinz\'{o}n and Clifford, Rapha\"{e}l and Mohamed, Manal},
title = {Necklace Swap Problem for Rhythmic Similarity Measures},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_27},
doi = {10.1007/11575832_27},
abstract = {Given two n-bit (cyclic) binary strings, A and B, represented on a circle (necklace instances). Let each sequence have the same number k of 1's. We are interested in computing the cyclic swap distance between A and B, i.e., the minimum number of swaps needed to convert A to B, minimized over all rotations of B. We show that this distance may be computed in O(k2).},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {234–245},
numpages = {12},
keywords = {cyclic strings, rhythmic/melodic similarity, repeated patterns, music retrieval, swap distance},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_28,
author = {Russo, Lu\'{\i}s M. S. and Oliveira, Arlindo L.},
title = {Faster Generation of Super Condensed Neighbourhoods Using Finite Automata},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_28},
doi = {10.1007/11575832_28},
abstract = {We present a new algorithm for generating super condensed neighbourhoods. Super condensed neighbourhoods have recently been presented as the minimal set of words that represent a pattern neighbourhood. These sets play an important role in the generation phase of hybrid algorithms for indexed approximate string matching. An existing algorithm for this purpose is based on a dynamic programming approach, implemented using bit-parallelism. In this work we present a bit-parallel algorithm based on automata which is faster, conceptually much simpler and uses less memory than the existing method.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {246–255},
numpages = {10},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_29,
author = {Hyyr\"{o}, Heikki},
title = {Restricted Transposition Invariant Approximate String Matching under Edit Distance},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_29},
doi = {10.1007/11575832_29},
abstract = {Let A and B be strings with lengths m and n, respectively, over a finite integer alphabet. Two classic string mathing problems are computing the edit distance between A and B, and searching for approximate occurrences of A inside B. We consider the classic Levenshtein distance, but the discussion is applicable also to indel distance. A relatively new variant [8] of string matching, motivated initially by the nature of string matching in music, is to allow transposition invariance for A. This means allowing A to be “shifted” by adding some fixed integer t to the values of all its characters: the underlying string matching task must then consider all possible values of t. M\"{a}kinen et al. [12,13] have recently proposed O(mn loglog m) and O(dn loglog m) algorithms for transposition invariant edit distance computation, where d is the transposition invariant distance between A and B, and an O(mn loglog m) algorithm for transposition invariant approximate string matching. In this paper we first propose a scheme to construct transposition invariant algorithms that depend on d or k. Then we proceed to give an O(n + d3) algorithm for transposition invariant edit distance, and an O(k2n) algorithm for transposition invariant approximate string matching.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {256–266},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_30,
author = {Mozgovoy, Maxim and Fredriksson, Kimmo and White, Daniel and Joy, Mike and Sutinen, Erkki},
title = {Fast Plagiarism Detection System},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_30},
doi = {10.1007/11575832_30},
abstract = {The large class sizes typical for an undergraduate programming course mean that it is nearly impossible for a human marker to accurately detect plagiarism, particularly if some attempt has been made to hide the copying. While it would be desirable to be able to detect all possible code transformations we believe that there is a minimum level of acceptable performance for the application of detecting student plagiarism. It would be useful if the detector operated at a level that meant for a piece of work to fool the algorithm would require that the student spent a large amount of time on the assignment and had a good enough understanding to do the work without plagiarising.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {267–270},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_31,
author = {Brini, Asma H. and Boughanem, Mohand and Dubois, Didier},
title = {A Model for Information Retrieval Based on Possibilistic Networks},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_31},
doi = {10.1007/11575832_31},
abstract = {This paper proposes a model for Information Retrieval (IR) based on possibilistic directed networks. Relations documents-terms and query-terms are modeled through possibility and necessity measures rather than a probability measure. The relevance value for the document given the query is measured by two degrees: the necessity and the possibility. More precisely, the user's query triggers a propagation process to retrieve necessarily or at least possibly relevant documents. The possibility degree is convenient to filter documents out from the response (retrieved documents) and the necessity degree is useful for document relevance confirmation. Separating these notions may account for the imprecision pervading the retrieval process. Moreover, an improved weighting of terms in a query not present in the document is introduced. Experiments carried out on a sub-collection of CLEF, namely LeMonde 1994, a French newspapers collection, showed the effectiveness of the model.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {271–282},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_32,
author = {Silva, Ilm\'{e}rio R. and Souza, Jo\~{a}o N. and Oliveira, Luciene C.},
title = {Comparison of Representations of Multiple Evidence Using a Functional Framework for IR},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_32},
doi = {10.1007/11575832_32},
abstract = {The combination of sources of evidence is an important subject of research in information retrieval and can be a good strategy for improving the quality of rankings. Another active research topic is modeling and is one of the central tasks in the development of information retrieval systems. In this paper, we analyze the combination of multiple evidence using a functional framework, presenting two case studies of the use of the framework to combine multiple evidence in contexts bayesian belief networks and in the vector space model. This framework is a meta-theory that represents IR models in a unique common language, allowing the representation, formulation and comparison of these models without the need to carry out experiments. We show that the combination of multiple evidence in the bayesian belief network can be carried at in of several ways, being that each form corresponds to a similarity function in the vector model. The analysis of this correspondence is made through the functional framework. We show that the framework allows us to design new models and helps designers to modify these models to extend them with new evidence sources.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {283–294},
numpages = {12},
keywords = {combination of multiple evidence, information retrieval models, functional framework},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_33,
author = {Elkan, Charles},
title = {Deriving TF-IDF as a Fisher Kernel},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_33},
doi = {10.1007/11575832_33},
abstract = {The Dirichlet compound multinomial (DCM) distribution has recently been shown to be a good model for documents because it captures the phenomenon of word burstiness, unlike standard models such as the multinomial distribution. This paper investigates the DCM Fisher kernel, a function for comparing documents derived from the DCM. We show that the DCM Fisher kernel has components that are similar to the term frequency (TF) and inverse document frequency (IDF) factors of the standard TF-IDF method for representing documents. Experiments show that the DCM Fisher kernel performs better than alternative kernels for nearest-neighbor document classification, but that the TF-IDF representation still performs best.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {295–300},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_34,
author = {Bergroth, Lasse},
title = {Utilizing Dynamically Updated Estimates in Solving the Longest Common Subsequence Problem},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_34},
doi = {10.1007/11575832_34},
abstract = {The running time of longest common subsequence (lcs) algorithms is shown to be dependent of several parameters. To such parameters belong e. g. the size of the input alphabet, the distribution of the characters in the input strings and the degree of similarity between the strings. Therefore it is very difficult to establish an lcs algorithm that could be efficient enough for all relevant problem instances. As a consequence of that fact, many of those algorithms are planned to be applied only on a restricted set of all possible inputs. Some of them are besides quite tricky to implement.In order to speed up the running time of lcs algorithms in common, one of the most crucial prerequisities is that preliminary information about the input strings could be utilized. In addition, this information should be available after a reasonably quick preprocessing phase. One informative a priori -value to calculate is a lower bound estimate for the length of the lcs. However, the obtained lower bound might not be as accurate as desired and thus no appreciable advantages of the preprocessing can be drawn.In this paper, a straightforward method for updating dynamically the lower bound value for the lcs is presented. The purpose is to refine the estimate gradually to prune more effectively the search space of the used exact lcs algorithm. Furthermore, simulation tests for the new presented method will be performed in order to convince us of the benefits of it.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {301–314},
numpages = {14},
keywords = {string algorithms, heuristic algorithms, longest common subsequence},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_35,
author = {Kim, Jin Wook and Amir, Amihood and Landau, Gad M. and Park, Kunsoo},
title = {Computing Similarity of Run-Length Encoded Strings with Affine Gap Penalty},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_35},
doi = {10.1007/11575832_35},
abstract = {The problem of computing similarity of two run-length encoded strings has been studied for various scoring metrics. Many algorithms have been developed for the longest common subsequence metric and some algorithms for the Levenshtein distance metric and the weighted edit distance metric. In this paper we consider similarity based on the affine gap penalty metric which is a more general and rather complicated scoring metric than the weighted edit distance. To compute similarity in this model efficiently, we convert the problem to a path problem on a directed acyclic graph and use some properties of maximum paths in this graph. We present an O(nm′+n′m) time algorithm for computing similarity of two run-length encoded strings in the affine gap penalty model, where n′ and m′ are the lengths of given two run-length encoded strings, and n and m are the decoded lengths of given two strings, respectively.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {315–326},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_36,
author = {Lipsky, Ohad and Porat, Ely},
title = {<i>L</i><sub>1</sub> Pattern Matching Lower Bound},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_36},
doi = {10.1007/11575832_36},
abstract = {Let a text string T = t0,...,tn−−1 and a pattern string P = p0,..., pm−−1ti, pj∈IN be given. In The Approximate Pattern Matching in the L1metric problem (L1-matching for short) the output is, for every text location i, the L1 distance between the pattern and the length m substring of the text starting at i, i.e. Σ$_{j=0}^{m-1}|{it t}_{i+{it j}}$ – pj | . The Less Than Matching problem is that of finding all locations i of T where t$_{i+{it j}}$ ≥ pjj = 0,..., m–1 . The String Matching with Mismatches problem is that of finding the number of mismatches between the pattern and every length m substring of the text. For the three above problems, the fastest known deterministic solution is $O(nsqrt{m{rm log}m})$ time.In this paper we show that the latter two problems can be linearly reduced to the problem of L1-matching.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {327–330},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_37,
author = {Lipsky, Ohad and Porat, Ely},
title = {Approximate Matching in the <i>L</i><sub>∞</sub> Metric},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_37},
doi = {10.1007/11575832_37},
abstract = {Let a text T=t0,...,tn−−1 and a pattern P=p0,..., pm−−1, strings of natural numbers, be given. In the Approximate Matching in the L∞metric problem the output is, for every text location i, the L∞ distance between the pattern and the length m substring of the text starting at i, i.e. Max$_{j=0}^{m--1}$|t$_{i+{it j}}$–pj|. We consider the Approximate k–L∞distance problem. Given text T and pattern P as before, and a natural number k the output of the problem is the L∞ distance of the pattern from the text only at locations i in the text where the distance is bounded by k. For the locations where the distance exceeds k the output is φ. We show an algorithm that solves this problem in O(n(k+log(min(m, |Σ|)))logm) time.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {331–334},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_38,
author = {Guignon, Valentin and Chauve, Cedric and Hamel, Sylvie},
title = {An Edit Distance between RNA Stem-Loops},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_38},
doi = {10.1007/11575832_38},
abstract = {We introduce the notion of conservative edit distance and mapping between two RNA stem-loops. We show that unlike the general edit distance between RNA secondary structures, the conservative edit distance can be computed in polynomial time and space, and we describe an algorithm for this problem. We show how this algorithm can be used in the more general problem of complete RNA secondary structures comparison.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {335–347},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_39,
author = {Allali, Julien and Sagot, Marie-France},
title = {A Multiple Graph Layers Model with Application to RNA Secondary Structures Comparison},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_39},
doi = {10.1007/11575832_39},
abstract = {We introduce a new data structure, called MiGaL for “Multiple Graph Layers”, that is composed of various graphs linked together by relations of abstraction/refinement. The new structure is useful for representing information that can be described at different levels of abstraction, each level corresponding to a graph. We then propose an algorithm for comparing two MiGaLs. The algorithm performs a step-by-step comparison starting with the most “abstract” level. The result of the comparison at a given step is communicated to the next step using a special colouring scheme. MiGaLs represent a very natural model for comparing RNA secondary structures that may be seen at different levels of detail, going from the sequence of nucleotides, single or paired with another to participate in a helix, to the network of multiple loops that is believed to represent the most conserved part of RNAs having similar function. We therefore show how to use MiGaLs to very efficiently compare two RNAs of any size at different levels of detail simultaneously.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {348–359},
numpages = {12},
keywords = {RNA, edit distance, secondary structure, graph layers, graph comparison},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_40,
author = {Backofen, Rolf and Hermelin, Danny and Landau, Gad M. and Weimann, Oren},
title = {Normalized Similarity of RNA Sequences},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_40},
doi = {10.1007/11575832_40},
abstract = {We introduce a normalized version of the LCS metric as a new local similarity measure for comparing two RNAs. An $mathcal{O}(n^{2}m{rm lg}m)$ time algorithm is presented for computing the maximum normalized score of two RNA sequences, where n and m are the lengths of the sequences and n ≤ m. This algorithm has the same time complexity as the currently best known global LCS algorithm.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {360–369},
numpages = {10},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_41,
author = {Valiente, Gabriel},
title = {A Fast Algorithmic Technique for Comparing Large Phylogenetic Trees},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_41},
doi = {10.1007/11575832_41},
abstract = {The comparison of rooted phylogenetic trees is essential to querying phylogenetic databases such as TreeBASE. Current comparison methods are based on either tree edit distances or common subtrees. However, a limitation of such methods is their inherent complexity. In this paper, a new distance over fully resolved phylogenetic trees, the transposition distance, is described which is based on a well-known bijection between perfect matchings and phylogenetic trees, and simple linear-time algorithms are presented for computing the new distance.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {370–375},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_42,
author = {Fredriksson, Kimmo and Grabowski, Szymon},
title = {Practical and Optimal String Matching},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_42},
doi = {10.1007/11575832_42},
abstract = {We develop a new exact bit-parallel string matching algorithm, based on the Shift-Or algorithm (Baeza-Yates &amp; Gonnet, 1992). Assuming that the pattern representation fits into a single computer word, this algorithm has optimal O(n logσm / m) average running time, as well as optimal O(n) worst case running time, where n, m and σ are the sizes of the text, the pattern, and the alphabet, respectively. We also study several implementation details. The experimental results show that our algorithm is the fastest in most of the cases where it can be applied, displacing even the long-standing BNDM (Navarro &amp; Raffinot, 2000) family of algorithms. Finally, we show how to adapt our techniques for the Shift-Add algorithm (Baeza-Yates &amp; Gonnet, 1992), obtaining optimal time for searching under Hamming distance.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {376–387},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_43,
author = {Tsuji, Hisashi and Ishino, Akira and Takeda, Masayuki},
title = {A Bit-Parallel Tree Matching Algorithm for Patterns with Horizontal VLDC's},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_43},
doi = {10.1007/11575832_43},
abstract = {The tree pattern matching problem is, given two labeled trees P and T, respectively called pattern tree and target tree, to find all occurrences of P within T. Many studies have been undertaken on this problem for both the cases of ordered and unordered trees. To realize flexible matching, a kind of variable-length-don't-care's (VLDC's) have been introduced. In particular, the path-VLDC's appear in XPath, a language for addressing parts of an XML document. In this paper, we introduce horizontal VLDC's, each matches a sequence of trees whose root nodes are consecutive siblings in ordered trees. We address the tree pattern matching problem for patterns with horizontal VLDC's. In our setting, the target tree is given as a tagged sequence such as XML data stream. We present an algorithm that solves the problem in O(mn) time using O(mh) space, where m and n are the sizes of P and T, respectively, and h is the height of T. We adopt the bit-parallel technique to obtain a practically fast algorithm.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {388–398},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

@inproceedings{10.1007/11575832_44,
author = {Liu, Ping and Liu, Yan-bing and Tan, Jian-long},
title = {A Partition-Based Efficient Algorithm for Large Scale Multiple-Strings Matching},
year = {2005},
isbn = {3540297405},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575832_44},
doi = {10.1007/11575832_44},
abstract = {Filtering plays an important role in the Internet security and information retrieval fields, and usually employs multiple-strings matching algorithm as its key part. All the classical matching algorithms, however, perform badly when the number of the keywords exceeds a critical point, which made large scale multiple-strings matching problem a great challenge. Based on the observation that the speed of the classical algorithms depends mainly on the length of the shortest keyword, a partition strategy was proposed to decompose the keywords set into a series of subsets on which the classical algorithms was performed. For the optimal partition, it was proved that the keywords with same length locate in one subset, and length of keywords in different subsets would not interlace each other. In this paper, we proposed a shortest-path model for the optimal partition finding problem. Experiments on both random and real data demonstrate that our algorithms generally has about a 100-300% speed-up compared with the classical ones.},
booktitle = {Proceedings of the 12th International Conference on String Processing and Information Retrieval},
pages = {399–404},
numpages = {6},
location = {Buenos Aires, Argentina},
series = {SPIRE'05}
}

