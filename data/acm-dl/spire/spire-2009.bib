@inproceedings{10.1007/978-3-642-03784-9_1,
author = {Gagie, Travis and Puglisi, Simon J. and Turpin, Andrew},
title = {Range Quantile Queries: Another Virtue of Wavelet Trees},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_1},
doi = {10.1007/978-3-642-03784-9_1},
abstract = {We show how to use a balanced wavelet tree as a data structure that stores a list of numbers and supports efficient <em>range quantile queries</em> . A range quantile query takes a rank and the endpoints of a sublist and returns the number with that rank in that sublist. For example, if the rank is half the sublist's length, then the query returns the sublist's median. We also show how these queries can be used to support space-efficient <em>coloured range reporting</em> and <em>document listing</em> .},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {1–6},
numpages = {6},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_2,
author = {Fukagawa, Daiji and Akutsu, Tatsuya and Takasu, Atsuhiro},
title = {Constant Factor Approximation of Edit Distance of Bounded Height Unordered Trees},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_2},
doi = {10.1007/978-3-642-03784-9_2},
abstract = {The edit distance problem on two unordered trees is known to be MAX SNP-hard. In this paper, we present an approximation algorithm whose approximation ratio is 2<em>h</em> + 2, where we consider unit cost edit operations and <em>h</em> is the maximum height of the two input trees. The algorithm is based on an embedding of unit cost tree edit distance into <em>L</em> 1 distance. We also present an efficient implementation of the algorithm using randomized dimension reduction.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {7–17},
numpages = {11},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_3,
author = {Brisaboa, Nieves R. and Ladra, Susana and Navarro, Gonzalo},
title = {K2-Trees for Compact Web Graph Representation},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_3},
doi = {10.1007/978-3-642-03784-9_3},
abstract = {This paper presents a Web graph representation based on a compact tree structure that takes advantage of large empty areas of the adjacency matrix of the graph. Our results show that our method is competitive with the best alternatives in the literature, offering a very good compression ratio (3.3---5.3 bits per link) while permitting fast navigation on the graph to obtain direct as well as reverse neighbors (2---15 microseconds per neighbor delivered). Moreover, it allows for extended functionality not usually considered in compressed graph representations.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {18–30},
numpages = {13},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_4,
author = {Lee, Taehyung and Na, Joong Chae and Park, Kunsoo},
title = {On-Line Construction of Parameterized Suffix Trees},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_4},
doi = {10.1007/978-3-642-03784-9_4},
abstract = {We consider on-line construction of a suffix tree for a parameterized string, where we always have the suffix tree of the input string read so far. This situation often arises from source code management systems where, for example, a source code repository is gradually increasing in its size as users commit new codes into the repository day by day. We present an on-line algorithm which constructs a parameterized suffix tree in randomized <em>O</em> (<em>n</em> ) time, where <em>n</em> is the length of the input string. Our algorithm is the first randomized linear time algorithm for the on-line construction problem.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {31–38},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_5,
author = {Tam, Alan and Wu, Edward and Lam, Tak-Wah and Yiu, Siu-Ming},
title = {Succinct Text Indexing with Wildcards},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_5},
doi = {10.1007/978-3-642-03784-9_5},
abstract = {A succinct text index uses space proportional to the text itself, say, two times <em>n</em> log<em>***</em> for a text of <em>n</em> characters over an alphabet of size <em>***</em> . In the past few years, there were several exciting results leading to succinct indexes that support efficient pattern matching. In this paper we present the first succinct index for a text that contains wildcards. The space complexity of our index is (3 + <em>o</em> (1))<em>n</em> log<em>***</em> + <em>O</em> (***log<em>n</em> ) bits, where *** is the number of wildcard groups in the text. Such an index finds applications in indexing genomic sequences that contain single-nucleotide polymorphisms (SNP), which could be modeled as wildcards.In the course of deriving the above result, we also obtain an alternate succinct index of a set of <em>d</em> patterns for the purpose of dictionary matching. When compared with the succinct index in the literature, the new index doubles the size (precisely, from <em>n</em> log<em>***</em> to 2 <em>n</em> log<em>***</em> , where <em>n</em> is the total length of all patterns), yet it reduces the matching time to <em>O</em> (<em>m</em> log<em>***</em> + <em>m</em> log<em>d</em> + <em>occ</em> ), where <em>m</em> is the length of the query text. It is worth-mentioning that the time complexity no longer depends on the total dictionary size.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {39–50},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_6,
author = {Ohlebusch, Enno and Gog, Simon},
title = {A Compressed Enhanced Suffix Array Supporting Fast String Matching},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_6},
doi = {10.1007/978-3-642-03784-9_6},
abstract = {Index structures like the suffix tree or the suffix array are of utmost importance in stringology, most notably in exact string matching. In the last decade, research on compressed index structures has flourished because the main problem in many applications is the space consumption of the index. It is possible to simulate the matching of a pattern against a suffix tree on an enhanced suffix array by using range minimum queries or the so-called <em>child table</em> . In this paper, we show that the Super-Cartesian tree of the LCP-array (with which the suffix array is enhanced) very naturally explains the child table. More important, however, is the fact that the balanced parentheses representation of this tree constitutes a very natural compressed form of the child table which admits to locate all <em>occ</em> occurrences of pattern <em>P</em> of length <em>m</em> in <em>O</em> (<em>m</em> log|Σ| + <em>occ</em> ) time, where Σ is the underlying alphabet. Our compressed child table uses less space than previous solutions to the problem. An implementation is available.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {51–62},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_7,
author = {Sir\'{e}n, Jouni},
title = {Compressed Suffix Arrays for Massive Data},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_7},
doi = {10.1007/978-3-642-03784-9_7},
abstract = {We present a fast space-efficient algorithm for constructing compressed suffix arrays (CSA). The algorithm requires <em>O</em> (<em>n</em> log<em>n</em> ) time in the worst case, and only <em>O</em> (<em>n</em> ) bits of extra space in addition to the CSA. As the basic step, we describe an algorithm for merging two CSAs. We show that the construction algorithm can be parallelized in a symmetric multiprocessor system, and discuss the possibility of a distributed implementation. We also describe a parallel implementation of the algorithm, capable of indexing several gigabytes per hour.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {63–74},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_8,
author = {Hon, Wing-Kai and Shah, Rahul and Thankachan, Sharma V. and Vitter, Jeffrey Scott},
title = {On Entropy-Compressed Text Indexing in External Memory},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_8},
doi = {10.1007/978-3-642-03784-9_8},
abstract = {A new trend in the field of pattern matching is to design indexing data structures which take space very close to that required by the indexed text (in entropy-compressed form) and also simultaneously achieve good query performance. Two popular indexes, namely the FM-index [Ferragina and Manzini, 2005] and the CSA [Grossi and Vitter 2005], achieve this goal by exploiting the Burrows-Wheeler transform (BWT) [Burrows and Wheeler, 1994]. However, due to the intricate permutation structure of BWT, no locality of reference can be guaranteed when we perform pattern matching with these indexes. Chien <em>et al.</em> [2008] gave an alternative text index which is based on sparsifying the traditional suffix tree and maintaining an auxiliary 2-D range query structure. Given a text <em>T</em> of length <em>n</em> drawn from a <em>***</em> -sized alphabet set, they achieved <em>O</em> (<em>n</em> log<em>***</em> )-bit index for <em>T</em> and showed that this index can preserve locality in pattern matching and hence is amenable to be used in external-memory settings. We improve upon this index and show how to apply entropy compression to reduce index space. Our index takes <em>O</em> (<em>n</em> (<em>H</em>  <em>k</em>  + 1)) + <em>o</em> (<em>n</em> log<em>***</em> ) bits of space where <em>H</em>  <em>k</em>  is the <em>k</em> th-order empirical entropy of the text. This is achieved by creating variable length blocks of text using arithmetic coding.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {75–89},
numpages = {15},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_9,
author = {Okanohara, Daisuke and Sadakane, Kunihiko},
title = {A Linear-Time Burrows-Wheeler Transform Using Induced Sorting},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_9},
doi = {10.1007/978-3-642-03784-9_9},
abstract = {To compute Burrows-Wheeler Transform (BWT), one usually builds a suffix array (SA) first, and then obtains BWT using SA, which requires much redundant working space. In previous studies to compute BWT directly [5,12], one constructs BWT incrementally, which requires O(<em>n</em> log<em>n</em> ) time where <em>n</em> is the length of the input text. We present an algorithm for computing BWT directly in linear time by modifying the suffix array construction algorithm based on induced sorting [15]. We show that the working space is O(<em>n</em> log<em>***</em> loglog <em>***</em>  <em>n</em> ) for any <em>***</em> where <em>***</em> is the alphabet size, which is the smallest among the known linear time algorithms.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {90–101},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_10,
author = {Inagaki, Kazumasa and Tomizawa, Yoshihiro and Yokoo, Hidetoshi},
title = {Novel and Generalized Sort-Based Transform for Lossless Data Compression},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_10},
doi = {10.1007/978-3-642-03784-9_10},
abstract = {We propose a new sort-based transform for lossless data compression that can replace the BWT transform in the block-sorting data compression algorithm. The proposed transform is a parametric generalization of the BWT and the RadixZip transform proposed by Vo and Manku (VLDB, 2008), which is a rather new variation of the BWT. For a class of parameters, the transform can be performed in time linear in the data length. We give an asymptotic compression bound attained by our algorithm.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {102–113},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_11,
author = {Adiego, Joaqu\'{\i}n and Brisaboa, Nieves R. and Mart\'{\i}nez-Prieto, Miguel A. and S\'{a}nchez-Mart\'{\i}nez, Felipe},
title = {A Two-Level Structure for Compressing Aligned Bitexts},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_11},
doi = {10.1007/978-3-642-03784-9_11},
abstract = {A <em>bitext</em> , or <em>bilingual parallel corpus</em> , consists of two texts, each one in a different language, that are mutual translations. Bitexts are very useful in linguistic engineering because they are used as source of knowledge for different purposes. In this paper we propose a strategy to efficiently compress and use bitexts, saving, not only space, but also processing time when exploiting them. Our strategy is based on a two-level structure for the vocabularies, and on the use of <em>biwords</em> , a pair of associated words, one from each language, as basic symbols to be encoded with an <emphasis type="SmallCaps">ETDC</emphasis> [2] compressor. The resulting compressed bitext needs around 20% of the space and allows more efficient implementations of the different types of searches and operations that linguistic engineerings need to perform on them. In this paper we discuss and provide results for compression, decompression, different types of searches, and bilingual snippets extraction.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {114–121},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_12,
author = {Brisaboa, Nieves R. and Ladra, Susana and Navarro, Gonzalo},
title = {Directly Addressable Variable-Length Codes},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_12},
doi = {10.1007/978-3-642-03784-9_12},
abstract = {We introduce a symbol reordering technique that implicitly synchronizes variable-length codes, such that it is possible to directly access the <em>i</em> -th codeword without need of any sampling method. The technique is practical and has many applications to the representation of ordered sets, sparse bitmaps, partial sums, and compressed data structures for suffix trees, arrays, and inverted indexes, to name just a few. We show experimentally that the technique offers a competitive alternative to other data structures that handle this problem.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {122–130},
numpages = {9},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_13,
author = {Mendoza, Marcelo and Zamora, Juan},
title = {Identifying the Intent of a User Query Using Support Vector Machines},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_13},
doi = {10.1007/978-3-642-03784-9_13},
abstract = {In this paper we introduce a high-precision query classification method to identify the intent of a user query given that it has been seen in the past based on informational, navigational, and transactional categorization. We propose using three vector representations of queries which, using support vector machines, allow past queries to be classified by user's intents. The queries have been represented as vectors using two factors drawn from click-through data: the time users take to review the documents they select and the popularity (quantity of preferences) of the selected documents. Experimental results show that time is the factor that yields higher precision in classification. The experiments shown in this work illustrate that the proposed classifiers can effectively identify the intent of past queries with high-precision.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {131–142},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_14,
author = {Balasubramanian, Niranjan and Allan, James},
title = {Syntactic Query Models for Restatement Retrieval},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_14},
doi = {10.1007/978-3-642-03784-9_14},
abstract = {We consider the problem of retrieving sentence level restatements. Formally, we define restatements as sentences that contain all or some subset of information present in a query sentence. Identifying restatements is useful for several applications such as multi-document summarization, document provenance, text reuse and novelty detection. Spurious partial matches and term dependence become important issues for restatement retrieval in these settings. To address these issues, we focus on query models that capture relative term importance and sequential term dependence. In this paper, we build query models using syntactic information such as subject-verb-objects and phrases. Our experimental results on two different collections show that syntactic query models are consistently more effective than purely statistical alternatives.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {143–155},
numpages = {13},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_15,
author = {Craveiro, Olga and Macedo, Joaquim and Madeira, Henrique},
title = {Use of Co-Occurrences for Temporal Expressions Annotation},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_15},
doi = {10.1007/978-3-642-03784-9_15},
abstract = {The annotation or extraction of temporal information from text documents is becoming increasingly important in many natural language processing applications such as text summarization, information retrieval, question answering, etc.. This paper presents an original method for easy recognition of temporal expressions in text documents. The method creates semantically classified temporal patterns, using word co-occurrences obtained from training corpora and a pre-defined seed keywords set, derived from the used language temporal references. A participation on a Portuguese named entity evaluation contest showed promising effectiveness and efficiency results. This approach can be adapted to recognize other type of expressions or languages, within other contexts, by defining the suitable word sets and training corpora.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {156–164},
numpages = {9},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_16,
author = {Geraldo, Andr\'{e} Pinto and Moreira, Viviane P. and Gon\c{c}alves, Marcos A.},
title = {On-Demand Associative Cross-Language Information Retrieval},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_16},
doi = {10.1007/978-3-642-03784-9_16},
abstract = {This paper proposes the use of algorithms for mining association rules as an approach for Cross-Language Information Retrieval. These algorithms have been widely used to analyse market basket data. The idea is to map the problem of finding associations between sales items to the problem of finding term translations over a parallel corpus. The proposal was validated by means of experiments using queries in two distinct languages: Portuguese and Finnish to retrieve documents in English. The results show that the performance of our proposed approach is comparable to the performance of the monolingual baseline and to query translation via machine translation, even though these systems employ more complex Natural Language Processing techniques. The combination between machine translation and our approach yielded the best results, even outperforming the monolingual baseline.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {165–173},
numpages = {9},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_17,
author = {Adsett, Connie R. and Marchand, Yannick},
title = {A Comparison of Data-Driven Automatic Syllabification Methods},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_17},
doi = {10.1007/978-3-642-03784-9_17},
abstract = {Although automatic syllabification is an important component in several natural language tasks, little has been done to compare the results of data-driven methods on a wide range of languages. This article compares the results of five data-driven syllabification algorithms (Hidden Markov Support Vector Machines, IB1, Liang's algorithm, the Look Up Procedure, and Syllabification by Analogy) on nine European languages in order to determine which algorithm performs best over all. Findings show that all algorithms achieve a mean word accuracy across all lexicons of over 90%. However, Syllabification by Analogy performs better than the other algorithms tested with a mean word accuracy of 96.84% (standard deviation of 2.93) whereas Liang's algorithm, the standard for hyphenation (used in $mboxTeX$), produces the second best results with a mean of 95.67% (standard deviation of 5.70).},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {174–181},
numpages = {8},
keywords = {machine learning, automatic syllabification, Natural language processing},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_18,
author = {Hon, Wing-Kai and Shah, Rahul and Wu, Shih-Bin},
title = {Efficient Index for Retrieving Top-k Most Frequent Documents},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_18},
doi = {10.1007/978-3-642-03784-9_18},
abstract = {In the <em>document retrieval problem</em> [9], we are given a collection of documents (strings) of total length <em>D</em> in advance, and our target is to create an index for these documents such that for any subsequent input pattern <em>P</em> , we can identify which documents in the collection contain <em>P</em> . In this paper, we study a natural extension to the above document retrieval problem. We call this <em>top-k</em> <em> frequent document retrieval</em> , where instead of listing all documents containing <em>P</em> , our focus is to identify the top <em>k</em> documents having most occurrences of <em>P</em> . This problem forms a basis for search engine tasks of retrieving documents ranked with TFIDF metric.A related problem was studied by [9] where the emphasis was on retrieving all the documents whose number of occurrences of the pattern <em>P</em> exceeds some frequency threshold <em>f</em> . However, from the information retrieval point of view, it is hard for a user to specify such a threshold value <em>f</em> and have a sense of how many documents will be outputted. We develop some additional building blocks which help the user overcome this limitation. These are used to derive an efficient index for top-<em>k</em> frequent document retrieval problem, answering queries in <em>O</em> (<em>P</em> + log<em>D</em> loglog<em>D</em> + <em>k</em> ) time and taking <em>O</em> (<em>D</em> log<em>D</em> ) space. Our approach is based on novel use of the suffix tree called <em>induced generalized suffix tree</em> (IGST).},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {182–193},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_19,
author = {Celikik, Marjan and Bast, Hannah},
title = {Fast Single-Pass Construction of a Half-Inverted Index},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_19},
doi = {10.1007/978-3-642-03784-9_19},
abstract = {We show how a half-inverted index can be constructed twice as fast as an ordinary inverted index. As shown in a series of recent works, the half-inverted index enables very fast prefix search, which in turn is the basis for very fast processing of many other types of advanced queries. Our construction algorithm is truly single-pass in that every posting (word occurrence) is touched (read and written) only once in the whole construction by avoiding an expensive merge of the index. The algorithm has been carefully engineered, with special attention paid to cache-efficiency and disk cost. We compared our algorithm against the state-of-the-art index construction from Zettair.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {194–205},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_20,
author = {Feuerstein, Esteban and Marin, Mauricio and Mizrahi, Michel and Gil-Costa, Veronica and Baeza-Yates, Ricardo},
title = {Two-Dimensional Distributed Inverted Files},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_20},
doi = {10.1007/978-3-642-03784-9_20},
abstract = {Term-partitioned indexes are generally inefficient for the evaluation of conjunctive queries, as they require the communication of long posting lists. On the other side, document-partitioned indexes incur in excessive overheads as the evaluation of every query involves the participation of all the processors, therefore their scalability is not adequate for real systems. We propose to arrange a set of processors in a two-dimensional array, applying term-partitioning at row level and document-partitioning at column level. Choosing the adequate number of rows and columns given the available number of processors, together with the selection of the proper ways of partitioning the index over that topology is the subject of this paper.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {206–213},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_21,
author = {Navarro, Gonzalo and Salmela, Leena},
title = {Indexing Variable Length Substrings for Exact and Approximate Matching},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_21},
doi = {10.1007/978-3-642-03784-9_21},
abstract = {We introduce two new index structures based on the <em>q</em> -gram index. The new structures index substrings of variable length instead of <em>q</em> -grams of fixed length. For both of the new indexes, we present a method based on the suffix tree to efficiently choose the indexed substrings so that each of them occurs almost equally frequently in the text. Our experiments show that the resulting indexes are up to 40% faster than the <em>q</em> -gram index when they use the same space.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {214–221},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_22,
author = {Pizzi, Cinzia and Bianco, Mauro},
title = {Expectation of Strings with Mismatches under Markov Chain Distribution},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_22},
doi = {10.1007/978-3-642-03784-9_22},
abstract = {We study a problem related to the extraction of over-represented words from a given source text <em>x</em> , of length <em>n</em> . The words are allowed to occur with <em>k</em> mismatches, and <em>x</em> is produced by a source over an alphabet Σ according to a Markov chain of order <em>p</em> . We propose an online algorithm to compute the expected number of occurrences of a word <em>y</em> of length <em>m</em> in <em>O</em> (<em>mk</em> |Σ| <em>p</em> + 1). We also propose an offline algorithm to compute the probability of any word that occurs in the text in <em>O</em> (<em>k</em> |Σ|2) after <em>O</em> (<em>nk</em> |Σ| <em>p</em> + 1) pre-processing. This algorithm allows us to compute the expectation for all the words in a text of length <em>n</em> in <em>O</em> (<em>kn</em> 2|Σ|2 + <em>nk</em> |Σ| <em>p</em> + 1), rather than in <em>O</em> (<em>n</em> 3 |Σ| <em>p</em> + 1) that can be obtained with other methods. Although this study was motivated by the motif discovery problem in bioinformatics, the results find their applications in any other domain involving combinatorics on words.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {222–233},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_23,
author = {Amir, Amihood and Landau, Gad M. and Na, Joong Chae and Park, Heejin and Park, Kunsoo and Sim, Jeong Seop},
title = {Consensus Optimizing Both Distance Sum and Radius},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_23},
doi = {10.1007/978-3-642-03784-9_23},
abstract = {The consensus string problem is finding a representative string (consensus) of a given set $mathbb{S}$ of strings. In this paper we deal with the consensus string problems optimizing both distance sum and radius, where the distance sum is the sum of (Hamming) distances from the strings in $mathbb{S}$ to the consensus and the radius is the longest (Hamming) distance from the strings in $mathbb{S}$ to the consensus. Although there have been results considering either distance sum or radius, there have been no results considering both as far as we know.We present two algorithms to solve the consensus string problems optimizing both distance sum and radius for three strings. The first algorithm finds the optimal consensus string that minimizes both distance sum and radius, and the second algorithm finds the bounded consensus string such that, given constants <em>s</em> and <em>r</em> , the distance sum is at most <em>s</em> and the radius is at most <em>r</em> . Both algorithms take linear time.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {234–242},
numpages = {9},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_24,
author = {Boucher, Christina},
title = {Faster Algorithms for Sampling and Counting Biological Sequences},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_24},
doi = {10.1007/978-3-642-03784-9_24},
abstract = {A set of sequences <em>S</em> is <em>pairwise bounded</em> if the Hamming distance between any pair of sequences in <em>S</em> is at most 2<em>d</em> . The <emphasis type="SmallCaps">Consensus Sequence</emphasis> problem aims to discern between pairwise bounded sets that have a consensus, and if so, finding one such sequence <em>s</em> *, and those that do not. This problem is closely related to the motif-recognition problem, which abstractly models finding important subsequences in biological data. We give an efficient algorithm for sampling pairwise bounded sets, referred to as MarkovSampling, and show it generates pairwise bounded sets uniformly at random. We illustrate the applicability of MarkovSampling to efficiently solving motif-recognition instances. Computing the expected number of motif sets has been a long-standing open problem in motif-recognition [1,3]. We consider the related problem of counting the number of pairwise bounded sets, give new bounds on number of pairwise bounded sets, and present an algorithmic approach to counting the number of pairwise bounded sets.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {243–253},
numpages = {11},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_25,
author = {Amir, Amihood and Parienty, Haim},
title = {Towards a Theory of Patches},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_25},
doi = {10.1007/978-3-642-03784-9_25},
abstract = {Many applications have a need for indexing unstructured data. It turns out that a similar ad-hoc method is being used in many of them - that of considering small particles of the data.In this paper we formalize this concept as a tiling problem and consider the efficiency of dealing with this model. We present an efficient algorithm for the one dimension tiling problem, and prove the two dimension problem is hard. We then develop an approximation algorithm with an approximation ratio converging to 2. We show that the "one-and-a-half" dimensional version of the problem is also hard.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {254–265},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_26,
author = {Feigenblat, Guy and Itzhaki, Ofra and Porat, Ely},
title = {The Frequent Items Problem, under Polynomial Decay, in the Streaming Model},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_26},
doi = {10.1007/978-3-642-03784-9_26},
abstract = {We consider the problem of estimating the frequency count of data stream elements under polynomial decay functions. In these settings every element arrives in the stream is assigned with a time decreasing weight, using a non increasing polynomial function. Decay functions are used in applications where older data is less significant  interesting  reliable than recent data. We propose 3 poly-logarithmic algorithms for the problem. The first one, deterministic, uses $ O (frac{1}{epsilon ^{2}} log N ( log log N + log U) ) $ bits. The second one, probabilistic, uses $O ( frac{1}{epsilon ^{2}} log frac{1}{epsilon delta} log N )$ bits and the third one, deterministic in the stochastic model, uses $O(frac{1}{epsilon ^{2}} log N)$ bits. In addition we show that using additional additive error can improve, in some cases, the space bounds. This variant of the problem is important and has many applications. To our knowledge it was never studied before.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {266–276},
numpages = {11},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_27,
author = {Gotthilf, Zvi and Lewenstein, Moshe},
title = {Improved Approximation Results on the Shortest Common Supersequence Problem},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_27},
doi = {10.1007/978-3-642-03784-9_27},
abstract = {The problem of finding the Shortest Common Supersequence (<em>SCS</em> ) of an arbitrary number of input strings is a well-studied problem. Given a set <em>L</em> of <em>k</em> strings, <em>s</em> 1 , <em>s</em> 2 , ..., <em>s</em>  <em>k</em>  , over an alphabet Σ, we say that their <em>SCS</em> is the shortest string that contains each of the input strings as a subsequence. The problem is known to be <em>NP-hard</em> [8] even over binary alphabet [12]. In this paper we focus on approximating two <em>NP-hard</em> variants of the <em>SCS</em> problem. For the first variant, where all input strings are of length 2, we present a $2 - frac {2}{1 + log{n}log{log{n}}}$ approximation algorithm, where |Σ| = <em>n</em> . This result immediately improves the $2 - frac {4}{n+1}$ approximation algorithm presented in [17]. Moreover, we present a $frac{7}{6}$ ($approx 1.166bar{6}$) approximation algorithm for the restricted variant (but still <em>NP-hard</em> ) where all input strings are of length 2 and every character in Σ has at most 3 occurrences in <em>L</em> .},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {277–284},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_28,
author = {Shiftan, Ariel and Porat, Ely},
title = {Set Intersection and Sequence Matching},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_28},
doi = {10.1007/978-3-642-03784-9_28},
abstract = {In the classical pattern matching problem, one is given a text and a pattern, both of which are sequences of letters, and is required to find all occurrences of the pattern in the text. We study two modifications of the classical problem, where each letter in the text and pattern is a set (<em>Set Intersection Matching</em> problem) or a sequence (<em>Sequence Matching</em> problem). Two "letters" are considered to be match if the intersection of the two corresponding sets is not empty, or if the two sequences have a common element in the same index. We show the first known non-trivial and efficient algorithms for these problems, for the case the maximum set/sequence size is small. The first, randomized, that takes $Thetaleft( 2^dnln nlog mright)$ time, where <em>d</em> is the maximum set/sequence size, and can also fit, with slight modifications, for the case one is also interested in up to <em>k</em> mismatches. The second is deterministic and takes $Thetaleft( 4^{d}nlog mright)$. The third algorithm, also deterministic, is able to count the number of matches at each index of the text in total running time $Thetaleft( sum_{i=1}^{d} {|Sigma| choose i} nlog m right)$.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {285–294},
numpages = {10},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_29,
author = {Clifford, Raphael and Harrow, Aram W. and Popa, Alexandru and Sach, Benjamin},
title = {Generalised Matching},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_29},
doi = {10.1007/978-3-642-03784-9_29},
abstract = {Given a pattern <em>p</em> over an alphabet Σ <em>p</em>  and a text <em>t</em> over an alphabet Σ <em>t</em>  , we consider the problem of determining a mapping <em>f</em> from Σ <em>p</em>  to ${Sigma}_{t}^{+}$ such that <em>t</em> = <em>f</em> (<em>p</em> 1 )<em>f</em> (<em>p</em> 2 )...<em>f</em> (<em>p</em>  <em>m</em>  ). This class of problems, which was first introduced by Amir and Nor in 2004, is defined by different constraints on the mapping <em>f</em> . We give NP-Completeness results for a wide range of conditions. These include when <em>f</em> is either many-to-one or one-to-one, when Σ <em>t</em>  is binary and when the range of <em>f</em> is limited to strings of constant length. We then introduce a related problem we term <em>pattern matching with string classes</em> which we show to be solvable efficiently. Finally, we discuss an optimisation variant of generalised matching and give a polynomial-time min $(1,sqrt{k/OPT})$-approximation algorithm for fixed <em>k</em> .},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {295–301},
numpages = {7},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_30,
author = {Ilie, Lucian and Tinta, Liviu},
title = {Practical Algorithms for the Longest Common Extension Problem},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_30},
doi = {10.1007/978-3-642-03784-9_30},
abstract = {The <em>Longest Common Extension</em> problem considers a string <em>s</em> and computes, for each of a number of pairs (<em>i</em> ,<em>j</em> ), the longest substring of <em>s</em> that starts at both <em>i</em> and <em>j</em> . It appears as a subproblem in many fundamental string problems and can be solved by linear-time preprocessing of the string that allows (worst-case) constant-time computation for each pair. The two known approaches use powerful algorithms: either constant-time computation of the Lowest Common Ancestor in trees or constant-time computation of Range Minimum Queries (RMQ) in arrays. We show here that, from practical point of view, such complicated approaches are not needed. We give two very simple algorithms for this problem that require no preprocessing. The first needs only the string and is significantly faster than all previous algorithms on the average. The second combines the first with a direct RMQ computation on the Longest Common Prefix array. It takes advantage of the superior speed of the cache memory and is the fastest on virtually all inputs.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {302–309},
numpages = {8},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_31,
author = {Ferrarotti, Flavio and Marin, Mauricio and Mendoza, Marcelo},
title = {A Last-Resort Semantic Cache for Web Queries},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_31},
doi = {10.1007/978-3-642-03784-9_31},
abstract = {We propose a method to evaluate queries using a last-resort semantic cache in a distributed Web search engine. The cache stores a group of frequent queries and for each of these queries it keeps minimal data, that is, the list of machines that produced their answers. The method for evaluating the queries uses the inverse frequency of the terms in the queries stored in the cache (<literal>Idf</literal> ) to determine when the results recovered from the cache are a good approximation to the exact answer set. Experiments show that the method is effective and efficient.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {310–321},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_32,
author = {Sushmita, Shanu and Joho, Hideo and Lalmas, Mounia},
title = {A Task-Based Evaluation of an Aggregated Search Interface},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_32},
doi = {10.1007/978-3-642-03784-9_32},
abstract = {This paper presents a user study that evaluated the effectiveness of an aggregated search interface in the context of non-navigational search tasks. An experimental system was developed to present search results aggregated from multiple information sources, and compared to a conventional tabbed interface. Sixteen participants were recruited to evaluate the performance of the two interfaces. Our results suggest that the aggregated search interface is a promising way of supporting non-navigational search tasks. The quantity and diversity of the retrieved items which participants accessed to complete a task, increased in the aggregated interface. Participants also found the aggregated presentation easier to access to retrieved items and to find relevant information, compared to the conventional interface.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {322–333},
numpages = {12},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_33,
author = {Magdy, Walid and Darwish, Kareem and El-Saban, Motaz},
title = {Efficient Language-Independent Retrieval of Printed Documents without OCR},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_33},
doi = {10.1007/978-3-642-03784-9_33},
abstract = {Recent book digitization initiatives have facilitated the access and search of millions of books. Although OCR remains essential for retrieving printed documents, OCR engines remain limited in the languages they handle and are generally expensive to build. This paper proposes a language independent approach that enables search through printed documents in a way that combines image-based matching with conventional IR techniques without using OCR. While image-based matching can be effective in finding similar words, complementing it with efficient retrieval techniques allows for sub-word matching, term weighting, and document ranking. The basic idea is that similar connected elements in printed documents are clustered and represented with ID's, which are then used to generate equivalent textual representations. The resultant representations are indexed using an IR engine and searched using the equivalent ID's of the connected elements in queries. Though, the main benefit of the proposed approach lies in languages for which no OCR exists, the technique was tested on English and Arabic to ascertain the relative effectiveness of the approach. The approach achieves more than 61% relative effectiveness compared to using OCR for both languages. While the reported numbers are lower than that of OCR-based approaches, the proposed method is fully automated, does not require any supervised training, and allows documents to be searchable within a few hours.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {334–343},
numpages = {10},
keywords = {OCR, Printed Documents Retrieval, Image Based Retrieval},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

@inproceedings{10.1007/978-3-642-03784-9_34,
author = {Bachrach, Yoram and Herbrich, Ralf and Porat, Ely},
title = {Sketching Algorithms for Approximating Rank Correlations in Collaborative Filtering Systems},
year = {2009},
isbn = {9783642037832},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03784-9_34},
doi = {10.1007/978-3-642-03784-9_34},
abstract = {Collaborative filtering (CF) shares information between users to provide each with recommendations. Previous work suggests using sketching techniques to handle massive data sets in CF systems, but only allows testing whether users have a high proportion of items they have both ranked. We show how to determine the <em>correlation</em> between the rankings of two users, using concise "sketches" of the rankings. The sketches allow approximating Kendall's Tau, a known rank correlation, with high accuracy <em>***</em> and high confidence 1 *** <em>***</em> . The required sketch size is logarithmic in the confidence and polynomial in the accuracy.},
booktitle = {Proceedings of the 16th International Symposium on String Processing and Information Retrieval},
pages = {344–352},
numpages = {9},
location = {Saariselk\"{a}, Finland},
series = {SPIRE '09}
}

