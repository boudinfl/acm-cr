@inproceedings{10.5555/2051073.2051074,
author = {Demaine, Erik D.},
title = {Constructing Strings at the Nano Scale via Staged Self-Assembly},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Tile self-assembly is an intriguing approach to manufacturing desired shapes with nano-scale feature size. A recent direction in this theory allows the use of multiple stages--operations performed by the experimenter, such as mixing two self-assembling systems together. This flexibility transforms the experimenter from a passive entity into a parallel algorithm, and vastly reduces the number of distinct parts required to construct a desired shape, possibly making the systems practical to build.We start with the relatively simple goal of constructing 1D strings of tiles with distinctive markers, while minimizing the number of mixing steps (work) performed by the staged assembly. In the practical situation of few different "glues", this problem turns out to be closely related to compressing a string into a context-free grammar with the fewest nonterminals. In general, however, the problems turn out to be quite different, as the implicit parallelism of a mixing operation can be exploited to reduce the number of steps.The staged-assembly perspective also enables the possibility of additional operations, such as adding an enzyme that destroys all tiles with a special label. By enabling destruction in addition to the usual construction, we can perform tasks impossible in a traditional self-assembly system. For example, we can build a Replicator, which transforms a given object of unknown shape or size into many copies of that shape; and we find a vastly more efficient way to construct a nano computer through self-assembly.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {1},
numpages = {1},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051075,
author = {Dupret, Georges},
title = {Discounted Cumulative Gain and User Decision Models},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose to explain Discounted Cumulative Gain (DCG) as the consequences of a set of hypothesis, in a generative probabilistic model, on how users browse the result page ranking list of a search engine. This exercise of reconstructing a user model from a metric allows us to show that it is possible to estimate from data the numerical values of the discounting factors. It also allows us to compare different candidate user models in terms of their ability to describe the observed data, and hence to select the best one. It is generally not possible to relate the performance of a ranking function in terms of DCG with the clicks observed after the function is deployed on a production environment. We show in this paper that a user model make this possible. Finally, we show that DCG can be interpreted as a measure of the utility a user gains per unit of effort she is ready to allocate. This contrasts nicely with a recent interpretation given to average precision (AP), another popular Information Retrieval metric, as a measure of effort needed to achieve a unit of utility [7].},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {2–13},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051076,
author = {Yahyaei, Sirvan and Bonzanini, Marco and Roelleke, Thomas},
title = {Cross-Lingual Text Fragment Alignment Using Divergence from Randomness},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes an approach to automatically align fragments of texts of two documents in different languages. A text fragment is a list of continuous sentences and an aligned pair of fragments consists of two fragments in two documents, which are content-wise related. Cross-lingual similarity between fragments of texts is estimated based on models of divergence from randomness. A set of aligned fragments based on the similarity scores are selected to provide an alignment between sections of the two documents. Similarity measures based on divergence show strong performance in the context of cross-lingual fragment alignment in the performed experiments.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {14–25},
numpages = {12},
keywords = {fragment alignment, divergence from randomness, summarisation},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051077,
author = {Alonso, Omar and Gertz, Michael and Baeza-Yates, Ricardo},
title = {Enhancing Document Snippets Using Temporal Information},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we propose an algorithm to enhance the quality of document snippets shown in a search engine by using temporal expressions. We evaluate our proposal in a subset of theWikipedia corpus using crowdsourcing, showing that snippets that have temporal information are preferred by the users.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {26–31},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051078,
author = {Egidi, Lavinia and Manzini, Giovanni},
title = {Spaced Seeds Design Using Perfect Rulers},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We consider the problem of lossless spaced seed design for approximate pattern matching. We show that, using mathematical objects known as perfect rulers, we can derive a family of spaced seeds for matching with up to two errors. We analyze these seeds with respect to the trade-off they offer between seed weight and the minimum length of the pattern to be matched. We prove that for patterns of length up to a few hundreds our seeds have a larger weight, hence a better filtration efficiency, than the ones known in the literature. In this context, we study in depth the specific case of Wichmann rulers and prove some preliminary results on the generalization of our approach to the larger class of unrestricted rulers.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {32–43},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051079,
author = {Amir, Amihood and Gotthilf, Zvi and Shalom, B. Riva},
title = {Weighted Shortest Common Supersequence},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Shortest Common Supersequence (SCS) is the problem of seeking a shortest possible sequence that contains each of the input sequences as a subsequence. In this paper we consider applying the problem to Position Weight Matrices (PWM). The Position Weight Matrix was introduced as a tool to handle a set of sequences that are not identical, yet, have many local similarities. Such a weighted sequence is a 'statistical image' of this set where we are given the probability of every symbol's occurrence at every text location. We consider two possible definitions of SCS on PWM. For the first, we give a polynomial time algorithm, having two input sequences. For the second, we prove NP-hardness.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {44–54},
numpages = {11},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051080,
author = {Belazzougui, Djamal and Raffinot, Mathieu},
title = {Approximate Regular Expression Matching with Multi-Strings},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we are interested in solving the approximate regular expression matching problem: we are given a regular expression R in advance and we wish to answer the following query: given a text T and a parameter k, find all the substrings of T which match the regular expression R with at most k errors (an error consist in deleting inserting, or substituting a character). There exists a well known solution for this problem in time O(mn) where m is the size of the regular expression (the number of operators and characters appearing in R) and n the length of the text. There also exists a solution for the case k = 0 (exact regular expression matching) which solves the problem in time O(dn), where d is the number of strings in the regular expression (a string is a sequence of characters connected with concatenation operator). In this paper, we show that both methods can be combined to solve the approximate regular approximate matching problem in time O(kdn) for arbitrary k. This bound can be much better than the bound O(mn/ logk+2n) achieved by the best actual regular expression matching algorithm in case d &lt; mk logk+2n (that is k is not too large and R contains much less occurrences of ∪ and * than occurrences of (undefined)).},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {55–66},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051081,
author = {Kopelowitz, Tsvi and Lewenstein, Moshe and Porat, Ely},
title = {Persistency in Suffix Trees with Applications to String Interval Problems},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The suffix tree has proven to be an invaluable indexing data structure, which is widely used as a building block in many applications. We study the problem of making a suffix tree persistent. Specifically, consider a streamed text T where characters are prepended to the beginning of the text. The suffix tree is updated for each character prepended. We wish to allow access to any previous version of the suffix tree. While it is possible to support basic persistence for suffix trees using classical persistence techniques, some applications which can make use of this persistency cannot be solved efficiently using these techniques alone.A collection of such problems is that of queries on string intervals of the text indexed by the suffix tree. In other words, if the text T = t1...tn is indexed, one may want to answer different queries on string intervals, ti...tj, of the text. These types of problems are known as position-restricted and contain querying, reporting, rank, selection etc. Persistency can be utilized to obtain solutions for these problems on prefixes of the text, by solving these problems on previous versions of the suffix tree. However, for substrings it is not sufficient to use the standard persistency.We propose more sophisticated persistent techniques which yield solutions for position-restricted querying, reporting, rank, and selection problems.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {67–80},
numpages = {14},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051082,
author = {Wang, Hung-Lung and Chen, Kuan-Yu},
title = {Approximate Point Set Pattern Matching with L<sub>p</sub>-Norm},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Given two sets of points, the text and the pattern, determining whether the pattern "appears" in the text is modeled as the point set pattern matching problem. Applications usually ask for not only exact matches between these two sets, but also approximate matches. In this paper, we investigate a one-dimensional approximate point set matching problem proposed in [T. Suga and S. Shimozono, Approximate point set pattern matching on sequences and planes, CPM'04]. What requested is an optimal match which minimizes the Lp-norm of the difference vector (|p2 -p1 -(t′2-t′1)|, |p3 -p2-(t′3 -t′2)|, . . . , |pm -pm-1 -(t′m -t′m-1)|), where p1, p2, . . . , pm is the pattern and t′1, t′2, . . . , t′m is a subsequence of the text. For p → ∞, the proposed algorithm is of time complexity O(mn), where m and n denote the lengths of the pattern and the text, respectively. For arbitrary p &lt; ∞, the time complexity is O(mnT(p)), where T(p) is the time of evaluating xp for x ∈ R.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {81–86},
numpages = {6},
keywords = {point set pattern matching, Lp-norm, dynamic programming},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051083,
author = {Fisichella, Marco and Stewart, Avar\'{e} and Cuzzocrea, Alfredo and Denecke, Kerstin},
title = {Detecting Health Events on the Social Web to Enable Epidemic Intelligence},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Content analysis and clustering of natural language documents becomes crucial in various domains, even in public health. Recent pandemics such as Swine Flu have caused concern for public health officials. Given the ever increasing pace at which infectious diseases can spread globally, officials must be prepared to react sooner and with greater epidemic intelligence gathering capabilities. Information should be gathered from a broader range of sources, including the Web which in turn requires more robust processing capabilities. To address this limitation, in this paper, we propose a new approach to detect public health events in an unsupervised manner. We address the problems associated with adapting an unsupervised learner to the medical domain and in doing so, propose an approach which combines aspects from different feature-based event detection methods. We evaluate our approach with a real world dataset with respect to the quality of article clusters. Our results show that we are able to achieve a precision of 62% and a recall of 75% evaluated using manually annotated, real-world data.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {87–103},
numpages = {17},
keywords = {retrospective medical event detection, epidemic intelligence, clustering},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051084,
author = {McCreadie, Richard and Macdonald, Craig and Ounis, Iadh},
title = {A Learned Approach for Ranking News in Real-Time Using the Blogosphere},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Newspaper websites and news aggregators rank news stories by their newsworthiness in real-time for display to the user. Recent work has shown that news stories can be ranked automatically in a retrospective manner based upon related discussion within the blogosphere. However, it is as yet undetermined whether blogs are sufficiently fresh to rank stories in real-time. In this paper, we propose a novel learning to rank framework which leverages current blog posts to rank news stories in a real-time manner. We evaluate our proposed learning framework within the context of the TREC Blog track top stories identification task. Our results show that, indeed, the blogosphere can be leveraged for the realtime ranking of news, including for unpredictable events. Our approach improves upon state-of-the-art story ranking approaches, outperforming both the best TREC 2009/2010 systems and its single best performing feature.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {104–116},
numpages = {13},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051085,
author = {Kopliku, Arlind and Pinel-Sauvagnat, Karen and Boughanem, Mohand},
title = {Attribute Retrieval from Relational Web Tables},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we propose an attribute retrieval approach which extracts and ranks attributes from HTML tables. Given an instance (e.g. Tower of Pisa), we want to retrieve from the Web its attributes (e.g. height, architect). Our approach uses HTML tables which are probably the largest source for attribute retrieval. Three recall oriented filters are applied over tables to check the following three properties: (i) is the table relational, (ii) has the table a header, and (iii) the conformity of its attributes and values. Candidate attributes are extracted from tables and ranked with a combination of relevance features. Our approach can be applied to all instances and is shown to have a high recall and a reasonable precision. Moreover, it outperforms state of the art techniques.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {117–128},
numpages = {12},
keywords = {information retrieval, attribute retrieval},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051086,
author = {Poblete, Barbara and Spiliopoulou, Myra and Mendoza, Marcelo},
title = {Query-Sets++: A Scalable Approach for Modeling Web Sites},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We explore an effective approach for modeling and classifying Web sites in the World Wide Web. The aim of this work is to classify Web sites using features which are independent of size, structure and vocabulary. We establish Web site similarity based on search engine query hits, which convey document relevance and utility in direct relation to users' needs and interests. To achieve this, we use a generic Web site representation scheme over different feature spaces, built upon query traffic to the site's documents. For this task we extend, in a non-trivial way, our prior work using query-sets for single document representation. We discuss why this previous methodology is not scalable for a large set of heterogeneous Web sites.We show that our models achieve very compactWeb site representations. Furthermore, our experiments on site classification show excellent performance and quality/dimensionality trade-off. In particular, we sustain a reduction in the feature space to 5% of the size of the bag-of-words representation, while achieving 99% precision in our classification experiments on DMOZ.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {129–134},
numpages = {6},
keywords = {query mining, classification, web sites},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051087,
author = {Lewenstein, Moshe},
title = {Indexing with Gaps},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In Indexing with Gaps one seeks to index a text to allow pattern queries that allow gaps within the pattern query. Formally a gappedpattern over alphabet Σ is a pattern of the form p = p1g1p2g2 ... glpl+1, where ∀i, pi ∈ Σ* and each gi is a gap length ∈ N. Often one considers these patterns with some bound constraints, for example, all gaps are bounded by a gap-bound G.Near-optimal solutions have, lately, been proposed for the case of one gap only with a predetermined size. More specifically, an indexing solution for patterns of the form p1 undefined g undefined p2, where g is known apriori. In this case the solutions mentioned are preprocessed in O(n log∈ n) time and O(n) space, where the pattern queries are answered in O(|p1| + |p2|), for constant sized alphabets. For the more general case when there is a bound G these results can be easily adapted with a multiplicative factor of O(G) for the preprocessing, i.e. O(n log∈ nG) preprocessing time and O(nG) preprocessing space. Alas, these solutions do not lend to more than one gap.In this paper we propose a solution for k gaps one with preprocessing time O(nG2k logk n log log n) and space of O(nG2k logk n) and query time O(m + 2k log log n), where m = Σi=1 |pi|.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {135–143},
numpages = {9},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051088,
author = {Brejov\'{a}, Bro\v{n}a and Landau, Gad M. and Vina\v{r}, Tom\'{a}\v{s}},
title = {Fast Computation of a String Duplication History under No-Breakpoint-Reuse},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we provide an O(n log2 n log log n log* n) algorithm to compute a duplication history of a string under no-breakpointreuse condition. Our algorithm is an efficient implementation of earlier work by Zhang et al. (2009). The motivation of this problem stems from computational biology, in particular from analysis of complex gene clusters. The problem is also related to computing edit distance with block operations, but in our scenario the start of the history is not fixed, but chosen to minimize the distance measure.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {144–155},
numpages = {12},
keywords = {dynamic text, duplication, sequence evolution, edit distance},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051089,
author = {Breslauer, Dany and Italiano, Giuseppe F.},
title = {Near Real-Time Suffix Tree Construction via the Fringe Marked Ancestor Problem},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We contribute a further step towards the plausible real time construction of suffix trees by presenting an on-line algorithm that spends O(log log n) time processing each input symbol and takes O(n log log n) time in total. Our results improve on a previously published algorithm that take O(log n) time per symbol and O(n log n) time in total. The improvements are achieved using a new data structure for the fringe marked ancestor problem, a special case of the nearest marked ancestor problem, which may be of independent interest.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {156–167},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051090,
author = {Amir, Amihood and Paryenty, Haim and Roditty, Liam},
title = {Approximations and Partial Solutions for the Consensus Sequence Problem},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The problem of finding the consensus of a given set of strings is formally defined as follows: given a set of strings S = {s1, . . . sk}, and a constant d, find, if it exists, a string s*, such that the Hamming distance of s* from each of the strings does not exceed d.In this paper we study an LP relaxation for the problem. We prove an additive upper bound, depending only in the number of strings k, and randomized bounds. We show that empirical results are much better. We also compare our program with some algorithms reported in the literature, and it is shown to perform well.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {168–173},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051091,
author = {K\"{a}rkk\"{a}inen, Juha and Puglisi, Simon J.},
title = {Fixed Block Compression Boosting in FM-Indexes},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A compressed full-text self-index occupies space close to that of the compressed text and simultaneously allows fast pattern matching and random access to the underlying text. Among the best compressed self-indexes, in theory and in practice, are several members of the FMindex family. In this paper, we describe new FM-index variants that combine nice theoretical properties, simple implementation and improved practical performance. Our main result is a new technique called fixed block compression boosting, which is a simpler and faster alternative to optimal compression boosting and implicit compression boosting used in previous FM-indexes.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {174–184},
numpages = {11},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051092,
author = {Claude, Francisco and Nicholson, Patrick K. and Seco, Diego},
title = {Space Efficient Wavelet Tree Construction},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Wavelet trees are one of the main building blocks in many space efficient data structures. In this paper, we present new algorithms for constructing wavelet trees, based on in-place sorting, that use virtually no extra space. Furthermore, we implement and confirm that these algorithms are practical by comparing them to a known construction algorithm. This represents a step forward for practical space-efficient data structures, by allowing their construction on more massive data sets.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {185–196},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051093,
author = {Beller, Timo and Gog, Simon and Ohlebusch, Enno and Schnattinger, Thomas},
title = {Computing the Longest Common Prefix Array Based on the Burrows-Wheeler Transform},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many sequence analysis tasks can be accomplished with a suffix array, and several of them additionally need the longest common prefix array. In large scale applications, suffix arrays are being replaced with full-text indexes that are based on the Burrows-Wheeler transform. In this paper, we present the first algorithm that computes the longest common prefix array directly on the wavelet tree of the Burrows-Wheeler transformed string. It runs in linear time and a practical implementation requires approximately 2.2 bytes per character.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {197–208},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051094,
author = {Thachuk, Chris},
title = {A Succinct Index for Hypertext},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Recent advances in nucleic acid sequencing technology has motivated research into succinct text indexes to represent reference genomes that support efficient pattern matching queries. Similar sequencing technology can also produce millions of reads (patterns) derived from transcripts which need to be aligned to a reference transcriptome. A transcriptome can be modeled as a hypertext. Motivated by this application, we propose the first succinct index for hypertext. The index can model any hypertext and places no restriction on the graph topology. We also propose a new pattern matching algorithm, capable of aligning a pattern to any path in the hypertext, that is especially efficient when few nodes of the hypertext share the same text--in this important case, our algorithm is a significant improvement over all existing approaches.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {209–220},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051095,
author = {Garcia-Fernandez, Anne and Ligozat, Anne-Laure and Dinarelli, Marco and Bernhard, Delphine},
title = {When Was It Written? Automatically Determining Publication Dates},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Automatically determining the publication date of a document is a complex task, since a document may contain only few intratextual hints about its publication date. Yet, it has many important applications. Indeed, the amount of digitized historical documents is constantly increasing, but their publication dates are not always properly identified via OCR acquisition. Accurate knowledge about publication dates is crucial for many applications, e.g. studying the evolution of documents topics over a certain period of time.In this article, we present a method for automatically determining the publication dates of documents, which was evaluated on a French newspaper corpus in the context of the DEFT 2011 evaluation campaign. Our system is based on a combination of different individual systems, relying both on supervised and unsupervised learning, and uses several external resources, e.g. Wikipedia, Google Books Ngrams, and etymological background knowledge about the French language. Our system detects the correct year of publication in 10% of the cases for 300-word excerpts and in 14% of the cases for 500-word excerpts, which is very promising given the complexity of the task.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {221–236},
numpages = {16},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051096,
author = {Henrique, Wallace Favoreto and Ziviani, Nivio and Cristo, Marco Ant\^{o}nio and de Moura, Edleno Silva and da Silva, Altigran Soares and Carvalho, Cristiano},
title = {A New Approach for Verifying URL Uniqueness in Web Crawlers},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Web has become a huge repository of pages and search engines allow users to find relevant information in this repository. Web crawlers are an important component of search engines. They find, download, parse content and store pages in a repository. In this paper, we present a new algorithm for verifying URL uniqueness in a large-scale web crawler. The verifier of uniqueness must check if a URL is present in the repository of unique URLs and if the corresponding page was already collected. The algorithm is based on a novel policy for organizing the set of unique URLs according to the server they belong to, exploiting a locality of reference property. This property is inherent in Web traversals, which follows from the skewed distribution of links within a web page, thus favoring references to other pages from a same server. We select the URLs to be crawled taking into account information about the servers they belong to, thus allowing the usage of our algorithm in the crawler without extra cost to pre-organize the entries. We compare our algorithm with a state-of-the-art algorithm found in the literature. We present a model for both algorithms and compare their performances. We carried out experiments using a crawling simulation of a representative subset of the Web which show that the adopted policy yields to a significant improvement in the time spent handling URL uniqueness verification.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {237–248},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051097,
author = {Min, Jinming and Jones, Gareth J. F.},
title = {External Query Reformulation for Text-Based Image Retrieval},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In text-based image retrieval, the Incomplete Annotation Problem (IAP) can greatly degrade retrieval effectiveness. A standard method used to address this problem is pseudo relevance feedback (PRF) which updates user queries by adding feedback terms selected automatically from top ranked documents in a prior retrieval run. PRF assumes that the target collection provides enough feedback information to select effective expansion terms. This is often not the case in image retrieval since images often only have short metadata annotations leading to the IAP. Our work proposes the use of an external knowledge resource (Wikipedia) in the process of refining user queries. In our method, Wikipedia documents strongly related to the terms in user query ("definition documents") are first identified by title matching between the query and titles of Wikipedia articles. These definition documents are used as indicators to re-weight the feedback documents from an initial search run on a Wikipedia abstract collection using the Jaccard coefficient. The new weights of the feedback documents are combined with the scores rated by different indicators. Query-expansion terms are then selected based on these new weights for the feedback documents. Our method is evaluated on the ImageCLEF WikipediaMM image retrieval task using text-based retrieval on the document metadata fields. The results show significant improvement compared to standard PRF methods.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {249–260},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051098,
author = {Nasir, Jamal Abdul and Karim, Asim and Tsatsaronis, George and Varlamis, Iraklis},
title = {A Knowledge-Based Semantic Kernel for Text Classification},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Typically, in textual document classification the documents are represented in the vector space using the "Bag of Words" (BOW) approach. Despite its ease of use, BOW representation cannot handle word synonymy and polysemy problems and does not consider semantic relatedness between words. In this paper, we overcome the shortages of the BOW approach by embedding a known WordNet-based semantic relatedness measure for pairs of words, namely Omiotis, into a semantic kernel. The suggested measure incorporates the TF-IDF weighting scheme, thus creating a semantic kernel which combines both semantic and statistical information from text. Empirical evaluation with real data sets demonstrates that our approach successfully achieves improved classification accuracy with respect to the standard BOW representation, when Omiotis is embedded in four different classifiers.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {261–266},
numpages = {6},
keywords = {semantic kernels, thesaurus, text classification},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051099,
author = {Hon, Wing-Kai and Ku, Tsung-Han and Shah, Rahul and Thankachan, Sharma V. and Vitter, Jeffrey Scott},
title = {Compressed Text Indexing with Wildcards},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Let T = T1φk1T2φk2 .... φkdTd+1 be a text of total length n, where characters of each Ti are chosen from an alphabet Σ of size σ, and φ denotes a wildcard symbol. The text indexing with wildcards problem is to index T such that when we are given a query pattern P, we can locate the occurrences of P in T efficiently. This problem has been applied in indexing genomic sequences that contain single-nucleotide polymorphisms (SNP) because SNP can be modeled as wildcards. Recently Tam et al. (2009) and Thachuk (2011) have proposed succinct indexes for this problem. In this paper, we present the first compressed index for this problem, which takes only nHh + o(n log σ) + O(d log n) bits space, where Hh is the hth-order empirical entropy (h = o(logσ n)) of T.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {267–277},
numpages = {11},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051100,
author = {Goto, Keisuke and Bannai, Hideo and Inenaga, Shunsuke and Takeda, Masayuki},
title = {Fast Q-Gram Mining on SLP Compressed Strings},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present simple and efficient algorithms for calculating q-gram frequencies on strings represented in compressed form, namely, as a straight line program (SLP). Given an SLP of size n that represents string T, we present an O(qn) time and space algorithm that computes the occurrence frequencies of all q-grams in T. Computational experiments show that our algorithm and its variation are practical for small q, actually running faster on various real string data, compared to algorithms that work on the uncompressed text. We also discuss applications in data mining and classification of string data, for which our algorithms can be useful.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {278–289},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051101,
author = {Russo, Lu\'{\i}s M. S. and Tischler, German},
title = {Succinct Gapped Suffix Arrays},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Gapped suffix arrays (also known as bi-factor arrays) were recently presented for approximate searching under the Hamming distance. These structures can be used to find occurrences of a pattern P, where the characters inside a gap do not have to match. This paper describes a succinct representation of gapped suffix arrays.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {290–294},
numpages = {5},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051102,
author = {Gagie, Travis and He, Meng and Munro, J. Ian and Nicholson, Patrick K.},
title = {Finding Frequent Elements in Compressed 2D Arrays and Strings},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We show how to store a compressed two-dimensional array such that, if we are asked for the elements with high relative frequency in a range, we can quickly return a short list of candidates that includes them. More specifically, given an m \texttimes{} n array A and a fraction α &gt; 0, we can store A in O(mn(H + 1)log2(1/α) bits, where H is the entropy of the elements' distribution in A, such that later, given a rectangular range in A and a fraction β ≥ α, in O(1/β) time we can return a list of O(1/β) distinct array elements that includes all the elements that have relative frequency at least β in that range. We do not verify that the elements in the list have relative frequency at least β, so the list may contain false positives. In the case when m = 1, i.e., A is a string, we improve this space bound by a factor of log(1/α), and explore a spacetime trade off for verifying the frequency of the elements in the list. This leads to an O(n min(log(1/α), H +1) log n) bit data structure for strings that, in O(1/β) time, can return the O(1/β) elements that have relative frequency at least β in a given range, without false positives, for β ≥ α.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {295–300},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051103,
author = {Breslauer, Dany and Italiano, Giuseppe F.},
title = {On Suffix Extensions in Suffix Trees},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Suffix trees are inherently asymmetric: prefix extensions only cause a few updates, while suffix extensions affect all suffixes causing a wave of updates. In his elegant linear-time on-line suffix tree algorithm Ukkonen relaxed the prevailing suffix tree representation and introduced two changes to avoid repeated structural updates and circumvent the inherent complexity of suffix extensions: (1) open ended edges that enjoy gratuitous leaf updates, and (2) the omission of implicit nodes. In this paper we study the implicit nodes as the suffix tree evolves. We partition the suffix tree's edges into collections of similar edges called bands, where implicit nodes exhibit identical behavior, and generalize the notion of open ended edges to allow implicit nodes to "float" within bands, only requiring updates when moving from one band to the next, adding up to only O(n) updates. We also show that internal implicit nodes are separated from each other by explicit suffix tree nodes and that all external implicit nodes are related to the same periodicity. These new properties may be used to keep track of the waves of implicit node updates and to build the suffix tree on-line in amortized linear time, providing access to all the implicit nodes in worst-case constant time.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {301–312},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051104,
author = {Tirdad, Kamran and Ghodsnia, Pedram and Munro, J. Ian and L\'{o}pez-Ortiz, Alejandro},
title = {COCA Filters: Co-Occurrence Aware Bloom Filters},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose an indexing data structure based on a novel variation of Bloom filters. Signature files have been proposed in the past as a method to index large text databases though they suffer from a high false positive error problem. In this paper we introduce COCA Filters, a new type of Bloom filters which exploits the co-occurrence probability of words in documents to reduce the false positive error. We show experimentally that by using this technique we can reduce the false positive error by up to 21.6 times for the same index size. Furthermore Bloom filters can be replaced by COCA filters wherever the co-occurrence of any two members of the universe is identifiable.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {313–325},
numpages = {13},
keywords = {information retrieval, signature files, locality sensitive hash functions, bloom filters},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051105,
author = {Kucherov, Gregory},
title = {On-Line Construction of Position Heaps},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a simple linear-time on-line algorithm for constructing a position heap for a string [EMOW11]. Our definition of position heap differs slightly from the one proposed in [EMOW11] in that it considers the suffixes ordered in the descending order of length. Our construction is based on classic suffix pointers and resembles the Ukkonen's algorithm for suffix trees [Ukk95]. Using suffix pointers, the position heap can be extended into the augmented position heap that allows for a linear-time string matching algorithm [EMOW11].},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {326–337},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051106,
author = {Christou, Michalis and Crochemore, Maxime and Flouri, Tom\'{a}\v{s} and Iliopoulos, Costas S. and Janou\v{s}ek, Jan and Melichar, Bo\v{r}ivoj and Pissis, Solon P.},
title = {Computing All Subtree Repeats in Ordered Ranked Trees},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We consider the problem of finding all subtree repeats in a given ordered ranked tree. Specifically, we transform the given tree to a string representing its postfix notation, and then propose an algorithm based on the bottom-up technique. The proposed algorithm is divided into two phases: the preprocessing phase, and the phase where all subtree repeats are computed. The linear runtime of the algorithm, as well as the use of linear auxiliary space, are important aspects of its quality.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {338–343},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051107,
author = {Gil-Costa, Veronica and Santos, Rodrygo L. T. and Macdonald, Craig and Ounis, Iadh},
title = {Sparse Spatial Selection for Novelty-Based Search Result Diversification},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Novelty-based diversification approaches aim to produce a diverse ranking by directly comparing the retrieved documents. However, since such approaches are typically greedy, they require O(n2) documentdocument comparisons in order to diversify a ranking of n documents. In this work, we propose to model novelty-based diversification as a similarity search in a sparse metric space. In particular, we exploit the triangle inequality property of metric spaces in order to drastically reduce the number of required document-document comparisons. Thorough experiments using three TREC test collections show that our approach is at least as effective as existing novelty-based diversification approaches, while improving their efficiency by an order of magnitude.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {344–355},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051108,
author = {Hagen, Matthias and Stein, Benno},
title = {Candidate Document Retrieval for Web-Scale Text Reuse Detection},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Given a document d, the task of text reuse detection is to find those passages in d which in identical or paraphrased form also appear in other documents. To solve this problem at web-scale, keywords representing d's topics have to be combined to web queries. The retrieved web documents can then be delivered to a text reuse detection system for an in-depth analysis. We focus on the query formulation problem as the crucial first step in the detection process and present a new query formulation strategy that achieves convincing results: compared to a maximal termset query formulation strategy [10, 14], which is the most sensible non-heuristic baseline, we save on average 70% of the queries in realistic experiments. With respect to the candidate documents' quality, our heuristic retrieves documents that are, on average, more similar to the given document than the results of previously published query formulation strategies [4, 8].},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {356–367},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051109,
author = {Gonz\'{a}lez-Caro, Cristina and Baeza-Yates, Ricardo},
title = {A Multi-Faceted Approach to Query Intent Classification},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we report results for automatic classification of queries in a wide set of facets that are useful to the identification of query intent. Our hypothesis is that the performance of single-faceted classification of queries can be improved by introducing information of multi-faceted training samples into the learning process. We test our hypothesis by performing a multi-faceted classification of queries based on the combination of correlated facets. Our experimental results show that this idea can significantly improve the quality of the classification. Since most of previous works in query intent classification are oriented to the study of single facets, these results are a first step to an integrated query intent classification model.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {368–379},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051110,
author = {Cummins, Ronan and Lalmas, Mounia and O'Riordan, Colm and Jose, Joemon M.},
title = {Navigating the User Query Space},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Query performance prediction (QPP) aims to automatically estimate the performance of a query. Recently there have been many attempts to use these predictors to estimate whether a perturbed version of a query will outperform the original version. In essence, these approaches attempt to navigate the space of queries in a guided manner.In this paper, we perform an analysis of the query space over a substantial number of queries and show that (1) users tend to be able to extract queries that perform in the top 5% of all possible user queries for a specific topic, (2) that post-retrieval predictors outperform preretrieval predictors at the high end of the query space. And, finally (3), we show that some post retrieval predictors are better able to select high performing queries from a group of user queries for the same topic.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {380–385},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051111,
author = {Belazzougui, Djamal and Navarro, Gonzalo},
title = {Improved Compressed Indexes for Full-Text Document Retrieval},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We give new space/time tradeoffs for compressed indexes that answer document retrieval queries on general sequences. On a collection of D documents of total length n, current approaches require at least |CSA| + O(n lgD/lg lgD) or 2|CSA| + o(n) bits of space, where CSA is a full-text index. Using monotone minimum perfect hash functions, we give new algorithms for document listing with frequencies and top-k document retrieval using just |CSA| + O(n lg lg lgD) bits. We also improve current solutions that use 2|CSA| + o(n) bits, and consider other problems such as colored range listing, top-k most important documents, and computing arbitrary frequencies.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {386–397},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051112,
author = {Maruyama, Shirou and Nakahara, Masaya and Kishiue, Naoya and Sakamoto, Hiroshi},
title = {ESP-Index: A Compressed Index Based on Edit-Sensitive Parsing},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a compressed self-index based the edit-sensitive parsing (ESP). Given a string S, its ESP tree is equivalent to a contextfree grammar deriving just S, which can be represented as a DAG G. Finding pattern P in S is reduced to embedding P into G. Succinct data structures are adopted and G is then decomposed into two LOUDS bit strings and a single array for permutation, requiring (1 + ε)n log n + 4n + o(n) bits for any 0 &lt; ε &lt; 1 where n corresponds to the number of different symbols in the grammar. The time to count the occurrences of P in S is in O(log*u/ε (mlog n+occc(logmlog u))), where m = |P|, u = |S|, and occc is the number of occurrences of a maximal common subtree in ESP trees of P and S. Using an additional array in n log u bits of space, our index supports locating P and displaying substring of S. Locating time is the same as counting time and displaying time for a substring of length m is O(m + log u).},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {398–409},
numpages = {12},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051113,
author = {Thankachan, Sharma V.},
title = {Compressed Indexes for Aligned Pattern Matching},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In many situations like protein sequences, the primary protein sequence is associated with secondary structure labels [6]. This can be treated as two sequences aligned character by character. Many other DNA and RNA sequences involve linkages which are aligned across or in the same or different strands. In this paper, we consider the most natural characterization of aligned string data.The aligned pattern matching problem is to index two input texts. T1[1...n] and T2[1...n], each having n characters taken from an alphabet set Σ of size σ = polylog(n), such that the following query can be answered efficiently: given two query patterns P1 and P2, find all the text. positions i such that P1 matches with T1[i...(i+|P1|-1)] and P2 matches with T2[i...(i + |P2| - 1)]. Our objective is to design a compressed space index for this problem and we obtained the following main results: when the query patterns are sufficiently long (|P1|, |P2| &gt; α = Θ(log2+2ε n), where ε &gt; 0), we can design an index which takes nH′k +nH″k +o(n log σ) bits space and O(|P1| + |P2| + log4+4ε n + t) query time, where H′k and H″k denotes the empirical kth-order entropy (k = o(logσ n)) of T1 and T2 respectively, t represents the number of outputs and ε &gt; 0. Further we show that designing a compressed/succinct space index with polylogarithmic query time, which works for query patterns of all lengths is at least as hard as designing a linear space index for 3-dimensional orthogonal range reporting with poly-logarithmic query time. However, we introduce another compressed index of nH′k + nH″k + O(n) + o(n log σ) bits space requirement with a query time of O(|P1|+|P2|+√nt log2+ε n) which works without any restriction on the length of the patterns.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {410–419},
numpages = {10},
location = {Pisa, Italy},
series = {SPIRE'11}
}

@inproceedings{10.5555/2051073.2051114,
author = {Kuruppu, Shanika and Puglisi, Simon J. and Zobel, Justin},
title = {Reference Sequence Construction for Relative Compression of Genomes},
year = {2011},
isbn = {9783642245824},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Relative compression, where a set of similar strings are compressed with respect to a reference string, is an effective method of compressing DNA datasets containing multiple similar sequences. Moreover, it supports rapid random access to the underlying data. The main difficulty of relative compression is in selecting an appropriate reference sequence. In this paper, we explore using the dictionary of repeats generated by COMRAD, RE-PAIR and DNA-X algorithms as reference sequences for relative compression. We show that this technique allows for better compression, and allows more general repetitive datasets to be compressed using relative compression.},
booktitle = {Proceedings of the 18th International Conference on String Processing and Information Retrieval},
pages = {420–425},
numpages = {6},
location = {Pisa, Italy},
series = {SPIRE'11}
}

