@inproceedings{10.1007/978-3-642-33247-0_1,
author = {Cassidy, Taylor and Ji, Heng and Deng, Hongbo and Zheng, Jing and Han, Jiawei},
title = {Analysis and Refinement of Cross-Lingual Entity Linking},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_1},
doi = {10.1007/978-3-642-33247-0_1},
abstract = {In this paper we propose two novel approaches to enhance cross-lingual entity linking (CLEL). One is based on cross-lingual information networks, aligned based on monolingual information extraction, and the other uses topic modeling to ensure global consistency. We enhance a strong baseline system derived from a combination of state-of-the-art machine translation and monolingual entity linking to achieve 11.2% improvement in B-Cubed+ F-measure. Our system achieved highly competitive results in the NIST Text Analysis Conference (TAC) Knowledge Base Population (KBP2011) evaluation. We also provide detailed qualitative and quantitative analysis on the contributions of each approach and the remaining challenges.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {1–12},
numpages = {12},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_2,
author = {Nordlie, Ragnar and Pharo, Nils},
title = {Seven Years of INEX Interactive Retrieval Experiments --- Lessons and Challenges},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_2},
doi = {10.1007/978-3-642-33247-0_2},
abstract = {This paper summarizes a major effort in interactive search investigation, the INEX i-track, a collective effort run over a seven-year period. We present the experimental conditions, report some of the findings of the participating groups, and examine the challenges posed by this kind of collective experimental effort.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {13–23},
numpages = {11},
keywords = {user studies, information search behavior, interactive information retrieval},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_3,
author = {Hanbury, Allan and M\"{u}ller, Henning and Langs, Georg and Weber, Marc Andr\'{e} and Menze, Bjoern H. and Fernandez, Tomas Salas},
title = {Bringing the Algorithms to the Data: Cloud---Based Benchmarking for Medical Image Analysis},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_3},
doi = {10.1007/978-3-642-33247-0_3},
abstract = {Benchmarks have shown to be an important tool to advance science in the fields of information analysis and retrieval. Problems of running benchmarks include obtaining large amounts of data, annotating it and then distributing it to the participants of a benchmark. Distribution of the data to participants is currently mostly done via data download that can take hours for large data sets and in countries with slow Internet connections even days. Sending physical hard disks was also used for distributing very large scale data sets (for example by TRECvid) but also this becomes infeasible if the data sets reach sizes of 5---10 TB. With cloud computing it is possible to make very large data sets available in a central place with limited costs. Instead of distributing the data to the participants, the participants can compute their algorithms on virtual machines of the cloud providers. This text presents reflections and ideas of a concrete project on using cloud---based benchmarking paradigms for medical image analysis and retrieval. It is planned to run two evaluation campaigns in 2013 and 2014 using the proposed technology.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {24–29},
numpages = {6},
keywords = {anatomy detection, benchmark, cloud computing, medical image analysis, case-based medical information retrieval},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_4,
author = {J\"{u}rgens, Julia J. and Hansen, Preben and Womser-Hacker, Christa},
title = {Going beyond CLEF-IP: The 'reality' for Patent Searchers?},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_4},
doi = {10.1007/978-3-642-33247-0_4},
abstract = {This paper gives an overview of several different approaches that have been applied by participants in the CLEF-IP evaluation initiative. On this basis, it is suggested that other techniques and experimental paradigms could be helpful in further improving the results and making the experiments more realistic. The field of information seeking is therefore incorporated and its potential gain for patent retrieval explained. Furthermore, the different search tasks that are undertaken by patent searchers are introduced as possible use cases. They can serve as a basis for development in patent retrieval research in that they present the diverse scenarios with their special characteristics and give the research community therefore a realistic picture of the patent user's work.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {30–35},
numpages = {6},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_5,
author = {Orio, Nicola and Liem, Cynthia C. S. and Peeters, Geoffroy and Schedl, Markus},
title = {MusiClef: Multimodal Music Tagging Task},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_5},
doi = {10.1007/978-3-642-33247-0_5},
abstract = {MusiClef is a multimodal music benchmarking initiative that will be running a MediaEval 2012 Brave New Task on Multimodal Music Tagging. This paper describes the setup of this task, showing how it complements existing benchmarking initiatives and fosters less explored methodological directions in Music Information Retrieval. MusiClef deals with a concrete use case, encourages multimodal approaches based on these, and strives for transparency of results as much as possible. Transparency is encouraged at several levels and stages, from the feature extraction procedure up to the evaluation phase, in which a dedicated categorization of ground truth tags will be used to deepen the understanding of the relation between the proposed approaches and experimental results.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {36–41},
numpages = {6},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_6,
author = {Berendsen, Richard and Tsagkias, Manos and de Rijke, Maarten and Meij, Edgar},
title = {Generating Pseudo Test Collections for Learning to Rank Scientific Articles},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_6},
doi = {10.1007/978-3-642-33247-0_6},
abstract = {Pseudo test collections are automatically generated to provide training material for learning to rank methods. We propose a method for generating pseudo test collections in the domain of digital libraries, where data is relatively sparse, but comes with rich annotations. Our intuition is that documents are annotated to make them better findable for certain information needs. We use these annotations and the associated documents as a source for pairs of queries and relevant documents. We investigate how learning to rank performance varies when we use different methods for sampling annotations, and show how our pseudo test collection ranks systems compared to editorial topics with editorial judgements. Our results demonstrate that it is possible to train a learning to rank algorithm on generated pseudo judgments. In some cases, performance is on par with learning on manually obtained ground truth.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {42–53},
numpages = {12},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_7,
author = {Piroi, Florina and Lupu, Mihai and Hanbury, Allan},
title = {Effects of Language and Topic Size in Patent IR: An Empirical Study},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_7},
doi = {10.1007/978-3-642-33247-0_7},
abstract = {We revisit the effects that various characteristics of the topic documents have on the effectiveness of the systems for the task of finding prior art in the patent domain. In doing so, we provide the reader interested in approaching the domain a guide of the issues that need to be addressed in this context.For the current study, we select two patent based test collections with a common document representation schema and look at topic characteristics specific to the objectives of the collections. We look at the effect of languages on retrieval and at the length of the topic documents. We present the correlations between these topic facets and their retrieval results, as well as their relevant documents.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {54–66},
numpages = {13},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_8,
author = {Gupta, Parth and Barr\'{o}n-Cede\~{n}o, Alberto and Rosso, Paolo},
title = {Cross-Language High Similarity Search Using a Conceptual Thesaurus},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_8},
doi = {10.1007/978-3-642-33247-0_8},
abstract = {This work addresses the issue of cross-language high similarity and near-duplicates search, where, for the given document, a highly similar one is to be identified from a large cross-language collection of documents. We propose a concept-based similarity model for the problem which is very light in computation and memory. We evaluate the model on three corpora of different nature and two language pairs English-German and English-Spanish using the Eurovoc conceptual thesaurus. Our model is compared with two state-of-the-art models and we find, though the proposed model is very generic, it produces competitive results and is significantly stable and consistent across the corpora.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {67–75},
numpages = {9},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_9,
author = {Keszler, Anita and Kov\'{a}cs, Levente and Szir\'{a}nyi, Tam\'{a}s},
title = {The Appearance of the Giant Component in Descriptor Graphs and Its Application for Descriptor Selection},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_9},
doi = {10.1007/978-3-642-33247-0_9},
abstract = {The paper presents a random graph based analysis approach for evaluating descriptors based on pairwise distance distributions on real data. Starting from the Erd\H{o}s-R\'{e}nyi model the paper presents results of investigating random geometric graph behaviour in relation with the appearance of the giant component as a basis for choosing descriptors based on their clustering properties. Experimental results prove the existence of the giant component in such graphs, and based on the evaluation of their behaviour the graphs, the corresponding descriptors are compared, and validated in proof-of-concept retrieval tests.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {76–81},
numpages = {6},
keywords = {graph analysis, feature selection, giant components},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_10,
author = {Yan, Xueliang and Gao, Guanglai and Su, Xiangdong and Wei, Hongxi and Zhang, Xueliang and Lu, Qianqian},
title = {Hidden Markov Model for Term Weighting in Verbose Queries},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_10},
doi = {10.1007/978-3-642-33247-0_10},
abstract = {It has been observed that short queries generally have better performance than their corresponding long versions when retrieved by the same IR model. This is mainly because most of the current models do not distinguish the importance of different terms in the query. Observed that sentence-like queries encode information related to the term importance in the grammatical structure, we propose a Hidden Markov Model (HMM) based method to extract such information to do term weighting. The basic idea of choosing HMM is motivated by its successful application in capturing the relationship between adjacent terms in NLP field. Since we are dealing with queries of natural language form, we think that HMM can also be used to capture the dependence between the weights and the grammatical structures. Our experiments show that our assumption is quite reasonable and that such information, when utilized properly, can greatly improve retrieval performance.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {82–87},
numpages = {6},
keywords = {Hidden Markov model, term weighting, verbose query},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_11,
author = {Agosti, Maristella and Di Buccio, Emanuele and Ferro, Nicola and Masiero, Ivano and Peruzzo, Simone and Silvello, Gianmaria},
title = {DIRECTions: Design and Specification of an IR Evaluation Infrastructure},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_11},
doi = {10.1007/978-3-642-33247-0_11},
abstract = {Information Retrieval (IR) experimental evaluation is an essential part of the research on and development of information access methods and tools. Shared data sets and evaluation scenarios allow for comparing methods and systems, understanding their behaviour, and tracking performances and progress over the time. On the other hand, experimental evaluation is an expensive activity in terms of human effort, time, and costs required to carry it out.Software and hardware infrastructures that support experimental evaluation operation as well as management, enrichment, and exploitation of the produced scientific data provide a key contribution in reducing such effort and costs and carrying out systematic and throughout analysis and comparison of systems and methods, overall acting as enablers of scientific and technical advancement in the field. This paper describes the specification for an Information Retrieval (IR) evaluation infrastructure by conceptually modeling the entities involved in Information Retrieval (IR) experimental evaluation and their relationships and by defining the architecture of the proposed evaluation infrastructure and the APIs for accessing it.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {88–99},
numpages = {12},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_12,
author = {Galu\v{s}\v{c}\'{a}kov\'{a}, Petra and Pecina, Pavel and Haji\v{c}, Jan},
title = {Penalty Functions for Evaluation Measures of Unsegmented Speech Retrieval},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_12},
doi = {10.1007/978-3-642-33247-0_12},
abstract = {This paper deals with evaluation of information retrieval from unsegmented speech. We focus on Mean Generalized Average Precision, the evaluation measure widely used for unsegmented speech retrieval. This measure is designed to allow certain tolerance in matching retrieval results (starting points of relevant segments) against a gold standard relevance assessment. It employs a Penalty Function which evaluates non-exact matches in the retrieval results based on their distance from the beginnings of their nearest true relevant segments. However, the choice of the Penalty Function is usually ad-hoc and does not necessary reflect users' perception of the speech retrieval quality. We perform a lab test to study satisfaction of users of a speech retrieval system to empirically estimate the optimal shape of the Penalty Function.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {100–111},
numpages = {12},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_13,
author = {Angelini, Marco and Ferro, Nicola and J\"{a}rvelin, Kalervo and Keskustalo, Heikki and Pirkola, Ari and Santucci, Giuseppe and Silvello, Gianmaria},
title = {Cumulated Relative Position: A Metric for Ranking Evaluation},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_13},
doi = {10.1007/978-3-642-33247-0_13},
abstract = {The development of multilingual and multimedia information access systems calls for proper evaluation methodologies to ensure that they meet the expected user requirements and provide the desired effectiveness. IR research offers a strong evaluation methodology and a range of evaluation metrics, such as MAP and (n)DCG. In this paper, we propose a new metric for ranking evaluation, the CRP. We start with the observation that a document of a given degree of relevance may be ranked too early or too late regarding the ideal ranking of documents for a query. Its relative position may be negative, indicating too early ranking, zero indicating correct ranking, or positive, indicating too late ranking. By cumulating these relative rankings we indicate, at each ranked position, the net effect of document displacements, the CRP. We first define the metric formally and then discuss its properties, its relationship to prior metrics, and its visualization. Finally we propose different visualizations of CRP by exploiting a test collection to demonstrate its behavior.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {112–123},
numpages = {12},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_14,
author = {Schaer, Philipp},
title = {Better than Their Reputation? On the Reliability of Relevance Assessments with Students},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_14},
doi = {10.1007/978-3-642-33247-0_14},
abstract = {During the last three years we conducted several information retrieval evaluation series with more than 180 LIS students who made relevance assessments on the outcomes of three specific retrieval services. In this study we do not focus on the retrieval performance of our system but on the relevance assessments and the inter-assessor reliability. To quantify the agreement we apply Fleiss' Kappa and Krippendorff's Alpha. When we compare these two statistical measures on average Kappa values were 0.37 and Alpha values 0.15. We use the two agreement measures to drop too unreliable assessments from our data set. When computing the differences between the unfiltered and the filtered data set we see a root mean square error between 0.02 and 0.12. We see this as a clear indicator that disagreement affects the reliability of retrieval evaluations. We suggest not to work with unfiltered results or to clearly document the disagreement rates.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {124–135},
numpages = {12},
keywords = {inter-assessor agreement, information retrieval, evaluation, relevance assessment, students, Krippendorff's alpha, Fleiss' Kappa, inter-rater agreement},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_15,
author = {K\"{u}rsten, Jens and Eibl, Maximilian},
title = {Comparing IR System Components Using Beanplots},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_15},
doi = {10.1007/978-3-642-33247-0_15},
abstract = {In this poster we demonstrate an approach to gain a better understanding of the interactions between search tasks, test collections and components and configurations of retrieval systems by testing a large set of experiment configurations against standard ad-hoc test collections.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {136–137},
numpages = {2},
keywords = {component-based evaluation, ad-hoc retrieval},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_16,
author = {Bhaskar, Pinaki and Bandyopadhyay, Sivaji},
title = {Language Independent Query Focused Snippet Generation},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_16},
doi = {10.1007/978-3-642-33247-0_16},
abstract = {The present paper describes the development of a language independent query focused snippet generation module. This module takes the query and content of each retrieved document and generates a query dependent snippet for each retrieved document. The algorithm of this module based on the sentence extraction, sentence scoring and sentence ranking. Subjective evaluation has been. English snippet got the best evaluation score, i.e. 1 and overall average evaluation score of 0.83 has been achieved in the scale of 0 to 1.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {138–140},
numpages = {3},
location = {Rome, Italy},
series = {CLEF'12}
}

@inproceedings{10.1007/978-3-642-33247-0_17,
author = {de L. Pertile, Solange and Moreira, Viviane P.},
title = {A Test Collection to Evaluate Plagiarism by Missing or Incorrect References},
year = {2012},
isbn = {9783642332463},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33247-0_17},
doi = {10.1007/978-3-642-33247-0_17},
abstract = {In recent years, several methods and tools been developed together with test collections to aid in plagiarism detection. However, both methods and collections have focused on content analysis, overlooking citation analysis. In this paper, we aim at filling this gap and present a test collection with cases of plagiarism by missing and incorrect references. The collection contains automatically generated academic papers in which passages from other documents have been inserted. Such passages were either: adequately referenced (i.e., not plagiarized), not referenced, or incorrectly referenced. Annotation files identifying each passage enable the evaluation of plagiarism detection systems.},
booktitle = {Proceedings of the Third International Conference on Information Access Evaluation: Multilinguality, Multimodality, and Visual Analytics},
pages = {141–143},
numpages = {3},
location = {Rome, Italy},
series = {CLEF'12}
}

