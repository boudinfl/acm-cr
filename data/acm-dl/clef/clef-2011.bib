@inproceedings{10.5555/2045274.2045276,
author = {Toms, Elaine G.},
title = {"Would You Trust Your IR System to Choose Your Date?" Re-Thinking IR Evaluation in the 21st Century},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This talk examines interactive IR system evaluation from the holistic approach, including some of the pitfalls in existing approaches, and the issues involved in designing more effective processes and procedures.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {1},
numpages = {1},
keywords = {interactive information retrieval, user-centred evaluation},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045277,
author = {Alonso, Omar},
title = {Crowdsourcing for Information Retrieval Experimentation and Evaluation},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Very recently, crowdsourcing has emerged as a viable alternative for conducting different types of experiments in a wide range of areas. Generally speaking and in the context of IR, crowdsourcing involves outsourcing tasks to a large group of people instead of assigning such tasks to an employee or editor. The availability of commercial crowdsourcing platforms offers vast access to an on-demand workforce. This new approach makes possible to conduct experiments extremely fast, with good results at a low cost. However, like in any experiment, there are several implementation details that would make an experiment work or fail. For large scale evaluation, deployment in practice is not that simple. Tasks have to be designed carefully with special emphasis on the user interface, instructions, content, and quality control.In this invited talk, I will explore some directions that may influence the outcome of a task and I will present a framework for conducting crowdsourcing experiments making some emphasis on a number of aspects that should be of importance for all sorts of IR-like tasks. Finally, I will outline research trends around human computation that promise to make this emerging field even more interesting in the near future.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {2},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045279,
author = {Mayfield, James and Lawrie, Dawn and McNamee, Paul and Oard, Douglas W.},
title = {Building a Cross-Language Entity Linking Collection in Twenty-One Languages},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe an efficient way to create a test collection for evaluating the accuracy of cross-language entity linking. Queries are created by semiautomatically identifying person names on the English side of a parallel corpus, using judgments obtained through crowdsourcing to identify the entity corresponding to the name, and projecting the English name onto the non-English document using word alignments. We applied the technique to produce the first publicly available multilingual cross-language entity linking collection. The collection includes approximately 55,000 queries, comprising between 875 and 4,329 queries for each of twenty-one non-English languages.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {3–13},
numpages = {11},
keywords = {multilingual corpora, crowdsourcing, entity linking, cross-language entity linking},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045280,
author = {Savenkov, Denis and Braslavski, Pavel and Lebedev, Mikhail},
title = {Search Snippet Evaluation at Yandex: Lessons Learned and Future Directions},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This papers surveys different approaches to evaluation of web search summaries and describes experiments conducted at Yandex. We hypothesize that the complex task of snippet evaluation is best solved with a range of different methods. Automation of evaluation based on available manual assessments and clickthrough analysis is a promising direction.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {14–25},
numpages = {12},
keywords = {experimentation, web search, snippets, evaluation, search summaries},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045281,
author = {Azzopardi, Leif and Balog, Krisztian},
title = {Towards a Living Lab for Information Retrieval Research and Development: A Proposal for a Living Lab for Product Search Tasks},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The notion of having a "living lab" to undertaken evaluations has been proposed by a number of proponents within the field of Information Retrieval (IR). However, what such a living lab might look like and how it might be setup has not been discussed in detail. Living labs have a number of appealing points such as realistic evaluation contexts where tasks are directly linked to user experience and the closer integration of research/academia and development/industry facilitating more efficient knowledge transfer. However, operationalizing a living lab opens up a number of concerns regarding security, privacy, etc. as well as challenges regarding the design, development and maintenance of the infrastructure required to support such evaluations. Here, we aim to further the discussion on living labs for IR evaluation and propose one possible architecture to create such an evaluation environment. To focus discussion, we put forward a proposal for a living lab on product search tasks within the context of an online shop.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {26–37},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045282,
author = {Amig\'{o}, Enrique and Gonzalo, Julio and Verdejo, Felisa},
title = {A Comparison of Evaluation Metrics for Document Filtering},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Although document filtering is simple to define, there is a wide range of different evaluation measures that have been proposed in the literature, all of which have been subject to criticism. We present a unified, comparative view of the strenghts and weaknesses of proposed measures based on two formal constraints (which should be satisfied by any suitable evaluation measure) and various properties (which help differentiating measures according to their behaviour). We conclude that (i) some smoothing process is necessary process to satisfy the basic constraints; and (ii) metrics can be grouped into three families, each satisfying one out of three formal properties, which are mutually exclusive, i.e. no metric can satisfy all three properties simultaneously.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {38–49},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045284,
author = {Spina, Damiano and Amig\'{o}, Enrique and Gonzalo, Julio},
title = {Filter Keywords and Majority Class Strategies for Company Name Disambiguation in Twitter},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Monitoring the online reputation of a company starts by retrieving all (fresh) information where the company is mentioned; and a major problem in this context is that company names are often ambiguous (apple may refer to the company, the fruit, the singer, etc.). The problem is particularly hard in microblogging, where there is little context to disambiguate: this was the task addressed in the WePS-3 CLEF lab exercise in 2010. This paper introduces a novel fingerprint representation technique to visualize and compare system results for the task. We apply this technique to the systems that originally participated in WePS-3, and then we use it to explore the usefulness of filter keywords (those whose presence in a tweet reliably signals either the positive or the negative class) and finding the majority class (whether positive or negative tweets are predominant for a given company name in a tweet stream) as signals that contribute to address the problem. Our study shows that both are key signals to solve the task, and we also find that, remarkably, the vocabulary associated to a company in the Web does not seem to match the vocabulary used in Twitter streams: even a manual extraction of filter keywords from web pages has substantially lower recall than an oracle selection of the best terms from the Twitter stream.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {50–61},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045285,
author = {Hammarstr\"{o}m, Harald},
title = {Automatic Annotation of Bibliographical References for Descriptive Language Materials},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The present paper considers the problem of annotating bibliographical references with labels/classes, given training data of references already annotated with labels. The problem is an instance of document categorization where the documents are short and written in a wide variety of languages. The skewed distributions of title words and labels calls for special carefulness when choosing a Machine Learning approach. The present paper describes how to induce Disjunctive Normal Form formulae (DNFs), which have several advantages over Decision Trees. The approach is evaluated on a large real-world collection of bibliographical references.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {62–73},
numpages = {12},
keywords = {supervised learning, language documentation, document categorization, decision trees, cross-lingual information retrieval},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045286,
author = {Kumar, N. Kiran and Santosh, G. S. K. and Varma, Vasudeva},
title = {A Language-Independent Approach to Identify the Named Entities in under-Resourced Languages and Clustering Multilingual Documents},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a language-independent Multilingual Document Clustering (MDC) approach on comparable corpora. Named entites (NEs) such as persons, locations, organizations play a major role in measuring the document similarity. We propose a method to identify these NEs present in under-resourced Indian languages (Hindi and Marathi) using the NEs present in English, which is a high resourced language. The identified NEs are then utilized for the formation of multilingual document clusters using the Bisecting k-means clustering algorithm. We didn't make use of any non-English linguistic tools or resources such as WordNet, Part-Of-Speech tagger, bilingual dictionaries, etc., which makes the proposed approach completely language-independent. Experiments are conducted on a standard dataset provided by FIRE1 for their 2010 Ad-hoc Cross-Lingual document retrieval task on Indian languages. We have considered English, Hindi and Marathi news datasets for our experiments. The system is evaluated using F-score, Purity and Normalized Mutual Information measures and the results obtained are encouraging.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {74–82},
numpages = {9},
keywords = {named entities, multilingual document clustering, languageindependent},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045287,
author = {Olvera-Lobo, Mar\'{\i}a-Dolores and Guti\'{e}rrez-Artacho, Juncal},
title = {Multilingual Question-Answering System in Biomedical Domain on the Web: An Evaluation},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Question-answering systems (QAS) are presented as an alternative to traditional systems of information retrieval, intended to offer precise responses to factual questions. An analysis has been made of the results offered by the QA multilingual biomedical system HONqa, available on the Web. The study has used a set of 120 biomedical definitional questions (What is...?), taken from the medical website WebMD, which were formulated in English, French, and Italian. The answers have been analysed using a serie of specific measures (MRR, TRR, FHS, precision, MAP).The study confirms that for all the languages analysed the functioning effectiveness needs to be improved, although in the multilingual context analysed the questions in the English language achieve better results for retrieving definitional information than in French and Italian.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {83–88},
numpages = {6},
keywords = {multilingual information, biomedical information, evaluation measures, HONqa, restricted-domain question answering systems, multilingual question answering systems},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045288,
author = {Ganguly, Debasis and Leveling, Johannes and Jones, Gareth J. F.},
title = {Simulation of Within-Session Query Variations Using a Text Segmentation Approach},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a generative model for automatic query reformulations from an initial query using the underlying subtopic structure of top ranked retrieved documents. We address three types of query reformulations: a) specialization; b) generalization; and c) drift. To test our model we generate three reformulation variants starting with selected fields from the TREC-8 topics as the initial queries. We use manual judgements from multiple assessors to measure the accuracy of the reformulated query variants and observe accuracies of 65%, 82% and 69% respectively for specialization, generalization and drift reformulations.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {89–94},
numpages = {6},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045290,
author = {Tsikrika, Theodora and de Herrera, Alba G. Seco and M\"{u}ller, Henning},
title = {Assessing the Scholarly Impact of ImageCLEF},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Systematic evaluation has an important place in information retrieval research starting with the Cranfield tests and currently with TREC (Text REtrieval Conference) and other evaluation campaigns. Such benchmarks are often mentioned to have an important impact in advancing a research field and making techniques comparable. Still, their exact impact is hard to measure. This paper aims at assessing the scholarly impact of the ImageCLEF image retrieval evaluation initiative. To this end, the papers in the proceedings published after each evaluation campaign and their citations are analysed using Scopus and Google Scholar. A significant impact of ImageCLEF could be shown through this bibliometric analysis. The differences between the employed analysis methods, each with its advantages and limitations, are also discussed.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {95–106},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045291,
author = {Harris, Christopher G. and Xu, Tao},
title = {The Importance of Visual Context Clues in Multimedia Translation},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {As video-sharing websites such as YouTube proliferate, the ability to rapidly translate video clips into multiple languages has become an essential component for enhancing their global reach and impact. Moreover, the ability to provide closed captioning in a variety of languages is paramount to reach a wider variety of viewers. We investigate the importance of visual context clues by comparing transcripts of multimedia clips (which allow transcriptionists to make use of visual context clues in their translations) with their corresponding written transcripts (which do not). Additionally, we contrast translations produced using crowdsourcing workers with those made by professional translators on cost and quality. Finally, we evaluate several genres of multimedia to examine the effects of visual context clues on each and demonstrate the results through heat maps.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {107–118},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045292,
author = {Di Buccio, Emanuele and Dussin, Marco and Ferro, Nicola and Masiero, Ivano and Santucci, Giuseppe and Tino, Giuseppe},
title = {To Re-Rank or to Re-Query: Can Visual Analytics Solve This Dilemma?},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Evaluation has a crucial role in Information Retrieval (IR) since it allows for identifying possible points of failure of an IR approach, thus addressing them to improve its effectiveness. Developing tools to support researchers and analysts when analyzing results and investigating strategies to improve IR system performance can help make the analysis easier and more effective. In this paper we discuss a Visual Analytics-based approach to support the analyst when deciding whether or not to investigate re-ranking to improve the system effectiveness measured after a retrieval run. Our approach is based on effectiveness measures that exploit graded relevance judgements and it provides both a principled and intuitive way to support analysis. A prototype is described and exploited to discuss some case studies based on TREC data.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {119–130},
numpages = {12},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045293,
author = {Schuth, Anne and Marx, Maarten},
title = {Evaluation Methods for Rankings of Facetvalues for Faceted Search},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We introduce two metrics aimed at evaluating systems that select facetvalues for a faceted search interface. Facetvalues are the values of meta-data fields in semi-structured data and are commonly used to refine queries. It is often the case that there are more facetvalues than can be displayed to a user and thus a selection has to be made. Our metrics evaluate these selections based on binary relevant assessments for the documents in a collection. Both our metrics are based on Normalized Discounted Cumulated Gain, an often used Information Retrieval metric.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {131–136},
numpages = {6},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.5555/2045274.2045294,
author = {Leong, Chee Wee and Hassan, Samer and Ruiz, Miguel Enrique and Mihalcea, Rada},
title = {Improving Query Expansion for Image Retrieval via Saliency and Picturability},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present a Wikipedia-based approach to query expansion for the task of image retrieval, by combining salient encyclopaedic concepts with the picturability of words. Our model generates the expanded query terms in a definite two-stage process instead of multiple iterative passes, requires no manual feedback, and is completely unsupervised. Preliminary results show that our proposed model is effective in a comparative study on the ImageCLEF 2010 Wikipedia dataset.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {137–142},
numpages = {6},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

