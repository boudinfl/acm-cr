@inproceedings{10.5555/2393955.2393957,
author = {Peters, Carol},
title = {What Happened in CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The organization of the CLEF 2006 evaluation campaign is described and details are provided concerning the tracks, test collections, evaluation infrastructure, and participation.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {1–10},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393958,
author = {Agosti, Maristella and Di Nunzio, Giorgio Maria and Ferro, Nicola},
title = {Scientific Data of an Evaluation Campaign: Do We Properly Deal with Them?},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper examines the current way of keeping the data produced during the evaluation campaigns and highlights some shortenings of it. As a consequence, we propose a new approach for improving the management evaluation campaigns' data. In this approach, the data are considered as scientific data to be cured and enriched in order to give full support to longitudinal statistical studies and long-term preservation.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {11–20},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393960,
author = {Di Nunzio, Giorgio M. and Ferro, Nicola and Mandl, Thomas and Peters, Carol},
title = {CLEF 2006: Ad Hoc Track Overview},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe the objectives and organization of the CLEF 2006 ad hoc track and discuss the main characteristics of the tasks offered to test monolingual, bilingual, and multilingual textual document retrieval systems. The track was divided into two streams. The main stream offered mono- and bilingual tasks using the same collections as CLEF 2005: Bulgarian, English, French, Hungarian and Portuguese. The second stream, designed for more experienced participants, offered the so-called "robust task" which used test collections from previous years in six languages (Dutch, English, French, German, Italian and Spanish) with the objective of privileging experiments which achieve good stable performance over all queries rather than high average performance. The performance achieved for each task is presented and the results are commented. The document collections used were taken from the CLEF multilingual comparable corpus of news documents.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {21–34},
numpages = {14},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393962,
author = {Pingali, Prasad and Tune, Kula Kekeba and Varma, Vasudeva},
title = {Hindi, Telugu, Oromo, English CLIR Evaluation},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the Cross Language Information Retrieval (CLIR) experiments of Language Technologies Research Centre (LTRC, IIIT-Hyderabad) as part of our participation in the ad-hoc track of CLEF 2006. This is our first participation in the CLEF evaluation campaign and we focused on Afaan Oromo, Hindi and Telugu as source (query) languages for retrieval of documents from English text collection. We have used a dictionary based approach for CLIR. After a brief description of our CLIR system we discuss the evaluation results of various experiments we conducted using CLEF 2006 dataset.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {35–42},
numpages = {8},
keywords = {CLEF 2006, hindi-english, telugu-english, CLIR experiments, oromo-english},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393963,
author = {Argaw, Atelach Alemu and Asker, Lars},
title = {Amharic-English Information Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe Amharic-English cross lingual information retrieval experiments in the ad hoc bilingual tracks of the CLEF 2006. The query analysis is supported by morphological analysis and part of speech tagging while we used two machine readable dictionaries supplemented by online dictionaries for term lookup in the translation process. Out of dictionary terms were handled using fuzzy matching and Lucene[4] was used for indexing and searching. Four experiments that differed in terms of utilized fields in the topic set, fuzzy matching, and term weighting, were conducted. The results obtained are reported and discussed.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {43–50},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393964,
author = {Cardoso, Nuno and Silva, M\'{a}rio J. and Martins, Bruno},
title = {The University of Lisbon at CLEF 2006 Ad-Hoc Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports the participation of the XLDB Group from the University of Lisbon in the CLEF 2006 ad-hoc monolingual and bilingual subtasks for Portuguese. We present our IR system, detail the query expansion strategy and the weighting scheme, describe the submitted runs and discuss the obtained results.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {51–56},
numpages = {6},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393965,
author = {Hayurani, Herika and Sari, Syandra and Adriani, Mirna},
title = {Query and Document Translation for English-Indonesian Cross Language IR},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a report on our participation in the Indonesian-English ad hoc bilingual task of the 2006 Cross-Language Evaluation Forum (CLEF). This year we compare the use of several language resources to translate Indonesian queries into English. We used several readable machine dictionaries to perform the translation. We also used two machine translation techniques to translate the Indonesian queries. In addition to translating an Indonesian query set into English, we also translated English documents into Indonesian using the machine readable dictionaries and a commercial machine translation tool. The results show performing the task by translating the queries is better than translating the documents. Combining several dictionaries produced better result than only using one dictionary. However, the query expansion that we applied to the translated queries using the dictionaries reduced the retrieval effectiveness of the queries.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {57–61},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393967,
author = {Noguera, Elisa and Llopis, Fernando},
title = {Passage Retrieval vs. Document Retrieval in the CLEF 2006 Ad Hoc Monolingual Tasks with the IR-n System},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The paper describes our participation in monolingual tasks at CLEF 2006. We submitted results for the following languages: English, French, Portuguese and Hungarian.We focused on studying different weighting schemes (okapi and dfr) and retrieval strategies (passage retrieval and document retrieval) to improve retrieval performance. After an analysis of our experiments and of the official results at CLEF 2006, we achieved considerably improved scores by using different configurations for different languages (French, Portuguese and Hungarian).},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {62–65},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393968,
author = {Gonzalez, Marco and De Lima, Vera L\'{u}cia Strube},
title = {The PUCRS NLP-Group Participation in CLEF2006: Information Retrieval Based on Linguistic Resources},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the 2006 participation of the PUCRS NLP-Group in the CLEF Monolingual Ad Hoc Task for Portuguese. We took part in this campaign using the TR+ Model, which is based on nominalization, binary lexical relations (BLR), Boolean queries, and the evidence concept. Our alternative strategy for lexical normalization, the nominalization, is to transform a word (adjective, verb, or adverb) into a semantically corresponding noun. BLRs identify relationships between nominalized terms and capture phrasal cohesion mechanisms, like those between subject and predicate, subject and object (direct or indirect), noun and adjective or verb and adverb. In our strategy, an index unit (a descriptor) may be a single term or a BLR, and we adopt the evidence concept: the descriptor weighting depends on the occurrence of phrasal cohesion mechanisms, besides depending on frequency of occurrence. We describe these features, which implement lexical normalization and term dependence in an information retrieval system based on linguistic resources.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {66–73},
numpages = {8},
keywords = {term dependence, search engine, lexical normalization, information retrieval evaluation},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393969,
author = {Arcoverde, Jo\~{a}o Marcelo Azevedo and Das Gra\c{c}as Volpe Nunes, Maria},
title = {NLP-Driven Constructive Learning for Filtering an IR Document Stream},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature engineering is known as one of the most important challenges for knowledge acquisition, since any inductive learning system depends upon an efficient representation model to find good solutions to a given problem. We present an NLP-driven constructive learning method for building features based upon noun phrases structures, which are supposed to carry the highest discriminatory information. The method was test at the CLEF 2006 Ad-Hoc, monolingual (Portuguese) IR track. A classification model was obtained using this representation scheme over a small subset of the relevance judgments to filter false-positives documents returned by the IR-system. The goal was to increase the overall precision. The experiment achieved a MAP gain of 41.3%, in average, over three selected topics. The best F1-measure for the text classification task over the proposed text representation model was 77.1%. The results suggest that relevant linguistic features can be exploited by NLP techniques in a domain specific application, and can be used suscesfully in text categorization, which can act as an important coadjuvant process for other high-level IR tasks.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {74–82},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393970,
author = {Mercier, Annabelle and Beigbeder, Michel},
title = {ENSM-SE at CLEF 2006: Fuzzy Proxmity Method with an Adhoc Influence Function},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We experiment a new influence function in our information retrieval method that uses the degree of fuzzy proximity of key terms in a document to compute the relevance of the document to the query. The model is based on the idea that the closer the query terms in a document are to each other the more relevant the document. Our model handles Boolean queries but, contrary to the traditional extensions of the basic Boolean information retrieval model, does not use a proximity operator explicitly. A single parameter makes it possible to control the proximity degree required. To improve our system we use a stemming algorithm before indexing, we take a specific influence function and we merge fuzzy proximity result lists built with different width of influence function. We explain how we construct the queries and report the results of our experiments in the ad-hoc monolingual French task of the CLEF 2006 evaluation campaign.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {83–90},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393971,
author = {Orengo, Viviane Moreira and Buriol, Luciana S. and Coelho, Alexandre Ramos},
title = {A Study on the Use of Stemming for Monolingual Ad-Hoc Portuguese Information Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For UFRGS's first participation in CLEF our goal was to compare the performance of heavier and lighter stemming strategies using the Portuguese data collections for monolingual Ad-hoc retrieval. The results show that the safest strategy was to use the lighter alternative (reducing plural forms only). On a query-by-query analysis, full stemming achieved the highest improvement but also the biggest decrease in performance when compared to no stemming. In addition, statistical tests showed that the only significant improvement in terms of mean average precision, precision at ten and number of relevant retrieved was achieved by our lighter stemmer.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {91–98},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393972,
author = {Hal\'{a}csy, P\'{e}ter and Tr\'{o}n, Viktor},
title = {Benefits of Resource-Based Stemming in Hungarian Information Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper discusses the impact of resource-driven stemming in information retrieval tasks. We conducted experiments in order to identify the relative benefit of various stemming strategies in a language with highly complex morphology. The results reveal the importance of various aspects of stemming in enhancing system performance in the IR task of the CLEF ad-hoc monolingual Hungarian track.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {99–106},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393973,
author = {Majumder, Prasenjit and Mitra, Mandar and Datta, Kalyankumar},
title = {Statistical vs. Rule-Based Stemming for Monolingual French Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our approach to the 2006 Adhoc Monolingual Information Retrieval run for French. The goal of our experiment was to compare the performance of a proposed statistical stemmer with that of a rule-based stemmer, specifically the French version of Porter's stemmer. The statistical stemming approach is based on lexicon clustering, using a novel string distance measure. We submitted three official runs, besides a baseline run that uses no stemming. The results show that stemming significantly improves retrieval performance (as expected) by about 9-10%, and the performance of the statistical stemmer is comparable with that of the rule-based stemmer.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {107–110},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393975,
author = {Vilares, Jes\'{u}s and Oakes, Michael P. and Tait, John I.},
title = {A First Approach to CLIR Using Character N-Grams Alignment},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the technique for translation of character n-grams we developed for our participation in CLEF 2006. This solution avoids the need for word normalization during indexing or translation, and it can also deal with out-of-vocabulary words. Since it does not rely on language-specific processing, it can be applied to very different languages, even when linguistic information and resources are scarce or unavailable. Our proposal makes considerable use of freely available resources and also tries to achieve a higher speed during the n-gram alignment process with respect to other similar approaches.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {111–118},
numpages = {8},
keywords = {association measures, alignment algorithms, translation algorithms, cross-language information retrieval, character n-grams},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393976,
author = {Mart\'{\i}nez-Santiago, Fernando and Montejo-R\'{a}ez, Arturo and Garc\'{\i}a-Cumbreras, Miguel \'{A}. and Ure\~{n}a-L\'{o}pez, L. Alfonso},
title = {SINAI at CLEF 2006 Ad Hoc Robust Multilingual Track: Query Expansion Using the Google Search Engine},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This year, we have participated in the Ad-Hoc Robust Multilingual track with the aim of evaluating two important issues in Cross-Lingual Information Retrieval (CLIR) systems. This paper first describes the method applied for query expansion in a multilingual environment by using web search results provided by the Google engine in order to increase retrieval robustness. Unfortunately, the results obtained are disappointing. The second issue reported alludes to the robustness of several common merging algorithms. We have found that 2-step RSV merging algorithms perform better than others algorithms when evaluating using geometric average.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {119–126},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393977,
author = {Mandl, Thomas and Hackl, Ren\'{e} and Womser-Hacker, Christa},
title = {Robust Ad-Hoc Retrieval Experiments with French and English at the University of Hildesheim},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports on experiments submitted for the robust task at CLEF 2006 ad intended to provide a baseline for other runs for the robust task. We applied a system previously tested for ad-hoc retrieval. Runs for monolingual English and French were submitted. Results on both training as well as test topics are reported. Only for French, positive results above 0.2 MAP were achieved.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {127–128},
numpages = {2},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393978,
author = {Tomlinson, Stephen},
title = {Comparing the Robustness of Expansion Techniques and Retrieval Measures},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We investigate which retrieval measures successfully discern robustness gains in the monolingual (Bulgarian, French, Hungarian, Portuguese and English) information retrieval tasks of the Ad-Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2006. In all 5 of our experiments with blind feedback (a technique known to impair robustness across topics), the mean scores of the Average Precision, Geometric MAP and Precision@10 measures increased (and most of these increases were statistically significant), implying that these measures are not suitable as robust retrieval measures. In contrast, we found that measures based on just the first relevant item, such as a Generalized Success@10 measure, successfully discerned some robustness gains, particularly the robustness advantage of expanding Title queries by using the Description field instead of blind feedback.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {129–136},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393979,
author = {Savoy, Jacques and Abdou, Samir},
title = {Experiments with Monolingual, Bilingual, and Robust Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For our participation in the CLEF 2006 campaign, our first objective was to propose and evaluate a decompounding algorithm and a more aggressive stemmer for the Hungarian language. Our second objective was to obtain a better picture of the relative merit of various search engines for the French, Portuguese/Brazilian and Bulgarian languages. To achieve this we evaluated the test-collections using the Okapi approach, some of the models derived from the Divergence from Randomness (DFR) family and a language model (LM), as well as two vector-processing approaches. In the bilingual track, we evaluated the effectiveness of various machine translation systems for a query submitted in English and automatically translated into the French and Portuguese languages. After blind query expansion, the MAP achieved by the best single MT system was around 95% for the corresponding monolingual search when French was the target language, or 83% with Portuguese. Finally, in the robust retrieval task we investigated various techniques in order to improve the retrieval performance of difficult topics.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {137–144},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393980,
author = {Zazo, Angel F. and Berrocal, Jose L. Alonso and Figuerola, Carlos G.},
title = {Local Query Expansion Using Terms Windows for Robust Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our work at CLEF 2006 Robust task. This is an ad-hoc task that explores methods for stable retrieval by focusing on poorly performing topics. We have participated in all subtasks: monolingual (English, French, Italian and Spanish), bilingual (Italian to Spanish) and multilingual (Spanish to [English, French, Italian and Spanish]). In monolingual retrieval we have focused our effort on local query expansion, i.e. using only the information from retrieved documents, not from the complete document collection or external corpora, such as the Web. Some local expansion techniques were applied for training topics. Regarding robustness the most effective one was the use of co-occurrence based thesauri, which were constructed using co-occurrence relations in windows of terms, not in complete documents. This is an effective technique that can be easily implemented by tuning only a few parameters. In bilingual and multilingual retrieval experiments several machine translation programs were used to translate topics. For each target language, translations were merged before performing a monolingual retrieval. We also applied the same local expansion technique. In multilingual retrieval, weighted max-min normalization was used to merge lists. In all the subtasks in which we participated our mandatory runs (using title and description fields of the topics) obtained very good rankings. Runs with short queries (only title field) also obtained high MAP and GMAP values using the same expansion technique.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {145–152},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393981,
author = {Lam-Adesina, Adenike M. and Jones, Gareth J. F.},
title = {Dublin City University at CLEF 2006: Robust Cross Language Track},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The main focus of the DCU group's participation in the CLEF 2006 Robust Track track was to explore a new method of re-ranking a retrieved document set based on the initial query with a pseudo relevance feedback (PRF) query expansion method. The aim of re-ranking using the initial query is to force the retrieved assumed relevant set to mimic the initial query more closely while not removing the benefits of PRF. Our results show that although our PRF is consistently effective for this task, the application of the current version of our new re-ranking method has little effect on the ranked output.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {153–156},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393982,
author = {McNamee, Paul},
title = {JHU/APL Ad Hoc Experiments at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {JHU/APL continued its participation in multilingual retrieval at CLEF in 2006. We again applied our hallmark technique for combating language diversity and morphological complexity: character n-gram tokenization. This year we participated in the ad hoc cross-language track and submitted both monolingual and bilingual runs. Our experimental results this year agree with our previous reports that n-grams perform especially well in linguistically complex languages, notably Bulgarian and Hungarian, where monolingual improvements of 27% and 70% respectively were observed compared to space-delimited word forms. As in CLEF 2005, our bilingual submissions made use of subword translation, statistical translation of character n-grams using aligned corpora, when parallel data were available, and webbased machine translation, when no suitable data was available to us.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {157–162},
numpages = {6},
keywords = {character n-gram tokenization, cross-language information retrieval, corpus-based translation},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393984,
author = {Stempfhuber, Maximilian and Baerisch, Stefan},
title = {The Domain-Specific Track at CLEF 2006: Overview of Approaches, Results and Assessment},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The CLEF domain-specific track uses databases in different languages from the social science domain as the basis for retrieval of documents relevant to a user's query. Predefined topics simulate the user's information needs and are used by the research groups participating in this track to generate actual queries. The documents are structured into individual elements so that they can be used for optimizing the search strategies. One type of the retrieval task was on cross-language retrieval, the finding of information using queries in a different language than the actual language of the documents. English, German and Russian were used as languages for queries and documents. Queries therefore had to be translated from one of the languages to one or more of the other languages. Besides the cross-language tasks also monolingual retrieval tasks were carried out where query and document are in the same language. The focus here was to map the query onto the language and internal structure of the documents.This paper gives an overview of the domain-specific track and reports on noteworthy trends in approaches and results as well as on the topic creation and assessment process.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {163–169},
numpages = {7},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393985,
author = {Leveling, Johannes},
title = {Reranking Documents with Antagonistic Terms},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For the participation of the IICS group at the domain-specific task (GIRT) of the CLEF campaign 2006, we employ cooccurrence of search terms and antagonistic terms (antonyms or cohyponyms) in documents to derive values for reranking an initial result set.A reranking test on GIRT 2004 data showed a significant increase in mean average precision (MAP), i. e. a change from 0.2446 MAP to 0.2986 MAP. Precision for the submitted runs for the domain-specific (DS) task did not change significantly, but the setup for the best experiment included a reranking of result documents (0.3539 MAP). For reranking a result set with an already high MAP (provided by the Berkeley group), a significant decrease in precision was observed (MAP dropped from 0.4343 to 0.3653).},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {170–173},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393986,
author = {Larson, Ray R.},
title = {Domain Specific Retrieval: Back to Basics},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper describe Berkeley's approach to the Domain Specific (DS) track for CLEF 2006. This year we did not use the tools for thesaurus-based query expansion and de-compounding for German used in previous years. This year Berkeley submitted 12 runs, including one for each subtask of the DS track. These include 3 Monolingual runs for English, German, and Russian, 7 Bilingual runs (3 X2EN, 1 X2DE, and 3 X2RU), and 2 Multilingual runs. For many DS sub-tasks our runs were the best performing runs, but sadly they were also the only runs for a number of sub-tasks. In the sub-tasks where there were other entries, our relative performance was above the mean performance in 2 sub-tasks and just below the mean in another.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {174–177},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393987,
author = {K\"{u}rsten, Jens and Eibl, Maximilian},
title = {Monolingual Retrieval Experiments with a Domain-Specific Document Corpus at the Chemnitz University of Technology},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This article describes the first participation of the Chair Media Informatics of the Chemnitz University of Technology in the Cross Language Evaluation Forum. An experimental prototype is introduced which implements several methods of optimizing search results. The configuration of the prototype is tested with the CLEF training data. The results of the Domain-Specific Monolingual German task suggest that combining the suffix stripping stemming and the decompounding approach is very useful. Also, a local document clustering (LDC) approach used to improve the query expansion (QE) based on pseudorelevance feedback (PRF) seems to be quite beneficial. Nevertheless, the evaluation of the English task using the same configuration suggests that the qualities of the results are highly speech dependent.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {178–185},
numpages = {8},
keywords = {pseudo-relevance feedback, evaluation, local clustering, data fusion},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393989,
author = {Karlgren, Jussi and Gonzalo, Julio and Clough, Paul},
title = {ICLEF 2006 Overview: Searching the Flickr WWW Photo-Sharing Repository},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper summarizes the task design for iCLEF 2006 (the CLEF interactive track). Compared to previous years, we proposed a radically new task: searching images in a naturally multilingual database, Flickr, which has millions of photographs shared by people all over the planet, tagged and described in a wide variety of languages. Participants were expected to build a multilingual search front-end to Flickr (using Flickr's search API) and study the behaviour of the users for a given set of searching tasks. The emphasis was put on studying the process, rather than evaluating its outcome.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {186–194},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393990,
author = {Artiles, Javier and Gonzalo, Julio and L\'{o}pez-Ostenero, Fernando and Peinado, V\'{\i}ctor},
title = {Are Users Willing to Search Cross-Language? An Experiment with the Flickr Image Sharing Repository},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper summarizes the participation of UNED in the CLEF 2006 interactive task. Our goal was to measure the attitude of users towards cross-language searching when the search system provides the possibility (as an option) of searching cross-language, and when the search tasks can clearly benefit from searching in multiple languages.Our results indicate that, even in the most favorable setting (the results are images that can be often interpreted as relevant without reading their descriptions, and the system can make translations in a transparent way to the user), users often avoid translating their query into unknown languages.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {195–204},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393991,
author = {Clough, Paul and Al-Maskari, Azzah and Darwish, Kareem},
title = {Providing Multilingual Access to Flickr for Arabic Users},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe our submission for iCLEF2006: an interface that allows users to search FLICKR in Arabic for images with captions in a range of languages. We report and discuss the results gained from a user experiment in accordance with directives given by iCLEF, including an analysis of the success of search tasks. To enable the searching of multilingual image annotations we use English as an interlingua. An Arabic-English dictionary is used for initial query translation, and then Babelfish is used to translate between English and French, German, Italian, Dutch and Spanish. Users are able to modify the English version of the query if they have the necessary language skills to do so. We have chosen to experiment with Arabic retrieval from FLICKR due to the growing numbers of online Middle Eastern users, the limited numbers of interactive Arabic user studies for cross-language IR to date, and the availability of resources to undertake a user study.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {205–216},
numpages = {12},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393992,
author = {Karlgren, Jussi and Olsson, Fredrik},
title = {Trusting the Results in Cross-Lingual Keyword-Based Image Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper gives a brief description of the starting points for the experiments the SICS team has performed in the 2006 interactive CLEF campaign.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {217–222},
numpages = {6},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393994,
author = {Magnini, Bernardo and Giampiccolo, Danilo and Forner, Pamela and Ayache, Christelle and Jijkoun, Valentin and Osenova, Petya and Pe\~{n}as, Anselmo and Rocha, Paulo and Sacaleanu, Bogdan and Sutcliffe, Richard},
title = {Overview of the CLEF 2006 Multilingual Question Answering Track},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Having being proposed for the fourth time, the QA at CLEF track has confirmed a still raising interest from the research community, recording a constant increase both in the number of participants and submissions. In 2006, two pilot tasks, WiQA and AVE, were proposed beside the main tasks, representing two promising experiments for the future of QA. Also in the main task some significant innovations were introduced, namely list questions and requiring text snippet(s) to support the exact answers. Although this had an impact on the work load of the organizers both to prepare the question sets and especially to evaluate the submitted runs, it had no significant influence on the performance of the systems, which registered a higher Best accuracy than in the previous campaign, both in monolingual and bilingual tasks. In this paper the preparation of the test set and the evaluation process are described, together with a detailed presentation of the results for each of the languages. The pilot tasks WiQA and AVE will be presented in dedicated articles.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {223–256},
numpages = {34},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393995,
author = {Pe\~{n}as, Anselmo and Rodrigo, \'{A}lvaro and Sama, Valent\'{\i}n and Verdejo, Felisa},
title = {Overview of the Answer Validation Exercise 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The first Answer Validation Exercise (AVE) has been launched at the Cross Language Evaluation Forum 2006. This task is aimed at developing systems able to decide whether the answer of a Question Answering system is correct or not. The exercise is described here together with the evaluation methodology and the systems results. The starting point for the AVE 2006 was the reformulation of Answer Validation as a Recognizing Textual Entailment problem, under the assumption that the hypothesis can be automatically generated instantiating hypothesis patterns with the QA systems' answers. 11 groups have participated with 38 runs in 7 different languages. Systems that reported the use of Logic have obtained the best results in their respective subtasks.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {257–264},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393996,
author = {Jijkoun, Valentin and De Rijke, Maarten},
title = {Overview of the WiQA Task at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe WiQA 2006, a pilot task aimed at studying question answering using Wikipedia. Going beyond traditional factoid questions, the task considered at WiQA 2006 was to return--given an source page from Wikipedia--to identify snippets from other Wikipedia pages, possibly in languages different from the language of the source page, that add new and important information to the source page, and that do so without repetition.A total of 7 teams took part, submitting 20 runs. Our main findings are two-fold: (i) while challenging, the tasks considered at WiQA are do-able as participants achieved impressive scores as measured in terms of yield, mean reciprocal rank, and precision, (ii) on the bilingual task, substantially higher scores were achieved than on the monolingual tasks.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {265–274},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393998,
author = {Tom\'{a}s, David and Vicedo, Jos\'{e} L.},
title = {Re-Ranking Passages with LSA in a Question Answering System},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {As in the previous QA@CLEF competition, two separate groups at the University of Alicante participated this year using different approaches. This paper describes the work of Alicante 1 group. We have continued with the research line established in the past competition, where the main goal was to obtain a fully data-driven system based on machine learning techniques. The main novelties introduced this year are focused on the information retrieval stage. We developed a module for passage re-ranking based on Latent Semantic Analysis (LSA). This year we extended our participation to monolingual Spanish task and bilingual Spanish-English task.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {275–279},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2393999,
author = {Desmontils, E. and Jacquin, C. and Monceaux, L.},
title = {Question Types Specification for the Use of Specialized Patterns in Prodicos System},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present the second version of the Prodicos query answering system which was developed by the TALN team from the LINA institute. The main improvements made concern in the one hand, the use of external knowledge (Wikipedia) to improve the passage selection step. And on the other hand, the answer extraction step is improved by the determination of four different strategies for locating the answer to a question regarding its type. Afterwards, for the passage selection and answer extraction modules, the evaluation is put forward to justify the results obtained.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {280–289},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394000,
author = {Bos, Johan and Nissim, Malvina},
title = {Answer Translation: An Alternative Approach to Cross-Lingual Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We approach cross-lingual question answering by using a mono-lingual QA system for the source language and by translating resulting answers into the target language. As far as we are aware, this is the first cross-lingual QA system in the history of CLEF that uses this method--almost without exception, cross-lingual QA systems use translation of the question or query terms instead. We demonstrate the feasibility of our alternative approach by using a mono-lingual QA system for English, and translating answers and finding appropriate documents in Italian and Dutch. For factoid and definition questions, we achieve overall accuracy scores ranging from 13% (EN→NL) to 17% (EN→IT) and lenient accuracy figures from 19% (EN→NL) to 25% (EN→IT). The advantage of this strategy to cross-lingual QA is that translation of answers is easier than translating questions--the disadvantage is that answers might be missing from the source corpus and additional effort is required for finding supporting documents of the target language.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {290–299},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394001,
author = {Cassan, Ad\'{a}n and Figueira, Helena and Martins, Andr\'{e} and Mendes, Afonso and Mendes, Pedro and Pinto, Cl\'{a}udia and Vidal, Daniel},
title = {Priberam's Question Answering System in a Cross-Language Environment},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Following last year's participation in the monolingual question answering (QA) track of CLEF, where Priberam's QA system has achieved state-of-the-art results, this year we decided to take part in both Portuguese and Spanish monolingual tasks, as well as in two bilingual (Portuguese-Spanish and Spanish-Portuguese) tasks of QA@CLEF. This paper describes the improvements and changes implemented in Priberam's QA system since last CLEF participation, detailing the work involved in its cross-lingual extension and discussing the results of the runs submitted to evaluation.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {300–309},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394002,
author = {Bowden, Mitchell and Olteanu, Marian and Suriyentrakor, Pasin and Clark, Jonathan and Moldovan, Dan},
title = {LCC's Poweranswer at QA@CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports on Language Computer Corporation's first QA@CLEF participation. For this exercise, LCC integrated its opendomain PowerAnswer Question Answering system with its statistical Machine Translation engine. For 2006, LCC participated in the English-to-Spanish, French and Portuguese cross-language tasks. The approach is that of intermediate translation, only processing English within the QA system regardless of the input or source languages. The output snippets were then mapped back into the source language documents for the final output of the system and submission. What follows is a description of the system and methodology.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {310–317},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394003,
author = {Bouma, Gosse and Fahmi, Ismail and Mur, Jori and Van Noord, Gertjan and Van Der Plas, Lonneke and Tiedemann, J\"{o}rg},
title = {Using Syntactic Knowledge for QA},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe the system of the University of Groningen for the monolingual Dutch and multilingual English to Dutch QA tasks. First, we give a brief outline of the architecture of our QA-system, which makes heavy use of syntactic information. Next, we describe the modules that were improved or developed especially for the CLEF tasks, among others incorporation of syntactic knowledge in IR, incorporation of lexical equivalences and coreference resolution, and a baseline multilingual (English to Dutch) QA system, which uses a combination of Systran and Wikipedia (for term recognition and translation) for question translation. For non-list questions, 31% (20%) of the highest ranked answers returned by the monolingual (multilingual) system were correct.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {318–327},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394004,
author = {Sacaleanu, Bogdan and Neumann, G\"{u}nter},
title = {A Cross-Lingual German-English Framework for Open-Domain Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The paper describes QUANTICO, a cross-language open domain question answering system for German and English. The main features of the system are: use of preemptive off-line document annotation with syntactic information like chunk structures, apposition constructions and abbreviation-extension pairs for the passage retrieval; use of online translation services, language models and alignment methods for the cross-language scenarios; use of redundancy as an indicator of good answer candidates; selection of the best answers based on distance metrics defined over graph representations. Based on the question type two different strategies of answer extraction are triggered: for factoid questions answers are extracted from best IR-matched passages and selected by their redundancy and distance to the question keywords; for definition questions answers are considered to be the most redundant normalized linguistic structures with explanatory role (i.e., appositions, abbreviation's extensions). The results of evaluating the system's performance by CLEF were as follows: for the best German-German run we achieved an overall accuracy (ACC) of 42.33% and a mean reciprocal rank (MRR) of 0.45; for the best English-German run 32.98% (ACC) and 0.35 (MRR); for the German-English run 17.89% (ACC) and 0.17 (MRR).},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {328–338},
numpages = {11},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394005,
author = {Laurent, Dominique and S\'{e}gu\'{e}la, Patrick and N\`{e}gre, Sophie},
title = {Cross Lingual Question Answering Using QRISTAL for CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {QRISTAL [9] is a question answering system making intensive use of natural language processing both for indexing documents and extracting answers. It ranked first in the EQueR evaluation campaign (Evalda, Technolangue [3]) and in CLEF 2005 for monolingual task (French-French) and multilingual task (English-French and Portuguese-French). This article describes the improvements of the system since last year. Then, it presents our benchmarked results for the CLEF 2006 campaign and a critical description of the system. Since Synapse D\'{e}veloppement is participating to Quaero project, QRISTAL is most likely to be integrated in a mass market search engine in the forthcoming years.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {339–350},
numpages = {12},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394006,
author = {Whittaker, E. W. D. and Novak, J. R. and Chatain, P. and Dixon, P. R. and Heie, M. H. and Furui, S.},
title = {CLEF2006 Question Answering Experiments at Tokyo Institute of Technology},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present the experiments performed at Tokyo Institute of Technology for the CLEF2006 Multiple Language Question Answering (QA@CLEF) track. Our approach to QA centres on a nonlinguistic, data-driven, statistical classification model that uses the redundancy of the web to find correct answers. For the cross-language aspect we employed publicly available web-based text translation tools to translate the question from the source into the corresponding target language, then used the corresponding mono-lingual QA system to find the answers. The hypothesised correct answers were then projected back on to the appropriate closed-domain corpus. Correct and supported answer performance on the mono-lingual tasks was around 14% for both Spanish and French. Performance on the cross-language tasks ranged from 5% for Spanish-English, to 12% for French-Spanish. Our method of projecting answers onto documents was shown not to work well: in the worst case on the French-English task we lost 84% of our otherwise correct answers. Ignoring the need for correct support information the exact answer accuracy increased to 29% and 21% correct on the Spanish and French mono-lingual tasks, respectively.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {351–361},
numpages = {11},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394007,
author = {Ahn, David and Jijkoun, Valentin and Van Rantwijk, Joris and De Rijke, Maarten and Sang, Erik Tjong Kim},
title = {Quartz: A Question Answering System for Dutch},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe a question answering system for Dutch that we used for our participation in the 2006 CLEF Question Answering Dutch monolingual task. We give an overview of the system and focus on the question classification module, the multi-dimensional markup for data storage and access, the answer processing component, and our probabilistic approach to estimating answer correctness.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {362–371},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394008,
author = {Filho, Pedro Paulo Balage and De Uz\^{e}da, Vin\'{\i}cius Rodrigues and Pardo, Thiago Alexandre Salgueiro and Das Gra\c{c}as Volpe Nunes, Maria},
title = {Experiments on Applying a Text Summarization System for Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present and analyze the results of the application of a text summarization system - GistSumm - to the task of monolingual question answering at CLEF 2006 for Portuguese texts. We hypothesized that topicoriented summarization techniques could be able to produce more accurate answers. However, our results showed that there is a big gap to be overcome for summarization to be directly applied to question-answering tasks.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {372–376},
numpages = {5},
keywords = {text summarization, question answering},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394009,
author = {Buscaldi, Davide and Gomez, Jos\'{e} Manuel and Rosso, Paolo and Sanchis, Emilio},
title = {N-Gram vs. Keyword-Based Passage Retrieval for Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe the participation of the Universidad Polit\'{e}cnica of Valencia to the 2006 edition, which was focused on the comparison between a Passage Retrieval engine (JIRS) specifically aimed to the Question Answering task and a standard, general use search engine such as Lucene. JIRS is based on n-grams, Lucene on keywords. We participated in three monolingual tasks: Spanish, Italian and French. The obtained results show that JIRS is able to return high quality passages, especially in Spanish.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {377–384},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394010,
author = {Pu\c{s}ca\c{s}u, Georgiana and Iftene, Adrian and Pistol, Ionu\c{t} and Trandab\u{a}\c{t}, Diana and Tufi\c{s}, Dan and Ceau\c{s}u, Alin and \c{S}tef\u{a}nescu, Dan and Ion, Radu and Dornescu, Iustin and Moruz, Alex and Cristea, Dan},
title = {Cross-Lingual Romanian to English Question Answering at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the development of a Question Answering (QA) system and its evaluation results in the Romanian-English cross-lingual track organized as part of the CLEF 2006 campaign. The development stages of the cross-lingual Question Answering system are described incrementally throughout the paper, at the same time pinpointing the problems that occurred and the way they were addressed. The system adheres to the classical architecture for QA systems, debuting with question processing followed, after term translation, by information retrieval and answer extraction. Besides the common QA difficulties, the track posed some specific problems, such as the lack of a reliable translation engine from Romanian into English, and the need to evaluate each module individually for a better insight into the system's failures.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {385–394},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394011,
author = {Besan\c{c}on, Romaric and Embarek, Mehdi and Ferret, Olivier},
title = {Finding Answers in the undefinedDipe System by Extracting and Applying Linguistic Patterns},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The simple techniques used in the OEdipe question answering system developed by the CEA-LIST/LIC2M did not perform well on definition questions in past CLEF-QA campaigns. We present in this article a new module for this QA system dedicated to this type of questions. This module is based on the automatic learning of lexico-syntactic patterns from examples. These patterns are then used for the extraction of short answers for definition questions. This technique has been experimented in the French monolingual track of the CLEF-QA 2006 evaluation, and results obtained have shown an improvement on this type of questions, compared to our previous participation.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {395–404},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394012,
author = {Costa, Lu\'{\i}s},
title = {Question Answering beyond CLEF Document Collections},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Esfinge is a general domain Portuguese question answering system, which uses the information available in the Web as an additional resource when searching for answers. The experiments described in this paper show that the Web helped more with this year's questions than in previous years and that using the Web to support answers also enables the system to answer correctly or at least to find relevant documents for more questions. In this third participation in CLEF, the main goals were (i) to check whether a database of word cooccurrences could improve the answer scoring algorithm and (ii) to get higher benefits from the use of a named entity recognizer that was used last year in sub-optimal conditions. 2006 results were slightly better than last year's, even though the question set included more definitions of type Que \'{e} X? (What is X?), with which the system had the worst results in previous participations.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {405–414},
numpages = {10},
keywords = {question answering, measurement, experimentation, performance, named entity recognition},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394013,
author = {Ju\'{a}rez-Gonz\'{a}lez, Antonio and T\'{e}llez-Valero, Alberto and Denicia-Carral, Claudia and Montes-y-G\'{o}mez, Manuel and Villase\~{n}or-Pineda, Luis},
title = {Using Machine Learning and Text Mining in Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes a QA system centered in a full data-driven architecture. It applies machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing resources such as named entity classifiers, parsers and ontologies. Experimental results on the Spanish Question Answering task at CLEF 2006 show that the proposed architecture can be a practical solution for monolingual question answering by reaching a precision as high as 51%.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {415–423},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394014,
author = {P\'{e}rez-Couti\~{n}o, Manuel and Montes-y-G\'{o}mez, Manuel and L\'{o}pez-L\'{o}pez, Aurelio and Villase\~{n}or-Pineda, Luis and Pancardo-Rodr\'{\i}guez, Aar\'{o}n},
title = {Applying Dependency Trees and Term Density for Answer Selection Reinforcement},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the experiments performed for the QA@CLEF- 2006 within the joint participation of the eLing Division at VEng and the Language Technologies Laboratory at INAOE. The aim of these experiments was to observe and quantify the improvements in the final step of the Question Answering prototype when some syntactic features were included into the decision process. In order to reach this goal, a shallow approach to answer ranking based on the term density measure has been integrated into the weighting schema. This approach has shown an interesting improvement against the same prototype without this module. The paper discusses the results achieved, the conclusions and further directions within this research.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {424–431},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394015,
author = {Hartrumpf, Sven and Leveling, Johannes},
title = {Interpretation and Normalization of Temporal Expressions for Question Answering},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The German question answering (QA) system InSicht participated in QA@CLEF for the third time. InSicht implements a deep QA approach: it builds on full sentence parses, inferences on semantic representations, and matching semantic representations derived from questions and documents. InSicht was improved for QA@CLEF 2006 as follows: temporal expressions are normalized and temporal deictic expressions are resolved to explicit date representations; the coreference module was extended by a fallback strategy for increased robustness; equivalence rules can introduce negated relations; answer candidates are clustered in order to avoid multiple occurrences of one real-world entity in the answers to a list question; and finally a shallow QA subsystem that produces a second answer stream was integrated into InSicht. The current system is evaluated in an ablation study on the German questions from QA@CLEF 2006.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {432–439},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394016,
author = {Gillard, Laurent and Sitbon, Laurianne and Blaudez, Eric and Bellot, Patrice and El-B\`{e}ze, Marc},
title = {Relevance Measures for Question Answering, the LIA at QA@CLEF-2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This article presents the first participation of the Avignon University Science Laboratory (LIA) to the Cross Language Evaluation Forum (CLEF). LIA participated to the monolingual Question Answering (QA) track dedicated to the French language, and to the cross-lingual English to French QA track. After a description of the strategies used by our system to answer questions, an analysis of the results obtained is done in order to discuss and outline possible improvements. Two strategies were used: for factoid questions, answer candidates are identified as Named Entities and selected by using key terms density measures ("compactness"); on the other hand, for definition questions, our approach is based on detection of frequent appositive informational nuggets appearing near the focus.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {440–449},
numpages = {10},
keywords = {questions beyond factoids, compactness, hey terms density metrics, question answering, definition questions, multilingual question answering},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394017,
author = {Ferr\'{a}ndez, S. and L\'{o}pez-Moreno, P. and Roger, S. and Ferr\'{a}ndez, A. and Peral, J. and Alvarado, X. and Noguera, E. and Llopis, F.},
title = {Monolingual and Cross-Lingual QA Using AliQAn and BRILI Systems for CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A previous version of AliQAn participated in the CLEF 2005 Spanish Monolingual Question Answering task. For this year's run, to the system are added new and representative patterns in question analysis and extraction of the answer. A new ontology of question types has been included. The inexact questions have been improved. The information retrieval engine has been modified considering only the detected keywords from the question analysis module. Besides, many PoS Tagger and SUPAR errors have been solved and finally, dictionaries about cities and countries have been incorporated. To deal with Cross-Lingual tasks, we employ the BRILI system. The achieved results are overall accuracy of 37.89% for monolingual and 21.58% for bilingual tasks.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {450–453},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394018,
author = {Grau, Brigitte and Ligozat, Anne-Laure and Robba, Isabelle and Bagur, Anne Vilnat Michael and S\'{e}journ\'{e}, Kevin},
title = {The Bilingual System MUSCLEF at QA@CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents our bilingual question answering system MUSCLEF. We underline the difficulties encountered when shifting from a mono to a cross-lingual system, then we focus on the evaluation of three modules of MUSCLEF: question analysis, answer extraction and answer fusion. We finally present how we re-used different modules of MUSCLEF to participate in AVE (Answer Validation Exercise).},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {454–462},
numpages = {9},
keywords = {multiword expressions, question answering, evaluation},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394019,
author = {De Pablo-S\'{a}nchez, C\'{e}sar and Gonz\'{a}lez-Ledesma, Ana and Moreno-Sandoval, Antonio and Vicente-D\'{\i}ez, Maria Teresa},
title = {MIRACLE Experiments in QA@CLEF 2006 in Spanish: Main Task, Real-Time QA and Exploratory QA Using Wikipedia (WiQA)},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe the participation of MIRACLE group in the QA track at CLEF. We participated in three subtasks and presented two systems that works in Spanish. The first system is a traditional QA system and was evaluated in the main task and the Real-Time QA pilot. The system features improved Named Entity recognition and shallow linguistic analysis and achieves moderate performance. In contrast, results obtained in RT-QA shows that this approach is promising to provide answers in constrained time. The second system focus in the WiQA pilot task, that aims at retrieving important snippets to complete a Wikipedia. The system uses collection link structure, cosine similarity and Named Entities to retrieve new and important snippets. Although the experiments have not been exhaustive it seems that the performance depends on the type of concept.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {463–472},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394020,
author = {Sarmento, Lu\'{\i}s},
title = {A First Step to Address Biography Generation as an Iterative QA Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The purpose of this article is two-fold: (i) to present RAPOSA, an open-domain automatic question answering system for portuguese that participated in QA track at CLEF 2006 for the first time, and (ii) to explain how RAPOSA is intended to be a key component of a larger information extraction framework that uses question-answering technology as the basis for automatic generation of biographies. We will make a first attempt to classify questions regarding their relevance for iterative biography generation and, according to such classification, we will then explain our motivation for participating in CLEF 2006. We will describe the architecture of RAPOSA, explaining the internal details of each of its composing modules. Next, we will present the results RAPOSA obtained on this year's edition of the QA track, and we will identify and comment on the main causes of error. Finally, we will present directions for future work.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {473–482},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394022,
author = {Rodrigo, \'{A}lvaro and Pe\~{n}as, Anselmo and Herrera, Jes\'{u}s and Verdejo, Felisa},
title = {The Effect of Entity Recognition on Answer Validation},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Answer Validation Exercise (AVE) 2006 is aimed at evaluating systems able to decide whether the responses of a Question Answering (QA) system are correct or not. Since most of the questions and answers contain entities, the use of a textual entailment relation between entities is studied here for the task of Answer Validation. We present some experiments concluding that the entity entailment relation is a feature that improves a SVM based classifier close to the best result in AVE 2006.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {483–489},
numpages = {7},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394023,
author = {Ferr\'{a}ndez, \'{O}. and Terol, R. M. and Mu\~{n}oz, R. and Mart\'{\i}nez-Barco, P. and Palomar, M.},
title = {A Knowledge-Based Textual Entailment Approach Applied to the AVE Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a system that has been initially used for Recognising Textual Entailment (RTE). Due to the fact that these two tasks (AVE and RTE) rely on the same main idea, our system was considered appropriate to be applied to AVE. Our system represents texts by means of logic forms and computes the semantic similarity between them. We have also designed a voting strategy between our system and the MLEnt system also developed at the University of Alicante. Although the results have not been very high, we consider them quite promising.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {490–493},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394024,
author = {Tatu, Marta and Iles, Brandon and Moldovan, Dan},
title = {Automatic Answer Validation Using COGEX},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports the performance of Language Computer Corporation's natural language logic prover for the English and Spanish subtasks of the Answer Validation Exercise. Cogex takes as input a pair of plain English text snippets, transforms them into semantically rich logic forms, automatically generates natural language axioms which will be used during the search for a proof, and determines the degree of entailment between the entailing text snippet and a possible relaxed version of the entailed text. The system labels an input pair as positive entailment if its proof score is above the threshold learned during training. Our semantic logic-based approach achieves 43.93% F-measure for the English data and 60.63% on Spanish.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {494–501},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394025,
author = {Bosma, Wauter and Callison-Burch, Chris},
title = {Paraphrase Substitution for Recognizing Textual Entailment},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe a method for recognizing textual entailment that uses the length of the longest common subsequence (LCS) between two texts as its decision criterion. Rather than requiring strict word matching in the common subsequences, we perform a flexible match using automatically generated paraphrases.We find that the use of paraphrases over strict word matches represents an average F-measure improvement from 0.22 to 0.36 on the CLEF 2006 Answer Validation Exercise for 7 languages.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {502–509},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394026,
author = {Zanzotto, Fabio Massimo and Moschitti, Alessandro},
title = {Experimenting a "General Purpose" Textual Entailment Learner in AVE},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present the use of a "general purpose" textual entailment recognizer in the Answer Validation Exercise (AVE) task. Our system is designed to learn entailment rules from annotated examples. Its main feature is the use of Support Vector Machines (SVMs) with kernel functions based on cross-pair similarity between entailment pairs. We experimented with our system using different training sets: RTE and AVE data sets. The comparative results show that entailment rules can be learned. Although, the high variability of the outcome prevents us to derive definitive conclusions, the results show that our approach is quite promising and improvable in the future.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {510–517},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394027,
author = {Gl\'{o}ckner, Ingo},
title = {Answer Validation through Robust Logical Inference},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The paper features MAVE, a knowledge-based system for answer validation through deep linguistic processing and logical inference. A relaxation loop is used to determine a robust indicator of logical entailment. The system not only validates answers directly, but also gathers evidence by proving the original question and comparing results with the answer candidate. This method boosts recall by up to 13%. Additional indicators signal false positives. The filter increases precision by up to 14.5% without compromising recall of the system.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {518–521},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394028,
author = {Kozareva, Zornitsa and V\'{a}zquez, Sonia and Montoyo, Andr\'{e}s},
title = {University of Alicante at QA@CLEF2006: Answer Validation Exercise},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present an Answer Validation System which is based on the combination of word overlap and Latent Semantic Indexing modules. The main contribution of our work consist in the adaptation of our already developed machine-learning textual entailment system MLEnt to the multilingual Answer Validation exercise.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {522–525},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394029,
author = {Kouylekov, Milen and Negri, Matteo and Magnini, Bernardo and Coppola, Bonaventura},
title = {Towards Entailment-Based Question Answering: ITC-Irst at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This year, besides providing support to other groups participating in cross-language Question Answering (QA) tasks, and submitting runs both for the monolingual Italian and for the cross-language Italian/English tasks, the ITC-irst participation in the CLEF campaign concentrated on the Answer Validation Exercise (AVE). The participation in the AVE task, with an answer validation module based on textual entailment recognition, is motivated by our objectives of (i) creating a modular framework for an entailment-based approach to QA, and (ii) developing, in compliance with this framework, a stand-alone component for answer validation which implements different approaches to the problem.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {526–536},
numpages = {11},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394031,
author = {Adafre, Sisay Fissaha and Jijkoun, Valentin and De Rijke, Maarten},
title = {Link-Based vs. Content-Based Retrieval for Question Answering Using Wikipedia},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe our participation in the WiQA 2006 pilot on question answering using Wikipedia, with a focus on comparing link-based vs content-based retrieval. Our system currently works for Dutch and English.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {537–540},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394032,
author = {Sutcliffe, Richard F. E. and Steinberger, Josef and Kruschwitz, Udo and Alexandrov-Kabadjov, Mijail and Poesio, Massimo},
title = {Identifying Novel Information Using Latent Semantic Analysis in the WiQA Task at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In our two-stage system for the English monolingual WiQA Task, snippets were first retrieved if they contained an exact match with the title. Candidates were then passed to the Latent Semantic Analysis component which judged them Novel if their match with the article text was less than a threshold. In Run1, the ten best snippets were returned and in Run 2 the twenty best. Run 1 was superior, with Average Yield per Topic 2.46 and Precision 0.37. Compared to other groups, our performance was in the middle of the range except for Precision where our system was the best. We attribute this to our use of exact title matches in the IR stage. In future work we will vary the approach used depending on the topic type, exploit co-references in conjunction with exact matches and make use of the elaborate hyperlink structure which is a unique and most interesting aspect of the Wikipedia.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {541–549},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394033,
author = {Buscaldi, Davide and Rosso, Paolo},
title = {A Bag-of-Words Based Ranking Method for the Wikipedia Question Answering Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a simple approach to the Wikipedia Question Answering pilot task in CLEF 2006. The approach ranks the snippets, retrieved using the Lucene search engine, by means of a similarity measure based on bags of words extracted from both the snippets and the articles in wikipedia. Our participation was in the monolingual English and Spanish tasks. We obtained the best results in the Spanish one.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {550–553},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394034,
author = {Ruiz, Antonio Toral and Pu\c{s}ca\c{s}u, Georgiana and Monteagudo, Lorenza Moreno and Bevi\'{a}, Rub\'{e}n Izquierdo and Bor\'{o}, Estela Saquete},
title = {University of Alicante at WiQA 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the participation of University of Alicante at the WiQA pilot task organized as part of the CLEF 2006 campaign. For a given set of topics, this task presupposes the discovery of important novel information distributed across different Wikipedia entries. The approach we adopted for solving this task uses Information Retrieval, query expansion by feedback, novelty re-ranking, as well as temporal ordering. Our system has participated both in the Spanish and English monolingual tasks. For each of the two participations the results are promising because, by employing a language independent approach, we obtain scores above the average. Moreover, in the case of Spanish, our result is very close to the best achieved score. Apart from introducing our system, the present paper also provides an in-depth result analysis, and proposes future lines of research, as well as follow-up experiments.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {554–560},
numpages = {7},
keywords = {information retrieval},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394035,
author = {Or\u{a}san, Constantin and Pu\c{s}ca\c{s}u, Georgiana},
title = {A High Precision Information Retrieval Method for WiQA},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents Wolverhampton University's participation in the WiQA competition. The method chosen for this task combines a high precision, but low recall information retrieval approach with a greedy sentence ranking algorithm. The high precision retrieval is ensured by querying the search engine with the exact topic, in this way obtaining only sentences which contain the topic. In one of the runs, the set of retrieved sentences is expanded using coreferential relations between sentences. The greedy algorithm used for ranking selects one sentence at a time, always the one which adds most information to the set of sentences without repeating the existing information too much. The evaluation revealed that it achieves a performance similar to other systems participating in the competition and that the run which uses coreference obtains the highest MRR score among all the participants.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {561–568},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394036,
author = {Santos, Diana and Costa, Lu\'{\i}s},
title = {QolA: Fostering Collaboration within QA},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we suggest a QA pilot task, dubbed QolA, whose joint rationale is allow for collaboration among systems, increase multilinguality and multicollection use, and investigate ways of dealing with different strengths and weaknesses of a population of QA systems. We claim that merging answers, weighting answers, choosing among contradictory answers or generating composite answers, and verifying and validating information, by posing related questions, should be part and parcel of the question answering process. The paper motivates these ideas and suggests a way to foster research in these areas by deploying QA systems as Web services.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {569–578},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394038,
author = {Clough, Paul and Grubinger, Michael and Deselaers, Thomas and Hanbury, Allan and M\"{u}ller, Henning},
title = {Overview of the ImageCLEF 2006 Photographic Retrieval and Object Annotation Tasks},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the general photographic retrieval and object annotation tasks of the ImageCLEF 2006 evaluation campaign. These tasks provided both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information systems for image retrieval and automatic image annotation. Both tasks offered something new for 2006 and attracted a large number of submissions: 12 groups participated in ImageCLEFphoto and 3 groups in the automatic annotation task. This paper summarises these two tasks including collections used in the benchmark, the tasks proposed, a summary of submissions from participating groups and the main findings.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {579–594},
numpages = {16},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394039,
author = {M\"{u}ller, Henning and Deselaers, Thomas and Deserno, Thomas and Clough, Paul and Kim, Eugene and Hersh, William},
title = {Overview of the ImageCLEFmed 2006 Medical Retrieval and Medical Annotation Tasks},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the medical image retrieval and annotation tasks of ImageCLEF 2006. Both tasks are described with respect to goals, databases, topics, results, and techniques. The ImageCLEFmed retrieval task had 12 participating groups (100 runs). Most runs were automatic, with only a few manual or interactive. Purely textual runs were in the majority compared to purely visual runs but most were mixed, using visual and textual information. None of the manual or interactive techniques were significantly better than automatic runs. The best-performing systems used visual and textual techniques combined, but combinations of visual and textual features often did not improve performance. Purely visual systems only performed well on visual topics. The medical automatic annotation used a larger database of 10,000 training images from 116 classes, up from 9,000 images from 57 classes in 2005. Twelve groups submitted 28 runs. Despite the larger number of classes, results were almost as good as in 2005 which demonstrates a clear improvement in performance. The best system of 2005 would have received a position in the middle in 2006.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {595–608},
numpages = {14},
keywords = {medical information retrieval, automatic image annotation, image retrieval},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394041,
author = {Larson, Ray R.},
title = {Text Retrieval and Blind Feedback for the ImageCLEFphoto Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we will describe Berkeley's approach to the ImageCLEFphoto task for CLEF 2006. This year is the first time that we have participated in ImageCLEF, and we chose to primarily establish a baseline for the Cheshire II system for this task, while we had originally hoped to use GeoCLEF methods for this task, in the end time constraints led us to restrict our submissions to the basic required runs for the task.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {609–612},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394042,
author = {Mart\'{\i}nez-Fern\'{a}ndez, J. L. and Garc\'{\i}a-Serrano, Ana M. and Rom\'{a}n, Julio Villena and Mart\'{\i}nez, Paloma},
title = {Expanding Queries through Word Sense Disambiguation},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The use of semantic information in the right way can lead to improved precision and recall in Information Retrieval (IR) systems. This assumption is the start point for the work carried out by the MIRACLE research team at ImageCLEF 2006. For this purpose, an implementation of the specification marks Word Sense Disambiguation (WSD) method Error! Reference source not found. has been developed. This method is based on WordNet Error! Reference source not found. and tries to select the right sense of each word appearing in the query. This allows the inclusion of only the correct synonyms when a semantic expansion is done. This selective expansion method has been combined with a deeper linguistic analysis to interpret negations and filter out common phrases and expressions used in query captions. Obtained results are included in Section 3 and, unfortunately, show that this approach to deal with semantics in IR systems does not lead to better precision and recall.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {613–616},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394043,
author = {Inoue, Masashi},
title = {Using Visual Linkages for Multilingual Image Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The use of visual features in text-based ad-hoc image retrieval is challenging. Visual and textual information have so far been treated equally in terms of their properties and combined with weighting mechanisms for balancing their contributions to the ranking. The use of visual and textual information in a single retrieval system sometimes limits its applicability due to the lack of modularity. In this paper, we propose an image retrieval method that separates the usage of visual information from that of textual information. Visual clustering establishes linkages between images and the relationships are later used for the reranking. By applying clustering on visual features prior to the ranking, the main retrieval process becomes purely text-based. Experimental results on the ImageCLEFphoto ad-hoc task show this scheme is suitable for querying multilingual collections on some search topics.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {617–624},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394044,
author = {Chang, Yih-Chen and Chen, Hsin-Hsi},
title = {Approaches of Using a Word-Image Ontology and an Annotated Image Corpus as Intermedia for Cross-Language Image Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Two kinds of intermedia are explored in ImageCLEFphoto2006. The approach of using a word-image ontology maps images to fundamental concepts in an ontology and measure the similarity between two images by using the kind-of relationship of the ontology. The approach of using an annotated image corpus maps images to texts describing concepts in the images, and the similarity of two images is measured by text counterparts using BM25. The official runs show that visual query and intermedia are useful. Comparing the runs using textual query only with the runs merging textual query and visual query, the latter improved 71%-119% of the performance of the former. Even in the situation which example images were removed from the image collection beforehand, the performance was still improved about 21%-43%.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {625–632},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394045,
author = {McDonald, Kieran and Jones, Gareth J. F.},
title = {Dublin City University at CLEF 2006: Experiments for the ImageCLEF Photo Collection Standard Ad Hoc Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For the CLEF 2006 Cross Language Image Retrieval (ImageCLEF) Photo Collection Standard Ad Hoc task, DCU performed monolingual and cross language retrieval using photo annotations with and without feedback, and also a combined visual and text retrieval approach. Topics are translated into English using the Babelfish online machine translation system. Text runs used the BM25 algorithm, while visual approach used simple low-level features with matching based on the Jeffrey Divergence measure. Our results consistently indicate that the fusion of text and visual features is best for this task, and that performing feedback for text consistently improves on the baseline nonfeedback BM25 text runs for all language pairs.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {633–637},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394047,
author = {M\"{u}ller, Henning and Gass, Tobias and Geissbuhler, Antoine},
title = {Image Classification with a Frequency-Based Information Retrieval Scheme for ImageCLEFmed 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This article describes the participation of the University and Hospitals of Geneva at the ImageCLEF 2006 image classification tasks (medical and non-medical). The techniques applied are based on classical tf/idf weightings of visual features as used in the GIFT (GNU Image Finding Tool). Based on the training data, features appearing in images of the same class are weighted higher than features appearing across classes. These feature weights are added to the classical weights. Several weightings and learning approaches are applied as well as quantisations of the features space with respect to grey levels. A surprisingly small number of grey levels leads to best results. Learning can improve the results only slightly and does not obtain as good results as classical image classification approaches. A combination of several classifiers leads to best final results, showing that the schemes have independent results.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {638–643},
numpages = {6},
keywords = {image retrieval, frequency-based, classification},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394048,
author = {Setia, Lokesh and Teynor, Alexandra and Halawani, Alaa and Burkhardt, Hans},
title = {Grayscale Radiograph Annotation Using Local Relational Features},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Content based automatic image classification systems are increasingly finding usage, e.g. in large medical image databases. This paper concentrates on a grayscale radiograph annotation task which was a part of the ImageCLEF 2006. We use local features calculated around interest points, which have recently received excellent results for various image recognition and classification tasks. We propose the use of relational features, which are highly robust to illumination changes, and thus quite suitable for X-Ray images. Results with various feature and classifier settings are reported. A significant improvement in results is seen when the relative positions of the interest points are also taken into account during matching. For the given test set, our best run had a classification error rate of 16.7 %, just 0.5 % higher than the best overall submission, and therewith was ranked second in the medical automatic annotation task at the ImageCLEF 2006. The proposed method is general, can be applied to other image classification tasks and can also be extended to colour images.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {644–651},
numpages = {8},
keywords = {image annotation, radiograph, local features, invariants},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394049,
author = {Daumke, Philipp and Paetzold, Jan and Marko, Kornel},
title = {MorphoSaurus in ImageCLEF 2006: The Effect of Subwords on Biomedical IR},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the 2006 ImageCLEF Medical Image Retrieval task we evaluate the effects of deep morphological analysis for mono-and cross-lingual document retrieval in the biomedical domain. The morphological analysis is based on the MorphoSaurus system in which subwords are introduced as morphologically meaningful word units. Subwords are organized in language specific lexica that were partly manually and partly automatically generated and currently cover six European languages. They are linked together in a multilingual thesaurus. The use of subwords instead of full words significantly reduces the number of lexical entries that are needed to sufficiently cover a specific language and domain. A further benefit of the approach is its independence from the underlying retrieval system. We combined MorphoSaurus with the open-source search engine Lucene and achieved precision gains of up to 25% over the baseline for a monolingual setting and promising results in a multilingual scenario.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {652–659},
numpages = {8},
keywords = {morpho-semantic indexing, biomedical information retrieval, stemming, subword indexing, cross language information retrieval},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394050,
author = {Hersh, William and Kalpathy-Cramer, Jayashree and Jensen, Jeffery},
title = {Medical Image Retrieval and Automated Annotation: OHSU at ImageCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Oregon Health &amp; Science University participated in both the medical retrieval and medical annotation tasks of ImageCLEF 2006. Our efforts in the retrieval task focused on manual modification of query statements and fusion of results from textual and visual retrieval techniques. Our results showed that manual modification of queries does improve retrieval performance, while data fusion of textual and visual techniques improves precision but lowers recall. However, since image retrieval may be a precision-oriented task, these data fusion techniques could be of value for many users. In the annotation task, we assessed a variety of learning techniques and obtained classification accuracy of up to 74% with test data.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {660–669},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394051,
author = {Florea, Filip and Rogozan, Alexandrina and Barbu, Eugen and Bensrhair, Abdelaziz and Darmoni, Stefan},
title = {MedIC at ImageCLEF 2006: Automatic Image Categorization and Annotation Using Combined Visual Representations},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The CISMeF group participated at the automatic annotation task of the 2006 ImageCLEF cross-language image retrieval track, employing the MedIC module. The module is designed to automatically extract annotations using image categorization. For the 2006 ImageCLEF annotation experiments we used two sets of visual representations: the first based on the PCA transformation of combined textural and statistic low-level visual features and the second oriented towards compact symbolic image descriptors extracted from the same low-level features. Only the first set of features was present in the actual competition and obtained the fourth rank, with only 1% of error more than the most accurate run of the competition. The comparison with the second representation set was conducted after the official benchmark, on the validation data set, and obtained similar results, but with significantly smaller image signatures.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {670–677},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394052,
author = {Liu, Jing and Hiu, Yang and Li, Mingjing and Ma, Songde and Ma, Wei-Ying},
title = {Medical Image Annotation and Retrieval Using Visual Features},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present the algorithms and results of our participation in the medical image annotation and retrieval tasks of ImageCLEFmed 2006. We exploit using global features and local features to describe medical images in the annotation task. Different kinds of global features are examined and the most descriptive ones are extracted to represent the radiographs, which effectively capture the intensity, texture and shape characters of the image content. We also evaluate the descriptive power of local features, i.e. local image patches, for medical images. A newly developed spatial pyramid matching algorithm is applied to measure the similarity between images represented by sets of local features. Both descriptors use multi-class SVM to classify the images. We achieve an error rate of 17.6% for global descriptor and 18.2% for the local one, which rank sixth and ninth respectively among all the submissions. For the medical image retrieval task, we only use visual features to describe the images. No textual information is considered. Different features are used to describe gray images and color images. Our submission achieves a mean average precision (MAP) of 0.0681, which ranks second in the 11 runs that also only use visual features.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {678–685},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394053,
author = {G\"{u}ld, Mark O. and Thies, Christian and Fischer, Benedikt and Deserno, Thomas M.},
title = {Baseline Results for the ImageCLEF 2006 Medical Automatic Annotation Task},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The ImageCLEF 2006 medical automatic annotation task encompasses 11,000 images from 116 categories, compared to 57 categories for 10,000 images of the similar task in 2005. As a baseline for comparison, a run using the same classifiers with the identical parameterization as in 2005 is submitted. In addition, the parameterization of the classifier was optimized according to the 9,000/1,000 split of the 2006 training data. In particular, texture-based classifiers are combined in parallel with classifiers, which use spatial intensity information to model common variabilities among medical images. However, all individual classifiers are based on global features, i.e. one feature vector describes the entire image. The parameterization from 2005 yields an error rate of 21.7%, which ranks 13th among the 28 submissions. The optimized classifier yields 21.4% error rate (rank 12), which is insignificantly better.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {686–689},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394054,
author = {Qiu, Bo},
title = {A Refined SVM Applied in Medical Image Annotation},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For the medical image annotation task of ImageCLEF2006, we developed a refined SVM method. This method includes two stages, in which accordingly the coarse and fine classifications are performed. At the coarse stage, the common Support Vector Machines (SVM) method is used with the corresponding image feature, down-scaled low resolution pixel map (32\texttimes{}32). At the refined stage, the results from the first stage are refined following the steps: 1) select those images near to the borders of SVM classifiers, which are to be re-classified and selected by a predefined threshold of SVM similarity value; 2) form a new training dataset, to eliminate the influence of the great volume unbalance among classes; 3) use three classification methods and image features: 20\texttimes{}50 low resolution pixel maps feed into SVMs; SIFT features feed into Euclidean distance classifiers; 16\texttimes{}16 low resolution pixel maps feed into PCA classifiers. 4) combine the results from 3). At last the results from the two stages are combined to form the final classification result. Our experimental results showed that with the two-stage method a significant improvement of the classification rate has been achieved.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {690–693},
numpages = {4},
keywords = {refined SVM, medical image annotation},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394055,
author = {Lacoste, Caroline and Chevallet, Jean-Pierre and Lim, Joo-Hwee and Le, Diem Thi Hoang and Xiong, Wei and Racoceanu, Daniel and Teodorescu, Roxana and Vuillenemot, Nicolas},
title = {Inter-Media Concept-Based Medical Image Indexing and Retrieval With UMLS at IPAL},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We promote the use of explicit medical knowledge to solve retrieval of information both visual and textual. For text, this knowledge is a set of concepts from a Meta-thesaurus, the Unified Medical Language System (UMLS). For images, this knowledge is a set of semantic features that are learned from examples using SVM within a structured learning framework. Image and text index are represented in the same way: a vector of concepts. The use of concepts allows the expression of a common index form: an inter-media index, offering the opportunity of homogeneous indexing/querying time fusion techniques. Top results obtained with concept based approaches show the potential of conceptual indexing.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {694–701},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394056,
author = {Ruiz, Miguel E.},
title = {UB at ImageCLEFmed 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the results of the University at Buffalo in the 2006 ImageCLEFmed task. Our approach for this task combines Content Based Image Retrieval (CBIR) and text retrieval to improve retrieval of medical images. Our results are comparable to other approaches presented in the task. Our results show that text retrieval performs well across the three different types of topics (visual, visual-semantic and semantic) and that the combination of CBIR and text retrieval yields moderate improvements.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {702–705},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394058,
author = {Gobeill, Julien and M\"{u}ller, Henning and Ruch, Patrick},
title = {Translation by Text Categorisation: Medical Image Retrieval in ImageCLEFmed 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present the fusion of simple retrieval strategies with thesaural resources to perform document and query translation by text categorisation for cross-language retrieval in a collection of medical images with case notes. The collection includes documents in French, English and German. The fusion of visual and textual content is also treated. Unlike most automatic categorisation systems our approach can be applied with any controlled vocabulary and does not require training data. For the experiments we use Medical Subject Headings (MeSH), a terminology maintained by the National Library of Medicine existing in 12 languages. The idea is to annotate every text of the collection (documents and queries) with a set of MeSH terms using our automatic text categoriser. Our results confirm that such an approach is competitive. Simple linear approaches were used to combine text and visual features.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {706–710},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394059,
author = {D\'{\i}az-Galiano, M. C. and Garc\'{\i}a-Cumbreras, M. \'{A}. and Mart\'{\i}n-Valdivia, M. T. and Montejo-R\'{a}ez, A. and Ure\~{n}a-L\'{o}pez, L. Alfonso},
title = {Using Information Gain to Improve the ImageCLEF 2006 Collection},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the SINAI team's participation in both the ad hoc task and the medical task. For the ad hoc task we use a new Machine Translation system which works with several translators and heuristics. For the medical task, we have processed the set of collections using Information Gain (IG) to identify the best tags that should be considered in the indexing process.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {711–714},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394060,
author = {Rahman, M. M. and Sood, V. and Desai, B. C. and Bhattacharya, P.},
title = {CINDI at ImageCLEF 2006: Image Retrieval &amp; Annotation Tasks for the General Photographic and Medical Image Collections},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the techniques and the analysis of different runs submitted by the CINDI group in ImageCLEF 2006. For the ad-hoc image retrieval from both the photographic and medical image collections, we experimented with cross-modal (image and text) interaction and integration approaches based on the relevance feedback in the form of textual query expansion and visual query point movement with adaptive similarity matching functions. For the automatic annotation tasks for both the medical and object collections, we experimented with a classifier combination approach, where several probabilistic multi-class support vector machine classifiers with features at different levels as inputs are combined to predict the final probability scores of each category as image annotation. Analysis of the results of the different runs we submitted for both the image retrieval and annotation tasks are reported in this paper.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {715–724},
numpages = {10},
keywords = {classifier combination, fusion, annotation, multi-modality, content-based image retrieval, relevance feedback},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394061,
author = {Deselaers, Thomas and Weyand, Tobias and Ney, Hermann},
title = {Image Retrieval and Annotation Using Maximum Entropy},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present and discuss our participation in the four tasks of the ImageCLEF 2006 Evaluation. In particular, we present a novel approach to learn feature weights in our content-based image retrieval system FIRE. Given a set of training images with known relevance among each other, the retrieval task is reformulated as a classification task and then the weights to combine a set of features are trained discriminatively using the maximum entropy framework. Experimental results for the medical retrieval task show large improvements over heuristically chosen weights. Furthermore the maximum entropy approach is used for the automatic image annotation tasks in combination with a part-based object model. Using our object classification methods, we obtained the best results in the medical and in the object annotation task.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {725–734},
numpages = {10},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394062,
author = {Maillot, Nicolas and Chevallet, Jean-Pierre and Lim, Joo Hwee},
title = {Inter-Media Pseudo-Relevance Feedback Application to ImageCLEF 2006 Photo Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper shows how to use a text retrieval and an image retrieval engine in a cooperative way. The proposed Inter-Media Pseudo-Relevance Feedback approach shows how the image modality can be used for expanding the text query and significantly improve the performance (MAP) of a multimedia information retrieval system. The document collection used for experiments is provided by the International Association for Pattern Recognition (IAPR) [1]. The proposed approach is completely automatic and has been used to participate to the ImageCLEF 2006 Photo Retrieval task.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {735–738},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394063,
author = {Wilhelm, Thomas and Eibl, Maximilian},
title = {ImageCLEF 2006 Experiments at the Chemnitz Technical University},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a report about our participation in the ImageCLEF photo task 2006 and a short description of our new framework for future use in further CLEF participations. We described and analysed our participation in the monolingual English task. A special Lucene-aligned query expansion and histogram comparison are helping to improve the baseline results.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {739–743},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394065,
author = {Oard, Douglas W. and Wang, Jianqiang and Jones, Gareth J. F. and White, Ryen W. and Pecina, Pavel and Soergel, Dagobert and Huang, Xiaoli and Shafran, Izhak},
title = {Overview of the CLEF-2006 Cross-Language Speech Retrieval Track},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The CLEF-2006 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Five teams participated in the English evaluation, performing both monolingual and cross-language searches of speech recognition transcripts, automatically generated metadata, and manually generated metadata. Results indicate that the 2006 English evaluation topics are more challenging than those used in 2005, but that cross-language searching continued to pose no unusual challenges when compared with monolingual searches of the same collection. Three teams participated in the monolingual Czech evaluation using a new evaluation measure based on differences between system-suggested and ground truth replay start times, with results that were broadly comparable to those observed for English.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {744–758},
numpages = {15},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394066,
author = {Ircing, Pavel and M\"{u}ller, Lud\^{e}k},
title = {Benefit of Proper Language Processing for Czech Speech Retrieval in the CL-SR Task at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The paper describes the system built by the team from the University of West Bohemia for participation in the CLEF 2006 CL-SR track. We have decided to concentrate only on the monolingual searching in the Czech test collection and investigate the effect of proper language processing on the retrieval performance. We have employed the Czech morphological analyser and tagger for that purposes. For the actual search system, we have used the classical tf.idf approach with blind relevance feedback as implemented in the Lemur toolkit. The results indicate that a suitable linguistic preprocessing is indeed crucial for the Czech IR performance.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {759–765},
numpages = {7},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394067,
author = {Terol, Rafael M. and Martinez-Barco, Patricio and Palomar, Manuel},
title = {Applying Logic Forms and Statistical Methods to CL-SR Performance},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes a CL-SR system that employs two different techniques: the first one is based on NLP rules that consist on applying logic forms to the topic processing while the second one basically consists on applying the IR-n statistical search engine to the spoken document collection. The application of logic forms to the topics allows to increase the weight of topic terms according to a set of syntactic rules. Thus, the weights of the topic terms are used by IR-n system in the information retrieval process.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {766–769},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394068,
author = {Aly, Robin and Hiemstra, Djoerd and Ordelman, Roeland and Van Der Werff, Laurens and De Jong, Franciska},
title = {XML Information Retrieval from Spoken Word Archives},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper the XML Information Retrieval System PF/Tijah is applied to retrieval tasks on large spoken document collections. The used example setting is the English CLEF-2006 CL-SR collection together with given English topics and self produced Dutch topics. The main findings presented in this paper are the easy way of adapting queries to use different kinds and combinations of metadata. Furthermore simple ways of combining different metadata kinds are shown to be beneficial in terms of mean average precision.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {770–777},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394069,
author = {Alzghool, Muath and Inkpen, Diana},
title = {Experiments for the Cross Language Speech Retrieval Task at CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the second participation of the University of Ottawa group in the Cross-Language Speech Retrieval (CL-SR) task at CLEF 2006. We present the results of the submitted runs for the English collection and very briefly for the Czech collection, followed by many additional experiments. We have used two Information Retrieval systems in our experiments: SMART and Terrier, with several query expansion techniques (including a new method based on log-likelihood scores for collocations). Our experiments showed that query expansion methods do not help much for this collection. We tested different Automatic Speech Recognition transcripts and combinations. The retrieval results did not improve, probably because the speech recognition errors happened for the words that are important in retrieval. We present cross-language experiments, where the queries are automatically translated by combining the results of several online machine translation tools. Our experiments showed that high quality automatic translations (for French) led to results comparable with monolingual English, while the performance decreased for the other languages. Experiments on indexing the manual summaries and keywords gave the best retrieval results.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {778–785},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394070,
author = {Wang, Jianqiang and Oard, Douglas W.},
title = {CLEF-2006 CL-SR at Maryland: English and Czech},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The University of Maryland participated in the English and Czech tasks. For English, one monolingual run using fields based on automatic transcription (the required condition) and one (otherwise identical) cross-language run using French queries were officially scored. Weighted use of alternative translations yielded an apparent improvement over one-best translation that was not statistically significant, but statistical translation models trained on European Parliament proceedings were found to be poorly matched to this task. Three contrastive runs in which manually generated metadata from the English collection was indexed were also officially scored. Results for Czech were not informative in this first year of that task.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {786–793},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394071,
author = {Jones, Gareth J. F. and Zhang, Ke and Lam-Adesina, Adenike M.},
title = {Dublin City University at CLEF 2006: Cross-Language Speech Retrieval (CL-SR) Experiments},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Dublin City University participation in the CLEF 2006 CL-SR task concentrated on exploring the combination of the multiple fields associated with the documents. This was based on use of the extended BM25F field combination model originally developed for multifield text documents. Additionally, we again conducted runs with our existing information retrieval methods based on the Okapi model. This latter method required an approach to determining approximate sentence boundaries within the free-flowing automatic transcription provided, to enable us to use our summary-based pseudo relevance feedback (PRF). Experiments were conducted for the English document collection with topics translated into English using Systran V3.0 machine translation.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {794–802},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394073,
author = {Balog, Krisztian and Azzopardi, Leif and Kamps, Jaap and De Rijke, Maarten},
title = {Overview of WebCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We report on the CLEF 2006 WebCLEF track devoted to crosslingual web retrieval. We provide details about the retrieval tasks, the used topic set, and the results of the participants. WebCLEF 2006 used a stream of known-item topics consisting of: (i) manual topics (including a selection of WebCLEF 2005 topics, and a set of new topics) and (ii) automatically generated topics (generated using two techniques). The results over all topics show that current CLIR systems are very effective, retrieving on average the target page in the top ranks. Manually constructed topics result in higher performance than and automatically generated ones. And finally, the resulting scores on automatic topics provide a reasonable ranking of the systems, showing that automatically generated topics are an attractive alternative in situations where manual topics are not readily available.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {803–819},
numpages = {17},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394074,
author = {Figuerola, Carlos G. and Berrocal, Jos\'{e} L. Alonso and Rodr\'{\i}guez, Angel F. Zazo and Rodr\'{\i}guez, Emilio},
title = {Improving Web Pages Retrieval Using Combined Fields},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This article describes the participation of the REINA Research Group of the University of Salamanca in WebCLEF 2006. This year we participated in the Monolingual Mixed Task in Spanish. The entire EuroGOV collection was processed to select all the pages in Spanish. All the pages with domain .es were also pre-selected. Our objective this year was to try pre-retrieval techniques of combining information fields or elements from web pages as well as the retrieval capability of these fields. In vector-based retrieval systems, the combining of terms coming from different sources can be achieved by operating on the frequency of the terms in the document using a weight scheme of tf\texttimes{}idf. The BODY field is, of course, the most useful from the retrieval perspective, but the text of the backlinks brings considerable improvement. META fields or tags, however, contribute little to retrieval improvement.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {820–825},
numpages = {6},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394075,
author = {Pinto, David and Rosso, Paolo and Jim\'{e}nez, Ernesto},
title = {A Penalisation-Based Ranking Approach for the Mixed Monolingual Task of WebCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an approach of a cross-lingual information retrieval which uses a ranking method based on a penalisation version of the Jaccard formula. The obtained results after the submission of a set of runs to the WebCLEF 2006 have shown that this simple ranking formula may be used in a cross-lingual environment. A comparison with runs submitted by other teams ranks us in a third place by using all the topics. A fourth place is obtained with our best overall results by using only the new topic set, and a second place was got by using only the automatic topics of the new topic set. An exact comparison with the rest of the participants is in fact difficult to obtain and, therefore, we consider that further detailed analysis of the components should be done in order to determine the best components of the proposed system.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {826–829},
numpages = {4},
keywords = {retrieval models, mixed-monolingual search process},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394076,
author = {Balog, Krisztian and De Rijke, Maarten},
title = {Index Combinations and Query Reformulations for Mixed Monolingual Web Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We examine the effectiveness on the multilingual WebCLEF 2006 test set of light-weight methods that have proved successful in other web retrieval settings: combinations of document representations on the one hand and query reformulation techniques on the other.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {830–833},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394077,
author = {Heuwing, Ben and Mandl, Thomas and Str\"{o}tgen, Robert},
title = {Multilingual Web Retrieval Experiments with Field Specific Indexing Strategies for WebCLEF 2006 at the University of Hildesheim},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Experiments with the analysis and extraction of the HTML structure of web documents were carried out for WebCLEF 2006. In addition, blind relevance feedback was applied. As for WebCLEF 2005, a language independent indexing strategy was pursued. We experimented with HTML title, H1 element and other elements emphasizing text. Our index contained title and H1, emphasized elements, full and partial content. The best results with the WebCLEF 2005 topics were achieved with a strong weight on the title-element and a very small weight on emphasized text leading to a marginal improvement over the best post submission runs for the mixed-monolingual task at Web-CLEF 2005. For the WebCLEF 2006 topics, improved results were achieved for manually generated topics. The best performance for manual topics for WebCLEF 2006 was achieved with a strong weight on both HTML title as well as H1 elements, and a decreased weight for the other elements. Blind relevance feedback could not yet improve the results.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {834–837},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394078,
author = {Rojas, Franco and Jim\'{e}nez-Salazar, H\'{e}ctor and Pinto, David},
title = {Vocabulary Reduction and Text Enrichment at WebCLEF},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Nowadays, cross-lingual Information Retrieval (IR) is one of the greatest challenges to deal with. Besides, one of the most important issues in IR consists of the corpus vocabulary reduction. In real situations some methods of IR such as the well-known vector space model, it is necessary to reduce the term space. In this work, we have considered a vocabulary reduction process based on the selection of mid-frequency terms. Our approach enhances precision, but in order to obtain a better recall, we have conducted an enrichment process based on the addition of co-ocurrence terms. By using this approach, we have obtained an improvement of 40%, using the BiEnEs topics of the WebCLEF 2005 task. The obtained results in the current mixed monolingual task of the WebCLEF 2006 have shown that the text enrichment must be done before the vocabulary reduction process in order to get the best performance.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {838–843},
numpages = {6},
keywords = {multilingual information, information retrieval, text representation},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394079,
author = {Tomlinson, Stephen},
title = {Experiments with the 4 Query Sets of WebCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the WebCLEF mixed monolingual retrieval task of the Cross-Language Evaluation Forum (CLEF) 2006, the system was given 1939 known-item queries, and the goal was to find the desired page in the 82GB EuroGOV collection (3.4 million pages crawled from government sites of 27 European domains). The 1939 queries included 124 new manually-created queries, 195 manually-created queries from last year, and two sets of 810 automatically-generated queries. In our experiments, the results on the automatically-generated queries were not always predictive of the results on the manually-created queries; in particular, our title-weighting and duplicate-filtering techniques were fairly effective on the manually-created queries but were detrimental on the automatically-generated queries. Further investigation uncovered serious encoding issues with the automatically-generated queries; for instance, the queries labelled as Greek actually used Latin characters.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {844–847},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394080,
author = {Wijaya, Syntia and Widhi, Bimo and Khoerniawan, Tommy and Adriani, Mirna},
title = {Applying Relevance Feedback for Retrieving Web-Page Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a report on our participation in the mixed monolingual web task of the 2006 Cross-Language Evaluation Forum (CLEF). We compared the result of web page retrieval based on the page content, page title, and anchor page. The retrieval effectiveness for the combination of page content, page title, and anchor texts was better than that of the combination of page title and page title only. Applying the pseudo-relevance feedback improved the retrieval performance of the queries.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {848–851},
numpages = {4},
keywords = {web retrieval, relevance feedback},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394082,
author = {Gey, Fredric and Larson, Ray and Sanderson, Mark and Bischoff, Kerstin and Mandl, Thomas and Womser-Hacker, Christa and Santos, Diana and Rocha, Paulo and Di Nunzio, Giorgio M. and Ferro, Nicola},
title = {GeoCLEF 2006: The CLEF 2006 Cross-Language Geographic Information Retrieval Track Overview},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {After being a pilot track in 2005, GeoCLEF advanced to be a regular track within CLEF 2006. The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR): retrieval for topics with a geographic specification. For GeoCLEF 2006, twenty-five search topics were defined by the organizing groups for searching English, German, Portuguese and Spanish document collections. Topics were translated into English, German, Portuguese, Spanish and Japanese. Several topics in 2006 were significantly more geographically challenging than in 2005. Seventeen groups submitted 149 runs (up from eleven groups and 117 runs in GeoCLEF 2005). The groups used a variety of approaches, including geographic bounding boxes, named entity extraction and external knowledge bases (geographic thesauri and ontologies and gazetteers).},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {852–876},
numpages = {25},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394083,
author = {Go\~{n}i-Menoyo, Jos\'{e} M. and Gonz\'{a}lez-Crist\'{o}bal, Jos\'{e} C. and Lana-Serrano, Sara and Mart\'{\i}nez-Gonz\'{a}lez, \'{A}ngel},
title = {MIRACLE's Ad-Hoc and Geographical IR Approaches for CLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the 2006 Miracle team's approaches to the Ad-Hoc and Geographical Information Retrieval tasks. A first set of runs was obtained using a set of basic components. Then, by putting together special combinations of these runs, an extended set was obtained. With respect to previous campaigns some improvements have been introduced in our system: an entity recognition prototype is integrated in our tokenization scheme, and the performance of our indexing and retrieval engine has been improved. For GeoCLEF, we tested retrieving using geo-entity and textual references separately, and then combining them with different approaches.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {877–880},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394084,
author = {Geoffrey, Andogah},
title = {GIR Experimentation},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Geographic Information Retrieval (GIR) community has generally accepted the thesis that both thematic and geographic aspects of documents are useful for GIR. This paper describes an experiment exploring this thesis by separately indexing and searching geographical relevant-terms (place names, geo-spatial relations, geographic concepts and geographic adjectives). Two indexes were created - one for extracted geographic relevant-terms (footprint document) and one for reference document collections. Footprint and reference document scores are combined by a linear interpolation to obtain an overall score for document relevance ranking. Experimentation with geographic query expansion provided no significant improvement though it stabilized the query result.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {881–888},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394085,
author = {Toral, A. and Ferr\'{a}ndez, O. and Noguera, E. and Kozareva, Z. and Montoyo, A. and Mu\~{n}oz, R.},
title = {GIR with Geographic Query Expansion},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For our participation in the second edition of GeoCLEF, we have researched the incorporation of geographic knowledge into Geographic Information Retrieval (GIR). Our system is made up of an IR module (IR-n) and a Geographic Knowledge module (Geonames). The results show that the addition of geographic knowledge has a negative impact on the precision. However, the fact that for some topics the obtained results are better, makes us conclude that the addition of this knowledge could be useful but further research is needed in order to determine how.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {889–892},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394086,
author = {Guill\'{e}n, Rocio},
title = {Monolingual and Bilingual Experiments in GeoCLEF2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the results of our experiments in the monolingual English, Spanish and Portuguese tasks and the Bilingual Spanish → English, Spanish → Portuguese, English → Spanish and Portuguese → Spanish tasks. We used the Terrier Information Retrieval Platform to run experiments for both tasks using the Inverse Document Frequency model with Laplace after-effect and normalization 2. Experiments included topics processed automatically as well as topics processed manually. Manual processing of topics was carried out using gazetteers. For the bilingual task we developed a component based on the transfer approach in machine translation. Topics were pre-processed automatically to eliminate stopwords. Then topics in the source language were translated to the target language. Evaluation of results show that the approach worked well in general, but applying more natural language processing techniques would improve retrieval performance.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {893–900},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394087,
author = {Leveling, Johannes and Veiel, Dirk},
title = {Experiments on the Exclusion of Metonymic Location Names from GIR},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For the GeoCLEF task of the CLEF campaign 2006, we investigate identifying literal (geographic) and metonymic senses of location names (the location name refers to another, related entity) and indexing them differently. In document preprocessing, location name senses are identified with a classifier relying on shallow features only. Different senses are stored in corresponding document fields, i. e. LOC (all senses), LOCLIT (literal senses), and LOCMET (metonymic senses). The classifier was trained on manually annotated data from German CoNLL- 2003 data and from a subset of the GeoCLEF newspaper corpus.The setup of our GIR (geographic information retrieval) system is a variant of our setup for GeoCLEF 2005. Results of the retrieval experiments indicate that excluding metonymic senses of location names (short: metonymic location names) improves mean average precision (MAP). Furthermore, using topic narratives decreases MAP, and query expansion with meronyms improves the performance of GIR in our experiments.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {901–904},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394088,
author = {Hu, You-Heng and Ge, Linlin},
title = {The University of New South Wales at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our participation in the GeoCLEF monolingual English task of the Cross Language Evaluation Forum 2006. The main objective of this study is to evaluate the retrieve performance of our geographic information retrieval system. The system consists of four modules: the geographic knowledge base that provides information about important geographic entities around the world and relationships between them; the indexing module that creates and maintains textual and geographic indices for document collections; the document retrieval module that uses the Boolean model to retrieve documents that meet both textual and geographic criteria; and the ranking module that ranks retrieved results based on ranking functions learned using Genetic Programming. Experiments results show that the geographic knowledge base, the indexing module and the retrieval module are useful for geographic information retrieval tasks, but the proposed ranking function learning method doesn't work well.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {905–912},
numpages = {8},
keywords = {geo-textual indexing, geographic knowledge base, geographic information retrieval, genetic programming},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394089,
author = {Garc\'{\i}a-Vega, Manuel and Garc\'{\i}a-Cumbreras, Miguel A. and Ure\~{n}a-L\'{o}pez, L. A. and Perea-Ortega, Jos\'{e} M.},
title = {GEOUJA System the First Participation of the University of Ja\'{e}n at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the first participation of the SINAI group of the University of Ja\'{e}n in GeoCLEF 2006. We have developed a system made up of three main modules: the Translation Subsystem, that works with queries into Spanish and German against English collection; the Query Expansion subsystem, that integrates a Named Entity Recognizer, a thesaurus expansion module and a geographical information-gazetteer module; and the Information Retrieval subsystem. We have participated in the monolingual and the bilingual tasks. The results obtained shown that the use of geographical and thesaurus information for query expansion does not improve the retrieval in our experiments.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {913–917},
numpages = {5},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394090,
author = {Garc\'{\i}a-Vega, Manuel and Garc\'{\i}a-Cumbreras, Miguel A. and Ure\~{n}a-L\'{o}pez, L. Alfonso and Perea-Ortega, Jos\'{e} M. and Ariza-L\'{o}pez, F. Javier and Ferr\'{a}ndez, Oscar and Toral, Antonio and Kozareva, Zornitsa and Noguera, Elisa and Montoyo, Andr\'{e}s and Mu\~{n}oz, Rafael and Buscaldi, Davide and Rosso, Paolo},
title = {R2D2 at GeoCLEF 2006: A Combined Approach},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the participation of a combined approach in GeoCLEF-2006. We have participated in Monolingual English Task and we present joint work of the three groups or teams belonging to the project R2D2 with a new system, combining the three individual systems of these teams. We consider that research in the area of GIR is still in its very early stages, therefore, although a voting system could improve the individual results of each system, we have to further investigate different ways to achieve a better combination of these systems.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {918–925},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394091,
author = {Li, Zhisheng and Wang, Chong and Xie, Xing and Wang, Xufa and Ma, Wei-Ying},
title = {MSRA Columbus at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the participation of Columbus Project of Microsoft Research Asia (MSRA) in GeoCLEF 2006. We participated in the Monolingual GeoCLEF evaluation (EN-EN) and submitted five runs based on different methods. In this paper, we describe our geographic information retrieval system, discuss the results and draw following conclusions: 1) if we just extract locations from the topics automatically without any expansions as geo-terms, the retrieval performance is barely satisfactory; 2) automatic query expansion weakens the performance; 3) if the queries are expanded manually, the performance is significantly improved.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {926–929},
numpages = {4},
keywords = {implicit location, geographic information retrieval, location extraction and disambiguation, geographic focus detection},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394092,
author = {Overell, Simon and Magalh\~{a}es, Jo\~{a}o and R\"{u}ger, Stefan},
title = {Forostar: A System for GIR},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We detail our methods for generating and applying co-occurrence models for the purpose of placename disambiguation. We explain in detail our use of co-occurrence models for placename disambiguation using a model generated from Wikipedia. The presented system is split into two stages: a batch text &amp; geographic indexer and a real time query engine. Four alternative query constructions and six methods of generating a geographic index are compared. The paper concludes with a full description of future work and ways in which the system could be optimised.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {930–937},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394093,
author = {Li, Yi and Stokes, Nicola and Cavedon, Lawrence and Moffat, Alistair},
title = {NICTA I2D2 Group at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We report on the experiments undertaken by the NICTA I2D2 Group as part of GeoCLEF 2006, as well as post-GeoCLEF evaluations and improvements to the submitted system. In particular, we used techniques to assign probabilistic likelihoods to geographic candidates for each identified geo-term, and a probabilistic IR engine. A normalisation process that adjusts term weights, so as to prevent expanded geo-terms from overwhelming non-geo terms, is shown to be crucial.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {938–945},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394094,
author = {Bischoff, Kerstin and Mandl, Thomas and Womser-Hacker, Christa},
title = {Blind Relevance Feedback and Named Entity Based Query Expansion for Geographic Retrieval at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In its participation at GeoCLEF 2006, the University of Hildesheim focused on the monolingual German and English and the bilingual German ↔ English tasks. Based on the results of GeoCLEF 2005, the weighting of and the expansion with geographic named entities (NE) within a Boolean retrieval approach was examined. Because the best results 2005 were achieved with Blind Relevance Feedback (BRF) in which NEs seemed to play a crucial role, the effects of adding particular geographic NEs within the BRF are explored. The paper presents a description of the system design, the submitted runs and results. A first analysis of unofficial post experiments indicates that geographic NEs can improve BRF and supports prior findings that the geographical expansion within a Boolean retrieval approach does not necessarily lead to better results - as it is often assumed.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {946–953},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394095,
author = {Buscaldi, Davide and Rosso, Paolo and Sanchis, Emilio},
title = {A Wordnet-Based Indexing Technique for Geographical Information Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an indexing technique based on Word-Net synonyms and holonyms. This technique has been developed for the Geographical Information Retrieval task. It may help in finding implicit geographic information contained in texts, particularly if the indication of the containing geographical entity is omitted. Our experiments were carried out with the Lucene search engine over the GeoCLEF 2006 set of topics. Results show that expansion can improve recall in some cases, although a specific ranking function is needed in order to obtain better results in terms of precision.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {954–957},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394096,
author = {Hauff, Claudia and Trieschnigg, Dolf and Rode, Henning},
title = {University of Twente at GeoCLEF 2006: Geofiltered Document Retrieval},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the approach of the University of Twente at its first participation in GeoCLEF. A large effort went into the construction of a geographic thesaurus which was utilized to add geographic knowledge to the documents and queries. Geographic filtering was applied to the results returned from a retrieval by content run. Employing such a geographic knowledge base however showed no added value - the content-only baseline outperformed all geographically filtered runs.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {958–961},
numpages = {4},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394097,
author = {Ferr\'{e}s, Daniel and Rodr\'{\i}guez, Horacio},
title = {TALP at GeoCLEF 2006: Experiments Using JIRS and Lucene with the ADL Feature Type Thesaurus},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our experiments in Geographical Information Retrieval (GIR) in the context of our participation in the CLEF 2006 GeoCLEF Monolingual English task. Our system, named TALP-GeoIR, follows a similar architecture of the GeoTALP-IR system presented at GeoCLEF 2005 with some changes in the retrieval modes and the Geographical Knowledge Base (KB).The system has four phases performed sequentially: i) a Keyword Selection algorithm based on a linguistic and geographical analysis of the topics, ii) a geographical retrieval with Lucene, iii) a document retrieval task with the JIRS Passage Retrieval (PR) software, and iv) a Document Ranking phase. A Geographical KB has been built using a set of publicly available geographical gazetteers and the Alexandria Digital Library (ADL) Feature Type Thesaurus.In our experiments we have used JIRS, a state-of-the-art PR system for Question Answering, for the GIR task. We also have experimented with an approach using both JIRS and Lucene. In this approach JIRS was used only for textual document retrieval and Lucene was used to detect the geographically relevant documents. These experiments show that applying only JIRS we obtain better results than combining JIRS and Lucene.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {962–969},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394098,
author = {Larson, Ray R. and Gey, Fredric C.},
title = {GeoCLEF Text Retrieval and Manual Expansion Approaches},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we will describe the Berkeley approaches to the GeoCLEF tasks for CLEF 2006. This year we used two separate systems for different tasks. Although of the systems both use versions of the same primary retrieval algorithm they differ in the supporting text pre-processing tools used.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {970–977},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394099,
author = {Ruiz, Miguel E. and Abbas, June and Mark, David and Shapiro, Stuart and Southwick, Silvia B.},
title = {UB at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper summarizes the work done at the State University of New York at Buffalo (UB) in the GeoCLEF 2006 track. The approach presented uses pure IR techniques (indexing of single word terms as well as word bigrams, and automatic retrieval feedback) to try to improve retrieval performance of queries with geographical references. The main purpose of this work is to identify the strengths and shortcomings of this approach so that it serves as a basis for future development of a geographical reference extraction system. We submitted four runs to the monolingual English task, two automatic runs and two manual runs, using the title and description fields of the topics. Our official results are above the median system (auto=0.2344 MAP, manual=0.2445 MAP). We also present an unofficial run that uses title description and narrative which shows a 10% improvement in results with respect to our baseline runs. Our manual runs were prepared by creating a Boolean query based on the topic description and manually adding terms from geographical resources available on the web. Although the average performance of the manual run is comparable to the automatic runs, a query by query analysis shows significant differences among individual queries. In general, we got significant improvements (more that 10% average precision) in 8 of the 25 queries. However, we also noticed that 5 queries in the manual runs perform significantly below the automatic runs.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {978–985},
numpages = {8},
location = {Alicante, Spain},
series = {CLEF'06}
}

@inproceedings{10.5555/2393955.2394100,
author = {Martins, Bruno and Cardoso, Nuno and Chaves, Marcirio Silveira and Andrade, Leonardo and Silva, M\'{a}rio J.},
title = {The University of Lisbon at GeoCLEF 2006},
year = {2006},
isbn = {3540749985},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper details the participation of the XLDB Group from the University of Lisbon at the 2006 GeoCLEF task. We tested text mining methods that use an ontology to extract geographic references from text, assigning documents to encompassing geographic scopes. These scopes are used in document retrieval through a ranking function that combines BM25 text weighting with a similarity function for geographic scopes. We also tested a topic augmentation method, based on the geographic ontology, as an alternative to the scope-based approach.},
booktitle = {Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-Modal Information Retrieval},
pages = {986–994},
numpages = {9},
location = {Alicante, Spain},
series = {CLEF'06}
}

