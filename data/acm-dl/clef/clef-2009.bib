@inproceedings{10.5555/1887364.1887366,
author = {Peters, Carol},
title = {What Happened in CLEF 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The organization of the CLEF 2009 evaluation campaign is described and details are provided concerning the tracks, test collections, evaluation infrastructure, and participation. The aim is to provide the reader of these proceedings with a complete picture of the entire campaign, covering both text and multimedia retrieval experiments. In the final section, the main results achieved by CLEF in the first ten years of activity are discussed and plans for the future of CLEF are presented.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {1–12},
numpages = {12},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887368,
author = {Ferro, Nicola and Peters, Carol},
title = {CLEF 2009 Ad Hoc Track Overview: TEL and Persian Tasks},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The design of the 2009 Ad Hoc track was to a large extent a repetition of the previous year's track, with the same three tasks: Tel@CLEF, Persian@CLEF, and Robust-WSD. In this first of the two track overviews, we describe the objectives and results of the TEL and Persian tasks and provide some statistical analyses.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {13–35},
numpages = {23},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887369,
author = {Agirre, Eneko and Di Nunzio, Giorgio Maria and Mandl, Thomas and Otegi, Arantxa},
title = {CLEF 2009 Ad Hoc Track Overview: Robust-WSD Task},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Robust-WSD at CLEF 2009 aims at exploring the contribution ofWord Sense Disambiguation to monolingual and multilingual Information Retrieval. The organizers of the task provide documents and topics which have been automatically tagged with Word Senses from WordNet using several state-of-the-art Word Sense Disambiguation systems. The Robust-WSD exercise follows the same design as in 2008. It uses two languages often used in previous CLEF campaigns (English, Spanish). Documents were in English, and topics in both English and Spanish. The document collections are based on the widely used LA94 and GH95 news collections. All instructions and datasets required to replicate the experiment are available from the organizers website (http://ixa2.si.ehu.es/clirwsd/). The results show that some top-scoring systems improve their IR and CLIR results with the use of WSD tags, but the best scoring runs do not use WSD.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {36–49},
numpages = {14},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887371,
author = {Anderka, Maik and Lipka, Nedim and Stein, Benno},
title = {Evaluating Cross-Language Explicit Semantic Analysis and Cross Querying},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our participation in the TEL@CLEF task of the CLEF 2009 ad-hoc track. The task is to retrieve items from various multilingual collections of library catalog records, which are relevant to a user's query. Two different strategies are employed: (i) the Cross-Language Explicit Semantic Analysis, CL-ESA, where the library catalog records and the queries are represented in a multilingual concept space that is spanned by aligned Wikipedia articles, and, (ii) a Cross Querying approach, where a query is translated into all target languages using Google Translate and where the obtained rankings are combined. The evaluation shows that both strategies outperform the monolingual baseline and achieve comparable results.Furthermore, inspired by the Generalized Vector Space Model we present a formal definition and an alternative interpretation of the CL-ESA model. This interpretation is interesting for real-world retrieval applications since it reveals how the computational effort for CL-ESA can be shifted from the query phase to a preprocessing phase.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {50–57},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887372,
author = {Leveling, Johannes and Zhou, Dong and Jones, Gareth J. F. and Wade, Vincent},
title = {Document Expansion, Query Translation and Language Modeling for Ad-Hoc IR},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For the multilingual ad-hoc document retrieval track (TEL) at CLEF, Trinity College Dublin and Dublin City University participated in collaboration. Our retrieval experiments focused on i) document expansion using an entry vocabulary module, ii) query translation with Google translate and a statistical MT system, and iii) a comparison of the retrieval models BM25 and language modeling (LM). The major results are that document expansion did not increase MAP; topic translation using the statistical MT system resulted in about 70% of the mean average precision (MAP) achieved compared to Google translate, and LM performs equally or slightly better than BM25. The bilingual retrieval French and German to English experiments obtained 89% and 90% of the best MAP for monolingual English.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {58–61},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887373,
author = {Zhou, Dong and Wade, Vincent},
title = {Smoothing Methods and Cross-Language Document Re-Ranking},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a report on our participation in the CLEF 2009 monolingual and bilingual ad hoc TEL@CLEF task involving three different languages: English, French and German. Language modeling was adopted as the underlying information retrieval model. While the data collection is extremely sparse, smoothing is particularly important when estimating a language model. The main purpose of the monolingual tasks is to compare different smoothing strategies and investigate the effectiveness of each alternative. This retrieval model was then used alongside a document re-ranking method based on Latent Dirichlet Allocation (LDA) which exploits the implicit structure of the documents with respect to original queries for the monolingual and bilingual tasks. Experimental results demonstrated that three smoothing strategies behave differently across testing languages while the LDA-based document re-ranking method should be considered further in order to bring significant improvement over the baseline language modeling systems in the cross-language setting.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {62–69},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887374,
author = {Jadidinejad, Amir Hossein and Mahmoudi, Fariborz},
title = {Cross-Language Information Retrieval Using Meta-Language Index Construction and Structural Queries},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Structural Query Language allows expert users to richly represent its information needs but unfortunately, the complexity of SQLs make them impractical in the Web search engines. Automatically detecting the concepts in an unstructured user's information need and generating a richly structured, multilingual equivalent query is an ideal solution. We utilize Wikipedia as a great concept repository and also some state of the art algorithms for extracting Wikipedia's concepts from the user's information need. This process is called "Query Wikification". Our experiments on the TEL corpus at CLEF2009 achieves +23% and +17% improvement in Mean Average Precision and Recall against the baseline. Our approach is unique in that, it does improve both precision and recall; two pans that often improving one, hurt the another.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {70–77},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887375,
author = {Tomlinson, Stephen},
title = {Sampling Precision to Depth 10000 at CLEF 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We conducted an experiment to test the completeness of the relevance judgments for the monolingual German, French, English and Persian (Farsi) information retrieval tasks of the Ad Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2009. In the ad hoc retrieval tasks, the system was given 50 natural language queries, and the goal was to find all of the relevant documents (with high precision) in a particular document set. For each language, we submitted a sample of the first 10000 retrieved items to investigate the frequency of relevant items at deeper ranks than the official judging depth of 60 for German, French and English and 80 for Persian. The results suggest that, on average, the percentage of relevant items assessed was less than 62% for German, 27% for French, 35% for English and 22% for Persian.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {78–85},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887376,
author = {Larson, Ray R.},
title = {Multilingual Query Expansion for CLEF Adhoc-TEL},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we will briefly describe the approaches taken by the Cheshire (Berkeley) Group for the CLEF Adhoc-TEL 2009 tasks (Mono and Bilingual retrieval). Recognizing that many potentially relevant documents in each of the TEL sub-collections are in other languages, we tried to use multiple translations of the topics for searching each subcollection, combined into a single query. Overall this strategy performed very poorly compared to the the basic monolingual approach used last year (and repeated for one run in each language this year). Once again this year we used probabilistic text retrieval based on logistic regression and incorporating blind relevance feedback for all of the runs. All translation for bilingual tasks was performed using the LEC Power Translator PC-based MT system. Our results this year, however, were surprising poor compared to last year's results. Additional analysis has shown that, for some cases, unexpected hyphenations in the machine translation and untranslated words were to blame.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {86–89},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887377,
author = {Machado, Jorge and Martins, Bruno and Borbinha, Jos\'{e}},
title = {Experiments with N-Gram Prefixes on a Multinomial Language Model versus Lucene's off-the-Shelf Ranking Scheme and Rocchio Query Expansion (TEL@CLEF Monolingual Task)},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We describe our participation in the TEL@CLEF task of the CLEF 2009 ad-hoc track, where we measured the retrieval performance of LGTE, an index engine for Geo-Temporal collections which is mostly based on Lucene, together with extensions for query expansion and multinomial language modelling. We experiment an N-Gram stemming model to improve our last year experiments which consisted in combinations of query expansion, Lucene's off-the-shelf ranking scheme and the ranking scheme based on multinomial language modeling. The N-Gram stemming model was based in a linear combination of N-Grams, with N between 2 and 5, using weight factors obtained by learning from last year topics and assessments. The Rocchio ranking function was also adapted to implement this N-Gram model. Results show that this stemming technique together with query expansion and multinomial language modeling both result in increased performance.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {90–97},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887379,
author = {Jadidinejad, Amir Hossein and Mahmoudi, Fariborz and Dehdari, Jon},
title = {Evaluation of Perstem: A Simple and Efficient Stemming Algorithm for Persian},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Persian is a challenging language in the field of NLP. Rightto-left orthography, complex morphology, complicated grammatical rules, and different forms of letters make it an interesting language for NLP research. In this paper we measure the effectiveness of a simple and efficient stemming algorithm, Perstem, on Persian information retrieval. Our experiments on the Hamshahri corpus at CLEF2009 show that the Perstem algorithm greatly improved both precision (+91%) and recall (+43%).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {98–101},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887380,
author = {Dolamic, Ljiljana and Savoy, Jacques},
title = {Ad Hoc Retrieval with the Persian Language},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our participation to the Persian ad hoc search during the CLEF 2009 evaluation campaign. In this task, we suggest using a light suffix-stripping algorithm for the Farsi (or Persian) language. The evaluations based on different probabilistic models demonstrated that our stemming approach performs better than a stemmer removing only the plural suffixes, or statistically better than an approach ignoring the stemming stage (around +4.5%) or a n-gram approach (around +4.7%). The use of a blind query expansion may significantly improve the retrieval effectiveness (between +7% to +11%). Combining different indexing and search strategies may further enhance the MAP (around +4.4%).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {102–109},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887381,
author = {Habibian, AmirHossein and AleAhmad, Abolfazl and Shakery, Azadeh},
title = {Ad Hoc Information Retrieval for Persian},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present an introduction to the Persian language and its morphology, and describe available resources for Persian text processing. We then propose and evaluate an information retrieval model, a variation of the vector space model which uses the relations existing between query terms. Our experiments on the Hamshahri collection show that the proposed model has better precision for top ranked documents in comparison with some popular IR models.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {110–119},
numpages = {10},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887383,
author = {Wolf, Elisabeth and Bernhard, Delphine and Gurevych, Iryna},
title = {Combining Probabilistic and Translation-Based Models for Information Retrieval Based on Word Sense Annotations},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The objective of our experiments in the monolingual robust word sense disambiguation (WSD) track at CLEF 2009 is twofold. On the one hand, we intend to increase the precision of WSD by a heuristic-based combination of the annotations of the two WSD systems. For this, we provide an extrinsic evaluation on different levels of word sense accuracy. On the other hand, we aim at combining an often used probabilistic model, namely the Divergence From Randomness BM25 model, with a monolingual translation-based model. Our best performing system with and without utilizing word senses ranked 1st overall in the monolingual task. However, we could not observe any improvement by applying the sense annotations compared to the retrieval settings based on tokens or lemmas only.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {120–127},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887384,
author = {Buscaldi, Davide and Rosso, Paolo},
title = {Indexing with Wordnet Synonyms May Improve Retrieval Results},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes a method developed for the Robust - Word Sense Disambiguation task at CLEF 2009. In our approach, a WordNet expanded index is generated from the disambiguated document collection. This index contains synonyms, hypernyms and holonyms of the disambiguated words contained in documents. Query words are integrated by terms extracted by means of a pseudo relevance feedback technique. The set of terms made of query words and terms resulting from pseudo relevance feedback are searched for in both the expanded WordNet index and the default index. The results show that the use of the extended index did not prove useful, obtaining 14-16% less in MAP with respect to the base system. However, for some queries, expanding index terms with synonyms resulted particularly useful.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {128–134},
numpages = {7},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887385,
author = {Borges, Thyago Bohrer and Moreira, Viviane P.},
title = {UFRGS@CLEF2009: Retrieval by Numbers},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For UFRGS's participation on CLEF's Robust task, our aim was to compare retrieval of plain documents to retrieval using information on word senses. The experimental runs which used word- sense disambiguation (WSD) consisted in indexing the synset codes of the senses which had scores higher than a predefined threshold. Several thresholds were tested. Our results have shown that the best WSD runs did not present a significant improvement in relation to the baseline run in which plain documents were used. In addition, a comparison between two alternative disambiguation systems has shown that one outperforms the other in all experimental runs.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {135–141},
numpages = {7},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887386,
author = {Kern, Roman and Juffinger, Andreas and Granitzer, Michael},
title = {Evaluation of Axiomatic Approaches to Crosslanguage Retrieval},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Integrating word sense disambiguation into an information retrieval system could potentially improve its performance. This is the major motivation for the Robust WSD tasks of the Ad-Hoc Track of the CLEF 2009 campaign. For these tasks we have build a customizable and flexible retrieval system. The best performing configuration of this system is based on research in the area of axiomatic approaches to information retrieval. Further, our experiments show that configurations that incorporate word sense disambiguation (WSD) information into the retrieval process did outperform those without. For the monolingual task the performance difference is more pronounced than for the bilingual task. Finally, we are able to show that our query translation approach does work effectively, even if applied in the monolingual task.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {142–149},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887387,
author = {Basile, Pierpaolo and Caputo, Annalina and Semeraro, Giovanni},
title = {UNIBA-SENSE @ CLEF 2009: Robust WSD Task},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the participation of the semantic Nlevels search engine SENSE at the CLEF 2009 Ad Hoc Robust-WSD Task. Our aim is to demonstrate that the combination of the N-levels model and WSD can improve the retrieval performance even when an effective retrieval model is adopted. To reach this aim, we worked on two different strategies. On one hand a model, based on Okapi BM25, was adopted at each level. On the other hand, we integrated a local relevance feedback technique, called Local Context Analysis, in both indexing levels of the system (keyword and word meaning). The hypothesis that Local Context Analysis can be effective even when it works on word meanings coming from a WSD algorithm is supported by experimental results. In monolingual task MAP increased of about 2% exploiting disambiguation, while GMAP increased from 4% to 9% when we used WSD in both mono- and bi- lingual tasks.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {150–157},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887388,
author = {Fern\'{a}ndez, Javi and Izquierdo, Rub\'{e}n and G\'{o}mez, Jos\'{e} M.},
title = {Using Wordnet Relations and Semantic Classes in Information Retrieval Tasks},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we explore the use of semantic classes in an existing information retrieval system in order to improve its results. Thus, we use two different ontologies of semantic classes (WordNet domain and Basic Level Concepts) in order to re-rank the retrieved documents and obtain better recall and precision. Finally, we implement a new method for weighting the expanded terms taking into account the weights of the original query terms and their relations in WordNet with respect to the new ones (which have demonstrated to improve the results). The evaluation of these approaches was carried out in the CLEF Robust-WSD Task, obtaining an improvement of 1.8% in GMAP for the semantic classes approach and 10% in MAP employing the WordNet term weighting approach.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {158–165},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887389,
author = {Agirre, Eneko and Otegi, Arantxa and Zaragoza, Hugo},
title = {Using Semantic Relatedness and Word Sense Disambiguation for (CL)IR},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we report the experiments for the CLEF 2009 Robust-WSD task, both for the monolingual (English) and the bilingual (Spanish to English) subtasks. Our main experimentation strategy consisted of expanding and translating the documents, based on the related concepts of the documents. For that purpose we applied a state-of-the art semantic relatedness method based on WordNet. The relatedness measure was used with and without WSD information. Even though we obtained positive results in our training and development datasets, we did not manage to improve over the baseline in the monolingual case. The improvement over the baseline in the bilingual case is marginal. We plan further work on this technique, which has attained positive results in the passage retrieval for question answering task at CLEF (ResPubliQA).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {166–173},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887391,
author = {Pe\~{n}as, Anselmo and Forner, Pamela and Sutcliffe, Richard and Rodrigo, \'{A}lvaro and For\u{a}scu, Corina and Alegria, I\~{n}aki and Giampiccolo, Danilo and Moreau, Nicolas and Osenova, Petya},
title = {Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the first round of ResPubliQA, a Question Answering (QA) evaluation task over European legislation, proposed at the Cross Language Evaluation Forum (CLEF) 2009. The exercise consists of extracting a relevant paragraph of text that satisfies completely the information need expressed by a natural language question. The general goals of this exercise are (i) to study if the current QA technologies tuned for newswire collections and Wikipedia can be adapted to a new domain (law in this case); (ii) to move to a more realistic scenario, considering people close to law as users, and paragraphs as system output; (iii) to compare current QA technologies with pure Information Retrieval (IR) approaches; and (iv) to introduce in QA systems the Answer Validation technologies developed in the past three years. The paper describes the task in more detail, presenting the different types of questions, the methodology for the creation of the test sets and the new evaluation measure, and analyzing the results obtained by systems and the more successful approaches. Eleven groups participated with 28 runs. In addition, we evaluated 16 baseline runs (2 per language) based only in pure IR approach, for comparison purposes. Considering accuracy, scores were generally higher than in previous QA campaigns.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {174–196},
numpages = {23},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887392,
author = {Turmo, Jordi and Comas, Pere R. and Rosset, Sophie and Galibert, Olivier and Moreau, Nicolas and Mostefa, Djamel and Rosso, Paolo and Buscaldi, Davide},
title = {Overview of QAST 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the experience of QAST 2009, the third time a pilot track of CLEF has been held aiming to evaluate the task of Question Answering in Speech Transcripts. Four sites submitted results for at least one of the three scenarios (European Parliament debates in English and Spanish and broadcast news in French). In order to assess the impact of potential errors of automatic speech recognition (ASR), for each task manual transcripts and three different ASR outputs were provided. In addition an original method of question creation was tried in order to get spontaneous oral questions resulting in two sets of questions (spoken and written). Each participant who had chosen a task was asked to submit a run for each condition. The QAST 2009 evaluation framework is described, along with descriptions of the three scenarios and their associated data, the system submissions for this pilot track and the official evaluation results.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {197–211},
numpages = {15},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887393,
author = {Santos, Diana and Cabral, Lu\'{\i}s Miguel},
title = {GikiCLEF: Expectations and Lessons Learned},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This overview paper is devoted to a critical assessment of GikiCLEF 2009, an evaluation contest specifically designed to expose and investigate cultural and linguistic issues inWikipedia search, with eight participant systems and 17 runs. After providing a maximally short but self contained overview of the GikiCLEF task and participation, we present the open source SIGA system, and discuss, for each of the main guiding ideas, the resulting successes or shortcomings, concluding with further work and still unanswered questions.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {212–222},
numpages = {11},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887395,
author = {Correa, Santiago and Buscaldi, Davide and Rosso, Paolo},
title = {NLEL-MAAT at ResPubliQA},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report presents the work carried out at NLE Lab for the QA@CLEF-2009 competition. We used the JIRS passage retrieval system, which is based on redundancy, with the assumption that it is possible to find the response to a question in a large enough document collection. The retrieved passages are ranked depending on the number, length and position of the question n-gram structures found in the passages. The best results were obtained in monolingual English, while the worst results were obtained for French. We suppose the difference is due to the question style that varies considerably from one language to another.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {223–228},
numpages = {6},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887396,
author = {Iftene, Adrian and Trandab\u{a}\c{t}, Diana and Moruz, Alex and Pistol, Ionu\c{t} and Husarciuc, Maria and Cristea, Dan},
title = {Question Answering on English and Romanian Languages},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {2009 marked UAIC1's fourth consecutive participation at the QA@CLEF competition, with continually improving results. This paper describes UAIC's QA systems participating in the Ro-Ro and En-En tasks. Both systems adhered to the classical QA architecture, with an emphasis on simplicity and real time answers: only shallow parsing was used for question processing, the indexes used by the retrieval module were at coarse-grained paragraph and document levels, and the answer extraction component used simple patternbased rules and lexical similarity metrics for candidate answer ranking. The results obtained for this year's participation were greatly improved from those of our team's previous participations, with an accuracy of 54% on the EN-EN task and 47% on the RO-RO task.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {229–236},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887397,
author = {Tannier, Xavier and Moriceau, V\'{e}ronique},
title = {Studying Syntactic Analysis in a QA System: FIDJI @ Respubliqa'09},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {FIDJI is an open-domain question-answering system for French. The main goal is to validate answers by checking that all the information given in the question is retrieved in the supporting texts. This paper presents FIDJI's results at ResPubliQA 2009, as well as additional experiments bringing to light the role of linguistic modules in this particular campaign.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {237–244},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887398,
author = {Rodrigo, \'{A}lvaro and P\'{e}rez-Iglesias, Joaqu\'{\i}n and Pe\~{n}as, Anselmo and Garrido, Guillermo and Araujo, Lourdes},
title = {Approaching Question Answering by Means of Paragraph Validation},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe the QA system developed for taking part in Res-PubliQA 2009. Our system was composed by an IR phase focused on improving QA results, a validation step for removing paragraphs that are not promising and a module based on ngrams overlapping for selecting the final answer. Furthermore, a selection module that uses lexical entailment in combination with ngrams overlapping was developed in English. The IR module achieved very promising results that were improved by the ngram ranking. Moreover, the ranking was slightly improved when lexical entailment was used.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {245–252},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887399,
author = {P\'{e}rez-Iglesias, Joaqu\'{\i}n and Garrido, Guillermo and Rodrigo, \'{A}lvaro and Araujo, Lourdes and Pe\~{n}as, Anselmo},
title = {Information Retrieval Baselines for the ResPubliQA Task},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The baselines proposed for the ResPubliQA 2009 task are described in this paper. The main aim for designing these baselines was to test the performance of a pure Information Retrieval approach on this task. Two baselines were run for each of the eight languages of the task. Both baselines used the Okapi-BM25 ranking function, with and without a stemming. In this paper we extend the previous baselines comparing the BM25 model with Vector Space Model performance on this task. The results prove that BM25 outperforms VSM for all cases.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {253–256},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887400,
author = {Ion, Radu and \c{S}tef\u{a}nescu, Dan and Ceau\c{s}u, Alexandru and Tufi\c{s}, Dan and Irimia, Elena and Mititelu, Verginica Barbu},
title = {A Trainable Multi-Factored QA System},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports on the construction and testing of a new Question Answering (QA) system, implemented as an workflow which builds on several web services developed at the Research Institute for Artificial Intelligence (RACAI).The evaluation of the system has been independently done by the organizers of the Romanian-Romanian task of the ResPubliQA 2009 exercise and has been rated the best performing system with the highest improvement due to the NLP technology over a baseline state-of-the-art IR system. We describe a principled way of combining different relevance measures for obtaining a general relevance (to the user's question) score that will serve as the sort key for the returned paragraphs. The system was trained on a specific corpus, but its functionality is independent on the linguistic register of the training data. The trained QA system that participated in the ResPubliQA shared task is available as a web application at http://www2.racai.ro/sir-resdec/.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {257–264},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887401,
author = {Gl\"{o}ckner, Ingo and Pelzer, Bj\"{o}rn},
title = {Extending a Logic-Based Question Answering System for Administrative Texts},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {LogAnswer is a question answering (QA) system for German that uses machine learning for integrating logic-based and shallow (lexical) validation features. For ResPubliQA 2009, LogAnswer was adjusted to specifics of administrative texts, as found in the JRC Acquis corpus. Moreover, support for a broader class of questions relevant to the domain was added, including questions that ask for a purpose, reason, or procedure. Results confirm the success of these measures to prepare LogAnswer for ResPubliQA, and of the general consolidation of the system. According to the C@1/Best IR baseline metric that tries to abstract from the language factor, LogAnswer was the third best of eleven systems participating in ResPubliQA. The system was especially successful at detecting wrong answers, with 73% correct rejections.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {265–272},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887402,
author = {Agirre, Eneko and Ansa, Olatz and Arregi, Xabier and De Lacalle, Maddalen Lopez and Otegi, Arantxa and Saralegi, Xabier and Zaragoza, Hugo},
title = {Elhuyar-IXA: Semantic Relatedness and Cross-Lingual Passage Retrieval},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This article describes the participation of the joint Elhuyar-IXA group in the ResPubliQA exercise at QA&amp;CLEF. In particular, we participated in the English-English monolingual task and in the Basque-English crosslingual one. Our focus has been threefold: (1) to check to what extent information retrieval (IR) can achieve good results in passage retrieval without question analysis and answer validation, (2) to check Machine Readable Dictionary (MRD) techniques for the Basque to English retrieval when faced with the lack of parallel corpora for Basque in this domain, and (3) to check the contribution of semantic relatedness based on WordNet to expand the passages to related words. Our results show that IR provides good results in the monolingual task, that our crosslingual system performs lower than the monolingual runs, and that semantic relatedness improves the results in both tasks (by 6 and 2 points, respectively).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {273–280},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887403,
author = {Vicente-D\'{\i}ez, Mar\'{\i}a Teresa and De Pablo-S\'{a}nchez, C\'{e}sar and Mart\'{\i}nez, Paloma and Schneider, Juli\'{a}n Moreno and Salazar, Marta Garrote},
title = {Are Passages Enough? The MIRACLE Team Participation in QA@CLEF2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper summarizes the participation of the MIRACLE team in the Multilingual Question Answering Track at CLEF 2009. In this campaign, we took part in the monolingual Spanish task at ResPubliQA and submitted two runs. We have adapted our QA system to the new JRC-Acquis collection and the legal domain. We tested the use of answer filtering and ranking techniques against a baseline system using passage retrieval with no success. The run using question analysis and passage retrieval obtained a global accuracy of 0.33, while the addition of an answer filtering resulted in 0.29. We provide an analysis of the results for different questions types to investigate why it is difficult to leverage previous QA techniques. Another task of our work has been the application of temporal management to QA. Finally we include some discussion of the problems found with the new collection and the complexities of the domain.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {281–288},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887405,
author = {Bernard, Guillaume and Rosset, Sophie and Galibert, Olivier and Adda, Gilles and Bilinski, Eric},
title = {The LIMSI Participation in the QAst 2009 Track: Experimenting on Answer Scoring},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present in this paper the three LIMSI question-answering systems on speech transcripts which participated to the QAst 2009 evaluation. These systems are based on a complete and multi-level analysis of both queries and documents. These systems use an automatically generated research descriptor. A score based on those descriptors is used to select documents and snippets. Three different methods are tried to extract and score candidate answers, and we present in particular a tree transformation based ranking method. We participated to all the tasks and submitted 30 runs (for 24 sub-tasks). The evaluation results for manual transcripts range from 27% to 36% for accuracy depending on the task and from 20% to 29% for automatic transcripts.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {289–296},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887406,
author = {Comas, Pere R. and Turmo, Jordi},
title = {Robust Question Answering for Speech Transcripts: UPC Experience in QAst 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the participation of the Technical University of Catalonia in the CLEF 2009 Question Answering on Speech Transcripts track. We have participated in the English and Spanish scenarios of QAST. For both manual and automatic transcripts we have used a robust factual Question Answering that uses minimal syntactic information. We have also developed a NERC designed to handle automatic transcripts. We perform a detailed analysis of our results and draw conclusions relating QA performance to word error rate and the difference between written and spoken questions.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {297–304},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887408,
author = {Cardoso, Nuno and Batista, David and Lopez-Pellicer, Francisco J. and Silva, M\'{a}rio J.},
title = {Where in the Wikipedia is That Answer? The XLDB at the GikiCLEF 2009 Task},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We developed a new semantic question analyser for a custom prototype assembled for participating in GikiCLEF 2009, which processes grounded concepts derived from terms, and uses information extracted from knowledge bases to derive answers. We also evaluated a newly developed named-entity recognition module, based in Conditional Random Fields, and a new world geoontology, derived from Wikipedia, which is used in the geographic reasoning process.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {305–309},
numpages = {5},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887409,
author = {Hartrumpf, Sven and Leveling, Johannes},
title = {Recursive Question Decomposition for Answering Complex Geographic Questions},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the GIRSA-WP system and the experiments performed for GikiCLEF 2009, the geographic information retrieval task in the question answering track at CLEF 2009. Three runs were submitted. The first one contained only results from the InSicht QA system; it showed high precision, but low recall. The combination with results from the GIR system GIRSA increased recall considerably, but reduced precision. The second run used a standard IR query, while the third run combined such queries with a Boolean query with selected keywords. The evaluation showed that the third run achieved significantly higher mean average precision (MAP) than the second run. In both cases, integrating GIR methods and QA methods was successful in combining their strengths (high precision of deep QA, high recall of GIR), resulting in the third-best performance of automatic runs in GikiCLEF. The overall performance still leaves room for improvements. For example, the multilingual approach is too simple. All processing is done in only one Wikipedia (the German one); results for the nine other languages are collected by following the translation links in Wikipedia.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {310–317},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887410,
author = {Cardoso, Nuno},
title = {GikiCLEF Topics and Wikipedia Articles: Did They Blend?},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a post-hoc analysis on how the Wikipedia collections fared in providing answers and justifications to GikiCLEF topics. Based on all solutions found by all GikiCLEF participant systems, this paper measures how self-sufficient the particular Wikipedia collections were to provide answers and justifications for the topics, in order to better understand the recall limit that a GikiCLEF system specialised in one single language has.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {318–321},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887411,
author = {Ferr\'{e}s, Daniel and Rodr\'{\i}guez, Horacio},
title = {TALP at GikiCLEF 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our experiments in Geographical Information Retrieval with the Wikipedia collection in the context of our participation in the GikiCLEF 2009 Multilingual task in English and Spanish. Our system, called gikiTALP, follows a simple approach that uses standard Information Retrieval with the Sphinx full-text search engine and some Natural Language Processing techniques without Geographical Knowdledge.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {322–325},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887412,
author = {Dornescu, Iustin},
title = {Semantic QA for Encyclopaedic Questions: EQUAL in GikiCLEF},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a new question answering (QA) approach and a prototype system, EQUAL, which relies on structural information from Wikipedia to answer open-list questions. The system achieved the highest score amongst the participants in the GikiCLEF 2009 task. Unlike the standard textual QA approach, EQUAL does not rely on identifying the answer within a text snippet by using keyword retrieval. Instead, it explores the Wikipedia page graph, extracting and aggregating information from multiple documents and enforcing semantic constraints. The challenges for such an approach and an error analysis are also discussed.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {326–333},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887413,
author = {Larson, Ray R.},
title = {Interactive Probabilistic Search for GikiCLEF},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we will briefly describe the approaches taken by the Berkeley Cheshire Group for the GikiCLEF task of the QA track. Because the task was intended to model some aspects of user search, and because of the complexity of the topics and their geographic elements, we decided to conduct interactive searching of the topics and selection of results. Because of the vagueness of the task specification early-on, some disagreements about what constituted a correct answer, and time constraints we were able to complete only 22 of the 50 topics. However, in spite of this limited submission the interactive approach was very effective and resulted in our submission being ranked third overall in the results.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {334–341},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887415,
author = {Besan\c{c}on, Romaric and Chaudiron, St\'{e}phane and Mostefa, Djamel and Timimi, Isma\"{\i}l and Choukri, Khalid and La\"{\i}b, Meriama},
title = {Information Filtering Evaluation: Overview of CLEF 2009 INFILE Track},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The INFILE@CLEF 2009 is the second edition of a track on the evaluation of cross-language adaptive filtering systems. It uses the same corpus as the 2008 track, composed of 300,000 newswires from Agence France Presse (AFP) in three languages: Arabic, English and French, and a set of 50 topics in general and specific domains (scientific and technological information). In 2009, we proposed two tasks : a batch filtering task and an interactive task to test adaptive methods. Results for the two tasks are presented in this paper.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {342–353},
numpages = {12},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887416,
author = {Qamar, Ali Mustafa and Gaussier, Eric and Denos, Nathalie},
title = {Batch Document Filtering Using Nearest Neighbor Algorithm},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose in this paper a batch algorithm to learn category specific thresholds in a multiclass environment where a document can belong to more than one class. The algorithm uses the k-nearest neighbor algorithm for filtering the 100,000 documents into 50 profiles. The experiments were run on the English corpus. Our experiments gave us a macro precision of 0.256 while the macro recall was 0.295. We had participated in the online task in INFILE 2008 where we had used an online algorithm using the feedbacks from the server. In comparison with INFILE 2008, the macro recall is significantly better in 2009, 0.295 vs 0.260. However the macro precision in 2008 were 0.306. Furthermore, the anticipation in 2009 was 0.43 as compared with 0.307 in 2008. We have also provided a detailed comparison between the batch and online algorithms.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {354–361},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887417,
author = {Dr\u{a}gu\c{s}anu, Cristian-Alexandru and Grigoriu, Alecsandru and Iftene, Adrian},
title = {UAIC: Participation in INFILE@CLEF Task},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This year marked UAIC's first participation at the INFILE@CLEF competition. The purpose of this campaign is the evaluation of cross-language filtering systems, which is to successfully build an automated system that separates relevant from non-relevant documents written in different languages with respect to a given profile. For the batch filtering task, participants are provided with the whole document collection and must return the list of relevant documents for each topic. We achieved good results in filtering documents, also obtaining the highest originality score, when having English as target language. Our team was also the only one who submitted runs for cross-lingual and multilingual batch filtering, with French and English/French as target languages. A brief description of our system, including presentation of the Parsing, Indexing and Filtering modules is given in this paper, as well as the results of the submitted runs.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {362–365},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887418,
author = {Damankesh, Asma and Oroumchian, Farhad and Shaalan, Khaled},
title = {Multilingual Information Filtering by Human Plausible Reasoning},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The theory of Human Plausible Reasoning (HPR) is an attempt by Collins and Michalski to explain how people answer questions when they are uncertain. The theory consists of a set of patterns and a set of inferences which could be applied on those patterns. This paper, investigates the application of HPR theory to the domain of cross language filtering. Our approach combines Natural Language Processing with HPR. The documents and topics are partially represented by automatically extracted concepts, logical terms and logical statements in a language neutral knowledge base. Reasoning provides the evidence of relevance. We have conducted hundreds of experiments especially with the depth of the reasoning, evidence combination and topic selection methods. The results show that HPR contributes to the overall performance by introducing new terms for topics. Also the number of inference paths from a document to a topic is an indication of its relevance.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {366–373},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887419,
author = {Ronald, John Anton Chrisostom and Rossi, Aur\'{e}lie and Fluhr, Christian},
title = {Hossur'Tech's Participation in CLEF 2009 INFILE Interactive Filtering},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the participation of our company formerly named Cadege / Hossur Tech and called now Geol Semantics in the task of filtering interactive CLEF 2009 INFILE and enhancements added after the experiment.The Interactive filtering is something different from traditional information retrieval systems. In CLEF 2009 INFILE adaptive filtering task we have only the knowledge about the 50 different topics which are used as queries and nothing about the input corpus to filter. Documents are received and filtered one by one.The fact that we know nothing about the corpus of the documents to filter, we were forced to use a linguistic approach for this filtering task. We have performed two CLEF 2009 INFILE interactive filtering French to French and French to English tasks, based on a deep linguistic process by using our own linguistic dictionaries.ACM categories and subject descriptors: H.3.3 Information Search and Retrieval, Information filtering},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {374–380},
numpages = {7},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887420,
author = {Montejo-R\'{a}ez, Arturo and Perea-Ortega, Jos\'{e} M. and D\'{\i}az-Galiano, Manuel Carlos and Ure\~{n}a-L\'{o}pez, L. Alfonso},
title = {Experiments with Google News for Filtering Newswire Articles},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes an approach based on the use of Google News as a source of information in order to generate a learning corpus for an information filtering task. The INFILE (INformation FILtering Evaluation) track of the CLEF (Cross-Lingual Evaluation Forum) 2009 campaign has been used as framework. The information filtering task can be seen as a document classification task, so a supervised learning scheme has been followed. Two learning corpora have been proved: one using the text of the topics as learning data to train a classifier, and another one where training data have been generated from Google News pages, using the keywords of topics as queries. Results show that the use of Google News for generating learning data does not improve the results obtained using only topic descriptions as learning corpora.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {381–384},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887422,
author = {Roda, Giovanna and Tait, John and Piroi, Florina and Zenz, Veronika},
title = {CLEF-IP 2009: Retrieval Experiments in the Intellectual Property Domain},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The CLEF-IP track ran for the first time within CLEF 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross-lingual information access. The track focused on the task of prior art search. The 15 European teams who participated in the track deployed a rich range of Information Retrieval techniques adapting them to this new specific domain and task. A large-scale test collection for evaluation purposes was created by exploiting patent citations.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {385–409},
numpages = {25},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887423,
author = {Magdy, Walid and Leveling, Johannes and Jones, Gareth J. F.},
title = {Exploring Structured Documents and Query Formulation Techniques for Patent Retrieval},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the experiments and results of DCU in CLEF-IP 2009. Our work applied standard information retrieval (IR) techniques to patent search. Different experiments tested various methods for the patent retrieval, including query formulation, structured index, weighted fields, document filtering, and blind relevance feedback. Some methods did not show expected good retrieval effectiveness such as blind relevance feedback, other experiments showed acceptable performance. Query formulation was the key to achieving better retrieval effectiveness, and this was performed through assigning higher weights to certain document fields. Further experiments showed that for longer queries, better results are achieved but at the expense of additional computations. For the best runs, the retrieval effectiveness is still lower than for IR applications for other domains, illustrating the difficulty of patent search. The official results have shown that among fifteen participants we achieved the seventh and the fourth ranks from the mean average precision (MAP) and recall point of view, respectively.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {410–417},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887424,
author = {Toucedo, Jos\'{e} Carlos and Losada, David E.},
title = {Formulating Good Queries for Prior Art Search},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe our participation in CLEF-IP 2009 (prior art search task). This was the first year of the task and we focused on how to build effectively a prior art query from a patent. Basically, we implemented simple strategies to extract terms from some textual fields of the patent documents and gave more weight to title terms. We ran experiments with the well-known BM25 model. Although we paid little attention to language-dependent issues, our performance was usually among the top 3 groups participating in the task.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {418–425},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887425,
author = {Iftene, Adrian and Ionescu, Ovidiu and Oancea, George-R\u{a}zvan},
title = {UAIC: Participation in CLEF-IP Track},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The CLEF-IP track was launched in 2009 to investigate IR techniques for patent retrieval, as part of the CLEF 2009 evaluation campaign. We built a system in order to participate in the CLEF-IP track. Our system has three main components: a filtering module, an indexing module, and a search module. Because the process of indexing all of the 75 GB of input patent documents took almost one day, we decided to work in a peer-to-peer environment with four computers. The problems encountered were related to the identification of relevant fields to be used for indexing and in the search processes.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {426–429},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887426,
author = {Lopez, Patrice and Romary, Laurent},
title = {PATATRAS: Retrieval Model Combination and Regression Models for Prior Art Search},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents PATATRAS (PATent and Article Tracking, Retrieval and AnalysiS), a system realized at the Humboldt University for the IP track of CLEF 2009. Our approach presents three main characteristics: 1. The usage of multiple retrieval models and term index definitions for the three languages considered in the present track producing ten different sets of ranked results. 2. The merging of the different results based on multiple regression models using an additional training set created from the patent collection. 3. The exploitation of patent metadata and the citation structures for creating restricted initial working sets of patents and for producing a final re-ranking regression model. The resulting architecture allowed us to exploit efficiently specific information of patent documents while remaining generic and easy to extend.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {430–437},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887427,
author = {Correa, Santiago and Buscaldi, Davide and Rosso, Paolo},
title = {NLEL-MAAT at CLEF-IP},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report presents the work carried out at NLE Lab for the CLEF-IP 2009 competition. We adapted the JIRS passage retrieval system for this task, with the objective to exploit the stylistic characteristics of the patents. Since JIRS was developed for the Question Answering task and this is the first time its model was used to compare entire documents, we had to carry out some transformations on the patent documents. The obtained results are not good and show that the modifications adopted in order to use JIRS represented a wrong choice, compromising the performance of the retrieval system.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {438–443},
numpages = {6},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887428,
author = {Gobeill, Julien and Pasche, Emilie and Teodoro, Douglas and Ruch, Patrick},
title = {Simple Pre and Post Processing Strategies for Patent Searching in CLEF Intellectual Property Track 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The objective of the 2009 CLEF-IP Track was to find documents that constitute prior art for a given patent. We explored a wide range of simple preprocessing and post-processing strategies, using Mean Average Precision (MAP) for evaluation purposes. Once determined the best document representation, we tuned a classical Information Retrieval engine in order to perform the retrieval step. Finally, we explored two different post-processing strategies. In our experiments, using the complete IPC codes for filtering purposes led to greater improvements than using 4-digits IPC codes. The second postprocessing strategy was to exploit the citations of retrieved patents in order to boost scores of cited patents. Combining all selected strategies, we computed optimal runs that reached a MAP of 0.122 for the training set, and a MAP of 0.129 for the official 2009 CLEF-IP XL set.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {444–451},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887429,
author = {Herbert, Benjamin and Szarvas, Gy\"{o}rgy and Gurevych, Iryna},
title = {Prior Art Search Using International Patent Classification Codes and All-Claims-Queries},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we describe the system we developed for the Intellectual Property track of the 2009 Cross-Language Evaluation Forum. The track addressed prior art search for patent applications. We used the Lucene library to conduct experiments with the traditional TFIDF-based ranking approach, indexing both the textual content and the IPC codes assigned to each document. We formulated our queries by using the title and claims of a patent application in order to measure the (weighted) lexical overlap between topics and prior art candidates. We also formulated a language-independent query using the IPC codes of a document to improve the coverage and to obtain a more accurate ranking of candidates. Using a simple model, our system remained efficient and had a reasonably good performance score: it achieved the 6th best Mean Average Precision score out of 14 participating systems on 500 topics, and the 4th best score out of 9 participants on 10,000 topics.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {452–459},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887430,
author = {J\"{a}rvelin, Antti and J\"{a}rvelin, Anni and Hansen, Preben},
title = {UTA and SICS at CLEF-IP'09},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper reports experiments performed in the course of the CLEF'09 Intellectual Property track, where our main goal was to study automatic query generation from the patent documents. Two simple word weighting algorithms (modified RATF formula, and tf undefined idf) for selecting query keys from the patent documents were tested. Also using different parts of the patent documents as sources of query keys was investigated. Our best runs placed relatively well compared to the other CLEF-IP'09 participants' runs. This suggests that tested approaches to the automatic query generation could be useful, and should be developed further. For three topics, the performance of the automatically extracted queries were compared to queries produced by three patent experts to see whether the automatic key word extraction algorithms seem to be able to extract relevant words from the topics.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {460–467},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887431,
author = {Alink, W. and Cornacchia, Roberto and De Vries, Arjen P.},
title = {Searching CLEF-IP by Strategy},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Tasks performed by intellectual property specialists are often ad hoc, and continuously require new approaches to search a collection of documents. We therefore investigate the benefits of a visual 'search strategy builder' to allow IP search experts to express their approach to searching the patent collection, without requiring IR or database expertise. These search strategies are executed on our probabilistic relational database framework. Search by strategy design can be very effective. We refined our search strategies after our initial submission to the CLEF-IP track, and with minor effort we could include techniques shown to be beneficial for other CLEF-IP participants.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {468–475},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887432,
author = {Fautsch, Claire and Savoy, Jacques},
title = {UniNE at CLEF-IP 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes our participation to the Intellectual Property task during the CLEF-2009 campaign. Our main objective was to evaluate different search models and try different strategies to select and weight relevant terms from a patent to form an effective query. We found out that the probabilistic models tend to perform better than others and that combining different indexing and search strategies may further improve retrieval. The final performance is still lower than expected and further investigations are therefore needed in this domain.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {476–479},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887433,
author = {Graf, Erik and Azzopardi, Leif and Van Rijsbergen, Keith},
title = {Automatically Generating Queries for Prior Art Search},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper outlines our participation in CLEF-IP's 2009 prior art search task. In the task's initial year our focus lay on the automatic generation of effective queries. To this aim we conducted a preliminary analysis of the distribution of terms common to topics and their relevant documents, with respect to term frequency and document frequency. Based on the results of this analysis we applied two methods to extract queries. Finally we tested the effectiveness of the generated queries on two state of the art retrieval models.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {480–490},
numpages = {11},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887434,
author = {Becks, Daniela and Womser-Hacker, Christa and Mandl, Thomas and K\"{o}lle, Ralph},
title = {Patent Retrieval Experiments in the Context of the CLEF IP Track 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {At CLEF 2009 the University of Hildesheim submitted experiments for the new Intellectual Property Track. We focused on the main task of this track that aims at finding prior art for a specified patent. Our experiments were split up into one official German run as well as different additional runs using English and German terms. The submitted run was based on a simple baseline approach including stopword elimination, stemming and simple term queries. Furthermore, we investigated the significance of the International Patent Classification (IPC). During the experiments, different parts of a patent were used to construct the queries. In a first stage, only title and claims were included. In contrast, for the post runs we generated a more complex boolean query, which combined terms of the title, claims, description and the IPC classes. The results made clear that using the IPC codes can particularly increase the recall of a patent retrieval system.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {491–496},
numpages = {6},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887435,
author = {Verberne, Suzan and D'hondt, Eva},
title = {Prior Art Retrieval Using the Claims Section as a Bag of Words},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe our participation in the 2009 CLEFIP task, which was targeted at prior-art search for topic patent documents. We opted for a baseline approach to get a feeling for the specifics of the task and the documents used. Our system retrieved patent documents based on a standard bag-of-words approach for both the Main Task and the English Task. In both runs, we extracted the claim sections from all English patents in the corpus and saved them in the Lemur index format with the patent IDs as DOCIDs. These claims were then indexed using Lemur's BuildIndex function. In the topic documents we also focused exclusively on the claims sections. These were extracted and converted to queries by removing stopwords and punctuation.We did not perform any term selection or query expansion. We retrieved 100 patents per topic using Lemur's RetEval function, retrieval model TF-IDF. Compared to the other runs submitted to the track, we obtained good results in terms of nDCG (0.46) and moderate results in terms of MAP (0.054).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {497–501},
numpages = {5},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887436,
author = {Guyot, Jacques and Falquet, Gilles and Benzineb, Karim},
title = {UniGE Experiments on Prior Art Search in the Field of Patents},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this experiment led at the University of Geneva (UniGE), we evaluated several similarity measures as well as the relevance of using automated classification to filter out search results. The patent field is particularly well suited to classification-based filtering because each patent is already classified. Our results show that such a filtering approach does not improve searching performances, but it does not have a negative impact on recall either. This last observation allows considering classification as a possible tool to reduce the search space without reducing the quality of search results.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {502–507},
numpages = {6},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887438,
author = {Mandl, Thomas and Agosti, Maristella and Di Nunzio, Giorgio Maria and Yeh, Alexander and Mani, Inderjeet and Doran, Christine and Schulz, Julia Maria},
title = {LogCLEF 2009: The CLEF 2009 Multilingual Logfile Analysis Track Overview},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Log data constitute a relevant aspect in the evaluation process of the quality of a search engine and the quality of a multilingual search service; log data can be used to study the usage of a search engine, and to better adapt it to the objectives the users were expecting to reach. The interest in multilingual log analysis was promoted by the Cross Language Evaluation Forum (CLEF) for the first time with a track named LogCLEF. LogCLEF is an evaluation initiative for the analysis of queries and other logged activities as expression of user behavior. The goal is the analysis and classification of queries in order to understand search behavior especially in multilingual contexts and ultimately to improve search systems. Two tasks were defined: Log Analysis and Geographic Query Identification (LAGI) which aimed at the identification of queries for geographic content and Log Analysis for Digital Societies (LADS) which was based on analyzing the user behavior of the search logs the service of The European Library. Five groups using a variety of approaches submitted experiments. The data for the track, the evaluation methodology and results are presented and discussed.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {508–517},
numpages = {10},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887439,
author = {Ghorab, M. Rami and Leveling, Johannes and Zhou, Dong and Jones, Gareth J. F. and Wade, Vincent},
title = {Identifying Common User Behaviour in Multilingual Search Logs},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The LADS (Log Analysis for Digital Societies) task at CLEF aims at investigating user actions in a multilingual setting. We carried out an analysis of search logs with the objectives of investigating how users from different linguistic or cultural backgrounds behave in search, and how the discovery of patterns in user actions could be used for community identification. The findings confirm that users from a different background behave differently, and that there are identifiable patterns in the user actions. The findings suggest that there is scope for further investigation of how search logs can be exploited to personalise and improve cross-language search as well as improve the TEL search system.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {518–525},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887440,
author = {Oakes, Michael and Xu, Yan},
title = {A Search Engine Based on Query Logs, and Search Log Analysis by Automatic Language Identification},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This work describes a variation on the traditional Information Retrieval paradigm, where instead of text documents being indexed according to their content, they are indexed according to the search terms previous users have used in finding them. We determine the effectiveness of this approach by indexing a sample of query logs from the European Library, and describe its usefulness for multilingual searching. In our analysis of the search logs, we determine the language of the past queries automatically, and annotate the search logs accordingly. From this information, we derive matrices to show that a) users tend to persist with the same query language throughout a query session, and b) submit queries in the same language as the interface they have selected.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {526–533},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887441,
author = {Iftene, Adrian},
title = {Identifying Geographical Entities in Users' Queries},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In 2009 we built a system in order to compete in the LAGI task (Log Analysis and Geographic Query Identification). The system uses an external resource built into GATE in combination with Wikipedia and Tumba in order to identify geographical entities in user's queries. The results obtained with and without Wikipedia resources are comparable. The main advantage of only using GATE resources is the improved run time. In the process of system evaluation we have identified the main problem of our approach: the system has insufficient external resources for the recognition of geographic entities.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {534–537},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887442,
author = {Lamm, Katrin and Mandl, Thomas and Koellex, Ralph},
title = {Search Path Visualization and Session Performance Evaluation with Log Files},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper discusses new strategies for the performance evaluation of user search behavior. For the Log Analysis for Digital Societies (LADS) task of LogCLEF 2009 are proposed three different levels of user performance: success, failure and strong failure. The goal is to compare and measure session performance on a qualitative as well as on a quantitative level. The results obtained with both methods are in good agreement. Primarily they show that it is possible to investigate user performance from interpreting the user interactions recorded in log files. Both the qualitative and quantitative results give rise to a refinement of our operational definition of performance.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {538–543},
numpages = {6},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887443,
author = {Bosca, Alessio and Dini, Luca},
title = {User Logs as a Means to Enrich and Refine Translation Dictionaries},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the participation of the CACAO prototype to the Log Analysis for Digital Societies (LADS) task of LogCLEF 2009 track. In our experiment we investigated the possibility to exploit the TEL logs data as a source for inferring new translations, thus enriching already existing translation dictionaries. The proposed approach is based on the assumption that in the context of a multilingual digital library the same query is likely to be repeated across different languages. We applied our approach to the logs from TEL and the results obtained are quite promising.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {544–551},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887445,
author = {Ferro, Nicola and Harman, Donna},
title = {CLEF 2009: Grid@CLEF Pilot Track Overview},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Grid@CLEF track is a long term activity with the aim of running a series of systematic experiments in order to improve the comprehension of MLIA systems and gain an exhaustive picture of their behaviour with respect to languages.In particular, Grid@CLEF 2009 is a pilot track that has started to move the first steps in this direction by giving the participants the possibility of getting experienced with the new way of carrying out experimentation that is needed in Grid@CLEF to test all the different combinations of IR components and languages. Grid@CLEF 2009 offered traditional monolingual ad-hoc tasks in 5 different languages (Dutch, English, French, German, and Italian) which make use of consolidated and very well known collections from CLEF 2001 and 2002 and used a set of 84 topics.Participants had to conduct experiments according to the CIRCO framework, an XML-based protocol which allows for a distributed, looselycoupled, and asynchronous experimental evaluation of IR systems. We provided a Java library which can be exploited to implement CIRCO and an example implementation with the Lucene IR system.The participation has been especially challenging also for the size of the XML files generated by CIRCO, which can become 50-60 times the size of the collection. Of the 9 initially subscribed participants, only 2 were able to submit runs in time and we received a total of 18 runs in 3 languages (English, French, and German) out of the 5 offered. The two participants used different IR systems or combination of them, namely Lucene, Terrier, and Cheshire II.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {552–565},
numpages = {14},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887446,
author = {Larson, Ray R.},
title = {Decomposing Text Processing for Retrieval: Cheshire Tries GRID@CLEF},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This short paper describes Berkeley's participation in the GRID@CLEF task. The GRID@CLEF task is intended to capture in XML form the intermediate results of the text processing phases of the indexing process used by IR systems. Our approach was to create a new instrumented version of the indexing program used with the Cheshire II system. Thanks to an extension by the organizers, we were able to submit runs derived from our system.The system used for this task is a modified version of the Cheshire II IR system, to which output files for the different intermediate streams have been added. The additions, like the original system were written in C. Developing this system required creating parallel modules for several elements of the Cheshire II indexing programs. The current version handles the simplest processing cases, and currently ignores the many specialized indexing modes in the system (such as geographic name extraction and georeferencing).},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {566–569},
numpages = {4},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887447,
author = {K\"{u}rsten, Jens and Eibl, Maximilian},
title = {Putting It All Together: The Xtrieval Framework at Grid@CLEF 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Xtrieval framework, built at the Chemnitz University of Technology, aims at analyzing the impact of different retrieval models and methods on retrieval effectiveness. For the Grid@CLEF task 2009, the CIRCO framework was integrated into the Xtrieval framework. 15 runs were performed 15 runs in the three languages German, English, and French. For each language two different stemmers and two different retrieval models were used. One run was a fusion run combining the results of the four other experiments. Whereas the different runs demonstrated that the impact of the used retrieval technologies is highly depending on the corpus, the merged approach produced the best results in each language.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {570–577},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887449,
author = {Kurimo, Mikko and Virpioja, Sami and Turunen, Ville T. and Blackwood, Graeme W. and Byrne, William},
title = {Overview and Results of Morpho Challenge 2009},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The goal of Morpho Challenge 2009 was to evaluate unsupervised algorithms that provide morpheme analyses for words in different languages and in various practical applications. Morpheme analysis is particularly useful in speech recognition, information retrieval and machine translation for morphologically rich languages where the amount of different word forms is very large. The evaluations consisted of: 1. a comparison to grammatical morphemes, 2. using morphemes instead of words in information retrieval tasks, and 3. combining morpheme and word based systems in statistical machine translation tasks. The evaluation languages were: Finnish, Turkish, German, English and Arabic. This paper describes the tasks, evaluation methods, and obtained results. The Morpho Challenge was part of the EU Network of Excellence PASCAL Challenge Program and organized in collaboration with CLEF.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {578–597},
numpages = {20},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887450,
author = {Bernhard, Delphine},
title = {MorphoNet: Exploring the Use of Community Structure for Unsupervised Morpheme Analysis},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper investigates a novel approach to unsupervised morphology induction relying on community detection in networks. In a first step, morphological transformation rules are automatically acquired based on graphical similarities between words. These rules encode substring substitutions for transforming one word form into another. The transformation rules are then applied to the construction of a lexical network. The nodes of the network stand for words while edges represent transformation rules. In the next step, a clustering algorithm is applied to the network to detect families of morphologically related words. Finally, morpheme analyses are produced based on the transformation rules and the word families obtained after clustering. While still in its preliminary development stages, this method obtained encouraging results at Morpho Challenge 2009, which demonstrate the viability of the approach.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {598–608},
numpages = {11},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887451,
author = {Virpioja, Sami and Kohonen, Oskar and Lagus, Krista},
title = {Unsupervised Morpheme Analysis with Allomorfessor},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Allomorfessor extends the unsupervised morpheme segmentation method Morfessor to account for the linguistic phenomenon of allomorphy, where one morpheme has several different surface forms. The method discovers common base forms for allomorphs from an unannotated corpus by finding small modifications, called mutations, for them. Using Maximum a Posteriori estimation, the model is able to decide the amount and types of the mutations needed for the particular language. In Morpho Challenge 2009 evaluations, the effect of the mutations was discovered to be rather small. However, Allomorfessor performed generally well, achieving the best results for English in the linguistic evaluation, and being in the top three in the application evaluations for all languages.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {609–616},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887452,
author = {Lavall\'{e}e, Jean-Fran\c{c}ois and Langlais, Philippe},
title = {Unsupervised Morphological Analysis by Formal Analogy},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {While classical approaches to unsupervised morphology acquisition often rely on metrics based on information theory for identifying morphemes, we describe a novel approach relying on the notion of formal analogy. A formal analogy is a relation between four forms, such as: reader is to doer as reading is to doing. Our assumption is that formal analogies identify pairs of morphologically related words. We first describe an approach which simply identifies all the formal analogies involving words in a lexicon. Despite its promising results, this approach is computationally too expensive. Therefore, we designed a more practical system which learns morphological structures using only a (small) subset of all formal analogies. We tested those two approaches on the five languages used in Morpho Challenge 2009.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {617–624},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887453,
author = {Spiegler, Sebastian and Gol\'{e}nia, Bruno and Flach, Peter A.},
title = {Unsupervised Word Decomposition with the Promodes Algorithm},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present PROMODES an algorithm for unsupervised word decomposition, which is based on a probabilistic generative model. The model considers segment boundaries as hidden variables and includes probabilities for letter transitions within segments. For the Morpho Challenge 2009, we demonstrate three versions of PROMODES. The first one uses a simple segmentation algorithm on a subset of the data and applies maximum likelihood estimates for model parameters when decomposing words of the original language data. The second version estimates its parameters through expectation maximization (EM). A third method is a committee of unsupervised learners where learners correspond to different EM initializations. The solution is found by majority vote which decides whether to segment at a word position or not. In this paper, we describe the probabilistic model, parameter estimation and how the most likely decomposition of an input word is found. We have tested PROMODES on non-vowelized and vowelized Arabic as well as on English, Finnish, German and Turkish. All three methods achieved competitive results.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {625–632},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887454,
author = {Gol\'{e}nia, Bruno and Spiegler, Sebastian and Flach, Peter A.},
title = {Unsupervised Morpheme Discovery with Ungrade},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present an unsupervised algorithm for morpheme discovery called UNGRADE (UNsupervised GRAph DEcomposition). UNGRADE works in three steps and can be applied to languages whose words have the structure prefixes-stem-suffixes. In the first step, a stem is obtained for each word using a sliding window, such that the description length of the window is minimised. In the next step prefix and suffix sequences are sought using a morpheme graph. The last step consists in combining morphemes found in the previous steps. UNGRADE has been experimentally evaluated on 5 languages (English, German, Finnish, Turkish and Arabic) with encouraging results.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {633–640},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887455,
author = {Can, Burcu and Manandhar, Suresh},
title = {Clustering Morphological Paradigms Using Syntactic Categories},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a new clustering algorithm for the induction of the morphological paradigms. Our method is unsupervised and exploits the syntactic categories of the words acquired by an unsupervised syntactic category induction algorithm [1]. Previous research [2,3] on joint learning of morphology and syntax has shown that both types of knowledge affect each other making it possible to use one type of knowledge to help learn the other one.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {641–648},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887456,
author = {Monson, Christian and Hollingshead, Kristy and Roark, Brian},
title = {Simulating Morphological Analyzers with Stochastic Taggers for Confidence Estimation},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a method for providing stochastic confidence estimates for rule-based and black-box natural language (NL) processing systems. Our method does not require labeled training data: We simply train stochastic models on the output of the original NL systems. Numeric confidence estimates enable both minimum Bayes risk-style optimization as well as principled system combination for these knowledge-based and black-box systems. In our specific experiments, we enrich ParaMor, a rule-based system for unsupervised morphology induction, with probabilistic segmentation confidences by training a statistical natural language tagger to simulate ParaMor's morphological segmentations. By adjusting the numeric threshold above which the simulator proposes morpheme boundaries, we improve F1 of morpheme identification on a Hungarian corpus by 5.9% absolute. With numeric confidences in hand, we also combine ParaMor's segmentation decisions with those of a second (blackbox) unsupervised morphology induction system, Morfessor. Our joint ParaMor-Morfessor system enhances F1 performance by a further 3.4% absolute, ultimately moving F1 from 41.4% to 50.7%.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {649–657},
numpages = {9},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887457,
author = {Lignos, Constantine and Chan, Erwin and Marcus, Mitchell P. and Yang, Charles},
title = {A Rule-Based Acquisition Model Adapted for Morphological Analysis},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We adapt the cognitively-oriented morphology acquisition model proposed in (Chan 2008) to perform morphological analysis, extending its concept of base-derived relationships to allow multi-step derivations and adding features required for robustness on noisy corpora. This results in a rule-based morphological analyzer which attains an F-score of 58.48% in English and 33.61% in German in the Morpho Challenge 2009 Competition 1 evaluation. The learner's performance shows that acquisition models can effectively be used in text-processing tasks traditionally dominated by statistical approaches.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {658–665},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

@inproceedings{10.5555/1887364.1887458,
author = {Tchoukalov, Tzvetan and Monson, Christian and Roark, Brian},
title = {Morphological Analysis by Multiple Sequence Alignment},
year = {2009},
isbn = {364215753X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In biological sequence processing, Multiple Sequence Alignment (MSA) techniques capture information about long-distance dependencies and the three-dimensional structure of protein and nucleotide sequences without resorting to polynomial complexity context-free models. But MSA techniques have rarely been used in natural language (NL) processing, and never for NL morphology induction. Our MetaMorph algorithm is a first attempt at leveraging MSA techniques to induce NL morphology in an unsupervised fashion. Given a text corpus in any language, MetaMorph sequentially aligns words of the corpus to form an MSA and then segments the MSA to produce morphological analyses. Over corpora that contain millions of unique word types, MetaMorph identifies morphemes at an F1 below state-of-the-art performance. But when restricted to smaller sets of orthographically related words, Meta-Morph outperforms the state-of-the-art ParaMor-Morfessor Union morphology induction system. Tested on 5,000 orthographically similar Hungarian word types, MetaMorph reaches 54.1% and ParaMor-Morfessor just 41.9%. Hence, we conclude that MSA is a promising algorithm for unsupervised morphology induction. Future research directions are discussed.},
booktitle = {Proceedings of the 10th Cross-Language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments},
pages = {666–673},
numpages = {8},
location = {Corfu, Greece},
series = {CLEF'09}
}

