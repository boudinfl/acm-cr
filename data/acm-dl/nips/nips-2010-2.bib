@inproceedings{10.5555/2997046.2997188,
author = {Luxburg, Ulrike von and Radl, Agnes and Hein, Matthias},
title = {Getting Lost in Space: Large Sample Analysis of the Commute Distance},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The commute distance between two vertices in a graph is the expected time it takes a random walk to travel from the first to the second vertex and back. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph at all and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. As an alternative we introduce the amplified commute distance that corrects for the undesired large sample effects.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2622–2630},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997187,
author = {Hasselt, Hado van},
title = {Double Q-Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2613–2621},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997186,
author = {Mosci, Sofia and Villa, Silvia and Verri, Alessandro and Rosasco, Lorenzo},
title = {A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups defined a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2604–2612},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997185,
author = {Zinkevich, Martin A. and Weimer, Markus and Smola, Alex and Li, Lihong},
title = {Parallelized Stochastic Gradient Descent},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2595–2603},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997184,
author = {Zhu, Jun and Li, Li-Jia and Li, Fei-Fei and Xing, Eric P.},
title = {Large Margin Learning of Upstream Scene Understanding Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an unbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2586–2594},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997183,
author = {Zhou, Hongbo and Cheng, Qiang},
title = {Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact definition for a novel representation called sparse grouping representation (SGR), and prove a set of sufficient conditions for generating such group level sparsity. Under these sufficient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classification setting.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2577–2585},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997182,
author = {Zhang, Yu and Yeung, Dit-Yan},
title = {Worst-Case Linear Discriminant Analysis},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis (LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis (WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2568–2576},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997181,
author = {Zhang, Yu and Yeung, Dit-Yan and Xu, Qian},
title = {Probabilistic Multi-Task Feature Selection},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, some variants of the l1 norm, particularly matrix norms such as the l1,2 and l1,∞ norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the l1,2 and l1,∞ norms by considering a family of l1,q norms for 1 &lt; q &lt; ∞ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the l1,q norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including q, automatically. Experiments have been conducted on two cancer classification applications using microarray gene expression data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2559–2567},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997180,
author = {Zhang, Yi and Schneider, Jeff},
title = {Learning Multiple Tasks with a Sparse Matrix-Normal Penalty},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose a matrix-variate normal penalty with sparse inverse co-variances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overfitting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via ℓ1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple fields and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2550–2558},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997179,
author = {Zhang, Xinhua and Saha, Ankan and Vishwanathan, S. V. N.},
title = {Lower Bounds on Rate of Convergence of Cutting Plane Methods},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In a recent paper Joachims [1] presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an ε accurate solution in O(1/ε2) iterations. By tightening the analysis, Teo et al. [2] showed that O(1/ε) iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a multivariate performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in O(1/√ε) iterations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2541–2549},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997178,
author = {Yu, Yaoliang and Yang, Min and Xu, Linli and White, Martha and Schuurmans, Dale},
title = {Relaxed Clipping: A Global Training Method for Robust Regression and Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Robust regression and classification are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of "loss clipping" can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2532–2540},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997177,
author = {Yu, Stella X.},
title = {Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and find out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2523–2531},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997176,
author = {Xu, Yang and Kemp, Charles},
title = {Inference and Communication in the Game of Password},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other's perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2514–2522},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997175,
author = {Xu, Huan and Mannor, Shie},
title = {Distributionally Robust Markov Decision Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound. We propose a decision criterion based on distributional robustness: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information). We show that finding the optimal distributionally robust policy can be reduced to a standard robust MDP where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2505–2513},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997174,
author = {Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},
title = {Robust PCA via Outlier Pursuit},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted.We present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2496–2504},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997173,
author = {Wu, Yi-Da and Lin, Shi-Jie and Chen, Hsin},
title = {A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuous-time paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the log-domain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2487–2495},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997172,
author = {Wu, Shuang and He, Xuming and Lu, Hongjing and Yuille, Alan},
title = {A Unified Model of Short-Range and Long-Range Motion Perception},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2478–2486},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997171,
author = {Wohrer, Adrien and Romo, Ranulfo and Machens, Christian},
title = {Linear Readout from a Neural Population with Partial Correlation Data},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reflect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to "fill the gaps" in noise correlations matrices using an iterative application of the Wishart distribution over positive definitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2469–2477},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997170,
author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
title = {Copula Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2460–2468},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997169,
author = {Williams, Oliver and McSherry, Frank},
title = {Probabilistic Inference and Differential Privacy},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We find that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2451–2459},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997168,
author = {Wiens, Jenna and Guttag, John V.},
title = {Active Learning Applied to Patient-Adaptive Heartbeat Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2442–2450},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997167,
author = {White, Martha and White, Adam},
title = {Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however, do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent's confidence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent-environment interaction rarely satisfies traditional assumptions. Samples of value-estimates are dependent, likely non-normally distributed and often limited, particularly in early learning when confidence estimates are pivotal. In this work, we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2433–2441},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997166,
author = {Welinder, Peter and Branson, Steve and Belongie, Serge and Perona, Pietro},
title = {The Multidimensional Wisdom of Crowds},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different "schools of thought" amongst the annotators, and can group together images belonging to separate categories.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2424–2432},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997165,
author = {Weiss, David and Sapp, Benjamin and Taskar, Ben},
title = {Sidestepping Intractable Inference with Structured Ensemble Cascades},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2415–2423},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997164,
author = {Wauthier, Fabian L. and Jordan, Michael I.},
title = {Heavy-Tailed Process Priors for Selective Shrinkage},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space. Often, however, we are confronted with "outliers" in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed stochastic processes (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs when the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2406–2414},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997163,
author = {Wang, Yang and Mori, Greg},
title = {A Discriminative Latent Model of Image Region and Object Tag Correspondence},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth region-to-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2397–2405},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997162,
author = {Wang, Wei and Zhou, Zhi-Hua},
title = {Multi-View Active Learning in the Non-Realizable Case},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be \~{O}(log 1/ε), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is \~{O}(1/ε), where the order of 1/ε is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/ε is related to the parameter in Tsybakov noise.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2388–2396},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997161,
author = {Wang, Meihong and Sha, Fei and Jordan, Michael I.},
title = {Unsupervised Kernel Dimension Reduction},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classification, our methods lead to lower classification errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2379–2387},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997160,
author = {Wang, Eric and Liu, Dehong and Silva, Jorge and Dunson, David and Carin, Lawrence},
title = {Joint Analysis of Time-Evolving Binary Matrices and Associated Documents},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efficient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2370–2378},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997159,
author = {Vishwanathan, S. V. N. and Sun, Zhaonan and Theera-Ampornpunt, Nawanol and Varma, Manik},
title = {Multiple Kernel Learning and the SMO Algorithm},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efficiently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efficiency and is significantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2361–2369},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997158,
author = {Viappiani, Paolo and Boutilier, Craig},
title = {Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian approaches to utility elicitation typically adopt (myopic) expected value of information (EVOI) as a natural criterion for selecting queries. However, EVOI-optimization is usually computationally prohibitive. In this paper, we examine EVOI optimization using choice queries, queries in which a user is ask to select her most preferred product from a set. We show that, under very general assumptions, the optimal choice query w.r.t. EVOI coincides with the optimal recommendation set, that is, a set maximizing the expected utility of the user selection. Since recommendation set optimization is a simpler, submodular problem, this can greatly reduce the complexity of both exact and approximate (greedy) computation of optimal choice queries. We also examine the case where user responses to choice queries are error-prone (using both constant and mixed multinomial logit noise models) and provide worst-case guarantees. Finally we present a local search technique for query optimization that works extremely well with large outcome spaces.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2352–2360},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997157,
author = {Vert, Jean-Philippe and Bleakley, Kevin},
title = {Fast Detection of Multiple Change-Points Shared by Many Signals Using Group LARS},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2343–2351},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997156,
author = {Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Poline, Jean-Baptiste and Thirion, Bertrand},
title = {Brain Covariance Selection: Better Individual Functional Connectivity Models Using Population Prior},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. Learning such models entails two main challenges: i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. We describe subject-level brain functional connectivity structure as a multivari-ate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2334–2342},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997155,
author = {Vinyals, Meritxell and Cerquides, Jes\'{u}s and Farinelli, Alessandro and Rodr\'{\i}guez-Aguilar, Juan Antonio},
title = {Worst-Case Bounds on the Quality of Max-Product Fixed-Points},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start providing a bound independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with specific structures such as bipartite graphs or grids. Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% optimal) on MRFs with large variable-disjoint cycles1.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2325–2333},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997154,
author = {Urry, Matthew J. and Sollich, Peter},
title = {Exact Learning Curves for Gaussian Process Regression on Large Random Graphs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difficult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a finite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2316–2324},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997153,
author = {Triefenbach, Fabian and Jalalvand, Azarakhsh and Schrauwen, Benjamin and Martens, Jean-Pierre},
title = {Phoneme Recognition with Large Hierarchical Reservoirs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2307–2315},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997152,
author = {Todorov, Emanuel},
title = {Policy Gradients in Linearly-Solvable MDPs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present policy gradient results within the framework of linearly-solvable MDPs. For the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2298–2306},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997151,
author = {Thiesson, Bo and Wang, Chong},
title = {Fast Large-Scale Mixture Modeling with Component-Specific Data Partitions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is significantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2289–2297},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997150,
author = {Taylor, Graham W. and Fergus, Rob and Williams, George and Spiro, Ian and Bregler, Christoph},
title = {Pose-Sensitive Embedding by Nonlinear NCA Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that real-world performance can be improved through the use of synthetic data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2280–2288},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997149,
author = {Takiyama, Ken and Okada, Masato},
title = {Switching State Space Model for Simultaneously Estimating State Transitions and Nonstationary Firing Rates},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary firing rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean firing rates but also discontinuous changes in temporal profiles of firing rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary firing rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of firing rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2271–2279},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997148,
author = {Syed, Zeeshan and Guttag, John},
title = {Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2262–2270},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997147,
author = {Syed, Umar and Schapire, Robert E.},
title = {A Reduction from Apprenticeship Learning to Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate ε, the difference between the value of the apprentice's policy and the expert's policy is O(√ε). Further, we prove that this difference is only O(ε) when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2253–2261},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997146,
author = {Syed, Umar and Taskar, Ben},
title = {Semi-Supervised Learning with Adversarially Missing Label Information},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efficient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2244–2252},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997145,
author = {Sun, Yi and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
title = {Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new way of converting a reversible finite Markov chain into a non-reversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2235–2243},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997144,
author = {Sun, Deqing and Sudderth, Erik B. and Black, Michael J.},
title = {Layered Image Motion with Explicit Occlusions, Temporal Consistency, and Depth Ordering},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2226–2234},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997143,
author = {Strehl, Alexander L. and Langford, John and Li, Lihong and Kakade, Sham M.},
title = {Learning from Logged Implicit Exploration Data},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide a sound and consistent foundation for the use of nonrandom exploration data in "contextual bandit" or "partially labeled" settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which "offline" data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2217–2225},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997142,
author = {Stobbe, Peter and Krause, Andreas},
title = {Efficient Minimization of Decomposable Submodular Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2208–2216},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997141,
author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
title = {Smoothness, Low-Noise and Fast Rates},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We establish an excess risk bound of \~{O}(HR2n + √HL*Rn) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity Rn, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = √R/n, this translates to a learning rate of \~{O}(RH/n) in the separable (L* = 0) case and \~{O}(RH/n + √L*RH/n) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2199–2207},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997140,
author = {Sorg, Jonathan and Singh, Satinder and Lewis, Richard L.},
title = {Reward Design via Online Gradient Ascent},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's. This gives rise to the optimization problem of designing the artificial agent's goals—in the RL framework, designing the agent's reward function. Existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure. In this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime. We show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2190–2198},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997139,
author = {Sontag, David and Meshi, Ofer and Jaakkola, Tommi and Globerson, Amir},
title = {More Data Means Less Inference: A Pseudo-Max Approach to Structured Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of learning to predict structured labels is of key importance in many applications. However, for general graph structure both learning and inference are intractable. Here we show that it is possible to circumvent this difficulty when the distribution of training examples is rich enough, via a method similar in spirit to pseudo-likelihood. We show that our new method achieves consistency, and illustrate empirically that it indeed approaches the performance of exact methods when sufficiently large training sets are used.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2181–2189},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997138,
author = {Singh, Anand and Jolivet, Renaud and Magistretti, Pierre J. and Weber, Bruno},
title = {Sodium Entry Efficiency during Action Potentials: A Novel Single-Parameter Family of Hodgkin-Huxley Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sodium entry during an action potential determines the energy efficiency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of Hodgkin-Huxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2173–2180},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997137,
author = {Silver, David and Veness, Joel},
title = {Monte-Carlo Planning in Large POMDPs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 x 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2164–2172},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997136,
author = {Shojaie, Ali and Michailidis, George},
title = {Penalized Principal Component Regression on Graphs for Analysis of Subnetworks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a flexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2155–2163},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997135,
author = {Shenoy, Pradeep and Rao, Rajesh P. N. and Yu, Angela J.},
title = {A Rational Decision-Making Framework for Inhibitory Control},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2146–2154},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997134,
author = {Sharpnack, James and Singh, Aarti},
title = {Identifying Graph-Structured Activation Patterns in Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of identifying an activation pattern in a complex, large-scale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2137–2145},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997133,
author = {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},
title = {Online Learning in the Manifold of Low-Rank Matrices},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute, and so is the projection operator that approximates it, we describe another second-order retraction that can be computed efficiently, with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m x n, given rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model, and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classification task.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2128–2136},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997132,
author = {Seth, Sohan and Park, Il "Memming" and Brockmeier, Austin J. and Semework, Mulugeta and Choi, John and Francis, Joseph T. and Pr\'{\i}ncipe, Jos\'{e} C.},
title = {A Novel Family of Non-Parametric Cumulative Based Divergences for Point Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Hypothesis testing on point processes has several applications such as model fitting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean firing rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov-Smirnov and Cram\'{e}r-von-Mises tests to the space of spike trains via stratification, and show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to find optimally matched point processes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2119–2127},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997131,
author = {Schmiedt, Joscha T. and Albers, Christian and Pawelzik, Klaus},
title = {Spike Timing-Dependent Plasticity as Dynamic Filter},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modifications. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We find that our model reproduces data from to recent experimental studies with a small number of biophysically in-terpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear filter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modifications are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also find emphasis of specific baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2110–2118},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997130,
author = {Scheinberg, Katya and Ma, Shiqian and Goldfarb, Donald},
title = {Sparse Inverse Covariance Selection via Alternating Linearization Methods},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an ℓ1-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an ε-optimal solution in O(1/ε) iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2101–2109},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997129,
author = {Sayedi, Amin and Zadimoghaddam, Morteza and Blum, Avrim},
title = {Trading off Mistakes and Don't-Know Predictions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We discuss an online learning framework in which the agent is allowed to say "I don't know" as well as making incorrect predictions on given examples. We analyze the trade off between saying "I don't know" and making mistakes. If the number of don't-know predictions is required to be zero, the model reduces to the well-known mistake-bound model introduced by Littlestone [Lit88]. On the other hand, if no mistakes are allowed, the model reduces to KWIK framework introduced by Li et. al. [LLW08]. We propose a general, though inefficient, algorithm for general finite concept classes that minimizes the number of don't-know predictions subject to a given bound on the number of allowed mistakes. We then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators with a margin.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2092–2100},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997128,
author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},
title = {Active Estimation of F-Measures},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of estimating the Fα-measure of a given model as accurately as possible on a fixed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα-measures are more accurate than estimates based on instances sampled from the test distribution.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2083–2091},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997127,
author = {Sato, Issei and Kurihara, Kenichi and Nakagawa, Hiroshi},
title = {Deterministic Single-Pass Algorithm for LDA},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2074–2082},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997126,
author = {Salzmann, Mathieu and Urtasun, Raquel},
title = {Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2065–2073},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997125,
author = {Salakhutdinov, Ruslan and Srebro, Nathan},
title = {Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled Netflix dataset.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2056–2064},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997124,
author = {Saberian, Mohammad J. and Vasconcelos, Nuno},
title = {Boosting Classifier Cascades},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classification and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classification risk and complexity. It searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2047–2055},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997123,
author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
title = {Tight Sample Complexity of Large-Margin Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the γ-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the γ-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2038–2046},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997122,
author = {Ruvolo, Paul and Movellan, Javier R.},
title = {An Alternative to Low-Level-Synchrony-Based Methods for Speech Detection},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction. One popular approach to this problem is audio-visual synchrony detection [10, 21, 12]. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal. Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all. Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models. The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot). Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection. The work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2029–2037},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997121,
author = {Reichert, David P. and Series, Peggy and Storkey, Amos J.},
title = {Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: A Deep Boltzmann Machine Model},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative findings in CBS. We also introduce a modification to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2020–2028},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997120,
author = {Rawlik, Konrad C. and Toussaint, Marc and Vijayakumar, Sethu},
title = {An Approximate Inference Approach to Temporal Optimization in Optimal Control},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems. However, they generally require the temporal parameters (for e.g. the movement duration or the time point of reaching an intermediate goal) to be specified a priori. Here, we present a methodology that is capable of jointly optimizing the temporal parameters in addition to the control command profiles. The presented approach is based on a Bayesian canonical time formulation of the optimal control problem, with the temporal mapping from canonical to real time parametrised by an additional control variable. An approximate EM algorithm is derived that efficiently optimizes both the movement duration and control commands offering, for the first time, a practical approach to tackling generic via point problems in a systematic way under the optimal control framework. The proposed approach, which is applicable to plants with non-linear dynamics as well as arbitrary state dependent and quadratic control costs, is evaluated on realistic simulations of a redundant robotic plant.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2011–2019},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997119,
author = {Ranzato, Marc'Aurelio and Mnih, Volodymyr and Hinton, Geoffrey E.},
title = {Generating More Realistic Images Using Gated MRF's},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2002–2010},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997118,
author = {Haefner, Ralf M. and Bethge, Matthias},
title = {Evaluating Neuronal Codes for Inference Using Fisher Information},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and find it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We find that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1993–2001},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997117,
author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
title = {Online Learning: Random Averages, Combinatorial Parameters, and Learnability},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fat-shattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1984–1992},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997116,
author = {Rajan, Kanaka and Abbott, L. F. and Sompolinsky, Haim},
title = {Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of fluctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1975–1983},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997115,
author = {Richard, Emile and Baskiotis, Nicolas and Evgeniou, Theodoros and Vayatis, Nicolas},
title = {Link Discovery Using Graph Feature Tracking},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1966–1974},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997114,
author = {Quadrianto, Novi and Smola, Alex and Caetano, Tib\'{e}rio and Vishwanathan, S. V. N. and Petterson, James},
title = {Multitask Learning without Label Correspondences},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces, such as integrating Yahoo! and DMOZ web directories.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1957–1965},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997113,
author = {Qin, Tao and Geng, Xiubo and Liu, Tie-Yan},
title = {A New Probabilistic Model for Rank Aggregation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-of-the-art ranking accuracy, and are much more efficient than previous algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1948–1956},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997112,
author = {Puertas, Gervasio and Bornschein, J\"{o}rg and L\"{u}cke, J\"{o}rg},
title = {The Maximal Causes of Natural Scenes Are Edge Filters},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1939–1947},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997111,
author = {Pfau, David and Bartlett, Nicholas and Wood, Frank},
title = {Probabilistic Deterministic Infinite Automata},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop a sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1930–1938},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997110,
author = {Petterson, James and Smola, Alex and Caetano, Tiberio and Buntine, Wray and Narayanamurthy, Shravan},
title = {Word Features for Latent Dirichlet Allocation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1921–1929},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997109,
author = {Petterson, James and Caetano, Tiberio},
title = {Reverse Multi-Label Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-label classification is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classification and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised. We optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods, showing excellent performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1912–1920},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997108,
author = {Peel, Thomas and Anthoine, Sandrine and Ralaivola, Liva},
title = {Empirical Bernstein Inequalities for U-Statistics},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernstein-type inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1903–1911},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997107,
author = {Pechyony, Dmitry and Vapnik, Vladimir},
title = {On the Theory of Learning with Privileged Information},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to find a classifier with a low generalization error in the decision space. We consider an empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space. We outline the conditions on the correcting space that, if satisfied, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1894–1902},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997106,
author = {Payet, Nadia and Todorovic, Sinisa},
title = {(RF)<sup>2</sup> — Random Forest Random Field},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)2. Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2. (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1885–1893},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997105,
author = {Pathak, Manas A. and Rane, Shantanu and Raj, Bhiksha},
title = {Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1876–1884},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997104,
author = {Parameswaran, Shibin and Weinberger, Kilian Q.},
title = {Large Margin Multi-Task Metric Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to support vector machines (svm) by Evgeniou et al. [15]. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (1mnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multi-task learning. We evaluate the resulting multi-task 1mnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1867–1875},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997103,
author = {Papandreou, George and Yuille, Alan L.},
title = {Gaussian Sampling by Local Perturbations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a technique for exact simulation of Gaussian Markov random fields (GMRFs), which can be interpreted as locally injecting noise to each Gaussian factor independently, followed by computing the mean/mode of the perturbed GMRF. Coupled with standard iterative techniques for the solution of symmetric positive definite systems, this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models. Apart from synthesizing data under a Gaussian model, the proposed technique directly leads to an efficient unbiased estimator of marginal variances. Beyond Gaussian models, the proposed algorithm is also very useful for handling highly non-Gaussian continuously-valued MRFs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of Gaussians with the help of local or distributed latent mixture assignment variables. The Bayesian treatment of such models most naturally involves a block Gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate Gaussian continuous vector and we show that it can directly benefit from the proposed methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1858–1866},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997102,
author = {P\'{a}l, D\'{a}vid and P\'{o}czos, Barnab\'{a}s and Szepesv\'{a}ri, Csaba},
title = {Estimation of R\'{e}Nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present simple and computationally efficient nonparametric estimators of R\'{e}nyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over ℝd. The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the 'generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1849–1857},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997101,
author = {Orabona, Francesco and Crammer, Koby},
title = {New Adaptive Algorithms for Online Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a general framework to online learning for classification problems with time-varying potential functions in the adversarial setting. This framework allows to design and prove relative mistake bounds for any generic loss function. The mistake bounds can be specialized for the hinge loss, allowing to recover and improve the bounds of known online classification algorithms. By optimizing the general bound we derive a new online classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order information. We analyze the properties of the algorithm and illustrate its performance using synthetic dataset.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1840–1848},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997100,
author = {Opper, Manfred and Ruttor, Andreas and Sanguinetti, Guido},
title = {Approximate Inference in Continuous Time Gaussian-Jump Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1831–1839},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997099,
author = {Noh, Yung-Kyun and Zhang, Byoung-Tak and Lee, Daniel D.},
title = {Generative Local Metric Learning for Nearest Neighbor Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning a local metric to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1822–1830},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997098,
author = {Nie, Feiping and Huang, Heng and Cai, Xiao and Ding, Chris},
title = {Efficient and Robust Feature Selection via Joint ℓ<sub>2,1</sub>-Norms Minimization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint ℓ2,1-norm minimization on both loss function and regularization. The ℓ2,1-norm based loss function is robust to outliers in data points and the ℓ2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1813–1821},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997097,
author = {Neu, Gergely and Gy\"{o}rgy, Andr\'{a}s and Szepesv\'{a}ri, Csaba and Antos, Andr\'{a}s},
title = {Online Markov Decision Processes under Bandit Feedback},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider online learning in finite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and, assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O (T2/3 (ln T)1/3), giving the first rigorously proved regret bound for the problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1804–1812},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997096,
author = {Navarro, Daniel J.},
title = {Learning the Context of a Category},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is fit to multiple data sets, and provides a parsimonious method for describing how humans learn context specific conceptual representations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1795–1803},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997095,
author = {Narayanan, Hariharan and Mitter, Sanjoy},
title = {Sample Complexity of Testing the Manifold Hypothesis},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of O(k/ε2 + log 1/δ/ε2) for the sample complexity of Empirical Risk minimization on k-means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here ε is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of O(k2/ε2 + log 1/δ/ε2) to O(k/ε2 (min (k, + log4 k/ε/ε2)) + log 1/δ/ε2). Based on these results, we devise a simple algorithm for k-means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to high dimensional data, where the sample complexity is independent of the ambient dimension.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1786–1794},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997094,
author = {Narayanan, Hariharan and Rakhlin, Alexander},
title = {Random Walk Approach to Regret Minimization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1777–1785},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997093,
author = {Nakajima, Shinichi and Sugiyama, Masashi and Tomioka, Ryota},
title = {Global Analytic Solution for Variational Bayesian Matrix Factorization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1768–1776},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997092,
author = {Nagano, Kiyohito and Kawahara, Yoshinobu and Iwata, Satoru},
title = {Minimum Average Cost Clustering},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1759–1767},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997091,
author = {M\o{}rup, Morten and Madsen, Kristoffer Hougaard and Dogonowski, Anne Marie and Siebner, Hartwig and Hansen, Lars Kai},
title = {Infinite Relational Modeling of Functional Connectivity in Resting State FMRI},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the infinite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1750–1758},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997090,
author = {Myers, Seth A. and Leskovec, Jure},
title = {On the Convexity of Latent Social Network Inference},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1741–1749},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997089,
author = {Murray, Iain and Adams, Ryan Prescott},
title = {Slice Sampling Covariance Hyperparameters of Latent Gaussian Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1732–1740},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997088,
author = {Muralidharan, Kritika and Vasconcelos, Nuno},
title = {A Biologically Plausible Network for the Computation of Orientation Dominance},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justification for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1723–1731},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997087,
author = {Mukherjee, Indraneel and Schapire, Robert E.},
title = {A Theory of Multiclass Boosting},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the "correct" requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1714–1722},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997086,
author = {Mozer, Michael C. and Pashler, Harold and Wilder, Matthew and Lindsey, Robert V. and Jones, Matt C. and Jones, Michael N.},
title = {Decontaminating Human Judgments by Removing Sequential Dependencies},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1705–1713},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997085,
author = {Motwani, Kamiya and Adluru, Nagesh and Hinrichs, Chris and Alexander, Andrew and Singh, Vikas},
title = {Epitome Driven 3-D Diffusion Tensor Image Segmentation: On Extracting <i>Specific</i> Structures},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of segmenting specific white matter structures of interest from Diffusion Tensor (DT-MR) images of the human brain. This is an important requirement in many Neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images. Typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today. To address this problem, we endow an image segmentation algorithm with "advice" encoding some global characteristics of the region(s) we want to extract. This is accomplished by constructing (using expert-segmented images) an epitome of a specific region - as a histogram over a bag of 'words' (e.g., suitable feature descriptors). Now, given such a representation, the problem reduces to segmenting a new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features. We present combinatorial approximation algorithms to incorporate such domain specific constraints for Markov Random Field (MRF) segmentation. Making use of recent results on image co-segmentation, we derive effective solution strategies for our problem. We provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in Neuroscience can be extracted reliably from 3-D brain image volumes using our algorithm.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1696–1704},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997084,
author = {Mooij, Joris M. and Stegle, Oliver and Janzing, Dominik and Zhang, Kun and Sch\"{o}lkopf, Bernhard},
title = {Probabilistic Latent Variable Models for Distinguishing between Cause and Effect},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1687–1695},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997083,
author = {Montavon, Gr\'{e}goire and Braun, Mikio L. and M\"{u}ller, Klaus-Robert},
title = {Layer-Wise Analysis of Deep Networks with Gaussian Kernels},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1678–1686},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997082,
author = {Mizutani, Eiji and Dreyfus, Stuart},
title = {An Analysis on Negative Curvature Induced by Singularity in Multi-Layer Neural-Network Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the neural-network parameter space, an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a flat region). Therefore, it may be confused with "attractive" local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efficient methods have been previously developed that avoided plateaus.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1669–1677},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997081,
author = {Miyamae, Atsushi and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},
title = {Natural Policy Gradient Methods with Parameter-Based Exploration for Control Tasks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose an efficient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1660–1668},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997080,
author = {Mitra, Kaushik and Sheorey, Sameer and Chellappa, Rama},
title = {Large-Scale Matrix Factorization with Missing Data under Additional Constraints},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that: 1) an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and 2) additional constraints such as ortho-normality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem, non-rigid SfM and photometric stereo problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1651–1659},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997079,
author = {Millner, Sebastian and Gr\"{u}bl, Andreas and Meier, Karlheinz and Schemmel, Johannes and Schwartz, Marc-Olivier},
title = {A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1642–1650},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997078,
author = {Miller, Benjamin A. and Bliss, Nadya T. and Wolfe, Patrick J.},
title = {Subgraph Detection Using Eigenvector L1 Norms},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a "detection theory" for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph's so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets confirms the efficacy of this approach.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1633–1641},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997077,
author = {Fard, Mahdi Milani and Pineau, Joelle},
title = {PAC-Bayesian Model Selection for Reinforcement Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces the first set of PAC-Bayesian bounds for the batch reinforcement learning problem in finite state spaces. These bounds hold regardless of the correctness of the prior distribution. We demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment, or on the value of actions. Our empirical results confirm that PAC-Bayesian model-selection is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignores them when they are misleading.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1624–1632},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997076,
author = {Micchelli, Charles A. and Morales, Jean M. and Pontil, Massimiliano},
title = {A Family of Penalty Functions for Structured Sparsity},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the li norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1612–1623},
numpages = {12},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997075,
author = {Memisevic, Roland and Zach, Christopher and Hinton, Geoffrey and Pollefeys, Marc},
title = {Gated Softmax Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a "log-bilinear" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1603–1611},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997074,
author = {McAllester, David and Hazan, Tamir and Keshet, Joseph},
title = {Direct Loss Minimization for Structured Prediction},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1594–1602},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997073,
author = {Maurits, Luke and Perfors, Amy and Navarro, Daniel},
title = {Why Are Some Word Orders More Common than Others? A Uniform Information Density Account},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an information-theoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1585–1593},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997072,
author = {Masnadi-Shirazi, Hamed and Vasconcelos, Nuno},
title = {Variable Margin Losses for Classifier Design},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of controlling the margin of a classifier is studied. A detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classification margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1576–1584},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997071,
author = {Maron, Yariv and Lamar, Michael and Bienenstock, Elie},
title = {Sphere Embedding: An Application to Part-of-Speech Induction},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a high-dimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1567–1575},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997070,
author = {Mairal, Julien and Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
title = {Network Flow Algorithms for Structured Sparsity},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of ℓ∞-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1558–1566},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997069,
author = {Maillard, Odalric-Ambrym and Munos, Remi},
title = {Scrambled Objects for Least-Squares Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider least-squares regression using a randomly generated subspace GP ⊂ F of finite dimension P, where F is a function space of infinite dimension, e.g. L2([0, 1]d). GP is defined as the span of P random features that are linear combinations of the basis functions of F weighted by random Gaussian i.i.d. coefficients. In particular, we consider multi-resolution random combinations at all scales of a given mother function, such as a hat function or a wavelet. In this latter case, the resulting Gaussian objects are called scrambled wavelets and we show that they enable to approximate functions in Sobolev spaces Hs([0, l]d). As a result, given N data, the least-squares estimate undefined built from P scrambled wavelets has excess risk ‖f* - undefined‖2P = O(‖f*‖2Hs([0,1]d)(log N)/P + P(log N)/N) for target functions f* ∈ Hs ([0,1]d) of smoothness order s &gt; d/2. An interesting aspect of the resulting bounds is that they do not depend on the distribution P from which the data are generated, which is important in a statistical regression setting considered here. Randomization enables to adapt to any possible distribution.We conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity \~{O}(2dN3/2 log N + N2), where d is the dimension of the input space.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1549–1557},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997068,
author = {Mahadevan, Sridhar and Liu, Bo},
title = {Basis Construction from Power Series Expansions of Value Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations. The first two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same benefits at lower computational cost.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1540–1548},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997067,
author = {Magdon-Ismail, Malik},
title = {Permutation Complexity Bound on Out-Sample Error},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We define a data dependent permutation complexity for a hypothesis set H, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based (like the maximum discrepancy) on dependent sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1531–1539},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997066,
author = {Lyu, Siwei},
title = {Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Divisive normalization (DN) has been advocated as an effective nonlinear efficient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1522–1530},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997065,
author = {Luss, Ronny and Rosset, Saharon and Shahar, Moni},
title = {Decomposing Isotonic Regression for Efficiently Solving Large Problems},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem, and prove that this sequence of partitions converges to the global solution. These network flow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2 x 105 variables and 107 constraints.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1513–1521},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997064,
author = {Jie, Luo and Orabona, Francesco},
title = {Learning from Candidate Labeling Sets},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classifiers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difficult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efficient algorithm to solve it. Experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1504–1512},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997063,
author = {Lu, Hongjing and Lin, Tungyou and Lee, Alan L. F. and Vese, Luminita and Yuille, Alan},
title = {Functional Form of Motion Priors in Human Motion Perception},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More specifically, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, first-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance. We then measured human performance for motion tasks and found that we obtained better fit for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better fit to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best fit models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction - ranging from 3% to 8% for five human subjects over ten experimental conditions - and give further support that the human visual system uses an L1-norm (Laplace) prior.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1495–1503},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997062,
author = {Lozano, Aur\'{e}lie C. and Sindhwani, Vikas},
title = {Block Variable Selection in Multivariate Regression and High-Dimensional Causal Inference},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique. This extension accounts for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. Within this framework, we then formulate the problem of inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to the classic PageRank algorithm traditionally applied to hyperlink graphs. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1486–1494},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997061,
author = {Lowd, Daniel and Domingos, Pedro},
title = {Approximate Inference by Compilation to Arithmetic Circuits},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1477–1485},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997060,
author = {Liu, Yuzong and Sharma, Mohit and Gaona, Charles M. and Breshears, Jonathan D. and Roland, Jarod and Freudenburg, Zachary V. and Weinberger, Kilian Q. and Leuthardt, Eric C.},
title = {Decoding Ipsilateral Finger Movements from ECoG Signals in Humans},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several motor related Brain Computer Interfaces (BCIs) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices. Contralateral primary motor cortex is also the region most severely affected by hemispheric stroke. Recent studies have identified ipsilateral cortical activity in planning of motor movements and its potential implications for a stroke relevant BCI. The most fundamental functional loss after a hemispheric stroke is the loss of fine motor control of the hand. Thus, whether ipsilateral cortex encodes finger movements is critical to the potential feasibility of BCI approaches in the future. This study uses ipsilateral cortical signals from humans (using ECoG) to decode finger movements. We demonstrate, for the first time, successful finger movement detection using machine learning algorithms. Our results show high decoding accuracies in all cases which are always above chance. We also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features are consistent with previous physiological findings. The results of this study have substantial implications for advancing neuroprosthetic approaches to stroke populations not currently amenable to existing BCI techniques.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1468–1476},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997059,
author = {Liu, Jun and Ye, Jieping},
title = {Moreau-Yosida Regularization for Grouped Tree Structure Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty, where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1459–1467},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997058,
author = {Liu, Ji and Wonka, Peter and Ye, Jieping},
title = {Multi-Stage Dantzig Selector},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ ℝn x m (m ≫ n) and a noisy observation vector y ∈ ℝn satisfying y = X β* + ε where ε is the noise vector following a Gaussian distribution N(0, σ2I), how to recover the signal (or parameter vector) β* when the signal is sparse?The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal β*. We show that if X obeys a certain condition, then with a large probability the difference between the solution ^β estimated by the proposed method and the true solution β* measured in terms of the lp norm (p ≥ 1) is bounded as‖^β - β ‖p ≤ (C(s - N)1/p √log m + Δ) σ,where C is a constant, s is the number of nonzero entries in β*, A is independent of m and is much smaller than the first term, and N is the number of entries of β* larger than a certain value in the order of O(σ/√log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs1/p √log mσ to C(s - N)1/p√log mσ where the value N depends on the number of large entries in β*. When N = s, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1450–1458},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997057,
author = {Liu, Han and Chen, Xi},
title = {Multivariate Dyadic Regression Trees for Sparse Learning Problems},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1441–1449},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997056,
author = {Liu, Han and Roeder, Kathryn and Wasserman, Larry},
title = {Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1432–1440},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997055,
author = {Liu, Han and Chen, Xi and Lafferty, John and Wasserman, Larry},
title = {Graph-Valued Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Undirected graphical models encode in a graph G the dependency structure of a random vector Y. In many applications, it is of interest to model Y given another random vector X as input. We refer to the problem of estimating the graph G(x) of Y conditioned on X = x as "graph-valued regression". In this paper, we propose a semiparametric method for estimating G(x) that builds a tree on the X space just as in CART (classification and regression trees), but at each leaf of the tree estimates a graph. We call the method "Graph-optimized CART", or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1423–1431},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997054,
author = {Liu, Hairong and Latecki, Longin Jan and Yan, Shuicheng},
title = {Robust Clustering as Ensembles of Affinity Relations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations. The average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efficient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a unified solution to clustering from k-ary affinity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1414–1422},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997053,
author = {Lin, Yuanqing and Zhang, Tong and Zhu, Shenghuo and Yu, Kai},
title = {Deep Coding Network},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a principled extension of the traditional single-layer flat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1405–1413},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997052,
author = {Lin, Dahua and Grimson, Eric and Fisher, John},
title = {Construction of Dependent Dirichlet Processes Based on Poisson Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1396–1404},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997051,
author = {Li, Ping and K\"{o}nig, Arnd Christian and Gui, Wenhao},
title = {B-Bit Minwise Hashing for Estimating Three-Way Similarities},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Computing1 two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest 6 bits (where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1387–1395},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997050,
author = {Li, Li-Jia and Su, Hao and Xing, Eric P. and Fei-Fei, Li},
title = {Object Bank: A High-Level Image Representation for Scene Classification &amp; Semantic Feature Sparsification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1378–1386},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997049,
author = {Li, Kaiming and Guo, Lei and Faraco, Carlos and Zhu, Dajiang and Deng, Fan and Zhang, Tuo and Jiang, Xi and Zhang, Degang and Chen, Hanbo and Hu, Xintao and Miller, Steve and Liu, Tianming},
title = {Individualized ROI Optimization via Maximization of Group-Wise Consistency of Structural and Functional Profiles},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1369–1377},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997048,
author = {Li, Fuxin and Sminchisescu, Cristian},
title = {Convex Multiple-Instance Learning by Estimating Likelihood Ratio},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an approach to multiple-instance learning that reformulates the problem as a convex optimization on the likelihood ratio between the positive and the negative class for each training instance. This is casted as joint estimation of both a likelihood ratio predictor and the target (likelihood ratio variable) for instances. Theoretically, we prove a quantitative relationship between the risk estimated under the 0-1 classification loss, and under a loss function for likelihood ratio. It is shown that likelihood ratio estimation is generally a good surrogate for the 0-1 loss, and separates positive and negative instances well. The likelihood ratio estimates provide a ranking of instances within a bag and are used as input features to learn a linear classifier on bags of instances. Instance-level classification is achieved from the bag-level predictions and the individual likelihood ratios. Experiments on synthetic and real datasets demonstrate the competitiveness of the approach.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1360–1368},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997046.2997047,
author = {Li, Congcong and Kowdle, Adarsh and Saxena, Ashutosh and Chen, Tsuhan},
title = {Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classifiers already exist for many sub-tasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classifier.We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a 'black-box' interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about what error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classification.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {1351–1359},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@proceedings{10.5555/2997046,
title = {NIPS'10: Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Vancouver, British Columbia, Canada}
}

