@inproceedings{10.5555/2969239.2969441,
author = {Perrot, Micha\"{e}l and Habrard, Amaury},
title = {Regressive Virtual Metric Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We are interested in supervised metric learning of Mahalanobis like distances. Existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples. In this paper, instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points. Hence, each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy. We show that our approach admits a closed form solution which can be kernelized. We provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods. Furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport. Lastly, we evaluate our approach on several state of the art datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1810–1818},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969440,
author = {Procaccia, Ariel D. and Shah, Nisarg},
title = {Is Approval Voting Optimal given Approval Votes?},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives. It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved. We challenge this assertion by proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative, given k-approval votes. While the answer is generally positive, our theoretical and empirical results call attention to situations where approval voting is suboptimal.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1801–1809},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969439,
author = {Komiyama, Junpei and Honda, Junya and Nakagawa, Hiroshi},
title = {Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1792–1800},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969438,
author = {Andreas, Jacob and Rabinovich, Maxim and Jordan, Michael I. and Klein, Dan},
title = {On the Accuracy of Self-Normalized Log-Linear Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as "self-normalization", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking.We prove upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1783–1791},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969437,
author = {Qian, Chao and Yu, Yang and Zhou, Zhi-Hua},
title = {Subset Selection by Pareto Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection, sparse regression, dictionary learning, etc. In this paper, we propose the POSS approach which employs evolutionary Pareto optimization to find a small-sized subset with good performance. We prove that for sparse regression, POSS is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently. Particularly, for the Exponential Decay subclass, POSS is proven to achieve an optimal solution. Empirical study verifies the theoretical results, and exhibits the superior performance of POSS to greedy and convex relaxation methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1774–1782},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969436,
author = {Chen, Jianshu and He, Ji and Shen, Yelong and Xiao, Lin and He, Xiaodong and Gao, Jianfeng and Song, Xinying and Deng, Li},
title = {End-to-End Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1765–1773},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969435,
author = {Arjevani, Yossi and Shamir, Ohad},
title = {Communication Complexity of Distributed Convex Learning and Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1756–1764},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969434,
author = {Nguyen, Quoc Phong and Low, Kian Hsiang and Jaillet, Patrick},
title = {Inverse Reinforcement Learning with Locally Consistent Reward Functions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Existing inverse reinforcement learning (IRL) algorithms have assumed each expert's demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts' behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1747–1755},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969433,
author = {Wang, Ye and Dunson, David},
title = {Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning. One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold. There are a rich variety of manifold learning methods available, which allow mapping of data points to the manifold. However, there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data. The best attempt is the Gaussian process latent variable model (GP-LVM), but identifiability issues lead to poor performance. We solve these issues by proposing a novel Coulomb repulsive process (Corp) for locations of points on the manifold, inspired by physical models of electrostatic interactions among particles. Combining this process with a GP prior for the mapping function yields a novel electrostatic GP (electroGP) process. Focusing on the simple case of a one-dimensional manifold, we develop efficient inference algorithms, and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1738–1746},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969432,
author = {Norouzi, Mohammad and Collins, Maxwell D. and Johnson, Matthew and Fleet, David J. and Kohli, Pushmeet},
title = {Efficient Non-Greedy Optimization of Decision Trees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d2), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1729–1737},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969431,
author = {Shpitser, Ilya},
title = {Segregated Graphs and Marginals of Chain Graph Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables. Markov random fields (MRFs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks. A chain graph model under the Lauritzen-Wermuth-Frydenberg interpretation (hereafter a chain graph model) generalizes both Bayesian networks and MRFs, and can represent asymmetric and symmetric relationships together.As in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model. One recent approach to the study of marginal graphical models is to consider a well-behaved supermodel. Such a supermodel of marginals of Bayesian networks, defined only by conditional independences, and termed the ordinary Markov model, was studied at length in [6].In this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a Markov property, with supermodels of marginals of chain graphs defined only by conditional independences. Special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and the ordinary Markov model carry over. Our results suggest that segregated graphs define an analogue of the ordinary Markov model for marginals of chain graph models.We illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1720–1728},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969430,
author = {Dance, Christopher and Silander, Tomi},
title = {When Are Kalman-Filter Restless Bandits Indexable?},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is indexable in the sense that the Whittle index is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about Schur-convexity and mechanical words, which are particular binary strings intimately related to palindromes.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1711–1719},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969429,
author = {Mueller, Jonas and Jaakkola, Tommi},
title = {Principal Differences Analysis: Interpretable Characterization of Differences between Distributions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1702–1710},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969428,
author = {Hermann, Karl Moritz and Ko\v{c}isk\'{y}, Tom\'{a}\v{s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
title = {Teaching Machines to Read and Comprehend},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1693–1701},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969427,
author = {Corneil, Dane and Gerstner, Wulfram},
title = {Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze-like Environments},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial. While the mechanism for rapid path planning is unknown, the CA3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal "preplay" periods may trace out future goal-directed trajectories. Here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model, resulting in bump-like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1684–1692},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969426,
author = {Shibagaki, Atsushi and Suzuki, Yoshiki and Karasuyama, Masayuki and Takeuchi, Ichiro},
title = {Regularization Path of Cross-Validation Error Lower Bounds},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1675–1683},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969425,
author = {Valera, Isabel and Ruiz, Francisco J. R. and Svensson, Lennart and Perez-Cruz, Fernando},
title = {Infinite Factorial Dynamical Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose the infinite factorial dynamic model (iFDM), a general Bayesian non-parametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1666–1674},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969424,
author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
title = {Less is More: Nystr\"{o}m Computational Regularization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study Nystr\"{o}m type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr\"{o}m Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1657–1665},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969423,
author = {Hensman, James and Matthews, Alexander G. de G. and Filippone, Maurizio and Ghahramani, Zoubin},
title = {MCMC for Variationally Sparse Gaussian Processes},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper is available at github.com/sparseMCMC.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1648–1656},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969422,
author = {Sugiyama, Mahito and Borgwardt, Karsten M.},
title = {Halting in Random Walk Kernels},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Random walk kernels measure graph similarity by counting matching walks in two graphs. In their most popular form of geometric random walk kernels, longer walks of length k are downweighted by a factor of λk (λ &lt; 1) to ensure convergence of the corresponding geometric series. We know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting: Longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1. This is a naive kernel between edges and vertices. We theoretically show that halting may occur in geometric random walk kernels. We also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets. Our findings promise to be instrumental in future graph kernel development and applications of random walk kernels.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1639–1647},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969421,
author = {Rosasco, Lorenzo and Villa, Silvia},
title = {Learning with Incremental Iterative Regularization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Within a statistical learning setting, we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method. In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and prove strong universal consistency, i.e. almost sure convergence of the risk, as well as sharp finite sample bounds for the iterates. Our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1630–1638},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969420,
author = {Tian, Tian and Zhu, Jun},
title = {Max-Margin Majority Voting for Learning from Crowds},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting (M3V) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and M3V. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1621–1629},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969419,
author = {Ma, Tengyu and Wigderson, Avi},
title = {Sum-of-Squares Lower Bounds for Sparse PCA},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the Sparse Principal Component Analysis (Sparse PCA) problem, and the family of Sum-of-Squares (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension p, a planted k-sparse unit vector can be in principle detected using only n ≈ k log p (Gaussian or Bernoulli) samples, but all efficient (polynomial time) algorithms known require n ≈ k2 samples. It was also known that this quadratic gap cannot be improved by the the most basic semi-definite (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or "pseudo-expectations") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1612–1620},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969418,
author = {Harel, Yuval and Meir, Ron and Opper, Manfred},
title = {A Tractable Approximation to Optimal Point Process Filtering: Application to Neural Encoding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1603–1611},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969417,
author = {Hazan, Elad and Levy, Kfir Y. and Shalev-Shwartz, Shai},
title = {Beyond Convexity: Stochastic Quasi-Convex Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD).The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1594–1602},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969416,
author = {Lim, Zhan Wei and Hsu, David and Lee, Wee Sun},
title = {Adaptive Stochastic Optimization: From Sets to Paths},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1585–1593},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969415,
author = {Shafieezadeh-Abadeh, Soroosh and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
title = {Distributionally Robust Logistic Regression},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper proposes a distributionally robust approach to logistic regression. We use the Wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples. If the radius of this ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence. We then formulate a distribution-ally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball. We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases. We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier. These bounds are given by the optimal values of two highly tractable linear programs. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1576–1584},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969414,
author = {Yi, Xinyang and Caramanis, Constantine},
title = {Regularized EM Algorithms: A Unified Framework and Statistical Guarantees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art high-dimensional prescriptions (e.g., \`{a} la [19]) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1567–1575},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969413,
author = {Mroueh, Youssef and Voinea, Stephen and Poggio, Tomaso},
title = {Learning with Group Invariant Features: A Kernel Perspective},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1558–1566},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969412,
author = {Yi, Xinyang and Wang, Zhaoran and Caramanis, Constantine and Liu, Han},
title = {Optimal Linear Estimation under Unknown Nonlinear Transform},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Linear regression studies the problem of estimating a model parameter β* ∈ ℝp, from n observations {(yi, xi)}ni=1 from linear model yi = 〈xi, β*〉 + ∊i. We consider a significant generalization in which the relationship between 〈xi, β*〉 and yi is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover β* in settings (i.e., classes of link function f) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between yi and 〈xi, β*〉. We also consider the high dimensional setting where β* is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p ≫ n. For a broad class of link functions between 〈xi, β*〉 and yi, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1549–1557},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969411,
author = {Pentina, Anastasia and Lampert, Christoph H.},
title = {Lifelong Learning with Non-i.i.d. Tasks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this work we aim at extending the theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead, we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time in a consistent way. In the first case we prove a PAC-Bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1540–1548},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969410,
author = {Chaturapruek, Sorathan and Duchi, John C. and R\'{e}, Christopher},
title = {Asynchronous Stochastic Convex Optimization: The Noise is in the Noise and SGD Don't Care},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures. Roughly, the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony. We also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods. In short, we show that for many stochastic approximation problems, as Freddie Mercury sings in Queen's Bohemian Rhapsody, "Nothing really matters."},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1531–1539},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969409,
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
title = {Risk-Sensitive and Robust Decision-Making: A CVaR Optimization Approach},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1522–1530},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969408,
author = {Rothenh\"{a}usler, Dominik and Heinze, Christina and Peters, Jonas and Meinshausen, Nicolai},
title = {BACKSHIFT: Learning Causal Cyclic Graphs from Unknown Shift Interventions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a simple method to learn linear causal cyclic models in the presence of latent variables. The method relies on equilibrium data of the model recorded under a specific kind of interventions ("shift interventions"). The location and strength of these interventions do not have to be known and can be estimated from the data. Our method, called BACKSHIFT, only uses second moments of the data and performs simple joint matrix diagonalization, applied to differences between covariance matrices. We give a sufficient and necessary condition for identifiability of the system, which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings, one of which can be pure observational data. We demonstrate the performance on some simulated data and applications in flow cytometry and financial time series.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1513–1521},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969407,
author = {Dauphin, Yann N. and Vries, Harm de and Bengio, Yoshua},
title = {Equilibrated Adaptive Learning Rates for Non-Convex Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1504–1512},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969406,
author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
title = {Decoupled Deep Neural Network for Semi-Supervised Semantic Segmentation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in PASCAL VOC dataset.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1495–1503},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969405,
author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
title = {Deep Generative Image Models Using a Laplacian Pyramid of Adversarial Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1486–1494},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969404,
author = {Koren, Tomer and Levy, Kfir Y.},
title = {Fast Rates for Exp-Concave Empirical Risk Minimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses—a general optimization framework that captures several important learning problems including linear and logistic regression, learning SVMs with the squared hinge-loss, portfolio selection and more. In this setting, we establish the first evidence that ERM is able to attain fast generalization rates, and show that the expected loss of the ERM solution in d dimensions converges to the optimal expected loss in a rate of d/n. This rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1477–1485},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969403,
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
title = {Policy Gradient for Coherent Risk Measures},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1468–1476},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969402,
author = {Hsu, Daniel and Kontorovich, Aryeh and Szepesv\'{a}ri, Csaba},
title = {Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic Markov chain at a prescribed confidence level. The interval is computed from a single finite-length sample path from the Markov chain, and does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time trelax, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a √n rate, where n is the length of the sample path. Upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy. The lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before seeing each state at least Ω(trelax) times on the average. Finally, future directions of research are identified.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1459–1467},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969401,
author = {Kveton, Branislav and Wen, Zheng and Ashkan, Azin and Szepesv\'{a}ri, Csaba},
title = {Combinatorial Cascading Bandits},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1450–1458},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969400,
author = {Giordano, Ryan and Broderick, Tamara and Jordan, Michael},
title = {Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, a well known major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables—both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1441–1449},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969399,
author = {Liu, Qiang and Fisher, John and Ihler, Alexander},
title = {Probabilistic Variational Bounds for Graphical Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an "any-time" fashion, expending more computation for tighter bounds. On the other hand, Monte Carlo estimators such as importance sampling have excellent any-time behavior, but depend critically on the proposal distribution. We propose a simple Monte Carlo based inference method that augments convex variational bounds by adding importance sampling (IS). We argue that convex variational methods naturally provide good IS proposals that "cover" the target probability, and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our IS estimator. This both provides an accurate estimator and enables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art variational bounds, and provide certificates of accuracy given enough samples relative to the error in the initial bound.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1432–1440},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969398,
author = {Banerjee, Siddhartha and Lofgren, Peter},
title = {Fast Bidirectional Probability Estimation in Markov Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a new bidirectional algorithm for estimating Markov chain multi-step transition probabilities: given a Markov chain, we want to estimate the probability of hitting a given target state in ℓ steps after starting from a given source distribution. Given the target state t, we use a (reverse) local power iteration to construct an 'expanded target distribution', which has the same mean as the quantity we want to estimate, but a smaller variance - this can then be sampled efficiently by a Monte Carlo algorithm. Our method extends to any Markov chain on a discrete (finite or countable) state-space, and can be extended to compute functions of multi-step transition probabilities such as PageRank, graph diffusions, hitting/return times, etc. Our main result is that in 'sparse' Markov Chains - wherein the number of transitions between states is comparable to the number of states -the running time of our algorithm for a uniform-random target node is order-wise smaller than Monte Carlo and power iteration based algorithms; in particular, our method can estimate a probability p using only O(1/√p) running time.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1423–1431},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969397,
author = {Dezfouli, Amir and Bonilla, Edwin V.},
title = {Scalable Inference for Gaussian Process Models with Black-Box Likelihoods},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a sparse method for scalable automated variational inference (AVI) in a large class of models with Gaussian process (GP) priors, multiple latent functions, multiple outputs and non-linear likelihoods. Our approach maintains the statistical efficiency property of the original AVI method, requiring only expectations over univariate Gaussian distributions to approximate the posterior with a mixture of Gaussians. Experiments on small datasets for various problems including regression, classification, Log Gaussian Cox processes, and warped GPs show that our method can perform as well as the full method under high sparsity levels. On larger experiments using the MNIST and the SARCOS datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1414–1422},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969396,
author = {Yoshikawa, Yuya and Iwata, Tomoharu and Sawada, Hiroshi and Yamada, Takeshi},
title = {Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1405–1413},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969395,
author = {Musco, Cameron and Musco, Christopher},
title = {Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Since being analyzed by Rokhlin, Szlam, and Tygert [1] and popularized by Halko, Martinsson, and Tropp [2], randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for any matrix, independently of singular value gaps. After \~{O}(1/∊) iterations, it gives a low-rank approximation within (1 + ∊) of optimal for spectral norm error.We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just \~{O}(1/√∊) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice. Furthermore, while it is a simple accuracy benchmark, even (1 + ∊) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1396–1404},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969394,
author = {Fan, Kai and Wang, Ziteng and Beck, Jeffrey and Kwok, James T. and Heller, Katherine},
title = {Fast Second-Order Stochastic Backpropagation for Variational Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1387–1395},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969393,
author = {Sun, Qing and Batra, Dhruv},
title = {SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image. Since the number of possible bounding boxes in an image is very large O(#pixels2), even a single linear scan to perform the greedy augmentation for submodular maximization is intractable. Thus, we formulate the greedy augmentation step as a Branch-and-Bound scheme. In order to speed up repeated application of B&amp;B, we propose a novel generalization of Minoux's 'lazy greedy' algorithm to the B&amp;B tree. Theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as Sliding Window+Non-Maximal Suppression (NMS) and and Efficient Subwindow Search (ESS) as special cases. Empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1378–1386},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969392,
author = {Borgs, Christian and Chayes, Jennifer T. and Smith, Adam},
title = {Private Graphon Estimation for Sparse Graphs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We design algorithms for fitting a high-dimensional statistical model to a large, sparse network without revealing sensitive information of individual members. Given a sparse input graph G, our algorithms output a node-differentially private nonparametric block model approximation. By node-differentially private, we mean that our output hides the insertion or removal of a vertex and all its adjacent edges. If G is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon W, our model guarantees consistency: as the number of vertices tends to infinity, the output of our algorithm converges to W in an appropriate version of the L2 norm. In particular, this means we can estimate the sizes of all multi-way cuts in G.Our results hold as long as W is bounded, the average degree of G grows at least like the log of the number of vertices, and the number of blocks goes to infinity at an appropriate rate. We give explicit error bounds in terms of the parameters of the model; in several settings, our bounds improve on or match known nonprivate results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1369–1377},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969391,
author = {Wu, Yifan and Gy\"{o}rgy, Andr\'{a}s and Szepesv\'{a}ri, Csaba},
title = {Online Learning with Gaussian Payoffs and Side Observations},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i, j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1360–1368},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969390,
author = {Balsubramani, Akshay and Freund, Yoav},
title = {Scalable Semi-Supervised Aggregation of Classifiers},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1351–1359},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969389,
author = {Bareinboim, Elias and Forney, Andrew and Pearl, Judea},
title = {Bandits with Unobserved Confounders: A Causal Approach},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1342–1350},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969388,
author = {Huang, Jiaji and Qiu, Qiang and Sapiro, Guillermo and Calderbank, Robert},
title = {Discriminative Robust Transformation Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, ∊)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild, demonstrate the value of being able to balance discrimination and robustness.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1333–1341},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969387,
author = {Lee, Jason D. and Sun, Yuekai and Taylor, Jonathan},
title = {Evaluating the Statistical Significance of Biclusters},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Biclustering (also known as submatrix localization) is a problem of high practical relevance in exploratory analysis of high-dimensional data. We develop a framework for performing statistical inference on biclusters found by score-based algorithms. Since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm, this is a form of selective inference. Our framework gives exact (non-asymptotic) confidence intervals and p-values for the significance of the selected biclusters.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1324–1332},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969386,
author = {Kopp, Tim and Singla, Parag and Kautz, Henry},
title = {Lifted Symmetry Detection and Breaking for MAP Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability. In this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes MAP inference in Markov Logic and similar statistical-relational languages. We introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory. We provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time. We show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain. We demonstrate the effectiveness of these techniques through experiments in two relational domains. We also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1315–1323},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969385,
author = {Sun, Ruoyu and Hong, Mingyi},
title = {Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The iteration complexity of the block-coordinate descent (BCD) type algorithm has been under extensive investigation. It was recently shown that for convex problems the classical cyclic BCGD (block coordinate gradient descent) achieves an O(1/r) complexity (r is the number of passes of all blocks). However, such bounds are at least linearly depend on K (the number of variable blocks), and are at least K times worse than those of the gradient descent (GD) and proximal gradient (PG) methods. In this paper, we close such theoretical performance gap between cyclic BCD and GD/PG. First we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic Block Coordinate Proximal Gradient (BCPG), a popular variant of BCD, can match those of the GD/PG in terms of dependency on K (up to a log2 (K) factor). Second, we establish an improved complexity bound for Coordinate Gradient Descent (CGD) for general convex problems which can match that of GD in certain scenarios. Our bounds are sharper than the known bounds as they are always at least K times worse than GD. Our analyses do not depend on the update order of block variables inside each cycle, thus our results also apply to BCD methods with random permutation (random sampling without replacement, another popular variant).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1306–1314},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969384,
author = {Kawale, Jaya and Bui, Hung and Kveton, Branislav and Thanh, Long Tran and Chawla, Sanjay},
title = {Efficient Thompson Sampling for Online Matrix-Factorization Recommendation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Matrix factorization (MF) collaborative filtering is an effective and widely used method in recommendation systems. However, the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative filtering from cold-start, has not been previously addressed. In this paper, we present a novel algorithm for online MF recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efficient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle filter. Extensive experiments in collaborative filtering using several real-world datasets demonstrate that PTS significantly outperforms the current state-of-the-arts.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1297–1305},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969383,
author = {Ahn, Sungsoo and Park, Sejun and Chertkov, Michael and Shin, Jinwoo},
title = {Minimum Weight Perfect Matching via Blossom Belief Propagation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Max-product Belief Propagation (BP) is a popular message-passing algorithm for computing a Maximum-A-Posteriori (MAP) assignment over a distribution represented by a Graphical Model (GM). It has been shown that BP can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective Linear Programming (LP) relaxation is tight, i.e., no integrality gap is present. However, when LP shows an integrality gap, no model has been known which can be solved systematically via sequential applications of BP. In this paper, we develop the first such algorithm, coined Blossom-BP, for solving the minimum weight matching problem over arbitrary graphs. Each step of the sequential algorithm requires applying BP over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices. Our scheme guarantees termination in O(n2) of BP runs, where n is the number of vertices in the original graph. In essence, the Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom algorithm by jumping at once over many sub-steps with a single BP. Moreover, our result provides an interpretation of the Edmonds' algorithm as a sequence of LPs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1288–1296},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969382,
author = {Wang, Jie and Ye, Jieping},
title = {Multi-Layer Feature Reduction for Tree Structured Group Lasso via Hierarchical Projection},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features. It has been applied successfully in many real-world applications. However, with extremely large feature dimensions, solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper, we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover, we can integrate MLFre—that has a low computational cost—with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1279–1287},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969381,
author = {Quanrud, Kent and Khashabi, Daniel},
title = {Online Learning with Adversarial Delays},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that online-gradient-descent [1] and follow-the-perturbed-leader [2] achieve regret O(√D) in the delayed setting, where D is the sum of delays of each round's feedback. This bound collapses to an optimal O(√T) bound in the usual setting of no delays (where D = T). Our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves. Our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1270–1278},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969380,
author = {Saade, Alaa and Krzakala, Florent and Zdeborov\'{a}, Lenka},
title = {Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank r reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries. We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank r of a large n X m matrix from C(r)r √nm entries, where C(r) is a constant close to 1. We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1261–1269},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969379,
author = {Reed, Scott and Zhang, Yi and Zhang, Yuting and Lee, Honglak},
title = {Deep Visual Analogy-Making},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in predicting image labels, annotations and captions, but have only just begun to be used for generating high-quality images. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1252–1260},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969378,
author = {Bubeck, Sebastien and Eldan, Ronen and Lehec, Joseph},
title = {Finite-Time Analysis of Projected Langevin Monte Carlo},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze the projected Langevin Monte Carlo (LMC) algorithm, a close cousin of projected Stochastic Gradient Descent (SGD). We show that LMC allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood. This gives the first Markov chain to sample from a log-concave distribution with a first-order oracle, as the existing chains with provable guarantees (lattice walk, ball walk and hit-and-run) require a zeroth-order oracle. Our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand SGD and its variants.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1243–1251},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969377,
author = {Goroshin, Ross and Mathieu, Michael and LeCun, Yann},
title = {Learning to Linearize under Uncertainty},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1234–1242},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969376,
author = {Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
title = {Practical and Optimal LSH for Angular Distance},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1225–1233},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969375,
author = {Erdogdu, Murat A.},
title = {Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (n ≫ p ≫ 1). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1216–1224},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969374,
author = {Rabinovich, Maxim and Angelino, Elaine and Jordan, Michael I.},
title = {Variational Consensus Monte Carlo},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel [22]. A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC—achieving near-ideal speedup in some instances.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1207–1215},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969373,
author = {Hughes, Michael C. and Stephenson, William and Sudderth, Erik B.},
title = {Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1198–1206},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969372,
author = {Jawanpuria, Pratik and Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
title = {Efficient Output Kernel Learning for Multiple Tasks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1189–1197},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969371,
author = {Gunasekar, Suriya and Banerjee, Arindam and Ghosh, Joydeep},
title = {Unified View of Matrix Completion under General Structural Constraints},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably the recently proposed spectral k-support norm.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1180–1188},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969370,
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1171–1179},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969369,
author = {Briol, Fran\c{c}ois-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A.},
title = {Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {There is renewed interest in formulating integration as a statistical inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging Bayesian model choice problem in cellular biology.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1162–1170},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969368,
author = {McInerney, James and Ranganath, Rajesh and Blei, David},
title = {The Population Posterior and Bayesian Modeling on Streams},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which require conditioning on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We develop the population posterior for latent Dirichlet allocation and Dirichlet process mixtures. We study our method with several large-scale data sets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1153–1161},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969367,
author = {Sriperumbudur, Bharath K. and Szab\'{o}, Zolt\'{a}n},
title = {Optimal Rates for Random Fourier Features},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in Lr (1 ≤ r &lt; ∞) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1144–1152},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969366,
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
title = {Learning Both Weights and Connections for Efficient Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1135–1143},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969365,
author = {Jain, Prateek and Tewari, Ambuj},
title = {Alternating Minimization for Regression Problems with Vector-Valued Outputs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1126–1134},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969364,
author = {Esser, Steve K. and Appuswamy, Rathinakumar and Merolla, Paul A. and Arthur, John V. and Modha, Dharmendra S.},
title = {Backpropagation for Energy-Efficient Neuromorphic Computing},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets. For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses. Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of 64), we achieve 99.42% accuracy at 108 μJ per image, and with a high efficiency network (ensemble of 1) we achieve 92.7% accuracy at 0.268 μJ per image.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1117–1125},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969363,
author = {Vincent, Pascal and Br\'{e}bisson, Alexandre de and Bouthillier, Xavier},
title = {Efficient Exact Gradient Update for Training Deep Networks with Very Large Sparse Targets},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d, i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1108–1116},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969362,
author = {Yang, Jimei and Reed, Scott and Yang, Ming-Hsuan and Lee, Honglak},
title = {Weakly-Supervised Disentangling with Recurrent Transformations for 3D View Synthesis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1099–1107},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969361,
author = {Chaudhuri, Kamalika and Kakade, Sham M. and Netrapalli, Praneeth and Sanghavi, Sujay},
title = {Convergence Rates of Active Learning for Maximum Likelihood Estimation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well.Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting - maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields.We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in [12] and [13] (on active linear and logistic regression) shows the promise of this approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1090–1098},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969360,
author = {Sun, Wei and Wang, Zhaoran and Liu, Han and Cheng, Guang},
title = {Non-Convex Statistical Optimization for Sparse Tensor Graphical Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1081–1089},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969359,
author = {Jun, Kwang-Sung and Zhu, Xiaojin and Rogers, Timothy and Yang, Zhuoran and Yuan, Ming},
title = {Human Memory Search as Initial-Visit Emitting Random Walk},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Imagine a random walk that outputs a state only when visiting it for the first time. The observed output is therefore a repeat-censored version of the underlying walk, and consists of a permutation of the states or a prefix of it. We call this model initial-visit emitting random walk (INVITE). Prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks, which is of great interest in both the study of human cognition and various clinical applications. However, parameter estimation in INVITE is challenging, because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable. In this paper, we propose the first efficient maximum likelihood estimate (MLE) for INVITE by decomposing the censored output into a series of absorbing random walks. We also prove theoretical properties of the MLE including identifiability and consistency. We show that INVITE outperforms several existing methods on real-world human response data from memory search tasks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1072–1080},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969358,
author = {Malkomes, Gustavo and Kusner, Matt J. and Chen, Wenlin and Weinberger, Kilian Q. and Moseley, Benjamin},
title = {Fast Distributed <i>k</i>-Center Clustering with Outliers on Massive Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Clustering large data is a fundamental problem with a vast number of applications. Due to the increasing size of data, practitioners interested in clustering have turned to distributed computation methods. In this work, we consider the widely used k-center clustering problem and its variant used to handle noisy data, k-center with outliers. In the noise-free setting we demonstrate how a previously-proposed distributed method is actually an O(1)-approximation algorithm, which accurately explains its strong empirical performance. Additionally, in the noisy setting, we develop a novel distributed algorithm that is also an O(1)-approximation. These algorithms are highly parallel and lend themselves to virtually any distributed computing framework. We compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions. The algorithms are all one can hope for in distributed settings: they are fast, memory efficient and they match their sequential counterparts.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1063–1071},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969357,
author = {Li, Tianyang and Prasad, Adarsh and Ravikumar, Pradeep},
title = {Fast Classification Rates for High-Dimensional Gaussian Generative Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate Gaussian distributions. We focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier, derived via the Bayes rule, also called Linear Discriminant Analysis, has been shown to behave poorly in high-dimensional settings. We present a novel analysis of the classification error of any linear discriminant approach given conditional Gaussian models. This allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier. As we show, under a natural sparsity assumption, and letting s denote the sparsity of the Bayes classifier, p the number of covariates, and n the number of samples, the simple (ℓ1-regularized) logistic regression classifier achieves the fast misclassification error rates of O (s log p/n), which is much better than the other approaches, which are either inconsistent under high-dimensional settings, or achieve a slower rate of O (√s log p/n).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1054–1062},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969356,
author = {Bitzer, Sebastian and Kiebel, Stefan J.},
title = {The Brain Uses Reliability of Stimulus Information When Making Perceptual Decisions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made. However, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability. We here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability. We then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings. Our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1045–1053},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969355,
author = {Cao, Wei and Li, Jian and Tao, Yufei and Li, Zhize},
title = {On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric, up to a small relative error. We study the topic under two standard settings—multi-armed bandits and hidden bipartite graphs—which differ in the nature of the input distributions. In the former setting, each distribution can be sampled (in the i.i.d. manner) an arbitrary number of times, whereas in the latter, each distribution is defined on a population of a finite size m (and hence, is fully revealed after m samples). For both settings, we prove lower bounds on the total number of samples needed, and propose optimal algorithms whose sample complexities match those lower bounds.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1036–1044},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969354,
author = {Papa, Guillaume and Cl\'{e}men\c{c}on, St\'{e}phan and Bellet, Aur\'{e}lien},
title = {SGD Algorithms Based on Incomplete <i>U</i>-Statistics: Large-Scale Minimization of Empirical Risk},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many learning problems, ranging from clustering to ranking through metric learning, empirical estimates of the risk functional consist of an average over tuples (e.g., pairs or triplets) of observations, rather than over individual observations. In this paper, we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems. We argue that in the large-scale setting, gradient estimates should be obtained by sampling tuples of data points with replacement (incomplete U-statistics) instead of sampling data points without replacement (complete U-statistics based on subsamples). We develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the Stochastic Gradient Descent (SGD) algorithm. It reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost. Beyond the rate bound analysis, experiments on AUC maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1027–1035},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969353,
author = {Johansson, Fredrik D. and Chattoraj, Ankani and Bhattacharyya, Chiranjib and Dubhashi, Devdatt},
title = {Weighted Theta Functions and Embeddings with Applications to Max-Cut, Clustering and Summarization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a unifying generalization of the Lovasz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming, and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for Max-Cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1018–1026},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969352,
author = {Jain, Prateek and Natarajan, Nagarajan and Tewari, Ambuj},
title = {Predtron: A Family of Online Algorithms for General Prediction Problems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds. The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification. For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1009–1017},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969351,
author = {Wang, Yining and Wang, Yu-Xiang and Singh, Aarti},
title = {Differentially Private Subspace Clustering},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple "clusters" so that data points in a single cluster lie approximately on a low-dimensional linear subspace. It is originally motivated by 3D motion segmentation in computer vision, but has recently been generically applied to a wide range of statistical machine learning problems, which often involves sensitive datasets about human subjects. This raises a dire concern for data privacy. In this work, we build on the framework of differential privacy and present two provably private subspace clustering algorithms. We demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice. Along the course of the proof, we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1000–1008},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969350,
author = {Wang, Yining and Tung, Hsiao-Yu and Smola, Alex and Anandkumar, Anima},
title = {Fast and Guaranteed Tensor Decomposition via Sketching},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {991–999},
numpages = {9},
keywords = {count sketch, tensor CP decomposition, randomized methods, topic modeling, spectral methods},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969349,
author = {Bourdoukan, Ralph and Deneve, Sophie},
title = {Enforcing Balance Allows Local Supervised Learning in Spiking Recurrent Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {982–990},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969348,
author = {Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {Unsupervised Learning by Program Synthesis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our techniques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures, and a technique for applying program synthesis tools to noisy data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {973–981},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969347,
author = {Lattimore, Tor and Crammer, Koby and Szepesv\'{a}ri, Csaba},
title = {Linear Multi-Resource Allocation with Semi-Bandit Feedback},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study an idealised sequential resource allocation problem. In each time step the learner chooses an allocation of several resource types between a number of tasks. Assigning more resources to a task increases the probability that it is completed. The problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy. Our main contribution is the new setting and an algorithm with nearly-optimal regret analysis. Along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise. We also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work, especially in the sparse case.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {964–972},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969346,
author = {Strathmann, Heiko and Sejdinovic, Dino and Livingstone, Samuel and Szabo, Zoltan and Gretton, Arthur},
title = {Gradient-Free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {955–963},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969345,
author = {Smith, David and Gogate, Vibhav},
title = {Bounding the Cost of Search-Based Lifted Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models (SRMs). These lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation. One drawback of these algorithms is that they use an inference-blind representation of the search space, which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion. In this paper, we present a principled approach to address this problem. We introduce a lifted analogue of the propositional And/Or search space framework, which we call a lifted And/Or schematic. Given a schematic-based representation of an SRM, we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic. We show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective Rao-Blackwellisation, and demonstrate experimentally that the Rao-Blackwellised version of the algorithm yields more accurate estimates on several real-world datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {946–954},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969344,
author = {Liang, Ming and Hu, Xiaolin and Zhang, Bo},
title = {Convolutional Neural Networks with Intra-Layer Recurrent Connections for Scene Labeling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Scene labeling is a challenging computer vision task. It requires the use of both local discriminative features and global context information. We adopt a deep recurrent convolutional neural network (RCNN) for this task, which is originally proposed for object recognition. Different from traditional convolutional neural networks (CNN), this model has intra-layer recurrent connections in the convolutional layers. Therefore each convolutional layer becomes a two-dimensional recurrent neural network. The units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods. While recurrent iterations proceed, the region of context captured by each unit expands. In this way, feature extraction and context modulation are seamlessly integrated, which is different from typical methods that entail separate modules for the two steps. To further utilize the context, a multi-scale RCNN is proposed. Over two benchmark datasets, Standford Background and Sift Flow, the model outperforms many state-of-the-art models in accuracy and efficiency.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {937–945},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969343,
author = {Kishimoto, Akihiro and Marinescu, Radu and Botea, Adi},
title = {Parallel Recursive Best-First AND/OR Search for Exact MAP Inference in Graphical Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The paper presents and evaluates the power of parallel search for exact MAP inference in graphical models. We introduce a new parallel shared-memory recursive best-first AND/OR search algorithm, called SPRBFAOO, that explores the search space in a best-first manner while operating with restricted memory. Our experiments show that SPRBFAOO is often superior to the current state-of-the-art sequential AND/OR search approaches, leading to considerable speed-ups (up to 7-fold with 12 threads), especially on hard problem instances.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {928–936},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969342,
author = {Johnson, Rie and Zhang, Tong},
title = {Semi-Supervised Convolutional Neural Networks for Text Categorization via Region Embedding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {919–927},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969341,
author = {Hosseini, Reshad and Sra, Suvrit},
title = {Matrix Manifold Optimization for Gaussian Mixtures},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We take a new look at parameter estimation for Gaussian Mixture Model (GMMs). Specifically, we advance Riemannian manifold optimization (on the manifold of positive definite matrices) as a potential replacement for Expectation Maximization (EM), which has been the de facto standard for decades. An out-of-the-box invocation of Riemannian optimization, however, fails spectacularly: it obtains the same solution as EM, but vastly slower. Building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences: it makes Riemannian optimization not only match EM (a nontrivial result on its own, given the poor record nonlinear programming has had against EM), but also outperforms it in many settings. To bring our ideas to fruition, we develop a well-tuned Riemannian LBFGS method that proves superior to known competing methods (e.g., Riemannian conjugate gradient). We hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {910–918},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969340,
author = {Ren, Jimmy SJ. and Xu, Li and Yan, Qiong and Sun, Wenxiu},
title = {Shepard Convolutional Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep learning has recently been introduced to the field of low-level computer vision and image processing. Promising results have been obtained in a number of tasks including super-resolution, inpainting, deconvolution, filtering, etc. However, previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators. We found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation (TVI). In this paper, we draw on Shepard interpolation and design Shepard Convolutional Neural Networks (ShCNN) which efficiently realizes end-to-end trainable TVI operators in the network. We show that by adding only a few feature maps in the new Shepard layers, the network is able to achieve stronger results than a much deeper architecture. Superior performance on both image in-painting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {901–909},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969339,
author = {Vovk, Vladimir and Petej, Ivan and Fedorova, Valentina},
title = {Large-Scale Probabilistic Predictors with and without Guarantees of Validity},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {892–900},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969338,
author = {Cho, Minhyung and Dhir, Chandra Shekhar and Lee, Jaehyung},
title = {Hessian-Free Optimization for Learning Deep Multidimensional Recurrent Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {883–891},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969337,
author = {Domke, Justin},
title = {Maximum Likelihood Learning with Arbitrary Treewidth via Fast-Mixing Parameter Sets},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of "fast-mixing parameters" where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution ∊-accurate in log-likelihood requires a total amount of effort cubic in 1/∊, disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution ∊-accurate in parameter distance with effort quadratic in 1/∊. Both of these provide of a fully-polynomial time randomized approximation scheme.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {874–882},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969336,
author = {Qu, Zheng and Richt\'{a}rik, Peter and Zhang, Tong},
title = {Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial and mini-batch variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {865–873},
numpages = {9},
keywords = {empirical risk minimization, arbitrary sampling, dual coordinate ascent, data-driven speedup},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969335,
author = {Berglund, Mathias and Raiko, Tapani and Honkala, Mikko and K\"{a}rkk\"{a}inen, Leo and Vetek, Akos and Karhunen, Juha},
title = {Bidirectional Recurrent Neural Networks as Generative Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult. Recently, two different frameworks, GSN and NADE, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible. As far as we know, neither GSN or NADE have been studied in the context of time series before. As an example of an un-supervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {856–864},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969334,
author = {Soma, Tasuku and Yoshida, Yuichi},
title = {A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice. We are motivated by real scenarios in machine learning that cannot be captured by (traditional) sub-modular set functions. We show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm. Our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy. The running time of our algorithm is roughly O(n log(nr) log r), where n is the size of the ground set and r is the maximum value of a coordinate. The dependency on r is exponentially better than the naive reduction algorithms. Several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms, while the running time is several orders of magnitude faster.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {847–855},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969333,
author = {Flach, Peter A. and Kull, Meelis},
title = {Precision-Recall-Gain Curves: PR Analysis Done Right},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracy-based performance assessment, many researchers have taken to report Precision-Recall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions - e.g., the area under a PR curve takes the arithmetic mean of precision values whereas the Fβ score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system, and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular, the area under Precision-Recall-Gain curves conveys an expected F1 score on a harmonic scale, and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of β values for which the point optimises Fβ. We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected F1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {838–846},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969332,
author = {Lloyd, James Robert and Ghahramani, Zoubin},
title = {Statistical Model Criticism Using Kernel Two Sample Tests},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {829–837},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969331,
author = {Takenouchi, Takashi and Kanamori, Takafumi},
title = {Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the information geometry. Some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {820–828},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969330,
author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
title = {GAP Safe Screening Rules for Sparse Multi-Task and Multi-Class Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with ℓ1 and ℓ1/ ℓ2 norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {811–819},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969329,
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
title = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {802–810},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969328,
author = {Wu, Anqi and Park, Il Memming and Pillow, Jonathan W.},
title = {Convolutional Spike-Triggered Covariance Analysis for Neural Subunit Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli. They are defined by a cascade of two linear-nonlinear (LN) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an output nonlinearity. Recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. However, fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima. Here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models. Specifically, we show that a "convolutional" decomposition of a spike-triggered average (STA) and covariance (STC) matrix provides an asymptotically efficient estimator for class of quadratic subunit models. We establish theoretical conditions for identifiability of the subunit and pooling weights, and show that our estimator performs well even in cases of model mismatch. Finally, we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model (GQM), and achieves nearly the same prediction performance as the full maximum-likelihood estimator, yet at substantially lower cost.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {793–801},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969327,
author = {Anava, Oren and Hazan, Elad and Mannor, Shie},
title = {Online Learning for Adversaries with Memory: Price of Past Mistakes},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretical results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969326,
author = {Alaoui, Ahmed El and Mahoney, Michael W.},
title = {Fast Randomized Kernel Ridge Regression with Statistical Guarantees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of statistical leverage scores to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the effective dimensionality of the problem. This latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom. We give an empirical evidence supporting this fact. Our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples. More precisely, the running time of the algorithm is O(np2) with p only depending on the trace of the kernel matrix and the regularization parameter. This is obtained via a variant of squared length sampling that we adapt to the kernel setting. Lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {775–783},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969325,
author = {Asteris, Megasthenis and Papailiopoulos, Dimitris and Kyrillidis, Anastasios and Dimakis, Alexandros G.},
title = {Sparse PCA via Bipartite Matchings},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. Such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the input data. We evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {766–774},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969324,
author = {Qu, Chao and Xu, Huan},
title = {Subspace Clustering with Irrelevant Features via Robust Dantzig Selector},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed "robust Dantzig selector" which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {757–765},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969323,
author = {Schulam, Peter and Saria, Suchi},
title = {A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {For many complex diseases, there is a wide variety of ways in which an individual can manifest the disease. The challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual's disease, which can in turn enable clinicians to optimize treatments. We represent an individual's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time. We propose a hierarchical latent variable model that individualizes predictions of disease trajectories. This model shares statistical strength across observations at different resolutions-the population, subpopulation and the individual level. We describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters. Finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma. We compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {748–756},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969322,
author = {Chen, Yuxin and Cand\'{e}s, Emmanuel J.},
title = {Solving Random Quadratic Systems of Equations is Nearly as Easy as Solving Linear Systems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper is concerned with finding a solution x to a quadratic system of equations yi = |〈ai, x〉|2, i = 1,..., m. We demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from O(n) equations in linear time, that is, in time proportional to reading the data {ai} and {yi}. This is accomplished by a novel procedure, which starting from an initial guess given by a spectral initialization procedure, attempts to minimize a nonconvex objective. The proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion, which discard terms bearing too much influence on the initial estimate or search directions. These careful selection rules—which effectively serve as a variance reduction scheme—provide a tighter initial guess, more robust descent directions, and thus enhanced practical performance. Further, this procedure also achieves a near-optimal statistical accuracy in the presence of noise. Empirically, we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {739–747},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969321,
author = {Bhatia, Kush and Jain, Himanshu and Kar, Purushottam and Varma, Manik and Jain, Prateek},
title = {Sparse Local Embeddings for Extreme Multi-Label Classification},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies, or scale to large problems as the low rank assumption is violated in most real world applications.In this paper we develop the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors.We conducted extensive experiments on several real-world, as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based (by as much as 35%) as well as tree-based (by as much as 6%) methods. SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {730–738},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969320,
author = {Bhatia, Kush and Jain, Prateek and Kar, Purushottam},
title = {Robust Regression via Hard Thresholding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X ∈ ℝp x n and an underlying model w*, the response vector is generated as y = XT w* + b where b ∈ ℝn is the corruption vector supported over at most C · n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X.In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* ∈ ℝp.Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery. and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20 x faster than the best L1 solver.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {721–729},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969319,
author = {Liu, Weiwei and Tsang, Ivor W.},
title = {On the Optimality of Classifier Chain for Multi-Label Classification},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To capture the interdependencies between labels in multi-label classification problems, classifier chain (CC) tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for CC. In this work, we first generalize the CC model over a random label order. Then, we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CC-Greedy algorithm achieves comparable prediction performance with CC-DP.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {712–720},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969318,
author = {Zhang, Chicheng and Chaudhuri, Kamalika},
title = {Active Learning from Weak and Strong Labelers},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {703–711},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969317,
author = {Ohsaka, Naoto and Yoshida, Yuichi},
title = {Monotone <i>k</i>-Submodular Function Maximization with Size Constraints},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain. Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions. In this paper, we give constant-factor approximation algorithms for maximizing monotone k-submodular functions subject to several size constraints. The running time of our algorithms are almost linear in the domain size. We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {694–702},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969316,
author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
title = {Deep Learning with Elastic Averaging SGD},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {685–693},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969315,
author = {Abbe, Emmanuel and Sandon, Colin},
title = {Recovering Communities in the General Stochastic Block Model without Knowing the Parameters},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The stochastic block model (SBM) has recently gathered significant attention due to new threshold phenomena. However, most developments rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in Abbe-Sandon FOCS15. In the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees. This lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees, and the model parameters are learned efficiently. For the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that achieves the CH-limit for exact recovery in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the SBM.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {676–684},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969314,
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R\'{e}mi},
title = {Black-Box Optimization of Noisy Functions with Unknown Smoothness},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of √ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {667–675},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969313,
author = {Adeli-Mosabbeb, Ehsan and Thung, Kim-Han and An, Le and Shi, Feng and Shen, Dinggang},
title = {Robust Feature-Sample Linear Discriminant Analysis for Brain Disorders Diagnosis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks. However, many existing discriminative methods assume that the input data is nearly noise-free, which limits their applications to solve real-world problems. Particularly for disease diagnosis, the data acquired by the neuroimaging devices are always prone to different sources of noise. Robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers. These methods focus on detecting either the sample-outliers or feature-noises. Moreover, they usually use unsupervised de-noising procedures, or separately de-noise the training and the testing data. All these factors may induce biases in the learning process, and thus limit its performance. In this paper, we propose a classification method based on the least-squares formulation of linear discriminant analysis, which simultaneously detects the sample-outliers and feature-noises. The proposed method operates under a semi-supervised setting, in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space. Therefore, the violating samples or feature values are identified as sample-outliers or feature-noises, respectively. We test our algorithm on one synthetic and two brain neurodegenerative databases (particularly for Parkinson's disease and Alzheimer's disease). The results demonstrate that our method outperforms all baseline and state-of-the-art methods, in terms of both accuracy and the area under the ROC curve.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {658–666},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969312,
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
title = {Character-Level Convolutional Networks for Text Classification},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {649–657},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969311,
author = {Gabri\'{e}, Marylou and Tramel, Eric W. and Krzakala, Florent},
title = {Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {640–648},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969310,
author = {Park, Gunwoong and Raskutti, Garvesh},
title = {Learning Large-Scale Poisson DAG Models Based on Overdispersion Scoring},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we address the question of identifiability and learning algorithms for large-scale Poisson Directed Acyclic Graphical (DAG) models. We define general Poisson DAG models as models where each node is a Poisson random variable with rate parameter depending on the values of the parents in the underlying DAG. First, we prove that Poisson DAG models are identifiable from observational data, and present a polynomial-time algorithm that learns the Poisson DAG model under suitable regularity conditions. The main idea behind our algorithm is based on overdispersion, in that variables that are conditionally Poisson are overdispersed relative to variables that are marginally Poisson. Our algorithms exploits overdispersion along with methods for learning sparse Poisson undirected graphical models for faster computation. We provide both theoretical guarantees and simulation results for both small and large-scale DAGs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {631–639},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969309,
author = {Korhonen, Janne H. and Parviainen, Pekka},
title = {Tractable Bayesian Network Structure Learning with Bounded Vertex Cover Number},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains NP-hard even for tree-width 2. In this paper, we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is W[1]-hard in parameter k. Furthermore, we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP), and show this is feasible in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {622–630},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969308,
author = {Kirillov, Alexander and Schlesinger, Dmitrij and Vetrov, Dmitry and Rother, Carsten and Savchynskyy, Bogdan},
title = {M-Best-Diverse Labelings for Submodular Energies and Beyond},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of finding M best diverse solutions of energy minimization problems for graphical models. Contrary to the sequential method of Batra et al., which greedily finds one solution after another, we infer all M solutions jointly. It was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones. The only obstacle for using this new technique is the complexity of the corresponding inference problem, since it is considerably slower algorithm than the method of Batra et al. In this work we show that the joint inference of M best diverse solutions can be formulated as a submodular energy minimization if the original MAP-inference problem is submodular, hence fast inference techniques can be used. In addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {613–621},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969307,
author = {Sz\"{o}r\'{e}nyi, Bai\"{y}zs and Busa-Fekete, R\'{o}bert and Paul, Adil and H\"{u}llermeier, Eyke},
title = {Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of online rank elicitation, assuming that rankings of a set of alternatives obey the Plackett-Luce distribution. Following the setting of the dueling bandits problem, the learner is allowed to query pairwise comparisons between alternatives, i.e., to sample pairwise marginals of the distribution in an online fashion. Using this information, the learner seeks to reliably predict the most probable ranking (or top-alternative). Our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure, for which the pairwise marginals provably coincide with the marginals of the Plackett-Luce distribution. In addition to a formal performance and complexity analysis, we present first experimental studies.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {604–612},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969306,
author = {Busa-Fekete, Robert and Sz\"{o}r\'{e}nyi, Bai\'{a}zs and Dembczy\'{n}ski, Krzysztof and H\"{u}llermeier, Eyke},
title = {Online F-Measure Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {595–603},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969305,
author = {Yang, Eunho and Lozano, Aur\'{e}lie C. and Ravikumar, Pradeep},
title = {Closed-Form Estimators for High-Dimensional Generalized Linear Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a class of closed-form estimators for GLMs under high-dimensional sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under high-dimensional settings, and (b) available in closed-form. We then perform thresholding operations on this MLE variant to obtain our class of estimators. We derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized GLM MLEs, even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators, as well as corollaries of our general theorem, for the special cases of logistic, exponential and Poisson regression models. We corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {586–594},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969304,
author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
title = {Attention-Based Models for Speech Recognition},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {577–585},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969303,
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
title = {Automatic Variational Inference in Stan},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {568–576},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969302,
author = {Zhao, Tuo and Wang, Zhaoran and Liu, Han},
title = {A Nonconvex Optimization Framework for Low Rank Matrix Estimation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {559–567},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969301,
author = {Ramasamy, Dinesh and Madhow, Upamanyu},
title = {Compressive Spectral Embedding: Sidestepping the SVD},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used "preprocessing" step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For an m x n matrix with T non-zeros, its time complexity is O ((T + m + n) log(m + n)), and the embedding dimension is O (log(m + n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the ℓ2-norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {550–558},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969300,
author = {Kuznetsov, Vitaly and Mohri, Mehryar},
title = {Learning Theory and Algorithms for Forecasting Non-Stationary Time Series},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {541–549},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969299,
author = {Krishnan, Rahul G. and Lacoste-Julien, Simon and Sontag, David},
title = {Barrier Frank-Wolfe for Marginal Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {532–540},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969298,
author = {Bahmani, Sohail and Romberg, Justin},
title = {Efficient Compressive Phase Retrieval with Constrained Sensing Vectors},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.In recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix. However, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.Deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace, then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is O(k log d/k), where k and d denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {523–531},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969297,
author = {Podosinnikova, Anastasia and Bach, Francis and Lacoste-Julien, Simon},
title = {Rethinking LDA: Moment Matching for Discrete ICA},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider moment matching techniques for estimation in latent Dirichlet allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {514–522},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969296,
author = {Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-Dickstein, Jascha},
title = {Deep Knowledge Tracing},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Knowledge tracing—where a machine models the knowledge of a student as they interact with coursework—is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {505–513},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969295,
author = {Lacoste-Julien, Simon and Jaggi, Martin},
title = {On the Global Linear Convergence of Frank-Wolfe Optimization Variants},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {496–504},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969294,
author = {Wang, Sida I. and Chaganty, Arun Tejasvi and Liang, Percy},
title = {Estimating Mixture Models via Mixtures of Polynomials},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. Simulations show good empirical performance on several models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {487–495},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969293,
author = {Qu, Xia and Doshi, Prashant},
title = {Individual Planning in Infinite-Horizon Multiagent Settings: Inference, Structure and Scalability},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization (EM). Our formalization in the context of infinite-horizon and finitely-nested interactive POMDPs (I-POMDP) is distinct from EM formulations for POMDPs and cooperative multiagent planning frameworks. We exploit the graphical model structure specific to I-POMDPs, and present a new approach based on block-coordinate descent for further speed up. Forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {478–486},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969292,
author = {Zhang, Chicheng and Song, Jimin and Chen, Kevin C and Chaudhuri, Kamalika},
title = {Spectral Learning of Large Structured HMMs for Comparative Epigenomics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure.The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {469–477},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969291,
author = {Chen, Po-Hsuan and Chen, Janice and Yeshurun, Yaara and Hasson, Uri and Haxby, James V. and Ramadge, Peter J.},
title = {A Reduced-Dimension FMRI Shared Response Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multi-subject fMRI data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity. We develop a shared response model for aggregating multi-subject fMRI data that accounts for different functional topographies among anatomically aligned datasets. Our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest. Furthermore, by removing the identified shared response, it allows improved detection of group differences. The ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fMRI studies.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {460–468},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969290,
author = {Wang, Xiangyu and Guo, Fangjian and Heller, Katherine A. and Dunson, David B.},
title = {Parallelizing MCMC with Random Partition Trees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to re-sample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {451–459},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969289,
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
title = {Tensorizing Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {442–450},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969288,
author = {Wu, Huasen and Srikant, R. and Liu, Xin and Jiang, Chong},
title = {Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {433–441},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969287,
author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
title = {3D Object Proposals for Accurate Object Class Detection},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {424–432},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969286,
author = {Gong, Pinghua and Ye, Jieping},
title = {HONOR: Hybrid Optimization for NOn-Convex Regularized Problems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {415–423},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969285,
author = {Paul, Saurabh and Magdon-Ismail, Malik and Drineas, Petros},
title = {Column Selection via Adaptive Sampling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm. Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms. Our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {406–414},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969284,
author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and P\'{o}czos, Barnab\"{y}s and Wasserman, Larry and Robins, James M.},
title = {Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are derived from the von Mises expansion and are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {397–405},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969283,
author = {Kundu, Abhisek and Drineas, Petros and Magdon-Ismail, Malik},
title = {Approximating Sparse PCA from Incomplete Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, O(∊-2k max{m, n}) elements gives an ∊-additive approximation to the sparse PCA problem (k is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {388–396},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969282,
author = {Li, Huan and Lin, Zhouchen},
title = {Accelerated Proximal Gradient Methods for Nonconvex Programming},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning. However, solving the nonconvex and nonsmooth optimization problems remains a big challenge. Accelerated proximal gradient (APG) is an excellent method for convex programming. However, it is still unknown whether the usual APG can ensure the convergence to a critical point in nonconvex programming. In this paper, we extend APG for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property. Accordingly, we propose a monotone APG and a nonmonotone APG. The latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration. To the best of our knowledge, we are the first to provide APG-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain O(1/k2) when the problems are convex, in which k is the number of iterations. Numerical results testify to the advantage of our algorithms in speed.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {379–387},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969281,
author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
title = {Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks. We propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters. This view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience. In simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances. Furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {370–378},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969280,
author = {Lin, Guosheng and Shen, Chunhua and Reid, Ian and Hengel, Anton van den},
title = {Deeply Learning the Messages in Message Passing Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that involve a large number of classes. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {361–369},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969279,
author = {Lin, Tian and Li, Jian and Chen, Wei},
title = {Stochastic Online Greedy Learning with Semi-Bandit Feedbacks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The greedy algorithm is extensively studied in the field of combinatorial optimization for decades. In this paper, we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time. We first propose the greedy regret and ∊-quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm. We then propose two online greedy learning algorithms with semi-bandit feedbacks, which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning, one for each of the regret metrics respectively. Both algorithms achieve O(log T) problem-dependent regret bound (T being the time horizon) for a general class of combinatorial structures and reward functions that allow greedy solutions. We further show that the bound is tight in T and other problem instance parameters.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {352–360},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969278,
author = {Asteris, Megasthenis and Papailiopoulos, Dimitris and Dimakis, Alexandros G.},
title = {Orthogonal NMF through Subspace Exploration},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Orthogonal Nonnegative Matrix Factorization (ONMF) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors, one of which has orthonormal columns. It yields potentially useful data representations as superposition of disjoint parts, while it has been shown to work well for clustering tasks where traditional methods underperform. Existing algorithms rely mostly on heuristics, which despite their good empirical performance, lack provable performance guarantees.We present a new ONMF algorithm with provable approximation guarantees. For any constant dimension k, we obtain an additive EPTAS without any assumptions on the input. Our algorithm relies on a novel approximation to the related Non-negative Principal Component Analysis (NNPCA) problem; given an arbitrary data matrix, NNPCA seeks k nonnegative components that jointly capture most of the variance. Our NNPCA algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component.We evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {343–351},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969277,
author = {Thomas, Philip S. and Niekum, Scott and Theocharous, Georgios and Konidaris, George},
title = {Policy Evaluation Using the Ω-Return},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose the Ω-return as an alternative to the λ-return currently used by the TD(λ) family of algorithms. The benefit of the Ω-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly, we suggest one way of approximating the Ω-return. We provide empirical studies that suggest that it is superior to the λ-return and γ-return for a variety of problems.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {334–342},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969276,
author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
title = {Top-k Multiclass SVM},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {325–333},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969275,
author = {Chen, Yen-Chi and Genovese, Christopher R. and Ho, Shirley and Wasserman, Larry},
title = {Optimal Ridge Detection Using Coverage Risk},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce the concept of coverage risk as an error measure for density ridge estimation. The coverage risk generalizes the mean integrated square error to set estimation. We propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk. We study the rate of convergence for coverage risk and prove consistency of the risk estimators. We apply our method to three simulated datasets and to cosmology data. In all the examples, the proposed method successfully recover the underlying density structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {316–324},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969274,
author = {Zoghi, Masrour and Karnin, Zohar and Whiteson, Shimon and Rijke, Maarten de},
title = {Copeland Dueling Bandits},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K2 log T) without requiring such assumptions. Our results offer the best of both worlds: O (K log T) bounds without restrictive assumptions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {307–315},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969273,
author = {Meshi, Ofer and Mahdavi, Mehrdad and Schwing, Alexander G.},
title = {Smooth and Strong: MAP Inference with Linear Convergence},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Maximum a-posteriori (MAP) inference is an important task for many applications. Although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years. We focus on linear programming (LP) relaxations, which have achieved state-of-the-art performance in many applications. However, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints.Therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity. Specifically, we introduce strong convexity by adding a quadratic term to the LP relaxation objective. We provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum. Further, we propose suitable optimization algorithms and analyze their convergence.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {298–306},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969272,
author = {Vondrick, Carl and Pirsiavash, Hamed and Oliva, Aude and Torralba, Antonio},
title = {Learning Visual Biases from Human Imagination},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {289–297},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969271,
author = {Campbell, Trevor and Straub, Julian and Fisher, John W. and How, Jonathan P.},
title = {Streaming, Distributed Variational Inference for Bayesian Nonparametrics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {280–288},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969270,
author = {Colin, Igor and Salmon, Joseph and Cl\'{e}men\c{c}on, St\'{e}phan and Bellet, Aur\'{e}lien},
title = {Extending Gossip Algorithms to Distributed Estimation of <i>U</i>-Statistics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {271–279},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969269,
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
title = {Texture Synthesis Using Convolutional Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {262–270},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969268,
author = {Carreira-Perpi\~{n}\'{a}n, Miguel \'{A}. and Vladymyrov, Max},
title = {A Fast, Universal Algorithm to Learn Parametric Nonlinear Embeddings},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {253–261},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969267,
author = {Dehaene, Guillaume and Barthelm\'{e}, Simon},
title = {Bounding Errors of Expectation-Propagation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of O(n-2) for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {244–252},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969266,
author = {Huang, Yan and Wang, Wei and Wang, Liang},
title = {Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Super resolving a low-resolution video is usually handled by either single-image super-resolution (SR) or multi-frame SR. Single-Image SR deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution. Multi-Frame SR generally extracts motion information, e.g., optical flow, to model the temporal dependency, which often shows high computational cost. Considering that recurrent neural networks (RNNs) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame SR. Different from vanilla RNNs, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling. With the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance. Due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {235–243},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969265,
author = {Gorham, Jackson and Mackey, Lester},
title = {Measuring Sample Quality with Stein's Method},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {226–234},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969264,
author = {Montanari, Andrea and Reichman, Daniel and Zeitouni, Ofer},
title = {On the Limitation of Spectral Methods: From the Gaussian Hidden Clique Problem to Rank-One Perturbations of Gaussian Tensors},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the following detection problem: given a realization of a symmetric matrix X of dimension n, distinguish between the hypothesis that all upper triangular variables are i.i.d. Gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix B of dimension L for which all upper triangular variables are i.i.d. Gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of X not in B are i.i.d. Gaussians variables with mean 0 and variance 1. We refer to this as the 'Gaussian hidden clique problem'. When L = (1 + ∊) √n (∊ &gt; 0), it is possible to solve this detection problem with probability 1 - on(1) by computing the spectrum of X and considering the largest eigenvalue of X. We prove that when L &lt; (1 - ∊) √n no algorithm that examines only the eigenvalues of X can detect the existence of a hidden Gaussian clique, with error probability vanishing as n → ∞. The result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional Gaussian tensors. In this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal cannot be detected.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {217–225},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969263,
author = {Lattimore, Tor},
title = {The Pareto Regret Frontier for Bandits},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Ω(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {208–216},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969262,
author = {Recasens, Adri\`{a} and Khosla, Aditya and Vondrick, Carl and Torralba, Antonio},
title = {Where Are They Looking?},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gaze-following is a challenging and important problem that deserves more attention from the community.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {199–207},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969261,
author = {Joulin, Armand and Mikolov, Tomas},
title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {190–198},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969260,
author = {Mahsereci, Maren and Hennig, Philipp},
title = {Probabilistic Line Searches for Stochastic Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {181–189},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969259,
author = {Maystre, Lucas and Grossglauser, Matthias},
title = {Fast and Accurate Inference of Plackett-Luce Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show that the maximum-likelihood (ML) estimate of models derived from Luce's choice axiom (e.g., the Plackett-Luce model) can be expressed as the stationary distribution of a Markov chain. This conveys insight into several recently proposed spectral inference algorithms. We take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the Plackett-Luce model. With a simple adaptation, this algorithm can be used iteratively, producing a sequence of estimates that converges to the ML estimate. The ML version runs faster than competing approaches on a benchmark of five datasets. Our algorithms are easy to implement, making them relevant for practitioners at large.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {172–180},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969258,
author = {Chakrabarti, Ayan},
title = {Color Constancy by Learning to Predict Chromaticity from Luminance},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes—without any spatial or semantic context—can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a "classifier" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier "end-to-end". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {163–171},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969257,
author = {Park, Mijung and Jitkrittum, Wittawat and Qamar, Ahmad and Szab\'{o}, Zolt\'{a}n and Buesing, Lars and Sahani, Maneesh},
title = {Bayesian Manifold Learning: The Locally Linear Latent Variable Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {154–162},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969256,
author = {Park, Mijung and Bohner, Gergo and Macke, Jakob H.},
title = {Unlocking Neural Population Non-Stationarity Using a Hierarchical Dynamics Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neural population activity often exhibits rich variability. This variability can arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as from modulations of neural firing properties on long time-scales, often referred to as neural non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics. We derive a Bayesian Laplace propagation algorithm for joint inference of parameters and population states. On neural population recordings from primary visual cortex, we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {145–153},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969255,
author = {Morgenstern, Jamie and Roughgarden, Tim},
title = {The Pseudo-Dimension of near-Optimal Auctions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper develops a general approach, rooted in statistical learning theory, to learning an approximately revenue-maximizing auction from data. We introduce t-level auctions to interpolate between simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby balancing the competing demands of expressivity and simplicity. We prove that such auctions have small representation error, in the sense that for every product distribution F over bidders' valuations, there exists a t-level auction with small t and expected revenue close to optimal. We show that the set of t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning error. One consequence of our results is that, in arbitrary single-parameter settings, one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {136–144},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969254,
author = {Wu, Jiajun and Yildirim, Ilker and Lim, Joseph J. and Freeman, William T. and Tenenbaum, Joshua B.},
title = {Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {127–135},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969253,
author = {He, Bryan and Yue, Yisong},
title = {Smooth Interactive Submodular Set Cover},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few (cost-weighted) actions as possible. It models settings where there is uncertainty regarding which submodular function to optimize. In this paper, we propose a new extension, which we call smooth interactive submodular set cover, that allows the target threshold to vary depending on the plausibility of each hypothesis. We present the first algorithm for this more general setting with theoretical guarantees on optimality. We further show how to extend our approach to deal with real-valued functions, which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {118–126},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969252,
author = {Zheng, Qinqing and Lafferty, John},
title = {A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With O(r3K2n log n) random measurements of a positive semidefinite n x n matrix of rank r and condition number K, our method is guaranteed to converge linearly to the global optimum.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {109–117},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969251,
author = {Sun, Ke and Wang, Jun and Kalousis, Alexandres and Marchand-Maillet, St\'{e}phane},
title = {Space-Time Local Embeddings},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Space-time is a profound concept in physics. This concept was shown to be useful for dimensionality reduction. We present basic definitions with interesting counter-intuitions. We give theoretical propositions to show that space-time is a more powerful representation than Euclidean space. We apply this concept to manifold learning for preserving local information. Empirical results on non-metric datasets show that more information can be preserved in space-time.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {100–108},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969250,
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {91–99},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969249,
author = {Pan, Xinghao and Papailiopoulos, Dimitris and Oymak, Samet and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I.},
title = {Parallel Correlation Clustering on Big Graphs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given a similarity graph between items, correlation clustering (CC) groups similar items together and dissimilar ones apart. One of the most popular CC algorithms is KwikCluster: an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio. Unfortunately, in practice KwikCluster requires a large number of clustering rounds, a potential bottleneck for large graphs. We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly linear speedups. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3 approximation ratio.We demonstrate experimentally that both algorithms outperform the state of the art, both in terms of clustering accuracy and running time. We show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15 x speedup.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {82–90},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969248,
author = {Park, Cesc Chunseong and Kim, Gunhee},
title = {Expressing an Image Stream with a Sequence of Natural Sentences},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherence recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {73–81},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969247,
author = {Yarkony, Julian and Fowlkes, Charless C.},
title = {Planar Ultrametrics for Image Segmentation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {64–72},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969246,
author = {Choromanska, Anna and Langford, John},
title = {Logarithmic Time Online Multiclass Prediction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {55–63},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969245,
author = {Qiu, Huitong and Han, Fang and Liu, Han and Caffo, Brian},
title = {Robust Portfolio Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {46–54},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969244,
author = {Shang, Xiaocheng and Zhu, Zhanxing and Leimkuhler, Benedict and Storkey, Amos J.},
title = {Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {37–45},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969243,
author = {Tsiligkaridis, Theodoros and Forsythe, Keith W.},
title = {Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyper-parameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {28–36},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969242,
author = {Alabdulmohsin, Ibrahim},
title = {Algorithmic Stability and Uniform Generalization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result. For instance, a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {19–27},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969241,
author = {Rooyen, Brendan van and Menon, Aditya Krishna and Williamson, Robert C.},
title = {Learning with Symmetric Label Noise: The Importance of Being Unhinged},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong ℓ2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss' SLN-robustness is borne out in practice. So, with apologies to Wilde [1895], while the truth is rarely pure, it can be simple.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {10–18},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969239.2969240,
author = {Shah, Nihar B. and Zhou, Dengyong},
title = {Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1–9},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@proceedings{10.5555/2969239,
title = {NIPS'15: Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
location = {Montreal, Canada}
}

