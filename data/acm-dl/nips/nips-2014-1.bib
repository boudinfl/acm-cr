@inproceedings{10.5555/2968826.2969032,
author = {Chan, Siu-On and Diakonikolas, Ilias and Servedio, Rocco A. and Sun, Xiaorui},
title = {Near-Optimal Density Estimation in near-Linear Time Using Variable-Width Histograms},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Let p be an unknown and arbitrary probability distribution over [0, 1). We consider the problem of density estimation, in which a learning algorithm is given i.i.d. draws from p and must (with high probability) output a hypothesis distribution that is close to p. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function.In more detail, for any k and ε, we give an algorithm that makes \~{O}(k/ε2) draws from p, runs in \~{O}(k/ε2) time, and outputs a hypothesis distribution h that is piece-wise constant with O(k log2(1/ε)) pieces. With high probability the hypothesis h satisfies dTV(p, h) ≤ C · optk (p) + ε, where dTV denotes the total variation distance (statistical distance), C is a universal constant, and optk (p) is the smallest total variation distance between p and any k-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The "approximation factor" C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and ε can achieve C&lt; 2 regardless of what kind of hypothesis distribution it uses.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1844–1852},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969031,
author = {Maillard, Odalric-Ambrym and Mann, Timothy A. and Mannor, Shie},
title = {"How Hard is My MDP?" The Distribution-Norm to the Rescue},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel p. In many problems, a good approximation of p is not needed. For instance, if from one state-action pair (s, a), one can only transit to states with the same value, learning p(· |s, a) accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) based on what we call the distribution-norm. The distribution-norm w.r.t. a measure v is defined on zero v-mean functions f by the standard variation of f with respect to v. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the problem-free, loose ‖ · ‖1 concentration inequalities used in most previous analysis of RL algorithms, with a tighter problem-dependent hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1835–1843},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969030,
author = {Blum, Avrim and Haghtalab, Nika and Procaccia, Ariel D.},
title = {Learning Optimal Commitment to Overcome Insecurity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Game-theoretic algorithms for physical security have made an impressive real-world impact. These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender's strategy and best-responds. In order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies. We design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities. In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1826–1834},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969029,
author = {Garreau, Damien and Lajugie, R\'{e}mi and Arlot, Sylvain and Bach, Francis},
title = {Metric Learning for Temporal Sequence Alignment},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1817–1825},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969028,
author = {Tang, Yichuan and Srivastava, Nitish and Salakhutdinov, Ruslan},
title = {Learning Generative Models with Visual Attention},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1808–1816},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969027,
author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
title = {Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1799–1807},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969026,
author = {Xu, Li and Ren, Jimmy S. J. and Liu, Ce and Jia, Jiaya},
title = {Deep Convolutional Neural Network for Image Deconvolution},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an ideal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1790–1798},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969025,
author = {Woodruff, David P.},
title = {Low Rank Approximation Lower Bounds in Row-Update Streams},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study low-rank approximation in the streaming model in which the rows of an n x d matrix A are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a k x d matrix R so that ‖A - AR† R‖2F ≤ (1 + ∊)‖A - Ak‖2F where Ak is the best rank-k approximation to A. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using O(dk/∊) words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of Ω(dk/∊) bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple Ω(dk) space lower bound.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1781–1789},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969024,
author = {Ruozzi, Nicholas and Jebara, Tony},
title = {Making Pairwise Binary Graphical Models Attractive},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Computing the partition function (i.e., the normalizing constant) of a given pair-wise binary graphical model is NP-hard in general. As a result, the partition function is typically estimated by approximate inference algorithms such as belief propagation (BP) and tree-reweighted belief propagation (TRBP). The former provides reasonable estimates in practice but has convergence issues. The later has better convergence properties but typically provides poorer estimates. In this work, we propose a novel scheme that has better convergence properties than BP and provably provides better partition function estimates in many instances than TRBP. In particular, given an arbitrary pairwise binary graphical model, we construct a specific "attractive" 2-cover. We explore the properties of this special cover and show that it can be used to construct an algorithm with the desired properties.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1772–1780},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969023,
author = {Liu, Yuanyuan and Shang, Fanhua and Fan, Wei and Cheng, James and Cheng, Hong},
title = {Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1763–1771},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969022,
author = {Henao, Ricardo and Yuan, Xin and Carin, Lawrence},
title = {Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1754–1762},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969021,
author = {Andersen, Michael Riis and Winther, Ole and Hansen, Lars Kai},
title = {Bayesian Inference for Structured Spike and Slab Priors},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1745–1753},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969020,
author = {Chen, Xianjie and Yuille, Alan},
title = {Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1736–1744},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969019,
author = {Lafond, Jean and Klopp, Olga and Moulines, \'{E}ric and Salmon, Joseph},
title = {Probabilistic Low-Rank Matrix Completion on Finite Alphabets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The task of reconstructing a matrix given a sample of observed entries is known as the matrix completion problem. It arises in a wide range of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite number of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (not necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1727–1735},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969018,
author = {Shen, Jie and Xu, Huan and Li, Ping},
title = {Online Optimization for Max-Norm Regularization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low rank estimation of the underlying data. However, max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck. In this paper, we propose an online algorithm for solving max-norm regularized problems that is scalable to large problems. Particularly, we consider the matrix decomposition problem as an example, although our analysis can also be applied in other problems such as matrix completion. The key technique in our algorithm is to reformulate the max-norm into a matrix factorization form, consisting of a basis component and a coefficients one. In this way, we can solve the optimal basis and coefficients alternatively. We prove that the basis produced by our algorithm converges to a stationary point asymptotically. Experiments demonstrate encouraging results for the effectiveness and robustness of our algorithm. See the full paper at arXiv:1406.3190.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1718–1726},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969017,
author = {Chen, Xu and Cheng, Xiuyuan and Mallat, St\'{e}phane},
title = {Unsupervised Deep Haar Scattering on Graphs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1709–1717},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969016,
author = {Mizrahi, Yariv D. and Denil, Misha and Freitas, Nando de},
title = {Distributed Parameter Estimation in Probabilistic Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1700–1708},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969015,
author = {Vanchinathan, Hastagiri P and Bart\`{o}k, Gabor and Krause, Andreas},
title = {Efficient Partial Monitoring with Prior Information},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1691–1699},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969014,
author = {Malinowski, Mateusz and Fritz, Mario},
title = {A Multi-World Approach to Question Answering about Real-World Scenes Based on Uncertain Input},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1682–1690},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969013,
author = {Xie, Cong and Yan, Ling and Li, Wu-Jun and Zhang, Zhihua},
title = {Distributed Power-Law Graph Computing: Theoretical and Empirical Analysis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing (DGC) frameworks has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning (GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called degree-based hashing (DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1673–1681},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969012,
author = {Bruer, John J. and Tropp, Joel A. and Cevher, Volkan and Becker, Stephen R.},
title = {Time–Data Tradeoffs by Aggressive Smoothing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1664–1672},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969011,
author = {Kong, Deguang and Fujimaki, Ryohei and Liu, Ji and Nie, Feiping and Ding, Chris},
title = {Exclusive Feature Learning on Arbitrary Structures via ℓ<sub>1,2</sub>-Norm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Group LASSO is widely used to enforce the structural sparsity, which achieves the sparsity at the inter-group level. In this paper, we propose a new formulation called "exclusive group LASSO", which brings out sparsity at intra-group level in the context of feature selection. The proposed exclusive group LASSO is applicable on any feature structures, regardless of their overlapping or non-overlapping structures. We provide analysis on the properties of exclusive group LASSO, and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis. We show applications of exclusive group LASSO for uncorrelated feature selection. Extensive experiments on both synthetic and real-world datasets validate the proposed method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1655–1663},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969010,
author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
title = {SAGA: A Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1646–1654},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969009,
author = {Song, Hyun Oh and Lee, Yong Jae and Jegelka, Stefanie and Darrell, Trevor},
title = {Weakly-Supervised Discovery of Visual Pattern Configurations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1637–1645},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969008,
author = {Wu, Anqi and Park, Mijung and Koyejo, Oluwasanmi and Pillow, Jonathan W.},
title = {Sparse Bayesian Structure Learning with Dependent Relevance Determination Prior},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as "region sparsity". Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g., group lasso and smooth RVM) for both simulated and real datasets from brain imaging.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1628–1636},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969007,
author = {Zhu, Changbo and Xu, Huan and Leng, Chenlei and Yan, Shuicheng},
title = {Convex Optimization Procedure for Clustering: Theoretical Revisit},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we present theoretical analysis of SON - a convex optimization procedure for clustering using a sum-of-norms (SON) regularization recently proposed in [8, 10, 11, 17]. In particular, we show if the samples are drawn from two cubes, each being one cluster, then SON can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster. To the best of our knowledge, this paper is the first to provide a rigorous analysis to understand why and when SON works. We believe this may provide important insights to develop novel convex optimization based algorithms for clustering.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1619–1627},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969006,
author = {Dekel, Ofer and Hazan, Elad and Koren, Tomer},
title = {The Blinded Bandit: Learning with Adaptive Feedback},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of adaptive feedback naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the blinded multi-armed bandit, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1610–1618},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969005,
author = {Long, Jonathan and Zhang, Ning and Darrell, Trevor},
title = {Do Convnets Learn Correspondence?},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1601–1609},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969004,
author = {Bartz, Daniel and M\"{u}ller, Klaus-Robert},
title = {Covariance Shrinkage for Autocorrelated Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1592–1600},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969003,
author = {Russo, Daniel and Roy, Benjamin Van},
title = {Learning to Optimize via Information-Directed Sampling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose information-directed sampling - a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between the square of expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation.We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. For the widely studied Bernoulli and linear bandit models, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, Thompson sampling, and knowledge gradient. Further, we present simple analytic examples illustrating that information-directed sampling can dramatically outperform upper confidence bound algorithms and Thompson sampling due to the way it measures information gain.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1583–1591},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969002,
author = {Nitanda, Atsushi},
title = {Stochastic Proximal Gradient Descent with Acceleration Techniques},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1574–1582},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969001,
author = {Refaat, Khaled S. and Choi, Arthur and Darwiche, Adnan},
title = {Decomposing Parameter Estimation Problems},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1565–1573},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2969000,
author = {Banerjee, Arindam and Chen, Sheng and Fazayeli, Farideh and Sivakumar, Vidyashankar},
title = {Estimation with Norm Regularization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects. We characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for a variety of noise models, design matrices, including sub-Gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models. Gaussian width, a geometric measure of size of sets, and associated tools play a key role in our generalized analysis.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1556–1564},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968999,
author = {Khan, Mohammad Emtiyaz},
title = {Decoupled Variational Gaussian Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. A difficulty remains in computation of the lower bound when the latent dimensionality L is large. Even though the lower bound is concave for many models, its computation requires optimization over O(L2) variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to O(N), where N is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when it is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples. Each gradient computation reduces to prediction in a pseudo linear regression model, thereby avoiding all direct computations of the covariance and only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1547–1555},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968998,
author = {Berg-Kirkpatrick, Taylor and Andreas, Jacob and Klein, Dan},
title = {Unsupervised Transcription of Piano Music},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording-specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F1 on real piano audio.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1538–1546},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968997,
author = {Gu, Quanquan and Wang, Zhaoran and Liu, Han},
title = {Sparse PCA with Oracle Property},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the estimation of the k-dimensional sparse principal sub-space of covariance matrix Σ in the high-dimensional setting. We aim to recover the oracle principal subspace solution, i.e., the principal subspace estimator obtained assuming the true support is known a priori. To this end, we propose a family of estimators based on the semidefinite relaxation of sparse PCA with novel regularizations. In particular, under a weak assumption on the magnitude of the population projection matrix, one estimator within this family exactly recovers the true support with high probability, has exact rank-k, and attains a √s/n statistical rate of convergence with s being the subspace sparsity level and n the sample size. Compared to existing support recovery results for sparse PCA, our approach does not hinge on the spiked covariance model or the limited correlation condition. As a complement to the first estimator that enjoys the oracle property, we prove that, another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse PCA, even when the previous assumption on the magnitude of the projection matrix is violated. We validate the theoretical results by numerical experiments on synthetic datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1529–1537},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968996,
author = {Mohan, Karthika and Pearl, Judea},
title = {Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of miss-ingness process. We extend the results of Mohan et al. [2013] by presenting more general conditions for recovering probabilistic queries of the form P(y|x) and P(y, x) as well as causal queries of the form P(y|do(x)). We show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable. Specifically, we derive graphical conditions for recovering causal effects of the form P(y|do(x)) when Y and its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1520–1528},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968995,
author = {Wang, Yining and Zhu, Jun},
title = {Spectral Methods for Supervised Topic Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on either variational approximation or Monte Carlo sampling. This paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent Dirichlet allocation (sLDA) models. The Spectral-sLDA algorithm is provably correct and computationally efficient. We prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on a diverse range of synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the algorithm.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1511–1519},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968994,
author = {Li, Nan and Jin, Rong and Zhou, Zhi-Hua},
title = {Top Rank Optimization in Linear Time},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1502–1510},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968993,
author = {Narasimhan, Harikrishna and Vaish, Rohit and Agarwal, Shivani},
title = {On the Statistical Consistency of Plug-in Classifiers for Non-Decomposable Performance Measures},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the 'true' posterior class probability) is available to a learning algorithm. In this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable 'estimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR), and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge, these are the first such results for these measures. In addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1493–1501},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968992,
author = {Tung, Hsiao-Yu Fish and Smola, Alexander J.},
title = {Spectral Methods for Indian Buffet Process Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1484–1492},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968991,
author = {Hajek, Bruce and Oh, Sewoong and Xu, Jiaming},
title = {Minimax-Optimal Inference from Partial Rankings},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cram\'{e}r-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cram\'{e}r-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1475–1483},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968990,
author = {Osband, Ian and Roy, Benjamin Van},
title = {Model-Based Reinforcement Learning and the Eluder Dimension},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as \~{O}(√dKdET) where T is time elapsed, dK is the Kolmogorov dimension and dE is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1466–1474},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968989,
author = {Liu, Zhe and Lafferty, John},
title = {Blossom Tree Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal "blossoms", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then "grown" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1458–1465},
numpages = {8},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968988,
author = {Stober, Sebastian and Cameron, Daniel J. and Grahn, Jessica A.},
title = {Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli - each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1449–1457},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968987,
author = {Razaviyayn, Meisam and Hong, Mingyi and Luo, Zhi-Quan and Pang, Jong-Shi},
title = {Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables. A popular approach to solve this problem is the block coordinate descent (BCD) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed. With the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the BCD method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm. In this work, we propose an inexact parallel BCD approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function. We investigate the convergence of this parallel BCD method for both randomized and cyclic variable selection rules. We analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions. The numerical experiments suggest that for a special case of Lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1440–1448},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968986,
author = {Jain, Prateek and Oh, Sewoong},
title = {Provable Tensor Factorization with Missing Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions, our method can recover a three-mode n \texttimes{} n \texttimes{} n dimensional rank-r tensor exactly from O(n3/2r5 log4 n) randomly sampled entries. In the process of proving this result, we solve two challenging sub-problems for tensors with missing data. First, in analyzing the initialization step, we prove a generalization of a celebrated result by Szemer\'{e}die et al. on the spectrum of random graphs. We show that this initialization step alone is sufficient to achieve the root mean squared error on the parameters bounded by C(r2n3/2(log n)4/|Ω|) from |Ω| observed entries for some constant C independent of n and r. Next, we prove global convergence of alternating minimization with this good initialization. Simulations suggest that the dependence of the sample size on the dimensionality n is indeed tight.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1431–1439},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968985,
author = {Gu, Quanquan and Gui, Huan and Han, Jiawei},
title = {Robust Tensor Decomposition with Gross Corruption},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the statistical performance of robust tensor decomposition with gross corruption. The observations are noisy realization of the superposition of a low-rank tensor W* and an entrywise sparse corruption tensor V*. Unlike conventional noise with bounded variance in previous convex tensor decomposition analysis, the magnitude of the gross corruption can be arbitrary large. We show that under certain conditions, the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously. Our theory yields nonasymptotic Frobenius-norm estimation error bounds for each tensor separately. We show through numerical experiments that our theory can precisely predict the scaling behavior in practice.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1422–1430},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968984,
author = {Tschiatschek, Sebastian and Iyer, Rishabh and Wei, Haochen and Bilmes, Jeff},
title = {Learning Mixtures of Submodular Functions for Image Collection Summarization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We address the problem of image collection summarization by learning mixtures of submodular functions. Submodularity is useful for this problem since it naturally represents characteristics such as fidelity and diversity, desirable for any summary. Several previously proposed image summarization scoring methodologies, in fact, instinctively arrived at submodularity. We provide classes of submodular component functions (including some which are instantiated via a deep neural network) over which mixtures may be learnt. We formulate the learning of such mixtures as a supervised problem via large-margin structured prediction. As a loss function, and for automatic summary scoring, we introduce a novel summary evaluation method called V-ROUGE, and test both submodular and non-submodular optimization (using the submodular-supermodular procedure) to learn a mixture of submodular functions. Interestingly, using non-submodular optimization to learn submodular functions provides the best results. We also provide a new data set consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using Amazon Mechanical Turk. We compare our method with previous work on this problem and show that our learning approach outperforms all competitors on this new data set. This paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new data set of image collections and human summaries.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1413–1421},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968983,
author = {Nguyen, Trung V. and Bonilla, Edwin V.},
title = {Automated Variational Inference for Gaussian Process Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop an automated variational method for approximate inference in Gaussian process (GP) models whose posteriors are often intractable. Using a mixture of Gaussians as the variational distribution, we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate Gaussian distributions and (ii) the gradients wrt the GP hyperparameters can be obtained analytically regardless of the model likelihood. We further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations. These results allow gradient-based optimization to be done efficiently in a black-box manner. Our approach is thoroughly verified on five models using six benchmark datasets, performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative MCMC sampling approaches. Our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1404–1412},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968982,
author = {Acharya, Jayadev and Jafarpour, Ashkan and Orlitsky, Alon and Suresh, Ananda Theertha},
title = {Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many important distributions are high dimensional, and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning, it approximates mixtures of k spherical Gaussians in d-dimensions to within ℓ1 distance ε using O(dk9(log2 d)/ε4) samples and Ok,ε(d3 log5 d) computation time. Conversely, we show that any estimator requires Ω(dk/ε2) samples, hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor Ok,ε is exponential in k, but much smaller than previously known.We also construct a simple estimator for one-dimensional Gaussian mixtures that uses \~{O}(k/ε2) samples and \~{O}((k/ε)3k+1) computation time.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1395–1403},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968981,
author = {Lindsey, Robert V. and Khajah, Mohammad and Mozer, Michael C.},
title = {Automatic Discovery of Cognitive Skills to Improve the Prediction of Student Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To master a discipline such as algebra or physics, students must acquire a set of cognitive skills. Traditionally, educators and domain experts use intuition to determine what these skills are and then select practice exercises to hone a particular skill. We propose a technique that uses student performance data to automatically discover the skills needed in a discipline. The technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice. Rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted Chinese restaurant process. We test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college. We obtain two surprising results. First, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills. Second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it. We discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1386–1394},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968980,
author = {Liu, Xianghang and Domke, Justin},
title = {Projecting Markov Random Field Parameters for Fast Mixing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1377–1385},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968979,
author = {Luo, Haipeng and Schapire, Robert E.},
title = {A Drifting-Games Analysis for Online Learning and Applications to Boosting},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1368–1376},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968978,
author = {Ott, Lionel and Pang, Linsey and Ramos, Fabio and Chawla, Sanjay},
title = {On Integrated Clustering and Outlier Detection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We model the joint clustering and outlier detection problem using an extension of the facility location formulation. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable. We provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence. Extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1359–1367},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968977,
author = {Yu, Adams Wei and Ma, Wanli and Yu, Yaoliang and Carbonell, Jaime G. and Sra, Suvrit},
title = {Efficient Structured Matrix Rank Minimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1350–1358},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968976,
author = {Zheng, Jinging and Jiang, Zhuolin and Chellappa, Rama and Phillips, P. Jonathon},
title = {Submodular Attribute Selection for Action Recognition in Video},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts e.g., jump forward or motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which are learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1341–1349},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968975,
author = {Chen, Weizhu and Wang, Zhenghao and Zhou, Jingren},
title = {Large-Scale L-BFGS Using MapReduce},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1332–1340},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968974,
author = {Chen, Chao and Liu, Han and Metaxas, Dimitris N. and Zhao, Tianqi},
title = {Mode Estimation for High Dimensional Discrete Tree Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading (δ, ρ)-modes of the underlying distributions. A point is defined to be a (δ, ρ)-mode if it is a local optimum of the density within a δ-neighborhood under metric ρ. As we increase the "scale" parameter δ, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the (δ, ρ)-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1323–1331},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968973,
author = {Mohri, Mehryar and Yang, Scott},
title = {Conditional Swap Regret and Conditional Correlated Equilibrium},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player's action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1314–1322},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968972,
author = {G\"{o}nen, Mehmet and Margolin, Adam A.},
title = {Localized Data Fusion for Kernel <i>k</i>-Means Clustering with Application to Cancer Biology},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1305–1313},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968971,
author = {Choi, Joon Hee and Vishwanathan, S. V. N.},
title = {DFacTo: Distributed Factorization of Tensors},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two sparse matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1296–1304},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968970,
author = {Chaudhuri, Kamalika and Hsu, Daniel and Song, Shuang},
title = {The Large Margin Mechanism for Differentially Private Maximization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning.Previous algorithms for this problem are either range-dependent—i.e., their utility diminishes with the size of the universe—or only apply to very restricted function classes. This work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1287–1295},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968969,
author = {Zaremba, Wojciech and Kurach, Karol and Fergus, Rob},
title = {Learning to Discover Efficient Mathematical Identities},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1278–1286},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968968,
author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
title = {Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1269–1277},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968967,
author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},
title = {Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1260–1268},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968966,
author = {Steinberg, Daniel M. and Bonilla, Edwin V.},
title = {Extended and Unscented Gaussian Processes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1251–1259},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968965,
author = {Zhou, Tianyi and Bilmes, Jeff and Guestrin, Carlos},
title = {Divide-and-Conquer Learning by Anchoring a Conical Hull},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We reduce a broad class of fundamental machine learning problems, usually addressed by EM or sampling, to the problem of finding the k extreme rays spanning the conical hull of a1 data point set. These k "anchors" lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the k anchors, we propose a novel divide-and-conquer learning scheme "DCA" that distributes the problem to O(k log k) same-type sub-problems on different low-D random hyperplanes, each can be solved independently by any existing solver. For the 2D sub-problem, we instead present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine inside other algorithms to check whether a point is covered in a conical hull, and thus improves these algorithms by providing significant speedups. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on large datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1242–1250},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968964,
author = {Tekin, Cem and Schaar, Mihaela van der},
title = {Discovering, Learning and Exploiting Relevance},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we consider the problem of learning online what is the information to consider when making sequential decisions. We formalize this as a contextual multi-armed bandit problem where a high dimensional (D-dimensional) context vector arrives to a learner which needs to select an action to maximize its expected reward at each time step. Each dimension of the context vector is called a type. We assume that there exists an unknown relation between actions and types, called the relevance relation, such that the reward of an action only depends on the contexts of the relevant types. When the relation is a function, i.e., the reward of an action only depends on the context of a single type, and the expected reward of an action is Lipschitz continuous in the context of its relevant type, we propose an algorithm that achieves \~{O}(Tγ) regret with a high probability, where γ = 2/(1 + √2). Our algorithm achieves this by learning the unknown relevance relation, whereas prior contextual bandit algorithms that do not exploit the existence of a relevance relation will have \~{O}(T(D+1)/(D+2)) regret. Our algorithm alternates between exploring and exploiting, it does not require reward observations in exploitations, and it guarantees with a high probability that actions with suboptimality greater than e are never selected in exploitations. Our proposed method can be applied to a variety of learning applications including medical diagnosis, recommender systems, popularity prediction from social networks, network security etc., where at each instance of time vast amounts of different types of information are available to the decision maker, but the effect of an action depends only on a single type.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1233–1241},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968963,
author = {Nakajima, Shinichi and Sato, Issei and Sugiyama, Masashi and Watanabe, Kazuho and Kobayashi, Hiroko},
title = {Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity than MAP},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1224–1232},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968962,
author = {Knudson, Karin C. and Yates, Jacob L. and Huk, Alexander C. and Pillow, Jonathan W.},
title = {Inferring Sparse Representations of Continuous Signals with Continuous Orthogonal Matching Pursuit},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many signals, such as spike trains recorded in multi-channel electrophysiological recordings, may be represented as the sparse sum of translated and scaled copies of waveforms whose timing and amplitudes are of interest. From the aggregate signal, one may seek to estimate the identities, amplitudes, and translations of the waveforms that compose the signal. Here we present a fast method for recovering these identities, amplitudes, and translations. The method involves greedily selecting component waveforms and then refining estimates of their amplitudes and translations, moving iteratively between these steps in a process analogous to the well-known Orthogonal Matching Pursuit (OMP) algorithm [11]. Our approach for modeling translations borrows from Continuous Basis Pursuit (CBP) [4], which we extend in several ways: by selecting a subspace that optimally captures translated copies of the waveforms, replacing the convex optimization problem with a greedy approach, and moving to the Fourier domain to more precisely estimate time shifts. We test the resulting method, which we call Continuous Orthogonal Matching Pursuit (COMP), on simulated and neural data, where it shows gains over CBP in both speed and accuracy.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1215–1223},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968961,
author = {Liu, Guangcan and Li, Ping},
title = {Recovery of Coherent Data via Low-Rank Dictionary Pursuit},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The recently established RPCA [4] method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR) [13], provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims. See the full paper at arXiv:1404.4032.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1206–1214},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968960,
author = {Mehta, Nishant A. and Williamson, Robert C.},
title = {From Stochastic Mixability to Fast Rates},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution P and returns a hypothesis f chosen from a fixed class ℱ with small loss ℓ. In the parametric setting, depending upon (ℓ, ℱ, P) ERM can have slow (1/√n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There exist several results that give sufficient conditions for fast rates in terms of joint properties of ℓ, ℱ, and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss ℓ (there being no role there for ℱ or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (ℓ, ℱ, P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1197–1205},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968959,
author = {Lim, Shiau Hong and Chen, Yudong and Xu, Huan},
title = {Clustering from Labels and Time-Varying Graphs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a general framework for graph clustering where a label is observed to each pair of nodes. This allows a very rich encoding of various types of pairwise interactions between nodes. We propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization. We analyze our algorithm under a general generative model, and provide both necessary and sufficient conditions for successful recovery of the underlying clusters. Our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition, weighted clustering and partially observed graphs. Furthermore, the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems. Our theoretical findings are further supported by empirical results on both synthetic and real data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1188–1196},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968958,
author = {Nickel, Maximilian and Jiang, Xueyan and Tresp, Volker},
title = {Reducing the Rank of Relational Factorization Models by Including Observable Patterns},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Tensor factorization has become a popular method for learning from multi-relational data. In this context, the rank of the factorization is an important parameter that determines runtime as well as generalization ability. To identify conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model to learn from latent and observable patterns on multi-relational data and present a scalable algorithm for computing the factorization. We show experimentally both that the proposed additive model does improve the predictive performance over pure latent variable methods and that it also reduces the required rank — and therefore runtime and memory complexity — significantly.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1179–1187},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968957,
author = {Cohen, Haim and Crammer, Koby},
title = {Learning Multiple Tasks in Parallel with a Shared Annotator},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a new multi-task framework, in which K online learners are sharing a single annotator with limited bandwidth. On each round, each of the K learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the K inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and then we proceed to the next round. We develop an online algorithm for multitask binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allow to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially achieves more (accuracy) for the same labour of the annotator.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1170–1178},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968956,
author = {Irfan, Mohammad T. and Ortiz, Luis E.},
title = {Causal Strategic Inference in Networked Microfinance Economies},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Performing interventions is a major challenge in economic policy-making. We propose causal strategic inference as a framework for conducting interventions and apply it to large, networked microfinance economies. The basic solution platform consists of modeling a microfinance market as a networked economy, learning the parameters of the model from the real-world microfinance data, and designing algorithms for various causal questions. For a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique. For the general case, we give a constructive proof of the existence of an equilibrium point. Our empirical study is based on the microfinance data from Bangladesh and Bolivia, which we use to first learn our models. We show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest rate cap, and subsidizing banks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1161–1169},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968955,
author = {Dubey, Avinava and Ho, Qirong and Williamson, Sinead and Xing, Eric P.},
title = {Dependent Nonparametric Trees for Dynamic Hierarchical Clustering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1152–1160},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968954,
author = {Liu, Lingqiao and Shen, Chunhua and Wang, Lei and Hengel, Anton van den and Wang, Chao},
title = {Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians.In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1143–1151},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968953,
author = {Zhang, Jian and Schwing, Alexander G. and Urtasun, Raquel},
title = {Message Passing Inference for Large Scale Graphical Models with High Order Potentials},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1134–1142},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968952,
author = {Zhu, Jun and Mao, Junhua and Yuille, Alan},
title = {Learning from Weakly Supervised Data by the Expectation Loss SVM (e-SVM) Algorithm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many situations we have some measurement of confidence on "positiveness" for a binary label. The "positiveness" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called expectation loss SVM (e-SVM) that is devoted to the problems where only the "positiveness" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1125–1133},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968951,
author = {Orabona, Francesco},
title = {Simultaneous Model Selection and Optimization through Parameter-Free Stochastic Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1116–1124},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968950,
author = {Netrapalli, Praneeth and Niranjan, U N and Sanghavi, Sujay and Anandkumar, Animashree and Jain, Prateek},
title = {Provable Non-Convex Robust PCA},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new method for robust PCA - the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is non-convex but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an m x n input matrix (m ≤ n), our method has a running time of O (r2mn) per iteration, and needs O (log(1/ ∊)) iterations to reach an accuracy of ∊. This is close to the running times of simple PCA via the power method, which requires O (rmn) per iteration, and O (log(1/ ∊)) iterations. In contrast, the existing methods for robust PCA, which are based on convex optimization, have O (m2n) complexity per iteration, and take O (1/∊) iterations, i.e., exponentially more iterations for the same accuracy.Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1107–1115},
numpages = {9},
keywords = {robust PCA, non-convex methods, matrix decomposition, alternating projections},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968949,
author = {Liu, Qiang and Ihler, Alexander},
title = {Distributed Estimation, Information Loss and Exponential Families},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE given the whole dataset. We study this framework's statistical properties, showing that the efficiency loss compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the "full-exponential-family-ness" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of both methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1098–1106},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968948,
author = {Carpentier, Alexandra and Valko, Michal},
title = {Extreme Bandits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the EXTREMEHUNTER algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1089–1097},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968947,
author = {Srivastava, Nisheeth and Vul, Edward and Schrater, Paul R},
title = {Magnitude-Sensitive Preference Formation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Our understanding of the neural computations that underlie the ability of animals to choose among options has advanced through a synthesis of computational modeling, brain imaging and behavioral choice experiments. Yet, there remains a gulf between theories of preference learning and accounts of the real, economic choices that humans face in daily life, choices that are usually between some amount of money and an item. In this paper, we develop a theory of magnitude-sensitive preference learning that permits an agent to rationally infer its preferences for items compared with money options of different magnitudes. We show how this theory yields classical and anomalous supply-demand curves and predicts choices for a large panel of risky lotteries. Accurate replications of such phenomena without recourse to utility functions suggest that the theory proposed is both psychologically realistic and econometrically viable.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1080–1088},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968946,
author = {Levine, Sergey and Abbeel, Pieter},
title = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1071–1079},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968945,
author = {Bresler, Guy and Gamarnik, David and Shah, Devavrat},
title = {Hardness of Parameter Estimation in Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see [1]) but no proof was known.Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1062–1070},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968944,
author = {Wang, Jie and Zhou, Jiayu and Liu, Jun and Wonka, Peter and Ye, Jieping},
title = {A Safe Screening Rule for Sparse Logistic Regression},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The ℓ1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the "0" components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Experiments demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression can be improved by one magnitude.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1053–1061},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968943,
author = {Wu, Yue and Lobato, Jos\'{e} Miguel Hern\'{a}ndez and Ghahramani, Zoubin},
title = {Gaussian Process Volatility Model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to over-fitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overriding problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1044–1052},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968942,
author = {Sz\"{o}r\'{e}nyi, Bal\'{a}zs and Kedenburg, Gunnar and Munos, Remi},
title = {Optimistic Planning in Markov Decision Processes Using a Generative Model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability 1 - δ, an ∊-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the "optimism in the face of uncertainty" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1035–1043},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968941,
author = {Acerbi, Luigi and Ma, Wei Ji and Vijayakumar, Sethu},
title = {A Framework for Testing Identifiability of Bayesian Models of Perception},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data. However, the intrinsic degeneracy of the Bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models, with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1026–1034},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968940,
author = {Needell, Deanna and Srebro, Nathan and Ward, Rachel},
title = {Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz Algorithm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We improve a recent guarantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1017–1025},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968939,
author = {Yen, Ian E. H. and Hsieh, Cho-Jui and Ravikumar, Pradeep and Dhillon, Inderjit},
title = {Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of these statistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1008–1016},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968938,
author = {Glazer, Assaf and Weissbrod, Omer and Lindenbaum, Michael and Markovitch, Shaul},
title = {Approximating Hierarchical MV-Sets for Hierarchical Clustering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The goal of hierarchical clustering is to construct a cluster tree, which can be viewed as the modal structure of a density. For this purpose, we use a convex optimization program that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. We further extend existing graph-based methods to approximate the cluster tree of a distribution. By avoiding direct density estimation, our method is able to handle high-dimensional data more efficiently than existing density-based approaches. We present empirical results that demonstrate the superiority of our method over existing ones.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {999–1007},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968937,
author = {Yao, Hengshuai and Szepesv\'{a}ri, Csaba and Sutton, Rich and Modayil, Joseph and Bhatnagar, Shalabh},
title = {Universal Option Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of learning models of options for real-time abstract planning, in the setting where reward functions can be specified at any time and their expected returns must be efficiently computed. We introduce a new model for an option that is independent of any reward function, called the universal option model (UOM). We prove that the UOM of an option can construct a traditional option model given a reward function, and also supports efficient computation of the option-conditional return. We extend the UOM to linear function approximation, and we show the UOM gives the TD solution of option returns and the value function of a policy over options. We provide a stochastic approximation algorithm for incrementally learning UOMs from data and prove its consistency. We demonstrate our method in two domains. The first domain is a real-time strategy game, where the controller must select the best game unit to accomplish a dynamically-specified task. The second domain is article recommendation, where each user query defines a new reward function and an article's relevance is the expected return from following a policy that follows the citations between articles. Our experiments show that UOMs are substantially more efficient than previously known methods for evaluating option returns and policies over options.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {990–998},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968936,
author = {Valera, Isabel and Ghahramani, Zoubin},
title = {General Table Completion Using a Bayesian Nonparametric Model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In this paper, we provide an efficient and robust table completion tool, based on a Bayesian nonparametric latent feature model. In particular, we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations. Then, we propose an inference algorithm that scales linearly with the number of observations. Finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {981–989},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968935,
author = {Meier, Franziska and Hennig, Philipp and Schaal, Stefan},
title = {Incremental Local Gaussian Regression},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {972–980},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968934,
author = {Soudry, Daniel and Hubara, Itay and Meir, Ron},
title = {Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a "mean-field" factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs.Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude.We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {963–971},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968933,
author = {Latimer, Kenneth W. and Chichilnisky, E. J. and Rieke, Fred and Pillow, Jonathan W.},
title = {Inferring Synaptic Conductances from Spike Trains under a Biophysically Inspired Point Process Model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite "push-pull" fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyper-polarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. The stimulus-dependence of both excitatory and inhibitory conductances can be well described by a linear-nonlinear cascade, with the filter driving inhibition exhibiting opposite sign and a slight delay relative to the filter driving excitation. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {954–962},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968932,
author = {Benson, Austin R. and Lee, Jason D. and Rajwa, Bartek and Gleich, David F.},
title = {Scalable Methods for Nonnegative Matrix Factorizations of Near-Separable Tall-and-Skinny Matrices},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called "tall-and-skinny matrices." One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {945–953},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968931,
author = {Wang, Shenlong and Schwing, Alexander G. and Urtasun, Raquel},
title = {Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property, we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programing. We demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising, and show that our method significantly outperforms existing techniques in terms of efficiency as well as quality of the retrieved solution.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {936–944},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968930,
author = {Treister, Eran and Turek, Javier},
title = {A Block-Coordinate Descent Approach for Large-Scale Sparse Inverse Covariance Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing. In this problem, the inverse of a covariance matrix of a multivariate normal distribution is estimated, assuming that it is sparse. An ℓ1 regularized log-determinant optimization problem is typically solved to approximate such matrices. Because of memory limitations, most existing algorithms are unable to handle large scale instances of this problem. In this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets. Our method treats the sought matrix block-by-block using quadratic approximations, and we show that this approach has advantages over existing methods in several aspects. Numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods, especially for large-scale problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {927–935},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968929,
author = {Henr\'{a}ndez-Lobato, Jos\'{e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
title = {Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {918–926},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968928,
author = {Weller, Adrian and Jebara, Tony},
title = {Clamping Variables and Approximate Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It was recently proved using graph covers (Ruozzi, 2012) that the Bethe partition function is upper bounded by the true partition function for a binary pairwise model that is attractive. Here we provide a new, arguably simpler proof from first principles. We make use of the idea of clamping a variable to a particular value. For an attractive model, we show that summing over the Bethe partition functions for each sub-model obtained after clamping any variable can only raise (and hence improve) the approximation. In fact, we derive a stronger result that may have other useful implications. Repeatedly clamping until we obtain a model with no cycles, where the Bethe approximation is exact, yields the result. We also provide a related lower bound on a broad class of approximate partition functions of general pairwise multi-label models that depends only on the topology. We demonstrate that clamping a few wisely chosen variables can be of practical value by dramatically reducing approximation error.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {909–917},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968927,
author = {Fogel, Fajwel and d'Aspremont, Alexandre and Vojnovic, Milan},
title = {SerialRank: Spectral Ranking Using Seriation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a seriation algorithm for ranking a set of n items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact even when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than other scoring methods. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {900–908},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968926,
author = {Mohler, George O.},
title = {Learning Convolution Filters for Inverse Covariance Estimation of Neural Network Connectivity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {891–899},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968925,
author = {Wu, Xiaojian and Sheldon, Daniel and Zilberstein, Shlomo},
title = {Stochastic Network Design in Bidirected Trees},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1 — ε)-optimal solutions for any problem instance in time polynomial in the input size and 1/ε. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {882–890},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968924,
author = {Marchand, Mario and Su, Hongyu and Morvant, Emilie and Rousu, Juho and Shawe-Taylor, John},
title = {Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show that the usual score function for conditional Markov networks can be written as the expectation over the scores of their spanning trees. We also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference. The experimental results confirm that practical learning is scalable to realistic datasets using this approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {873–881},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968923,
author = {Boix, Xavier and Roig, Gemma and Diether, Salomon and Gool, Luc Van},
title = {Self-Adaptable Templates for Feature Coding},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P) [1, 2, 3, 4, 5]. In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {864–872},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968922,
author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
title = {On the Computational Efficiency of Training Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {855–863},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968921,
author = {Lemonnier, R\'{e}mi and Seaman, Kevin and Vayatis, Nicolas},
title = {Tight Bounds for Influence in Diffusion Networks and Application to Bond Percolation and Epidemiology},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we derive theoretical bounds for the long-term influence of a node in an Independent Cascade Model (ICM). We relate these bounds to the spectral radius of a particular matrix and show that the behavior is sub-critical when this spectral radius is lower than 1. More specifically, we point out that, in general networks, the sub-critical regime behaves in O(√n) where n is the size of the network, and that this upper bound is met for star-shaped networks. We apply our results to epidemiology and percolation on arbitrary networks, and derive a bound for the critical value beyond which a giant connected component arises. Finally, we show empirically the tightness of our bounds for a large family of networks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {846–854},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968920,
author = {Hern\'{a}ndez-Lobato, Daniel and Sharmanska, Viktoriia and Kersting, Kristian and Lampert, Christoph H. and Quadrianto, Novi},
title = {Mind the Nuisance: Gaussian Process Classification Using Privileged Noise},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {837–845},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968919,
author = {Soare, Marta and Lazaric, Alessandro and Munos, R\'{e}mi},
title = {Best-Arm Identification in Linear Bandits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter θ* and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the G-optimality criterion used in optimal experimental design.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {828–836},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968918,
author = {Calandriello, Daniele and Lazaric, Alessandro and Restell, Marcello},
title = {Sparse Multi-Task Reinforcement Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In multi-task reinforcement learning (MTRL), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t. single-task learning. In this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features. This is equivalent to assuming that the weight vectors of the task value functions are jointly sparse, i.e., the set of their non-zero components is small and it is shared across tasks. Building on existing results in multi-task regression, we develop two multi-task extensions of the fitted Q-iteration algorithm. While the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation. For both algorithms we provide a sample complexity analysis and numerical simulations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {819–827},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968917,
author = {Sani, Amir and Neu, Gergely and Lazaric, Alessandro},
title = {Exploiting Easy Data in Online Optimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of online optimization, where a learner chooses a decision from a given decision set and suffers some loss associated with the decision and the state of the environment. The learner's objective is to minimize its cumulative regret against the best fixed decision in hindsight. Over the past few decades numerous variants have been considered, with many algorithms designed to achieve sub-linear regret in the worst case. However, this level of robustness comes at a cost. Proposed algorithms are often over-conservative, failing to adapt to the actual complexity of the loss sequence which is often far from the worst case. In this paper we introduce a general algorithm that, provided with a "safe" learning algorithm and an opportunistic "benchmark", can effectively combine good worst-case guarantees with much improved performance on "easy" data. We derive general theoretical bounds on the regret of the proposed algorithm and discuss its implementation in a wide range of applications, notably in the problem of learning with shifting experts (a recent COLT open problem). Finally, we provide numerical simulations in the setting of prediction with expert advice with comparisons to the state of the art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {810–818},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968916,
author = {Chakrabarty, Deeparnab and Jain, Prateek and Kothari, Pravesh},
title = {Provable Submodular Minimization Using Wolfe's Algorithm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time [10, 11]. However, these algorithms are typically not practical. In 1976, Wolfe [21] proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige [3] showed how Wolfe's algorithm can be used for SFM. For general submodular functions, this Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance.Despite its good practical performance, very little is known about Wolfe's minimum norm algorithm theoretically. To our knowledge, the only result is an exponential time analysis due to Wolfe [21] himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns an O(1/t)-approximate solution to the min-norm point on any polytope. We also prove a robust version of Fujishige's theorem which shows that an O(1/n2)-approximate solution to the min-norm point on the base polytope implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for unconstrained submodular function minimization.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {802–809},
numpages = {8},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968915,
author = {Gu, Shuhang and Zhang, Lei and Zuo, Wangmeng and Feng, Xiangchu},
title = {Projective Dictionary Pair Learning for Pattern Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the ℓ0 or ℓ1-norm sparsity constraint on the representation coefficients adopted in most DL methods makes the training and testing phases time consuming. We propose anew discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {793–801},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968914,
author = {Hazan, Elad and Levy, Kfir Y.},
title = {Bandit Convex Optimization: Towards Tight Bounds},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968913,
author = {Adametz, David and Roth, Volker},
title = {Distance-Based Network Recovery under Feature Correlation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present an inference method for Gaussian graphical models when only pair-wise distances of n objects are observed. Formally, this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances dMH(xi, xj), where object xi lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby, we generalize TiWnet [19], which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {775–783},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968912,
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
title = {Discriminative Unsupervised Feature Learning with Convolutional Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {766–774},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968911,
author = {Chen, Dongqu},
title = {Learning Shuffle Ideals under Restricted Distributions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set U is the collection of all strings containing some string u ∈ U as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learn-ability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {757–765},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968910,
author = {Drugowitsch, Jan and Moreno-Bote, Rub\'{e}n and Pouget, Alexandre},
title = {Optimal Decision-Making with Time-Varying Evidence Reliability},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {748–756},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968909,
author = {Schober, Michael and Duvenaud, David and Hennig, Philipp},
title = {Probabilistic ODE Solvers with Runge-Kutta Means},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {739–747},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968908,
author = {Stimberg, Florian and Ruttor, Andreas and Opper, Manfred},
title = {Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a model where the rate of an inhomogeneous Poisson process is modified by a Chinese restaurant process. Applying a MCMC sampler to this model allows us to do posterior Bayesian inference about the number of states in Poisson-like data. Our sampler is shown to get accurate results for synthetic data and we apply it to V1 neuron spike data to find discrete firing rate states depending on the orientation of a stimulus.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {730–738},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968907,
author = {Tran-Dinh, Quoc and Cevher, Volkan},
title = {Constrained Convex Minimization via Model-Based Excessive Gap},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a model-based excessive gap technique to analyze first-order primal-dual methods for constrained convex minimization. As a result, we construct first-order primal-dual methods with optimal convergence rates on the primal objective residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {721–729},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968906,
author = {Arpit, Devansh and Nwogu, Ifeoma and Govindaraju, Venu},
title = {Dimensionality Reduction with Subspace Structure Preservation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that 2K projection vectors are sufficient for the independence preservation of any K class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving state-of-the-art results compared to popular dimensionality reduction techniques.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {712–720},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968905,
author = {Plessis, Marthinus C. du and Niu, Gang and Sugiyama, Masashi},
title = {Analysis of Learning from Positive and Unlabeled Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than 2√2 times the fully supervised case. These theoretical findings are also validated through experiments.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {703–711},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968904,
author = {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},
title = {Online and Stochastic Gradient Methods for Non-Decomposable Loss Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {694–702},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968903,
author = {Jain, Prateek and Tewari, Ambuj and Kar, Purushottam},
title = {On Iterative Hard Thresholding Methods for High-Dimensional M-Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. Finally, we extend our analysis to the problem of low-rank matrix recovery.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {685–693},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968902,
author = {Koyejo, Oluwasanmi and Khanna, Rajiv and Ghosh, Joydeep and Poldrack, Russell A.},
title = {On Prior Distributions and Approximate Inference for Structured Variables},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {676–684},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968901,
author = {Oh, Sang-Yun and Dalal, Onkar and Khare, Kshitij and Rajaratnam, Bala},
title = {Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of ℓ1-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing ℓ1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for ℓ1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {667–675},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968900,
author = {Ridgway, James and Alquier, Pierre and Chopin, Nicolas and Liang, Feng},
title = {PAC-Bayesian AUC Classification and Scoring},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {658–666},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968899,
author = {Mittal, Happy and Goyal, Prasoon and Gogate, Vibhav and Singla, Parag},
title = {New Rules for Domain Independent Lifted MAP Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Lifted inference algorithms for probabilistic first-order logic frameworks such as Markov logic networks (MLNs) have received significant attention in recent years. These algorithms use so called lifting rules to identify symmetries in the first-order representation and reduce the inference problem over a large probabilistic model to an inference problem over a much smaller model. In this paper, we present two new lifting rules, which enable fast MAP inference in a large class of MLNs. Our first rule uses the concept of single occurrence equivalence class of logical variables, which we define in the paper. The rule states that the MAP assignment over an MLN can be recovered from a much smaller MLN, in which each logical variable in each single occurrence equivalence class is replaced by a constant (i.e., an object in the domain of the variable). Our second rule states that we can safely remove a subset of formulas from the MLN if all equivalence classes of variables in the remaining MLN are single occurrence and all formulas in the subset are tautology (i.e., evaluate to true) at extremes (i.e., assignments with identical truth value for groundings of a predicate). We prove that our two new rules are sound and demonstrate via a detailed experimental evaluation that our approach is superior in terms of scalability and MAP solution quality to the state of the art approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {649–657},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968898,
author = {Nishihara, Robert and Jegelka, Stefanie and Jordan, Michael I.},
title = {On the Convergence Rate of Decomposable Submodular Function Minimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of "simple" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {640–648},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968897,
author = {Meek, Christopher and Meil\u{a}, Marina},
title = {Recursive Inversion Models for Permutations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a new exponential family probabilistic model for permutations that can capture hierarchical structure and that has the Mallows and generalized Mallows models as subclasses. We describe how to do parameter estimation and propose an approach to structure search for this class of models. We provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {631–639},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968896,
author = {Amin, Kareem and Rostamizadeh, Afshin and Syed, Umar},
title = {Repeated Contextual Auctions with Strategic Buyers},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer's valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (\~{O}(T2/3)) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {622–630},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968895,
author = {Koc\'{a}k, Tom\'{a}\v{s} and Neu, Gergely and Valko, Michal and Munos, R\'{e}mi},
title = {Efficient Learning by Implicit Exploration in Bandit Problems with Side Observations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {613–621},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968894,
author = {Osband, Ian and Roy, Benjamin Van},
title = {Near-Optimal Reinforcement Learning in Factored MDPs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer Ω(SAT) regret on some MDP, where T is the elapsed time and S and A are the cardinalities of the state and action spaces. This implies T = Ω(SA) time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, S and A can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a factored MDP, it is possible to achieve regret that scales polynomially in the number of parameters encoding the factored MDP, which may be exponentially smaller than S or A. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning PSRL) and an upper confidence bound algorithm (UCRL-Factored).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {604–612},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968893,
author = {Oh, Sewoong and Shah, Devavrat},
title = {Learning Mixed Multinomial Logit Model from Ordinal Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture with two MNL components is infeasible in general.Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. We present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of r MNL components over n objects can be learnt using samples whose size scales polynomially in n and r (concretely, r3.5n3(log n)4, with r ≪ n2/7 when the model parameters are sufficiently incoherent). The algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using RANKCENTRALITY introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {595–603},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968892,
author = {Seiler, Christof and Rubinstein-Salzedo, Simon and Holmes, Susan},
title = {Positive Curvature and Hamiltonian Monte Carlo},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {586–594},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968891,
author = {Steeg, Greg Ver and Galstyan, Aram},
title = {Discovering Structure in High-Dimensional Data through Correlation Explanation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {577–585},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968890,
author = {Simonyan, Karen and Zisserman, Andrew},
title = {Two-Stream Convolutional Networks for Action Recognition in Videos},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {568–576},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968889,
author = {Rosman, Guy and Volkov, Mikhail and Feldman, Danny and III, John W. Fisher and Rus, Daniela},
title = {Coresets for <i>k</i>-Segmentation of Streaming Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by a k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals.A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task - in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(k log n/ε2) that provides a (1+ε)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel reduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {559–567},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968888,
author = {Lattimore, Tor and Munos, R\'{e}mi},
title = {Bounded Regret for Finite-Armed Structured Bandits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {550–558},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968887,
author = {Pachauri, Deepti and Kondor, Risi and Sargur, Gautam and Singh, Vikas},
title = {Permutation Diffusion Maps (PDM) with Application to the Image Association Problem in Computer Vision},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Consistently matching keypoints across images, and the related problem of finding clusters of nearby images, are critical components of various tasks in Computer Vision, including Structure from Motion (SfM). Unfortunately, occlusion and large repetitive structures tend to mislead most currently used matching algorithms, leading to characteristic pathologies in the final output. In this paper we introduce a new method, Permutations Diffusion Maps (PDM), to solve the matching problem, as well as a related new affinity measure, derived using ideas from harmonic analysis on the symmetric group. We show that just by using it as a preprocessing step to existing SfM pipelines, PDM can greatly improve reconstruction quality on difficult datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {541–549},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968886,
author = {Chen, Changyou and Zhu, Jun and Zhang, Xinhua},
title = {Robust Bayesian Max-Margin Clustering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {532–540},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968885,
author = {Wang, Xiaolong and Zhang, Liliang and Lin, Liang and Liang, Zhujin and Zuo, Wangmeng},
title = {Deep Joint Task Learning for Generic Object Extraction},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper investigates how to extract objects-of-interest without relying on handcraft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {523–531},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968884,
author = {Tyagi, Hemant and Krause, Andreas and G\"{a}rtner, Bernd},
title = {Efficient Sampling for Learning Sparse Additive Models in High Dimensions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of learning sparse additive models, i.e., functions of the form: f(x) = Σl∈S ϕl(xl), x ∈ ℝd from point queries of f. Here S is an unknown subset of coordinate variables with |S| = k ≪ d. Assuming ϕl's to be smooth, we propose a set of points at which to sample f and an efficient randomized algorithm that recovers a uniform approximation to each unknown ϕl. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise - either arbitrary but bounded, or stochastic - on the performance of our algorithm.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {514–522},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968883,
author = {Mi, Yuanyuan and Fung, C. C. Alan and Wong, K. Y. Michael and Wu, Si},
title = {Spike Frequency Adaptation Implements Anticipative Tracking in Continuous Attractor Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To extract motion information, the brain needs to compensate for time delays that are ubiquitous in neural signal transmission and processing. Here we propose a simple yet effective mechanism to implement anticipative tracking in neural systems. The proposed mechanism utilizes the property of spike-frequency adaptation (SFA), a feature widely observed in neuronal responses. We employ continuous attractor neural networks (CANNs) as the model to describe the tracking behaviors in neural systems. Incorporating SFA, a CANN exhibits intrinsic mobility, manifested by the ability of the CANN to support self-sustained travelling waves. In tracking a moving stimulus, the interplay between the external drive and the intrinsic mobility of the network determines the tracking performance. Interestingly, we find that the regime of anticipation effectively coincides with the regime where the intrinsic speed of the travelling wave exceeds that of the external drive. Depending on the SFA amplitudes, the network can achieve either perfect tracking, with zero-lag to the input, or perfect anticipative tracking, with a constant leading time to the input. Our model successfully reproduces experimentally observed anticipative tracking behaviors, and sheds light on our understanding of how the brain processes motion information in a timely manner.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {505–513},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968882,
author = {Turner, Ryan and Bottone, Steven and Avasarala, Bhargav},
title = {A Complete Variational Tracker},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a novel probabilistic tracking algorithm that incorporates combinatorial data association constraints and model-based track management using variational Bayes. We use a Bethe entropy approximation to incorporate data association constraints that are often ignored in previous probabilistic tracking algorithms. Noteworthy aspects of our method include a model-based mechanism to replace heuristic logic typically used to initiate and destroy tracks, and an assignment posterior with linear computation cost in window length as opposed to the exponential scaling of previous MAP-based approaches. We demonstrate the applicability of our method on radar tracking and computer vision problems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {496–504},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968881,
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
title = {Learning Deep Features for Scene Recognition Using Places Database},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {487–495},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968880,
author = {Makin, Joseph G. and Sabes, Philip N.},
title = {Sensory Integration and Density Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The integration of partially redundant information from multiple sensors is a standard computational problem for agents interacting with the world. In man and other primates, integration has been shown psychophysically to be nearly optimal in the sense of error minimization. An influential generalization of this notion of optimality is that populations of multisensory neurons should retain all the information from their unisensory afferents about the underlying, common stimulus [1]. More recently, it was shown empirically that a neural network trained to perform latent-variable density estimation, with the activities of the unisensory neurons as observed data, satisfies the information-preservation criterion, even though the model architecture was not designed to match the true generative process for the data [2]. We prove here an analytical connection between these seemingly different tasks, density estimation and sensory integration; that the former implies the latter for the model used in [2]; but that this does not appear to be true for all models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {478–486},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968879,
author = {Sabato, Sivan and Munos, Remi},
title = {Active Regression by Stratification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O(1/∊) cannot in general be improved upon. Nonetheless, the so-called 'constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {469–477},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968878,
author = {Akram, Sahar and Simon, Jonathan Z. and Shamma, Shihab and Babadi, Behtash},
title = {A State-Space Model for Decoding Auditory Attentional Modulation from MEG in a Competing-Speaker Environment},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {460–468},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968877,
author = {Guez, Arthur and Heess, Nicolas and Silver, David and Dayan, Peter},
title = {Bayes-Adaptive Simulation-Based Search with Value Function Approximation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises appropriately over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {451–459},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968876,
author = {Zhang, Chicheng and Chaudhuri, Kamalika},
title = {Beyond Disagreement-Based Agnostic Active Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithm for this problem is disagreement-based active learning, which has a high label requirement. Thus a major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems.In this paper, we provide such an algorithm. Our solution is based on two novel contributions; first, a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and second, a novel confidence-rated predictor.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {442–450},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968875,
author = {Vandermeulen, Robert A. and Scott, Clayton D.},
title = {Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {While robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting. We present a robust version of the popular kernel density estimator (KDE). As with other estimators, a robust version of the KDE is useful since sample contamination is a common issue with datasets. What "robustness" means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper. To construct a robust KDE we scale the traditional KDE and project it to its nearest weighted KDE in the L2 norm. This yields a scaled and projected KDE (SPKDE). Because the squared L2 norm penalizes point-wise errors superlinearly this causes the weighted KDE to allocate more weight to high density regions. We demonstrate the robustness of the SPKDE with numerical experiments and a consistency result which shows that asymptotically the SPKDE recovers the uncontaminated density under sufficient conditions on the contamination.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {433–441},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968874,
author = {Nam, Woonhyun and Doll\'{a}r, Piotr and Han, Joon Hee},
title = {Local Decorrelation for Improved Pedestrian Detection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {424–432},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968873,
author = {McWilliams, Brian and Krummenacher, Gabriel and Lucic, Mario and Buhmann, Joachim M.},
title = {Fast and Robust Least Squares Estimation in Corrupted Linear Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates.The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence - for which we also develop a randomized approximation - motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {415–423},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968872,
author = {Saade, Alaa and Krzakala, Florent and Zdeborov\'{a}, Lenka},
title = {Spectral Clustering of Graphs with the Bethe Hessian},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {406–414},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968871,
author = {Ghoshdastidar, Debarghya and Dukkipati, Ambedkar},
title = {Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hypergraphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {397–405},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968870,
author = {Minh, H\'{e} Quang and Biagio, Marco San and Murino, Vittorio},
title = {Log-Hilbert-Schmidt Metric between Positive Definite Operators on Hilbert Spaces},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper introduces a novel mathematical and computational framework, namely Log-Hilbert-Schmidt metric between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between co-variance operators on a Reproducing Kernel Hilbert Space (RKHS), for which we obtain explicit formulas via the corresponding Gram matrices. Empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the Log-Euclidean metric, Stein and Jeffreys divergences, achieving new state of the art results.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {388–396},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968869,
author = {Chen, Shouyuan and Lin, Tian and King, Irwin and Lyu, Michael R. and Chen, Wei},
title = {Combinatorial Pure Exploration of Multi-Armed Bandits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {379–387},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968868,
author = {Gottlieb, Lee-Ad and Kontorovich, Aryeh and Nisnevitch, Pinhas},
title = {Near-Optimal Sample Compression for Nearest Neighbors},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {370–378},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968867,
author = {Pareek, Harsh and Ravikumar, Pradeep},
title = {A Representation Theory for Ranking Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions. Point-wise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole. In many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the Bayes Optimal ranking functions might themselves be listwise, especially if the loss function is listwise. A key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions. We show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions. Our analysis draws from the theories of tensor analysis, functional analysis and De Finetti theorems. We also present experiments using a novel reranking method motivated by our representation theory.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {361–369},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968866,
author = {Mi, Yuanyuan and Li, Luozheng and Wang, Dahui and Wu, Si},
title = {A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Persistent activity refers to the phenomenon that cortical neurons keep firing even after the stimulus triggering the initial neuronal responses is moved. Persistent activity is widely believed to be the substrate for a neural system retaining a memory trace of the stimulus information. In a conventional view, persistent activity is regarded as an attractor of the network dynamics, but it faces a challenge of how to be closed properly. Here, in contrast to the view of attractor, we consider that the stimulus information is encoded in a marginally unstable state of the network which decays very slowly and exhibits persistent firing for a prolonged duration. We propose a simple yet effective mechanism to achieve this goal, which utilizes the property of short-term plasticity (STP) of neuronal synapses. STP has two forms, short-term depression (STD) and short-term facilitation (STF), which have opposite effects on retaining neuronal responses. We find that by properly combining STF and STD, a neural system can hold persistent activity of graded lifetime, and that persistent activity fades away naturally without relying on an external drive. The implications of these results on neural information representation are discussed.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {352–360},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968865,
author = {Archer, Evan and K\"{o}ster, Urs and Pillow, Jonathan and Macke, Jakob H.},
title = {Low-Dimensional Models of Neural Population Activity in Sensory Cortical Circuits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations, for which inference scales linearly in both population size and recording duration. We test this model to multi-channel recordings from primary visual cortex and show that it accounts for neural tuning properties as well as cross-neural correlations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {343–351},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968864,
author = {Deshpande, Yash and Montanari, Andrea},
title = {Sparse PCA via Covariance Thresholding},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n x p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components v1,..., vr have at most k1, · · · , kq non-zero entries respectively, and study the high-dimensional regime in which p is of the same order as n.In an influential paper, Johnstone and Lu [JL04] introduced a simple algorithm that estimates the support of the principal vectors v1,..., vr by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if kq ≤ C1 √n/ log p, and to fail with high probability if kq ≥ C2 √n/ log p for two constants 0 &lt; C1, C2 &lt; ∞. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees.Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik [KNV13]. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for k of order √n. Recent conditional lower bounds [BR13] suggest that it might be impossible to do significantly better.The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {334–342},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968863,
author = {Raiko, Tapani and Yao, Li and Cho, KyungHyun and Bengio, Yoshua},
title = {Iterative Neural Autoregressive Distribution Estimator (NADE-<i>k</i>)},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {325–333},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968862,
author = {Janoos, Firdaus and Denli, Huseyin and Subrahmanya, Niranjan},
title = {Multi-Scale Graphical Models for Spatio-Temporal Processes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc.. However, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection [17]. As we show, time-series graphical models based on vector auto-regressive processes[18] are inefficient in capturing such multi-scale structure. In this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems. We also propose algorithms to efficiently estimate the interaction structure from data. We demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {316–324},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968861,
author = {Ackerman, Margareta and Dasgupta, Sanjoy},
title = {Incremental Clustering: The Case for Extra Clusters},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {307–315},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968860,
author = {Silva, Ricardo and Evans, Robin},
title = {Causal Inference through a Witness Protection Program},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest "weak" paths in a unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of "path cancellations" that will imply conditional independencies but do not rule out the existence of confounding causal paths. The outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice to complement other default tools in observational studies.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {298–306},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968859,
author = {Ravanbakhsh, Siamak and Rabbany, Reihaneh and Greiner, Russell},
title = {Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing – for integral solutions in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a. community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with N3, demonstrating that augmentation is practical and efficient.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {289–297},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968858,
author = {Bareinboim, Elias and Pearl, Judea},
title = {Transportability from Multiple Environments with Limited Experiments: Completeness Results},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper addresses the problem of mz-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of mz-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the mz-transportability class.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {280–288},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968857,
author = {Hwang, Sung Ju and Sigal, Leonid},
title = {A Unified Semantic Embedding: Relating Taxonomies and Attributes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work, which only utilized them as side information, we explicitly embed these semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a supercategory + a sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a model with better generalization. This model also generates compact semantic description of each category, which enhances interoperability and enables humans to analyze what has been learned.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {271–279},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968856,
author = {Yang, Tianbao and Jin, Rong},
title = {Extracting Certainty from Uncertainty: Transductive Pairwise Classification from Pairwise Similarities},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this work, we study the problem of transductive pairwise classification from pairwise similarities. The goal of transductive pairwise classification from pair-wise similarities is to infer the pairwise class relationships, to which we refer as pairwise labels, between all examples given a subset of class relationships for a small set of examples, to which we refer as labeled examples. We propose a very simple yet effective algorithm that consists of two simple steps: the first step is to complete the sub-matrix corresponding to the labeled examples and the second step is to reconstruct the label matrix from the completed sub-matrix and the provided similarity matrix. Our analysis exhibits that under several mild preconditions we can recover the label matrix with a small error, if the top eigen-space that corresponds to the largest eigenvalues of the similarity matrix covers well the column space of label matrix and is subject to a low coherence, and the number of observed pairwise labels is sufficiently enough. We demonstrate the effectiveness of the proposed algorithm by several experiments.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {262–270},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968855,
author = {Feng, Jiashi and Xu, Huan and Mannor, Shie and Yan, Shuicheng},
title = {Robust Logistic Regression and Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {253–261},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968854,
author = {Djolonga, Josip and Krause, Andreas},
title = {From MAP to Marginals: Variational Inference in Bayesian Submodular Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-FIELD, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {244–252},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968853,
author = {Chang, Jason and III, John W. Fisher},
title = {Parallel Sampling of HDPs Using Sub-Cluster Splits},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a sampling technique for Hierarchical Dirichlet process models. The parallel algorithm builds upon [1] by proposing large split and merge moves based on learned sub-clusters. The additional global split and merge moves drastically improve convergence in the experimental results. Furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {235–243},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968852,
author = {Zoran, Daniel and Krishnan, Dilip and Bento, Jose and Freeman, William T.},
title = {Shape and Illumination from Shading Using the Generic Viewpoint Assumption},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Generic Viewpoint Assumption (GVA) states that the position of the viewer or the light in a scene is not special. Thus, any estimated parameters from an observation should be stable under small perturbations such as object, viewpoint or light positions. The GVA has been analyzed and quantified in previous works, but has not been put to practical use in actual vision tasks. In this paper, we show how to utilize the GVA to estimate shape and illumination from a single shading image, without the use of other priors. We propose a novel linearized Spherical Harmonics (SH) shading model which enables us to obtain a computationally efficient form of the GVA term. Together with a data term, we build a model whose unknowns are shape and SH illumination. The model parameters are estimated using the Alternating Direction Method of Multipliers embedded in a multi-scale estimation framework. In this prior-free framework, we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions, requiring fewer assumptions than competing methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {226–234},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968851,
author = {Zhu, Zhenyao and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
title = {Multi-View Perceptron: A Deep Model for Learning Face Identity and View Representations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Various factors, such as identity, view, and illumination, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of primate brain. Recent studies [5, 19] discovered that primate brain has a face-processing network, where view and identity are processed by different neurons. Taking into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and in the meanwhile infer a full spectrum of multi-view images, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {217–225},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968850,
author = {Feyereisl, Jan and Kwak, Suha and Son, Jeany and Han, Bohyung},
title = {Object Localization Based on Structural SVM Using Privileged Information},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a structured prediction algorithm for object localization based on Support Vector Machines (SVMs) using privileged information. Privileged information provides useful high-level knowledge for image understanding and facilitates learning a reliable model even with a small number of training examples. In our setting, we assume that such information is available only at training time since it may be difficult to obtain from visual data accurately without human supervision. Our goal is to improve performance by incorporating privileged information into ordinary learning framework and adjusting model parameters for better generalization. We tackle object localization problem based on a novel structural SVM using privileged information, where an alternating loss-augmented inference procedure is employed to handle the term in the objective function corresponding to privileged information. We apply the proposed algorithm to the Caltech-UCSD Birds 200-2011 dataset, and obtain encouraging results suggesting further investigation into the benefit of privileged information in structured prediction.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {208–216},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968849,
author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
title = {Stochastic Multi-Armed-Bandit Problem with Non-Stationary Rewards},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward "variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {199–207},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968848,
author = {Patrini, Giorgio and Nock, Richard and Rivera, Paul and Caetano, Tiberio},
title = {(Almost) No Label No Cry},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to ≈300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {190–198},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968847,
author = {Wang, Huahua and Banerjee, Arindam and Luo, Zhi-Quan},
title = {Parallel Direction Method of Multipliers},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of minimizing block-separable (non-smooth) convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of ADMM to multiple blocks is still unclear. In this paper, we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve optimization problems with multi-block linear constraints. At each iteration, PDMM randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {181–189},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968846,
author = {Qin, Danfeng and Chen, Xuanli and Guillaumin, Matthieu and Gool, Luc Van},
title = {Quantized Kernel Learning for Feature Matching},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability.In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {172–180},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968845,
author = {Shamir, Ohad},
title = {Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {163–171},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968844,
author = {Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},
title = {Just-in-Time Learning for Fast and Flexible Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {154–162},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968843,
author = {Yang, Yingzhen and Liang, Feng and Yan, Shuicheng and Wang, Zhangyang and Huang, Thomas S.},
title = {On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {145–153},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968842,
author = {Lee, Jason D. and Taylor, Jonathan E.},
title = {Exact Post Model Selection Inference for Marginal Screening},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected ("condition on selection" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+Lasso.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {136–144},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968841,
author = {Liu, Han and Wang, Lie and Zhao, Tuo},
title = {Multivariate Regression with Calibration},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity O(1/ ∈), where ∈ is a pre-specified numerical accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {127–135},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968840,
author = {Pan, Xinghao and Jegelka, Stefanie and Gonzalez, Joseph and Bradley, Joseph and Jordan, Michael I.},
title = {Parallel Double Greedy Submodular Maximization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many machine learning problems can be reduced to the maximization of sub-modular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results [1] only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. [2] and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the tradeoff space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {118–126},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968839,
author = {Kumar, M. Pawan},
title = {Rounding-Based Moves for Metric Labeling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {109–117},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968838,
author = {Wager, Stefan and Fithian, William and Wang, Sida and Liang, Percy},
title = {Altitude Training: Strong Bounds for Single-Layer Dropout},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {100–108},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968837,
author = {Lu, Yichao and Foster, Dean P.},
title = {Large Scale Canonical Correlation Analysis with Iterative Least Squares},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA , a iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {91–99},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968836,
author = {Felzenszwalb, Pedro F. and Oberlin, John G.},
title = {Multiscale Fields of Patterns},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {82–90},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968835,
author = {Osogami, Takayuki and Otsuka, Makoto},
title = {Restricted Boltzmann Machines Modeling Human Choice},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans. These phenomena include the similarity effect, the attraction effect, and the compromise effect. We formally quantify the strength of these phenomena that can be represented by our choice model, which illuminates the flexibility of our choice model. We then show that our choice model can be represented as a restricted Boltzmann machine and that its parameters can be learned effectively from data. Our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {73–81},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968834,
author = {Diego, Ferran and Hamprecht, Fred A.},
title = {Sparse Space-Time Deconvolution for Calcium Image Analysis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a unified formulation and algorithm to find an extremely sparse representation for Calcium image sequences in terms of cell locations, cell shapes, spike timings and impulse responses. Solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art, without the need for heuristic pre- or postprocessing. Experiments on real and synthetic data demonstrate the viability of the proposed method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {64–72},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968833,
author = {Fragkiadaki, Katerina and Salas, Marta and Arbel\'{a}ez, Pablo and Malik, Jitendra},
title = {Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation "leaking", optical flow "bleeding" etc. In this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and NRSfM for 3D reconstruction of objects in videos. For each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3D shape under different rank bounds of the trajectory matrix. We show that dense 3D shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera. We achieve competitive results on a public NRSfM benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches. We further test our approach on popular video segmentation datasets. To the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in Youtube or Hollywood movies, without object-specific priors.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {55–63},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968832,
author = {Xiong, Yuanjun and Liu, Wei and Zhao, Deli and Tang, Xiaoou},
title = {Zeta Hull Pursuits: Learning Nonconvex Data Hulls},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {46–54},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968831,
author = {Liu, Anqi and Ziebart, Brian D.},
title = {Robust Classification under Sample Selection Bias},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample reweighted empirical loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for learning a robust bias-aware (RBA) probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on binary classification tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {37–45},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968830,
author = {Yerebakan, Halid Z. and Rajwa, Bartek and Dundar, Murat},
title = {The Infinite Mixture of Infinite Gaussian Mixtures},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Dirichlet process mixture of Gaussians (DPMG) has been used in the literature for clustering and density estimation problems. However, many real-world data exhibit cluster distributions that cannot be captured by a single Gaussian. Modeling such data sets by DPMG creates several extraneous clusters even when clusters are relatively well-defined. Herein, we present the infinite mixture of infinite Gaussian mixtures (I2GMM) for more flexible modeling of data sets with skewed and multi-modal cluster distributions. Instead of using a single Gaussian for each cluster as in the standard DPMG model, the generative model of I2GMM uses a single DPMG for each cluster. The individual DPMGs are linked together through centering of their base distributions at the atoms of a higher level DP prior. Inference is performed by a collapsed Gibbs sampler that also enables partial parallelization. Experimental results on several artificial and real-world data sets suggest the proposed I2GMM model can predict clusters more accurately than existing variational Bayes and Gibbs sampler versions of DPMG.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {28–36},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968829,
author = {Li, Mu and Andersen, David G. and Smola, Alexander and Yu, Kai},
title = {Communication Efficient Distributed Machine Learning with the Parameter Server},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from ℓ1 -regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {19–27},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968828,
author = {Zhang, Yichuan and Sutton, Charles},
title = {Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparam-eters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {10–18},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2968826.2968827,
author = {Muandet, Krikamol and Sriperumbudur, Bharath and Sch\"{o}lkopf, Bernhard},
title = {Kernel Mean Estimation via Spectral Filtering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing "better" estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1–9},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@proceedings{10.5555/2968826,
title = {NIPS'14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
location = {Montreal, Canada}
}

