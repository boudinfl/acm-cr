@inproceedings{10.5555/2999325.2999503,
author = {Zhang, Chao and Zhang, Lei and Ye, Jieping},
title = {Generalization Bounds for Domain Adaptation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we provide a new framework to study the generalization bound of the learning process for domain adaptation. We consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data. In particular, we use the integral probability metric to measure the difference between two domains. Then, we develop the specific Hoeffding-type deviation inequality and symmetrization inequality for either kind of domain adaptation to achieve the corresponding generalization bound based on the uniform entropy number. By using the resultant generalization bound, we analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process. The numerical experiments support our results.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3320–3328},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999502,
author = {Swersky, Kevin and Tarlow, Daniel and Sutskever, Ilya and Salakhutdinov, Ruslan and Zemel, Richard S. and Adams, Ryan P.},
title = {Cardinality Restricted Boltzmann Machines},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Restricted Boltzmann Machine (RBM) is a popular density model that is also good for extracting features. A main source of tractability in RBM models is that, given an input, the posterior distribution over hidden variables is factorizable and can be easily computed and sampled from. Sparsity and competition in the hidden representation is beneficial, and while an RBM with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are typically not added, as the resulting posterior over the hidden units seemingly becomes intractable. In this paper we show that a dynamic programming algorithm can be used to implement exact sparsity in the RBM's hidden units. We also show how to pass derivatives through the resulting posterior marginals, which makes it possible to fine-tune a pre-trained neural network with sparse hidden layers.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3293–3301},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999501,
author = {Ralaivola, Liva},
title = {Confusion-Based Online Learning and a Passive-Aggressive Scheme},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper provides the first—to the best of our knowledge—analysis of online learning algorithms for multiclass problems when the confusion matrix is taken as a performance measure. The work builds upon recent and elegant results on noncommutative concentration inequalities, i.e. concentration inequalities that apply to matrices, and, more precisely, to matrix martingales. We do establish generalization bounds for online learning algorithms and show how the theoretical study motivates the proposition of a new confusion-friendly learning procedure. This learning algorithm, called COPA (for COnfusion Passive-Aggressive) is a passive-aggressive learning algorithm; it is shown that the update equations for COPA can be computed analytically and, henceforth, there is no need to recourse to any optimization package to implement it.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3284–3292},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999500,
author = {Sani, Amir and Lazaric, Alessandro and Munos, R\'{e}mi},
title = {Risk-Aversion in Multi-Armed Bandits},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we define two algorithms, investigate their theoretical guarantees, and report preliminary empirical results.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3275–3283},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999499,
author = {Frongillo, Rafael M. and Penna, Nicolas Delia and Reid, Mark D.},
title = {Interpreting Prediction Markets: A Stochastic Approach},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution. This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved. In particular, we show that under certain conditions the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis. Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3266–3274},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999498,
author = {Acharya, Jayadev and Das, Hirakendu and Orlitsky, Alon},
title = {Tight Bounds on Profile Redundancy and Distinguishability},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The minimax KL-divergence of any distribution from all distributions in a collection P has several practical implications. In compression, it is called redundancy and represents the least additional number of bits over the entropy needed to encode the output of any distribution in undefined. In online estimation and learning, it is the lowest expected log-loss regret when guessing a sequence of random values generated by a distribution in undefined. In hypothesis testing, it upper bounds the largest number of distinguishable distributions in undefined. Motivated by problems ranging from population estimation to text classification and speech recognition, several machine-learning and information-theory researchers have recently considered label-invariant observations and properties induced by i.i.d. distributions. A sufficient statistic for all these properties is the data's profile, the multiset of the number of times each data element appears. Improving on a sequence of previous works, we show that the redundancy of the collection of distributions induced over profiles by length-n i.i.d. sequences is between 0.3 · n1/3 and n1/3 log2 n, in particular, establishing its exact growth power.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3257–3265},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999497,
author = {Trevizan, Felipe W. and Veloso, Manuela M.},
title = {Trajectory-Based Short-Sighted Probabilistic Planning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3248–3256},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999496,
author = {Gens, Robert and Domingos, Pedro},
title = {Discriminative Learning of Sum-Product Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sum-product networks are a new deep architecture that can perform fast, exact inference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the first discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an efficient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably using "hard" gradient descent, where marginal inference is replaced by MPE inference (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classification tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architecture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3239–3247},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999495,
author = {Der, Matthew and Saul, Lawrence K.},
title = {Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model's continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model's main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3230–3238},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999494,
author = {Coudron, Matthew and Lerman, Gilad},
title = {On the Sample Complexity of Robust PCA},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We estimate the rate of convergence and sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix. This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA). Our model assumes a sub-Gaussian underlying distribution and an i.i.d. sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d. sample of size N is of order O(N-0.5+ε) for arbitrarily small ε &gt; 0 (affecting the probabilistic estimate); this rate of convergence is close to the one of direct covariance estimation, i.e., O(N-0.5). Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is O(D2+δ) for arbitrarily small δ &gt; 0 (whereas the sample complexity of direct covariance estimation with Frobenius norm is O(D2)). These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3221–3229},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999493,
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
title = {Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3212–3220},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999492,
author = {Zou, Will Y. and Zhu, Shenghuo and Ng, Andrew Y. and Yu, Kai},
title = {Deep Learning of Invariant Features via Simulated Fixations in Video},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We apply salient feature detection and tracking in videos to simulate fixations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classification accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3203–3211},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999491,
author = {Zhang, Yichuan and Sutton, Charles and Storkey, Amos and Ghahramani, Zoubin},
title = {Continuous Relaxations for Discrete Hamiltonian Monte Carlo},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3194–3202},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999490,
author = {Rai, Piyush and Kumar, Abhishek and III, Hal Daum\'{e}},
title = {Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multiple-output regression models require estimating multiple parameters, one for each output. Structural regularization is usually employed to improve parameter estimation in such models. In this paper, we present a multiple-output regression model that leverages the covariance structure of the latent model parameters as well as the conditional covariance structure of the observed outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike some of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3185–3193},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999489,
author = {Li, Ping and Zhang, Cun-Hui},
title = {Entropy Estimations Using Correlated Symmetric Stable Random Projections},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Methods for efficiently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to well approximate the Shannon entropy using small storage.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3176–3184},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999488,
author = {Jenatton, Rodolphe and Roux, Nicolas Le and Bordes, Antoine and Obozinski, Guillaume},
title = {A Latent Factor Model for Highly Multi-Relational Data},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3167–3175},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999487,
author = {Jiang, Ke and Kulis, Brian and Jordan, Michael I.},
title = {Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulfilled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the k-means and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3158–3166},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999486,
author = {He, He and Daum\'{e}, Hal and Eisner, Jason},
title = {Imitation Learning by Coaching},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Imitation Learning has been shown to be successful in solving many challenging real-world problems. Some recent approaches give strong performance guarantees by training the policy iteratively. However, it is important to note that these guarantees depend on how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space, we may fail to find a policy that has low error on the training set. In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle. By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle. We apply our algorithm to cost-sensitive dynamic feature selection, a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic feature selection and two static feature selection methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3149–3157},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999485,
author = {Khan, Mohammad Emtiyaz and Mohamed, Shakir and Murphy, Kevin P.},
title = {Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new variational inference algorithm for Gaussian process regression with non-conjugate likelihood functions, with application to a wide array of problems including binary and multi-class classification, and ordinal regression. Our method constructs a concave lower bound that is optimized using an efficient fixed-point updating algorithm. We show that the new algorithm has highly competitive computational complexity, matching that of alternative approximate inference methods. We also prove that the use of concave variational bounds provides stable and guaranteed convergence - a property not available to other approaches. We show empirically for both binary and multi-class classification that our new algorithm converges much faster than existing variational methods, and without any degradation in performance.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3140–3148},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999484,
author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
title = {Risk Aversion in Markov Decision Processes via Near-Optimal Chernoff Bounds},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The expected return is a widely used objective in decision making under uncertainty. Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize. We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded. Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3131–3139},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999483,
author = {Yang, Shulin and Bo, Liefeng and Wang, Jue and Shapiro, Linda},
title = {Unsupervised Template Learning for Fine-Grained Object Recognition},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fine-grained recognition refers to a subordinate level of recognition, such as recognizing different species of animals and plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape and structure shared cross different categories, and the differences are in the details of object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the cooccurrence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3122–3130},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999482,
author = {Li, Ping and Owen, Art B and Zhang, Cun-Hui},
title = {One Permutation Hashing},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Minwise hashing is a standard procedure in the context of search, for efficiently estimating set similarities in massive binary data such as text. Recently, 6-bit minwise hashing has been applied to large-scale learning and sublinear time near-neighbor search. The major drawback of minwise hashing is the expensive preprocessing, as the method requires applying (e.g.,) k = 200 to 500 permutations on the data. This paper presents a simple solution called one permutation hashing. Conceptually, given a binary data matrix, we permute the columns once and divide the permuted columns evenly into k bins; and we store, for each data vector, the smallest nonzero location in each bin. The probability analysis illustrates that this one permutation scheme should perform similarly to the original (k-permutation) minwise hashing. Our experiments with training SVM and logistic regression confirm that one permutation hashing can achieve similar (or even better) accuracies compared to the k-permutation scheme. See more details in arXiv:1208.1259.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3113–3121},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999481,
author = {Vintch, Brett and Zaharia, Andrew D. and Movshon, J. Anthony and Simoncelli, Eero P.},
title = {Efficient and Direct Estimation of a Neural Subunit Model for Sensory Coding},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of spatially shifted linear filters. These filters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple filters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the filters reside rather than the filters themselves. Here, we assume a linear-nonlinear-linear-nonlinear (LN-LN) cascade model in which the first linear stage is a set of shifted ('convolutional') copies of a common filter, and the first nonlinear stage consists of rectifying scalar nonlinearities that are identical for all filter outputs. We refer to these initial LN elements as the 'subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model significantly outperforms STA and STC in terms of cross-validated accuracy and efficiency.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999480,
author = {Huang, Jonathan and Alexander, Daniel},
title = {Probabilistic Event Cascades for Alzheimer's Disease},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the Alzheimer's Disease Neuroimaging Initiative.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3095–3103},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999479,
author = {Khaleghi, Azadeh and Ryabko, Daniil},
title = {Locating Changes in Highly Dependent Data with Unknown Number of Change Points},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of multiple change point estimation is considered for sequences with unknown number of change points. A consistency framework is suggested that is suitable for highly dependent time-series, and an asymptotically consistent algorithm is proposed. In order for the consistency to be established the only assumption required is that the data is generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form. The theoretical results are complemented with experimental evaluations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3086–3094},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999478,
author = {Wu, Xiao-Ming and Li, Zhenguo and So, Anthony Man-Cho and Wright, John and Chang, Shih-Fu},
title = {Learning with Partially Absorbing Random Walks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel stochastic process that is with probability αi being absorbed at current state i, and with probability 1 – αi follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set S of low conductance will be mostly absorbed in S. Moreover, the absorption probabilities vary slowly inside S, while dropping sharply outside, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in retrieval and classification.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3077–3085},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999477,
author = {Kim, Dongho and Kim, Kee-Eung and Poupart, Pascal},
title = {Cost-Sensitive Exploration in Bayesian Reinforcement Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3068–3076},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999476,
author = {Beck, Jeff and Heller, Katherine and Pouget, Alexandre},
title = {Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be naturally implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb's rule and describe an extension of this work which allows us to deal with time varying and correlated latent causes.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3059–3067},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999475,
author = {Swersky, Kevin and Tarlow, Daniel and Adams, Ryan P. and Zemel, Richard S. and Frey, Brendan J.},
title = {Probabilistic <i>n</i>-Choose-<i>k</i> Models for Classification and Ranking},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In categorical data there is often structure in the number of variables that take on each label. For example, the total number of objects in an image and the number of highly relevant documents per query in web search both tend to follow a structured distribution. In this paper, we study a probabilistic model that explicitly includes a prior distribution over such counts, along with a count-conditional likelihood that defines probabilities over all subsets of a given size. When labels are binary and the prior over counts is a Poisson-Binomial distribution, a standard logistic regression model is recovered, but for other count distributions, such priors induce global dependencies and combinatorics that appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can be derived for more general forms of this model. We illustrate the utility of the formulation by exploring applications to multi-object classification, learning to rank, and top-K classification.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3050–3058},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999474,
author = {Abbott, Joshua T. and Austerweil, Joseph L. and Griffiths, Thomas L.},
title = {Human Memory Search as a Random Walk in a Semantic Network},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3041–3049},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999473,
author = {Karklin, Yan and Ekanadham, Chaitanya and Simoncelli, Eero P.},
title = {Hierarchical Spike Coding of Sound},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important first step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The first layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of first layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while fine-scale structure is captured by recurrent interactions within the first layer. When fit to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a significant improvement over standard methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3032–3040},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999472,
author = {Fiterau, Madalina and Dubrawski, Artur},
title = {Projection Retrieval for Classification},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many applications, classification systems often require human intervention in the loop. In such cases the decision process must be transparent and comprehensible, simultaneously requiring minimal assumptions on the underlying data distributions. To tackle this problem, we formulate an axis-aligned subspace-finding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct views to classify query points, and facilitates visual evaluation by users.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3023–3031},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999471,
author = {Meshi, Ofer and Jaakkola, Tommi and Globerson, Amir},
title = {Convergence Rate Analysis of MAP Coordinate Minimization Algorithms},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Finding maximum a posteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used. Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However, these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence. Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3014–3022},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999470,
author = {Ortega, Pedro A. and Grau-Moya, Jordi and Genewein, Tim and Balduzzi, David and Braun, Daniel A.},
title = {A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel Bayesian approach to solve stochastic optimization problems that involve finding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of first, doing inference over the function space and second, finding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. Given t observations of the function, the posterior can be evaluated efficiently in time O(t2) up to a multiplicative constant. Finally, we show how to apply our model to optimize a noisy, non-convex, high-dimensional objective function.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3005–3013},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999469,
author = {Zou, James Y. and Adams, Ryan P.},
title = {Priors for Diversity in Generative Latent Variable Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic latent variable models are one of the cornerstones of machine learning. They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference. Such models are useful for exploratory analysis and visualization, for building density models of data, and for providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions on internal parameters. For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-occurring words only appear in a small number of topics. In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the underlying i.i.d. prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function. Using a kernel between probability distributions, we are able to define a DPP on probability measures. We show how to perform MAP inference with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2996–3004},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999468,
author = {Palla, Konstantina and Knowles, David A. and Ghahramani, Zoubin},
title = {A Nonparametric Variable Clustering Model},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a simple clustering, of observed variables into highly correlated subsets. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. Our Dirichlet process variable clustering (DPVC) model can discover block-diagonal covariance structures in data. We evaluate our method on both synthetic and gene expression analysis problems.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2987–2995},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999467,
author = {Zhou, Yu and Bai, Xiang and Liu, Wenyu and Latecki, Longin Jan},
title = {Fusion with Diffusion for Robust Visual Tracking},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. If the edge weights are determined by a single similarity measure, then it hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In particular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is proposed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of objects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the diffusion on the TPG is the same as the diffusion process on each of the original graphs. Moreover, it is not necessary to explicitly construct the TPG in our framework. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t - 1 and target object candidates in frame t are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2978–2986},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999466,
author = {Boyles, Levi and Welling, Max},
title = {The Time-Marginalized Coalescent Prior for Hierarchical Clustering},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clustering. The prior is constructed by marginalizing out the time information of Kingman's coalescent, providing a prior over tree structures which we call the Time-Marginalized Coalescent (TMC). This allows for models which factorize the tree structure and times, providing two benefits: more flexible priors may be constructed and more efficient Gibbs type inference can be used. We demonstrate this on an example model for density estimation and show the TMC achieves competitive experimental results.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2969–2977},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999465,
author = {Luo, Dijun and Ding, Chris and Huang, Heng and Nie, Feiping},
title = {Forging the Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many graph-based machine learning and data mining approaches, the quality of the graph is critical. However, in real-world applications, especially in semi-supervised learning and unsupervised learning, the evaluation of the quality of a graph is often expensive and sometimes even impossible, due the cost or the unavailability of ground truth. In this paper, we proposed a robust approach with convex optimization to "forge" a graph: with an input of a graph, to learn a graph with higher quality. Our major concern is that an ideal graph shall satisfy all the following constraints: non-negative, symmetric, low rank, and positive semidefinite. We develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees. With only one non-sensitive parameter, our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings. As a preprocessing of graphs, our method has a wide range of potential applications machine learning and data mining.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2960–2968},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999464,
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
title = {Practical Bayesian Optimization of Machine Learning Algorithms},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2951–2959},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999463,
author = {Dalvi, Nilesh and Parameswaran, Aditya and Rastogi, Vibhor},
title = {Minimizing Uncertainty in Pipelines},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2942–2950},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999462,
author = {Iyer, Rishabh and Bilmes, Jeff},
title = {Submodular-Bregman and the Lov\'{a}Sz-Bregman Divergences with Applications},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, defined either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lovasz extension of a submodular function, which we call the Lov\'{a}sz-Bregman divergence, is a continuous extension of a submodular Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm defined through the submodular Bregman divergence provides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov\'{a}sz Bregman divergence is natural in clustering scenarios where ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efficient unlike other order based distance measures.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2933–2941},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999461,
author = {Yoon, Sejong and Pavlovic, Vladimir},
title = {Distributed Probabilistic Learning for Camera Networks with Missing Data},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed affine structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2924–2932},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999460,
author = {Karasev, Vasiliy and Chiuso, Alessandro and Soatto, Stefano},
title = {Controlled Recognition Bounds for Visual Learning and Exploration},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of "visual search" of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a "passive" agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an "omnipotent" agent, capable of infinite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2915–2923},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999459,
author = {Zhang, Xinhua and Yu, Yaoliang and Schuurmans, Dale},
title = {Accelerated Training for Matrix-Norm Regularization: A Boosting Approach},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees e accuracy within O(1 /ε) iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization—exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2906–2914},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999458,
author = {Bouchard-C\^{o}t\'{e}, Alexandre and Kirkpatrick, Bonnie},
title = {Bayesian Pedigree Analysis Using Measure Factorization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease. With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites. Some pedigrees number in the thousands of individuals. Meanwhile, analysis methods have remained limited to pedigrees of &lt; 100 individuals which limits analyses to many small independent pedigrees.Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited. This is because linkage analysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order. LODs are difficult to interpret and nontrivial to extend to consider interactions among sites. These developments and difficulties call for the creation of modern methods of pedigree analysis.Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models. We show that these disease models can be turned into accurate and computationally efficient estimators. The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models. This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2897–2905},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999457,
author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
title = {Fast Variational Inference in the Conjugate Exponential Family},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic inference using our bound.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2888–2896},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999456,
author = {Calderhead, Ben and Sustik, M\'{a}ty\'{a}s A.},
title = {Sparse Approximate Manifolds for Differential Geometric MCMC},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of ℓ1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust Student-t error model, for which the Expected Fisher Information is analytically intractable.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2879–2887},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999455,
author = {Herbster, Mark and Pasteris, Stephen and Vitale, Fabio},
title = {Online Sum-Product Computation over Trees},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of performing efficient sum-product computations in an online setting over a tree. A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field. Belief propagation can be used to solve this problem, but requires time linear in the size of the tree, and is therefore too slow in an online setting where we are continuously receiving new data and computing individual marginals. With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less. We accomplish this via a hierarchical covering structure that caches previous local sum-product computations. Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-productlike algorithm to efficiently compute marginals with respect to this cover; and iii) apply "i" and "ii" to find an efficient algorithm with a regret bound for the online allocation problem in a multi-task setting.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2870–2878},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999454,
author = {Kpotufe, Samory and Boularias, Abdeslam},
title = {Gradient Weights Help Nonparametric Regressors},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In regression problems over ℝd, the unknown function f often varies more in some coordinates than in others. We show that weighting each coordinate i with the estimated norm of the ith derivative of f is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and k-NN regressors. We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed estimator is efficiently learned online.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2861–2869},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999453,
author = {Elliott, Lloyd T. and Teh, Yee Whye},
title = {Scalable Imputation of Genetic Data with a Discrete Fragmentation-Coagulation Process},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of partitions. The partitions at consecutive locations in the genome are related by the splitting and merging of their clusters. Our model can be thought of as a discrete analogue of the continuous fragmentation-coagulation process [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining accuracies comparable to other state-of-the-art genotype imputation methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2852–2860},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999452,
author = {Cire\c{s}an, Dan C. and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J\"{u}rgen},
title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \texttimes{} 512 \texttimes{} 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2843–2851},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999451,
author = {Richard, Emile and Ga\"{\i}ffas, St\'{e}phane and Vayatis, Nicolas},
title = {Link Prediction in Graphs with Autoregressive Features},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2834–2842},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999450,
author = {Vinyals, Oriol and Jia, Yangqing and Deng, Li and Darrell, Trevor},
title = {Learning with Recursive Perceptual Representations},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous -often more complicated- methods on several vision and speech benchmarks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2825–2833},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999449,
author = {Mnih, Andriy and Teh, Yee Whye},
title = {Learning Label Trees for Probabilistic Modelling of Implicit Feedback},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2816–2824},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999448,
author = {Krafft, Peter and Moore, Juston and Desmarais, Bruce and Wallach, Hanna},
title = {Topic-Partitioned Multinetwork Embeddings},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—specifically, the discovery and visualization of topic-specific subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model's ability to discover and visualize topic-specific communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2807–2815},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999447,
author = {Ahmed, Amr and Ravi, Sujith and Narayanamurthy, Shravan M. and Smola, Alexander J.},
title = {FastEx: Hash Clustering with Exponential Families},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2798–2806},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999446,
author = {Mroueh, Youssef and Poggio, Tomaso and Rosasco, Lorenzo and Slotine, Jean-Jacques E.},
title = {Multiclass Learning with Simplex Coding},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows us to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, we develop a relaxation error analysis that avoids constraints on the considered hypotheses class. Moreover, using this setting we derive the first provably consistent regularized method with training/tuning complexity that is independent to the number of classes. We introduce tools from convex analysis that can be used beyond the scope of this paper.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2789–2797},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999445,
author = {Du, Nan and Song, Le and Smola, Alex and Yuan, Ming},
title = {Learning Networks of Heterogeneous Influence},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting future events. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we address the challenging problem of uncovering the hidden network only from the cascades. The structure discovery problem is complicated by the fact that the influence between networked entities is heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the transmission functions among networked entities.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2780–2788},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999444,
author = {Flint, Alex and Blaschko, Matthew B.},
title = {Perceptron Learning of SAT},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2771–2779},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999443,
author = {Ermon, Stefano and Gomes, Carla P. and Sabharwal, Ashish and Selman, Bart},
title = {Density Propagation and Improved Bounds on the Partition Function},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a probabilistic graphical model, its density of states is a distribution that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this distribution. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decomposition, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2762–2770},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999442,
author = {Gerrish, Sean M. and Blei, David M.},
title = {How They Vote: Issue-Adjusted Models of Legislative Behavior},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues. Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2753–2761},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999441,
author = {Babacan, S. Derin and Nakajima, Shinichi and Do, Minh N.},
title = {Probabilistic Low-Rank Subspace Clustering},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in subspace clustering and identifying outliers.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2744–2752},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999440,
author = {Gillenwater, Jennifer and Kulesza, Alex and Taskar, Ben},
title = {Near-Optimal MAP Inference for Determinantal Point Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Determinantal point processes (DPPs) have recently been proposed as computationally efficient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, finding the most likely configuration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization problem, which also arises in experimental design and sensor placement, involves finding the largest principal minor of a positive semidefinite matrix. Because the objective is log-submodular, greedy algorithms have been used in the past with some empirical success; however, these methods only give approximation guarantees in the special case of monotone objectives, which correspond to a restricted class of DPPs. In this paper we propose a new algorithm for approximating the MAP problem based on continuous techniques for submodular function maximization. Our method involves a novel continuous relaxation of the log-probability function, which, in contrast to the multilinear extension used for general submodular functions, can be evaluated and differentiated exactly and efficiently. We obtain a practical algorithm with a 1/4-approximation guarantee for a more general class of non-monotone DPPs; our algorithm also extends to MAP inference under complex polytope constraints, making it possible to combine DPPs with Markov random fields, weighted matchings, and other models. We demonstrate that our approach outperforms standard and recent methods on both synthetic and real-world data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2735–2743},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999439,
author = {Jun, Seong-Hwan and Wang, Liangliang and Bouchard-Cote, Alexandre},
title = {Entangled Monte Carlo},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel method for scalable parallelization of SMC algorithms, Entangled Monte Carlo simulation (EMC). EMC avoids the transmission of particles between nodes, and instead reconstructs them from the particle genealogy. In particular, we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation. We explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed. We demonstrate using examples from Bayesian phylogenetic that the computational gain from parallelization using EMC significantly outweighs the cost of particle reconstruction. The timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2726–2734},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999438,
author = {Furmston, Thomas and Barber, David},
title = {A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being popular methods in this field. In this article we provide a unifying perspective of these two algorithms by showing that their search-directions in the parameter space are closely related to the search-direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative optimisation method for Markov Decision Processes. We are able to show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2717–2725},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999437,
author = {Larochelle, Hugo and Lauly, Stanislas},
title = {A Neural Autoregressive Topic Model},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2708–2716},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999436,
author = {Bryant, Michael and Sudderth, Erik B.},
title = {Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is in-feasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2699–2707},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999435,
author = {Coates, Adam and Karpathy, Andrej and Ng, Andrew Y.},
title = {Emergence of Object-Selective Features in Unsupervised Feature Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we find that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to significant global distortions like large translations and scale.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2690–2798},
numpages = {109},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999434,
author = {Coates, Adam and Karpathy, Andrej and Ng, Andrew Y.},
title = {Emergence of Object-Selective Features in Unsupervised Feature Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we find that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to significant global distortions like large translations and scale.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2681–2689},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999433,
author = {Jamieson, Kevin G. and Nowak, Robert D. and Recht, Benjamin},
title = {Query Complexity of Derivative-Free Optimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper provides lower bounds on the convergence rate of Derivative Free Optimization (DFO) with noisy function evaluations, exposing a fundamental and unavoidable gap between the performance of algorithms with access to gradients and those with access to only function evaluations. However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions. A distinctive feature of the algorithm is that it uses only Boolean-valued function comparisons, rather than function evaluations. This makes the algorithm useful in an even wider range of applications, such as optimization based on paired comparisons from human subjects, for example. We also show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999432,
author = {Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
title = {A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2663–2671},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999431,
author = {Bach, Stephen H. and Broecheler, Matthias and Getoor, Lise and O'Leary, Dianne P.},
title = {Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic graphical models are powerful tools for analyzing constrained, continuous domains. However, finding most-probable explanations (MPEs) in these models can be computationally expensive. In this paper, we improve the scalability of MPE inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains. We derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art. We show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2654–2662},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999430,
author = {Kapoor, Ashish and Jain, Prateek and Viswanathan, Raajay},
title = {Multilabel Classification Using Bayesian Compressed Sensing},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2645–2653},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999429,
author = {Ibrahimi, Morteza and Javanmard, Adel and Roy, Benjamin Van},
title = {Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, for the average cost LQ problem, a regret bound of O(√T) was shown, apart form logarithmic factors. However, this bound scales exponentially with p, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that achieves a regret bound of O(p√T), apart from logarithmic factors. In particular, our algorithm has an average cost of (1 + ε) times the optimum cost after T = polylog(p)O(1/ε2). This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of ε times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2636–2644},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999428,
author = {Ba, Demba and Babadi, Behtash and Purdon, Patrick and Brown, Emery},
title = {Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ℓ<sub>1</sub>-Minimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$-1 are Sk-sparse (with Sk typically smaller than S1), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where Ak and ek denote the measurement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk—we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ exactly. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2627–2635},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999427,
author = {Becker, S. and Fadili, M. J.},
title = {A Quasi-Newton Proximal Splitting Method},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classification.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2618–2626},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999426,
author = {Deisenroth, Marc Peter and Mohamed, Shakir},
title = {Expectation Propagation in Gaussian Process Dynamical Systems},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Rich and complex time-series data, such as those generated from engineering systems, financial markets, videos, or neural recordings are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires flexible and accurate models. In this paper, we promote Gaussian process dynamical systems as a rich model class that is appropriate for such an analysis. We present a new approximate message-passing algorithm for Bayesian state estimation and inference in Gaussian process dynamical systems, a non-parametric probabilistic generalization of commonly used state-space models. We derive our message-passing algorithm using Expectation Propagation and provide a unifying perspective on message passing in general state-space models. We show that existing Gaussian filters and smoothers appear as special cases within our inference framework, and that these existing approaches can be improved upon using iterated message passing. Using both synthetic and real-world data, we demonstrate that iterated message passing can improve inference in a wide range of tasks in Bayesian state estimation, thus leading to improved predictions and more effective decision making.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2609–2617},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999425,
author = {Blundell, Charles and Heller, Katherine A. and Beck, Jeffrey M.},
title = {Modelling Reciprocating Relationships with Hawkes Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a Bayesian nonparametric model that discovers implicit social structure from interaction time-series data. Social groups are often formed implicitly, through actions among members of groups. Yet many models of social networks use explicitly declared relationships to infer social structure. We consider a particular class of Hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals. We then extend the Infinite Relational Model by using these reciprocating Hawkes processes to parameterise its edges, making events associated with edges co-dependent through time. Our model outperforms general, unstructured Hawkes processes as well as structured Poisson process-based models at predicting verbal and email turn-taking, and military conflicts among nations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2600–2608},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999424,
author = {Lindsten, Fredrik and Jordan, Michael I. and Sch\"{o}n, Thomas B.},
title = {Ancestor Sampling for Particle Gibbs},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2591–2599},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999423,
author = {Paul, Michael J. and Dredze, Mark},
title = {Factorial LDA: Sparse Multi-Dimensional Text Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2582–2590},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999422,
author = {Kedem, Dor and Tyree, Stephen and Weinberger, Kilian Q. and Sha, Fei and Lanckriet, Gert},
title = {Non-Linear Metric Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we introduce two novel metric learning algorithms, Χ2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and easy-to-use. The two approaches achieve this goal in fundamentally different ways: Χ2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear Χ2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of Χ2-LMNN, obtain best results in 19 out of 20 learning settings.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2573–2581},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999421,
author = {Pacheco, Jason L. and Sudderth, Erik B.},
title = {Minimization of Continuous Bethe Approximations: A Positive Variation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions. While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties, and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2564–2572},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999420,
author = {Nguyen, Trung Thanh and Silander, Tomi and Leong, Tze-Yun},
title = {Transferring Expectations in Model-Based Reinforcement Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predefined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2555–2563},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999419,
author = {Zhou, Mingyuan and Carin, Lawrence},
title = {Augment-and-Conquer Negative Binomial Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2546–2554},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999418,
author = {Liu, Han and Lafferty, John and Wasserman, Larry},
title = {Exponential Concentration for Mutual Information Estimation with Application to Forests},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2537–2545},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999417,
author = {Hansen, Toke Jansen and Mahoney, Michael W.},
title = {Semi-Supervised Eigenvectors for Locally-Biased Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many applications, one has side information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks "nearby" that pre-specified target region. Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities. In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner. We also provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2528–2536},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999416,
author = {Cohen, Shay B. and Collins, Michael},
title = {Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe an approach to speed-up inference with latent-variable PCFGs, which have been shown to be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature. We also describe an error bound for this approximation, which gives guarantees showing that if the underlying tensors are well approximated, then the probability distribution over trees will also be well approximated. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2519–2527},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999415,
author = {Ba, Amadou and Sinn, Mathieu and Goude, Yannig and Pompey, Pascal},
title = {Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes an efficient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) filter. In order to quickly track changes in the model and put more weight on recent data, the RLS filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit\'{e} de France (EDF). Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2510–2518},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999414,
author = {Cheng, Weiwei and H\"{u}llermeier, Eyke and Waegeman, Willem and Welker, Volkmar},
title = {Label Ranking with Partial Abstention Based on Thresholded Probabilistic Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2501–2509},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999413,
author = {Canas, Guillermo D. and Rosasco, Lorenzo A.},
title = {Learning Probability Measures with Respect to Optimal Transport Metrics},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of empirical to population measures, which, unlike existing bounds, are applicable to a wide class of measures.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2492–2500},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999412,
author = {Yu, Yaoliang and Asian, \"{O}zlem and Schuurmans, Dale},
title = {A Polynomial-Time Form of Robust Regression},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression—Variational M-estimation—that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2483–2491},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999411,
author = {Negahban, Sahand and Oh, Sewoong and Shah, Devavrat},
title = {Iterative Ranking from Pair-Wise Comparisons},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR's TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding 'scores' for each object (e.g. player's rating) is of interest to understanding the intensity of the preferences.In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1].},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2474–2482},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999410,
author = {Canas, Guillermo D. and Poggio, Tomaso and Rosasco, Lorenzo A.},
title = {Learning Manifolds with K-Means and k-Flats},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-flats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-flats, both the results and the mathematical tools are new.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2465–2473},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999409,
author = {Balduzzi, David and Besserve, Michel},
title = {Towards a Learning-Theoretic Analysis of Spike-Timing Dependent Plasticity},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2456–2464},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999408,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
title = {A Better Way to Pretrain Deep Boltzmann Machines},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2447–2455},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999407,
author = {Kamilov, Ulugbek S. and Rangan, Sundeep and Fletcher, Alyson K. and Unser, Michael},
title = {Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the estimation of an i.i.d. vector x ∈ ℝn from measurements y ∈ ℝm obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x. Our method can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2438–2446},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999406,
author = {Kadri, Hachem and Rakotomamonjy, Alain and Bach, Francis and Preux, Philippe},
title = {Multiple Operator-Valued Kernel Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an ℓr-norm constraint on the combination coefficients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2429–2437},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999405,
author = {Pilanci, Mert and El Ghaoui, Laurent and Chandrasekaran, Venkat},
title = {Recovery of Sparse Probability Measures via Convex Programming},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. The classical ℓ1 regularizer fails to promote sparsity on the probability simplex since ℓ1 norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on ℓ1 norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2420–2428},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999404,
author = {Gopal, Siddharth and Yang, Yiming and Bai, Bing and Niculescu-Mizil, Alexandru},
title = {Bayesian Models for Large-Scale Hierarchical Classification},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for large scale problems. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivariate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parameters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present variational algorithms for tractable posterior inference in these models, and provide a parallel implementation that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach and shows improved performance over the other state-of-the-art hierarchical methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2411–2419},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999403,
author = {Streeter, Matthew and McMahan, H. Brendan},
title = {No-Regret Algorithms for Unconstrained Online Convex Optimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is ℝn. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point undefined are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of undefined. In particular, regret with respect to undefined = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2402–2410},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999402,
author = {Kalogeratos, Argyris and Likas, Aristidis},
title = {Dip-Means: An Incremental Clustering Method for Estimating the Number of Clusters},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that can be used as a wrapper around any iterative clustering algorithm of k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as an individual 'viewer' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of distances between the viewer and the cluster members. Important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2393–2401},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999401,
author = {Schwing, Alexander G. and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},
title = {Globally Convergent Dual MAP LP Relaxation Solvers Using Fenchel-Young Margins},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as they are not globally convergent. In this work we propose to augment these algorithms with an ε-descent approach and present a method to efficiently optimize for a descent direction in the sub-differential using a margin-based formulation of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interaction problems and show that our approach outperforms state-of-the-art solvers.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2384–2392},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999400,
author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur and Sachdeva, Sushant},
title = {Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form y = Ax + η where A is an unknown n \texttimes{} n matrix and x is a random variable whose components are independent and have a fourth moment strictly less than that of a standard Gaussian random variable and η is an n-dimensional Gaussian random variable with unknown covariance ∑: We give an algorithm that provable recovers A and ∑ up to an additive ε and whose running time and sample complexity are polynomial in n and 1/ε. To accomplish this, we introduce a novel "quasi-whitening" step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of A one by one via local search.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2375–2383},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999399,
author = {He, Yunlong and Qi, Yanjun and Kavukcuoglu, Koray and Park, Haesun},
title = {Learning the Dependency Structure of Latent Factors},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classification performance.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2366–2374},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999398,
author = {Ueno, Tsuyoshi and Hayashi, Kohei and Washio, Takashi and Kawahara, Yoshinobu},
title = {Weighted Likelihood Policy Search with Model Selection},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Reinforcement learning (RL) methods based on direct policy search (DPS) have been actively discussed to achieve an efficient approach to complicated Markov decision processes (MDPs). Although they have brought much progress in practical applications of RL, there still remains an unsolved problem in DPS related to model selection for the policy. In this paper, we propose a novel DPS method, weighted likelihood policy search (WLPS), where a policy is efficiently learned through the weighted likelihood estimation. WLPS naturally connects DPS to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to DPS problems directly. Hence, by following the idea of the information criterion, we develop a new measurement for model comparison in DPS based on the weighted log-likelihood.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2357–2365},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999397,
author = {Park, Mijung and Pillow, Jonathan W.},
title = {Bayesian Active Learning with Localized Priors for Fast Receptive Field Characterization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Active learning methods can dramatically improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (RF). Bayesian active learning methods specify a posterior distribution over the RF given the data collected so far in the experiment, and select a stimulus on each time step that maximally reduces posterior uncertainty. However, existing methods tend to employ simple Gaussian priors over the RF and do not exploit uncertainty at the level of hyperparameters. Incorporating this uncertainty can substantially speed up active learning, particularly when RFs are smooth, sparse, or local in space and time. Here we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors. Our algorithm uses sequential Markov Chain Monte Carlo sampling ("particle filtering" with MCMC) to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion. The core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments. We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2348–2356},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999396,
author = {Hardt, Moritz and Ligett, Katrina and McSherry, Frank},
title = {A Simple and Practical Algorithm for Differentially Private Data Release},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2339–2347},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999395,
author = {Hsieh, Cho-Jui and Dhillon, Inderjit S. and Ravikumar, Pradeep and Banerjee, Arindam},
title = {A Divide-and-Conquer Procedure for Sparse Inverse Covariance Estimation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the composite log-determinant optimization problem, arising from the ℓ1- regularized Gaussian maximum likelihood estimator of a sparse inverse covariance matrix, in a high-dimensional setting with a very large number of variables. Recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field, even in very high-dimensional regimes with a limited number of samples. In this paper, we are concerned with the computational cost in solving the above optimization problem. Our proposed algorithm partitions the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. Our key idea for the divide step to obtain a sub-problem partition is as follows: we first derive a tractable bound on the quality of the approximate solution obtained from solving the corresponding sub-divided problems. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, in order to find effective partitions of the variables. For the conquer step, we use the approximate solution, i.e., solution resulting from solving the sub-problems, as an initial point to solve the original problem, and thereby achieve a much faster computational procedure.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2330–2338},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999394,
author = {Greenwald, Amy and Sodomka, Eric and Li, Jiacui},
title = {Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many large economic markets, goods are sold through sequential auctions. Examples include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. In this paper, we combine methods from game theory and decision theory to search for approximate equilibria in sequential auction domains, in which bidders do not know their opponents' values for goods, bidders only partially observe the actions of their opponents', and bidders demand multiple goods. We restrict attention to two-phased strategies: first predict (i.e., learn); second, optimize. We use best-reply dynamics [4] for prediction (i.e., to predict other bidders' strategies), and then assuming fixed other-bidder strategies, we estimate and solve the ensuing Markov decision processes (MDP) [18] for optimization. We exploit auction properties to represent the MDP in a more compact state space, and we use Monte Carlo simulation to make estimating the MDP tractable. We show how equilibria found using our search procedure compare to known equilibria for simpler auction domains, and we approximate an equilibrium for a more complex auction domain where analytical solutions are unknown.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2321–2329},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999393,
author = {Terashima, Hiroki and Okada, Masato},
title = {The Topographic Unsupervised Learning of Natural Sounds in the Auditory Cortex},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2312–2320},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999392,
author = {Srivastava, Nisheeth and Schrater, Paul R.},
title = {Rational Inference of Relative Preferences},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2303–2311},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999391,
author = {Volkovs, Maksims N. and Zemel, Richard S.},
title = {Collaborative Ranking with 17 Parameters},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on datasets from one item domain yield excellent results on a dataset from very different item domain, without any retraining.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2294–2302},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999390,
author = {Bourdoukan, Ralph and Barrett, David G. T. and Machens, Christian K. and Den\`{e}ve, Sophie},
title = {Learning Optimal Spike-Based Representations},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How can neural networks learn to represent information optimally? We answer this question by deriving spiking dynamics and learning dynamics directly from a measure of network performance. We find that a network of integrate-and-fire neurons undergoing Hebbian plasticity can learn an optimal spike-based representation for a linear decoder. The learning rule acts to minimise the membrane potential magnitude, which can be interpreted as a representation error after learning. In this way, learning reduces the representation error and drives the network into a robust, balanced regime. The network becomes balanced because small representation errors correspond to small membrane potentials, which in turn results from a balance of excitation and inhibition. The representation is robust because neurons become self-correcting, only spiking if the representation error exceeds a threshold. Altogether, these results suggest that several observed features of cortical dynamics, such as excitatory-inhibitory balance, integrate-and-fire dynamics and Hebbian plasticity, are signatures of a robust, optimal spike-based code.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2285–2293},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999389,
author = {Shelton, Jacquelyn A. and Sterne, Philip and Bornschein, Jorg and Sheikh, Abdul-Saboor and L\"{u}cke, J\"{o}rg},
title = {Why MCA? Nonlinear Sparse Coding with Spike-Andslab Prior for Neurally Plausible Image Encoding},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level image components. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA) instead of a linear combination. The major challenge is parameter optimization because a model with either (1) or (2) results in strongly multimodal posteriors. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posteriors. We design an exact piece-wise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric and bimodal activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, we find that the model predicts a high percentage of globular receptive fields alongside Gabor-like fields. Similarly high percentages are observed in vivo. Our results thus argue in favor of improvements of the standard sparse coding model for simple cells by using flexible priors and nonlinear combinations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2276–2284},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999388,
author = {Pereira, Francisco and Botvinick, Matthew},
title = {A Systematic Approach to Extracting Semantic Information from Functional MRI Data},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2267–2275},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999387,
author = {Kasiviswanathan, Shiva Prasad and Wang, Huahua and Banerjee, Arindam and Melville, Prem},
title = {Online ℓ<sub>1</sub>-Dictionary Learning with Application to Novel Document Detection},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online ℓ1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the ℓ1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online ℓ1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2258–2266},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999386,
author = {Gopalan, Prem and Mimno, David and Gerrish, Sean M. and Freedman, Michael J. and Blei, David M.},
title = {Scalable Inference of Overlapping Communities},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a scalable algorithm for posterior inference of overlapping communities in large networks. Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel (MMSB). It naturally interleaves subsampling the network with estimating its community structure. We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2249–2257},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999385,
author = {Foti, Nicholas J. and Williamson, Sinead A.},
title = {Slice Sampling Normalized Kernel-Weighted Completely Random Measure Mixture Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality. However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a large class of dependent nonparametric processes, including several existing models, and present a slice sampler that allows efficient inference across this class of models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2240–2248},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999384,
author = {Wang, Zuoguan and Lyu, Siwei and Schalk, Gerwin and Ji, Qiang},
title = {Learning with Target Prior},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter θ that maximizes the log likelihood of fθ (X) on a uncorresponded training set with regards to p(y). Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2231–2239},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999383,
author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
title = {Multimodal Learning with Deep Boltzmann Machines},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a unified representation that fuses modalities together. We find that this representation is useful for classification and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and filling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model significantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2222–2230},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999382,
author = {Bellemare, Marc G. and Veness, Joel and Bowling, Michael},
title = {Sketch-Based Linear Value Function Approximation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2213–2221},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999381,
author = {Chen, Yudong and Sanghavi, Sujay and Xu, Huan},
title = {Clustering Sparse Graphs},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a new algorithm to cluster sparse unweighted graphs - i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difficult to solve.Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be penalized differently. We analyze our algorithm's performance on the natural, classical and widely studied "planted partition" model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2204–2212},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999380,
author = {Zhou, Dengyong and Platt, John C. and Basu, Sumit and Mao, Yi},
title = {Learning from the Wisdom of Crowds by Minimax Entropy},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2195–2203},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999379,
author = {Challis, Edward and Barber, David},
title = {Affine Independent Variational Inference},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider inference in a broad class of non-conjugate probabilistic models based on minimising the Kullback-Leibler divergence between the given target density and an approximating 'variational' density. In particular, for generalised linear models we describe approximating densities formed from an affine transformation of independently distributed latent variables, this class including many well known densities as special cases. We show how all relevant quantities can be efficiently computed using the fast Fourier transform. This extends the known class of tractable variational approximations and enables the fitting for example of skew variational densities to the target density.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2186–2194},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999378,
author = {Boularias, Abdeslam and Kr\"{o}mer, Oliver and Peters, Jan},
title = {Algorithms for Learning Markov Field Policies},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We use a graphical model for representing policies in Markov Decision Processes. This new representation can easily incorporate domain knowledge in the form of a state similarity graph that loosely indicates which states are supposed to have similar optimal actions. A bias is then introduced into the policy search process by sampling policies from a distribution that assigns high probabilities to policies that agree with the provided state similarity graph, i.e. smoother policies. This distribution corresponds to a Markov Random Field. We also present forward and inverse reinforcement learning algorithms for learning such policy distributions. We illustrate the advantage of the proposed approach on two problems: cart-balancing with swing-up, and teaching a robot to grasp unknown objects.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2177–2185},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999377,
author = {Wang, Zhuo and Stocker, Alan A. and Lee, Daniel D.},
title = {Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum <i>L<sub>p</sub></i> Loss},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general Lp norm. We generalize the Cramer-Rao lower bound and show how the Lp loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables. In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing Lp loss in the limit as p goes to zero.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2168–2176},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999376,
author = {Balle, Borja and Mohri, Mehryar},
title = {Spectral Learning of General Weighted Automata via Constrained Matrix Completion},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many tasks in text and speech processing and computational biology require estimating functions mapping strings to real numbers. A broad class of such functions can be defined by weighted automata. Spectral methods based on the singular value decomposition of a Hankel matrix have been recently proposed for learning a probability distribution represented by a weighted automaton from a training sample drawn according to this same target distribution. In this paper, we show how spectral methods can be extended to the problem of learning a general weighted automaton from a sample generated by an arbitrary distribution. The main obstruction to this approach is that, in general, some entries of the Hankel matrix may be missing. We present a solution to this problem based on solving a constrained matrix completion problem. Combining these two ingredients, matrix completion and spectral method, a whole new family of algorithms for learning general weighted automata is obtained. We present generalization bounds for a particular algorithm in this family. The proofs rely on a joint stability analysis of matrix completion and spectral learning.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2159–2167},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999375,
author = {Mehta, Nishant A. and Lee, Dongryeol and Gray, Alexander G.},
title = {Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks. We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2150–2158},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999374,
author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
title = {Relax and Randomize: From Value to Algorithms},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones, also capturing such "unorthodox" methods as Follow the Perturbed Leader and the R2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a "random playout". New versions of the Follow-the-Perturbed-Leader algorithms are presented, as well as methods based on the Littlestone's dimension, efficient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2141–2149},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999373,
author = {Ho, Qirong and Yin, Junming and Xing, Eric P.},
title = {On Triangular versus Edge Representations — towards Scalable Modeling of Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we argue for representing networks as a bag of triangular motifs, particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations. Such approaches require both 1-edges and 0-edges (missing edges) to be provided as input, and as a consequence, approximate inference algorithms for these models usually require Ω(N2) time per iteration, precluding their application to larger real-world networks. In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality. A triangular motif is a vertex triple containing 2 or 3 edges, and the number of such motifs is Θ(∑i D2i) (where Di is the degree of vertex i), which is much smaller than N2 for low-maximum-degree networks. Using this representation, we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networks with high maximum degree, the triangular motifs can be naturally subsampled in a node-centric fashion, allowing for much faster inference at a small cost in accuracy. Empirically, we demonstrate that our approach, when compared to that of an edge-based model, has faster runtime and improved accuracy for mixed-membership community detection. We conclude with a large-scale demonstration on an N ≈ 280,000-node network, which is infeasible for network models with Ω(N2) inference cost.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2132–2140},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999372,
author = {Shenoy, Pradeep and Yu, Angela J.},
title = {Strategic Impatience in Go/NoGo versus Forced-Choice Decision-Making},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by eliminating the need for response selection as in 2AFC, a consistent tendency for subjects to make more Go responses (both higher hits and false alarm rates) in the GNG task raises the concern that there may be fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1, 2]) and the related leaky competing accumulator models [3, 4], capture various aspects of behavioral performance, but do not clarify the provenance of the Go bias in GNG. We postulate that this "impatience" to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of the 2AFC and GNG tasks: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes not only error rate but also average decision delay naturally exhibits the experimentally observed Go bias. The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again just before the response deadline. The initial rise in the threshold is due to the diminishing temporal advantage of choosing the fast Go response compared to the fixed-delay NoGo response. We also show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such fixed-threshold approximations cannot reproduce the Go bias. Our results suggest that observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and thus need not imply any other difference in the underlying sensory and cognitive processes.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2123–2131},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999371,
author = {Fukumizu, Kenji and Leng, Chenlei},
title = {Gradient-Based Kernel Method for Feature Extraction and Variable Selection},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method. In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets. Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. [2]. Experimental results show that the proposed methods successfully find effective features and variables without parametric models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2114–2122},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999370,
author = {Giesen, Joachim and Laue, Soren and Mueller, Jens K. and Swiercy, Sascha},
title = {Approximating Concavely Parameterized Optimization Problems},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy ε &gt; 0 by a set of size O(1/√ε). A lower bound of size Ω(1/√ε) shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size O(1/√ε). Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2105–2113},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999369,
author = {Houlsby, Neil and Hern\'{a}ndez-Lobato, Jose Miguel and Husz\'{a}r, Ferenc and Ghahramani, Zoubin},
title = {Collaborative Gaussian Processes for Preference Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new model based on Gaussian processes (GPs) for learning pair-wise preferences expressed by multiple users. Inference is simplified by using a preference kernel for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2096–2104},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999368,
author = {Loh, Po-Ling and Wainwright, Martin J.},
title = {Structure Estimation for Discrete Graphical Models: Generalized Covariance Matrices and Their Inverses},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate a curious relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results that have previously been established only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the significance of the inverse covariance matrix of a non-Gaussian distribution. Based on our population-level results, we show how the graphical Lasso may be used to recover the edge structure of certain classes of discrete graphical models, and present simulations to verify our theoretical results.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2087–2095},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999367,
author = {Ramaswamy, Harish G. and Agarwal, Shivani},
title = {Classification Calibration Dimension for General Multiclass Losses},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of classification calibration dimension of a multiclass loss matrix, which measures the smallest 'size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al. (2010) for analyzing the difficulty of designing 'low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999366,
author = {Chen, Katherine and Bowling, Michael},
title = {Tractable Objectives for Robust Policy Optimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance. One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations. In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty. Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated. We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2069–2077},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999365,
author = {Ryabko, Daniil and Mary, J\'{e}r\'{e}mie},
title = {Reducing Statistical Time-Series Problems to Binary Classification},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show how binary classification methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2060–2068},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999364,
author = {Caron, Fran\c{c}ois},
title = {Bayesian Nonparametric Models for Bipartite Graphs},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, a generative process for network growth, and a simple Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2051–2059},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999363,
author = {El-Yaniv, Ran and Wiener, Yair},
title = {Pointwise Tracking the Optimal Regression Function},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper examines the possibility of a 'reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn 'selective' regressors that can ε-pointwise track the best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain. Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2042–2050},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999362,
author = {Dennis, Aaron and Ventura, Dan},
title = {Learning the Architecture of Sum-Product Networks Using Clustering on Variables},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2033–2041},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999361,
author = {Hauberg, S\o{}ren and Freifeld, Oren and Black, Michael J.},
title = {A Geometric Take on Metric Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2024–2032},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999360,
author = {Archer, Evan and Park, Il Memming and Pillow, Jonathan W.},
title = {Bayesian Estimation of Discrete Entropy with Mixtures of Stick-Breaking Priors},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of estimating Shannon's entropy H in the under-sampled regime, where the number of possible symbols may be unknown or countably infinite. Dirichlet and Pitman-Yor processes provide tractable prior distributions over the space of countably infinite discrete distributions, and have found major applications in Bayesian non-parametric statistics and machine learning. Here we show that they provide natural priors for Bayesian entropy estimation, due to the analytic tractability of the moments of the induced posterior distribution over entropy H. We derive formulas for the posterior mean and variance of H given data. However, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior on H, meaning the prior strongly determines the estimate in the under-sampled regime. We therefore define a family of continuous mixing measures such that the resulting mixture of Dirichlet or Pitman-Yor processes produces an approximately flat prior over H. We explore the theoretical properties of the resulting estimators and show that they perform well on data sampled from both exponential and power-law tailed distributions.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2015–2023},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999359,
author = {Park, Hyunsin and Yun, Sungrack and Park, Sanghyuk and Kim, Jongmin and Yoo, Chang D.},
title = {Phoneme Classification Using Constrained Variational Gaussian Process Dynamical System},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For phoneme classification, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and non-parametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classification performance, on the synthetic and benchmark datasets.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2006–2014},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999358,
author = {Ghosh, Soumya and Sudderth, Erik B. and Loper, Matthew and Black, Michael J.},
title = {From Deformations to Parts: Motion-Based Segmentation of 3D Objects},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a method for discovering the parts of an articulated object from aligned meshes of the object in various three-dimensional poses. We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation. To allow analysis of datasets in which object instances have varying 3D shapes, we model part variability across poses via affine transformations. By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty. Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better deformation predictions than conventional clustering methods.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1997–2005},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999357,
author = {Gong, Pinghua and Ye, Jieping and Zhang, Changshui},
title = {Multi-Stage Multi-Task Feature Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a MultiStage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1988–1996},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999356,
author = {Druckmann, Shaul and Hu, Tao and Chklovskii, Dmitri B.},
title = {A Mechanistic Model of Early Sensory Processing Based on Subtracting Sparse Representations},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1979–1987},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999355,
author = {Liu, Xianghang and Petterson, James and Caetano, Tiberio S.},
title = {Learning as MAP Inference in Discrete Graphical Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new formulation for binary classification. Instead of relying on convex losses and regularizers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but discrete formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex approaches, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for direct regularization through cardinality-based penalties, such as the ℓ0 pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1970–1978},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999354,
author = {Maillard, Odalric Ambrym and Carpentier, Alexandra},
title = {Online Allocation and Homogeneous Partitioning for Piecewise Constant Mean-Approximation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the setting of active learning for the multi-armed bandit, where the goal of a learner is to estimate with equal precision the mean of a finite number of arms, recent results show that it is possible to derive strategies based on finite-time confidence bounds that are competitive with the best possible strategy. We here consider an extension of this problem to the case when the arms are the cells of a finite partition P of a continuous sampling space Χ ⊂ ℝd. Our goal is now to build a piecewise constant approximation of a noisy function (where each piece is one region of undefined and undefined is fixed beforehand) in order to maintain the local quadratic error of approximation on each cell equally low. Although this extension is not trivial, we show that a simple algorithm based on upper confidence bounds can be proved to be adaptive to the function itself in a near-optimal way, when |undefined| is chosen to be of minimax-optimal order on the class of α-H\"{o}lder functions.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1961–1969},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999353,
author = {Lyons, Simon M. J. and S\"{a}rkk\"{a}, Simo and Storkey, Amos J.},
title = {The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic differential equations (SDE) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes. Crucial to the process of using SDE to build mathematical models is the ability to estimate parameters of those models from observed data. Over the past few decades, significant progress has been made on this problem, but we are still far from having a definitive solution. We describe a novel method of approximating a diffusion process that we show to be useful in Markov chain Monte-Carlo (MCMC) inference algorithms. We take the 'white' noise that drives a diffusion process and decompose it into two terms. The first is a 'coloured noise' term that can be deterministically controlled by a set of auxilliary variables. The second term is small and enables us to form a linear Gaussian 'small noise' approximation. The decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by MCMC methods. We explain why many state-of-the-art inference methods fail on highly nonlinear inference problems, and we demonstrate experimentally that our method performs well in such situations. Our results show that this method is a promising new tool for use in inference and parameter estimation problems.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1952–1960},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999352,
author = {Tsianos, Konstantinos I. and Lawlor, Sean and Rabbat, Michael G.},
title = {Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value r which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a k-regular expander graph [1] yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on r. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1943–1951},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999351,
author = {Harel, Maayan and Mannor, Shie},
title = {The Perturbed Variation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new discrepancy score between two distributions that gives an indication on their similarity. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity; it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1934–1942},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999350,
author = {Noh, Yung-Kyun and Park, Frank Chongwoo and Lee, Daniel D.},
title = {Diffusion Decision Making for Adaptive <i>k</i>-Nearest Neighbor Classification},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification. We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation. By applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification. Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors. Experiments with both synthetic and real datasets demonstrate the effectiveness of our classification criteria.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1925–1933},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999349,
author = {Rey, Melanie and Roth, Volker},
title = {Meta-Gaussian Information Bottleneck},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1916–1924},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999348,
author = {Papai, Tivadar and Kautz, Henry and Stefankovic, Daniel},
title = {Slice Normalized Dynamic Markov Logic Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the size of the discretized time-domain typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efficient online inference, and can directly model influences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1907–1915},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999347,
author = {Pillow, Jonathan W. and Scott, James G.},
title = {Fully Bayesian Inference for Neural Models with Negative-Binomial Spiking},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses. The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability. Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals. This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models. We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1898–1906},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999346,
author = {Petralia, Francesca and Rao, Vinayak and Dunson, David B.},
title = {Repulsive Mixtures},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning. Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool. One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant. Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings. Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components. To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components. We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation. The methods are illustrated using synthetic examples and an iris data set.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1889–1897},
numpages = {9},
keywords = {repulsive point process, gaussian mixture model, bayesian nonparametrics, well separated mixture, model-based clustering, dirichlet process},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999345,
author = {Gibson, Richard and Burch, Neil and Lanctot, Marc and Szafron, Duane},
title = {Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a smaller, sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1880–1888},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999344,
author = {Rombouts, Jaldert O. and Bohte, Sander M. and Roelfsema, Pieter R.},
title = {Neurally Plausible Reinforcement Learning of Working Memory Tasks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: by learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity [1]. It is however not well known how such neurons acquire these task-relevant working memories. Here we introduce a biologically plausible learning scheme grounded in Reinforcement Learning (RL) theory [2] that explains how neurons become selective for relevant information by trial and error learning. The model has memory units which learn useful internal state representations to solve working memory tasks by transforming partially observable Markov decision problems (POMDP) into MDPs. We propose that synaptic plasticity is guided by a combination of attentional feedback signals from the action selection stage to earlier processing levels and a globally released neuromodulatory signal. Feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. The neuromodulatory signal interacts with tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to 1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks [1, 3, 4] and 2) learn to optimally integrate probabilistic evidence for perceptual decision making [5, 6].},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1871–1879},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999343,
author = {Freno, Antonino and Keller, Mikaela and Tommasi, Marc},
title = {Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1862–1870},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999342,
author = {Ruiz, Francisco J. R. and Valera, Isabel and Blanco, Carlos and Perez-Cruz, Fernando},
title = {Bayesian Nonparametric Modeling of Suicide Attempts},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1853–1861},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999341,
author = {Belanger, David and Passos, Alexandre and Riedel, Sebastian and McCallum, Andrew},
title = {MAP Inference in Chains Using Column Generation},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Linear chains and trees are basic building blocks in many applications of graphical models, and they admit simple exact maximum a-posteriori (MAP) inference algorithms based on message passing. However, in many cases this computation is prohibitively expensive, due to quadratic dependence on variables' domain sizes. The standard algorithms are inefficient because they compute scores for hypotheses for which there is strong negative local evidence. For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate results. This paper presents new exact inference algorithms based on the combination of column generation and pre-computed bounds on terms of the model's scoring function. While we do not improve worst-case performance, our method substantially speeds real-world, typical-case inference in chains and trees. Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task. Our algorithm is also extendable to new techniques for approximate inference, to faster 0/1 loss oracles, and new opportunities for connections between inference and learning. We encourage further exploration of high-level reasoning about the optimization problem implicit in dynamic programs.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1844–1852},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999340,
author = {Bohte, Sander M.},
title = {Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. Recent spiking neuron models like the adaptive Spike Response Model implement adaptation as additive fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation accurately models neural spiking behavior over a limited dynamic input range. To extend efficient coding over large changes in dynamic input range, we propose a multiplicative adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that, unlike the additive adaptation model, the firing rate in our multiplicative adaptation model saturates to a realistic maximum spike-rate regardless of input magnitude. Additionally, when simulating variance switching experiments, the model quantitatively fits experimental data over a wide dynamic range. Dynamic threshold models of adaptation furthermore suggest a straightforward interpretation of neural activity in terms of dynamic differential signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1835–1843},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999339,
author = {Scherrer, Bruno and Lesner, Boris},
title = {On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider infinite-horizon stationary γ-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error e at each iteration, it is well-known that one can compute stationary policies that are 2γ/(1 - γ)2 ε-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to 2γ/1-γ ε-optimal, which constitutes a significant improvement in the usual situation when γ is close to 1. Surprisingly, this shows that the problem of "computing near-optimal non-stationary policies" is much simpler than that of "computing near-optimal stationary policies".},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1826–1834},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999338,
author = {Pokorny, Florian T. and Ek, Carl Henrik and Kjellstr\"{o}m, Hedvig and Kragic, Danica},
title = {Persistent Homology for Learning Densities with Bounded Support},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel method for learning densities with bounded support which enables us to incorporate 'hard' topological constraints. In particular, we show how emerging techniques from computational algebraic topology and the notion of persistent homology can be combined with kernel-based methods from machine learning for the purpose of density estimation. The proposed formalism facilitates learning of models with bounded support in a principled way, and - by incorporating persistent homology techniques in our approach - we are able to encode algebraic-topological constraints which are not addressed in current state of the art probabilistic models. We study the behaviour of our method on two synthetic examples for various sample sizes and exemplify the benefits of the proposed approach on a real-world dataset by learning a motion model for a race car. We show how to learn a model which respects the underlying topological structure of the racetrack, constraining the trajectories of the car.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1817–1825},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999337,
author = {Sinn, Mathieu and Chen, Bei},
title = {Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied in [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1808–1816},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999336,
author = {Guzman-Rivera, Abner and Batra, Dhruv and Kohli, Pushmeet},
title = {Multiple Choice Learning: Learning to Produce Multiple Structured Outputs},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of generating multiple hypotheses for structured prediction tasks that involve interaction with users or successive components in a cascaded architecture. Given a set of multiple hypotheses, such components/users typically have the ability to retrieve the best (or approximately the best) solution in this set. The standard approach for handling such a scenario is to first learn a single-output model and then produce M-Best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we learn to produce multiple outputs by formulating this task as a multiple-output structured-output prediction problem with a loss-function that effectively captures the setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on image segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this type of scenario and leads to substantial improvements in prediction accuracy.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1799–1807},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999335,
author = {Lorbert, Alexander and Ramadge, Peter J.},
title = {Kernel Hyperalignment},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We report experiments using real-world, multi-subject fMRI data.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1790–1798},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999334,
author = {Ashton, Simon R F and Sollich, Peter},
title = {Learning Curves for Multi-Task Gaussian Process Regression},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the average case performance of multi-task Gaussian process (GP) regression as captured in the learning curve, i.e. the average Bayes error for a chosen task versus the total number of examples n for all tasks. For GP covariances that are the product of an input-dependent covariance function and a free-form intertask covariance matrix, we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks T. We use these to study the asymptotic learning behaviour for large n. Surprisingly, multi-task learning can be asymptotically essentially useless, in the sense that examples from other tasks help only when the degree of inter-task correlation, ρ, is near its maximal value ρ = 1. This effect is most extreme for learning of smooth target functions as described by e.g. squared exponential kernels. We also demonstrate that when learning many tasks, the learning curves separate into an initial phase, where the Bayes error on each task is reduced down to a plateau value by "collective learning" even though most tasks have not seen examples, and a final decay that occurs once the number of examples is proportional to the number of tasks.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1781–1789},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999333,
author = {Yi, Jinfeng and Jin, Rong and Jain, Anil K. and Jain, Shaili and Yang, Tianbao},
title = {Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called semi-crowdsourced clustering that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects and from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1772–1780},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999332,
author = {Ortner, Ronald and Ryabko, Daniil},
title = {Online Regret Bounds for Undiscounted Continuous Reinforcement Learning},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are H\"{o}lder continuity of rewards and transition probabilities.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1763–1771},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999331,
author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},
title = {Active Comparison of Prediction Models},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of comparing the risks of two given predictive models—for instance, a baseline model and a challenger—as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1754–1762},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999330,
author = {Wipf, David and Wu, Yi},
title = {Dual-Space Analysis of the Sparse Linear Model},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients. These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters. Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation. The underlying cost functions can be related via a dual-space framework from [22], which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space. This perspective is useful because some analyses or extensions are more conducive to development in one space or the other. Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit. As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I. In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II. For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular ℓ1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1745–1753},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999329,
author = {Zoran, Daniel and Weiss, Yair},
title = {Natural Images, Gaussian Mixtures and Dead Leaves},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components — including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1736–1744},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999328,
author = {Chen, Zhitang and Zhang, Kun and Chan, Laiwan},
title = {Causal Discovery with Scale-Mixture Model for Spatiotemporal Variance Dependencies},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthetic and real world data are conducted to show the applicability of the proposed model and algorithms.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1727–1735},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999327,
author = {Hwang, Sung Ju and Grauman, Kristen and Sha, Fei},
title = {Semantic Kernel Forests from Multiple Taxonomies},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient. While an object taxonomy specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that are relevant. In light of these issues, we propose a discriminative feature learning approach that leverages multiple hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenicties, while another could reflect their habitats). For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes. Then, using the resulting semantic kernel forest, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class. To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure. We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1718–1726},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.5555/2999325.2999326,
author = {Gregor, Karol and Chklovskii, Dmitri B.},
title = {A Lattice Filter Model of the Visual Pathway},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Motivated by the cascade structure of the visual pathway (retina → lateral geniculate nucelus (LGN) → primary visual cortex, V1) we propose to model its function using lattice filters - signal processing devices for stage-wise decorrelation of temporal signals. Lattice filter models predict neuronal responses consistent with physiological recordings in cats and primates. In particular, they predict temporal receptive fields of two different types resembling so-called lagged and non-lagged cells in the LGN. Moreover, connection weights in the lattice filter can be learned using Hebbian rules in a stage-wise sequential manner reminiscent of the neuro-developmental sequence in mammals. In addition, lattice filters can model visual processing in insects. Therefore, lattice filter is a useful abstraction that captures temporal aspects of visual processing.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1709–1717},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@proceedings{10.5555/2999325,
title = {NIPS'12: Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Lake Tahoe, Nevada}
}

