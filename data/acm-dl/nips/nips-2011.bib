@inproceedings{10.5555/2986459.2986765,
author = {Ermon, Stefano and Gomes, Carla P. and Sabharwal, Ashish and Selman, Bart},
title = {Accelerated Adaptive Markov Chain for Partition Function Computation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel Adaptive Markov Chain Monte Carlo algorithm to compute the partition function. In particular, we show how to accelerate a flat histogram sampling technique by significantly reducing the number of "null moves" in the chain, while maintaining asymptotic convergence properties. Our experiments show that our method converges quickly to highly accurate solutions on a range of benchmark instances, outperforming other state-of-the-art methods such as IJGP, TRW, and Gibbs sampling both in run-time and accuracy. We also show how obtaining a so-called density of states distribution allows for efficient weight learning in Markov Logic theories.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2744–2752},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986764,
author = {Alexe, Bogdan and Petrescu, Viviana and Ferrari, Vittorio},
title = {Exploiting Spatial Overlap to Efficiently Compute Appearance Distances between Image Windows},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a computationally efficient technique to compute the distance of high-dimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efficiently solve tasks relevant to many computer vision applications, such as finding all pairs of windows between two images with distance smaller than a threshold, or finding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm finds the most similar pair of windows between two images while computing only 1% of all distances on average.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2735–2743},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986763,
author = {Loh, Po-Ling and Wainwright, Martin J.},
title = {High-Dimensional Regression with Noisy and Missing Data: Provable Guarantees with Non-Convexity},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2726–2734},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986762,
author = {Stimberg, Florian and Ruttor, Andreas and Opper, Manfred and Sanguinetti, Guido},
title = {Inference in Continuous-Time Change-Point Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of Bayesian inference for continuous-time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times. We propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a Poisson process and a two-state Markovian switch. We test the methodology on simulated data, and apply it to two real data sets in finance and systems biology. Our experimental results show that the approach leads to valid inferences and non-trivial insights.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2717–2725},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986761,
author = {Song, Le and Parikh, Ankur P. and Xing, Eric P.},
title = {Kernel Embeddings of Latent Tree Graphical Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision, bioinformatics and natural language processing problems. However, existing models are largely restricted to discrete and Gaussian variables due to computational constraints; furthermore, algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search. We present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-Gaussian variables. Our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efficient inference. Experiments on simulated and real data show the advantage of our proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2708–2716},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986760,
author = {Chen, Bo and Navalpakkam, Vidhya and Perona, Pietro},
title = {Predicting Response Time and Error Rates in Visual Search},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A model of human visual search is proposed. It predicts both response time (RT) and error rates (RT) as a function of image parameters such as target contrast and clutter. The model is an ideal observer, in that it optimizes the Bayes ratio of target present vs target absent. The ratio is computed on the firing pattern of V1/V2 neurons, modeled by Poisson distributions. The optimal mechanism for integrating information over time is shown to be a 'soft max' of diffusions, computed over the visual field by 'hypercolumns' of neurons that share the same receptive field and have different response properties to image features. An approximation of the optimal Bayesian observer, based on integrating local decisions, rather than diffusions, is also derived; it is shown experimentally to produce very similar predictions to the optimal observer in common psychophysics conditions. A psychophyisics experiment is proposed that may discriminate between which mechanism is used in the human brain.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2699–2707},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986759,
author = {G\"{o}rnitz, Nico and Widmer, Christian and Zeller, Georg and Kahles, Andr\'{e} and Sonnenburg, Soren and Ratsch, Gunnar},
title = {Hierarchical Multitask Structured Output Learning for Large-Scale Sequence Segmentation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel regularization-based Multitask Learning (MTL) formulation for Structured Output (SO) prediction for the case of hierarchical task relations. Structured output prediction often leads to difficult inference problems and hence requires large amounts of training data to obtain accurate models. We propose to use MTL to exploit additional information from related learning tasks by means of hierarchical regularization. Training SO models on the combined set of examples from multiple tasks can easily become infeasible for real world applications. To be able to solve the optimization problems underlying multitask structured output learning, we propose an efficient algorithm based on bundle-methods. We demonstrate the performance of our approach in applications from the domain of computational biology addressing the key problem of gene finding. We show that 1) our proposed solver achieves much faster convergence than previous methods and 2) that the Hierarchical SO-MTL approach outperforms considered non-MTL methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2690–2698},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986758,
author = {Kokkinos, Iasonas},
title = {Rapid Deformable Object Detection Using Dual-Tree Branch-and-Bound},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work we use Branch-and-Bound (BB) to efficiently detect objects with deformable part models. Instead of evaluating the classifier score exhaustively over image locations and scales, we use BB to focus on promising image locations. The core problem is to compute bounds that accommodate part deformations; for this we adapt the Dual Trees data structure [7] to our problem.We evaluate our approach using Mixture-of-Deformable Part Models [4]. We obtain exactly the same results but are 10-20 times faster on average. We also develop a multiple-object detection variation of the system, where hypotheses for 20 categories are inserted in a common priority queue. For the problem of finding the strongest category in an image this results in a 100-fold speedup.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2681–2689},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986757,
author = {Li, Ping and Shrivastava, Anshumali and Moore, Joshua and K\"{o}nig, Arnd Christian},
title = {Hashing Algorithms for Large-Scale Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Minwise hashing is a standard technique in the context of search for efficiently computing set similarities. The recent development of b-bit minwise hashing provides a substantial improvement by storing only the lowest b bits of each hashed value. In this paper, we demonstrate that b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression, to solve large-scale and high-dimensional statistical learning tasks, especially when the data do not fit in memory. We compare b-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW) algorithms, which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than VW (and random projections) for binary data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2672–2680},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986756,
author = {Qi, Yuan and Yan, Feng},
title = {EigenNet: A Bayesian Hybrid of Generative and Conditional Models for Sparse Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For many real-world applications, we often need to select correlated variables— such as genetic variations and imaging features associated with Alzheimer's disease—in a high dimensional space. The correlation between variables presents a challenge to classical variable selection methods. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables. To overcome this limitation, we present a novel hybrid model, EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We develop an efficient active-set algorithm to estimate the model via evidence maximization. Experimental results on synthetic data and imaging genetics data demonstrate the superior predictive performance of the EigenNet over the lasso, the elastic net, and the automatic relevance determination.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2663–2671},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986755,
author = {Brendel, Wieland and Romo, Ranulfo and Machens, Christian K.},
title = {Demixed Principal Component Analysis},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difficult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2654–2662},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986754,
author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
title = {On the Universality of Online Mirror Descent},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2645–2653},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986753,
author = {Pajarinen, Joni and Peltonen, Jaakko},
title = {Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Applications such as robot control and wireless communication require planning under uncertainty. Partially observable Markov decision processes (POMDPs) plan policies for single agents under uncertainty and their decentralized versions (DEC-POMDPs) find a policy for multiple agents. The policy in infinite-horizon POMDP and DEC-POMDP problems has been represented as finite state controllers (FSCs). We introduce a novel class of periodic FSCs, composed of layers connected only to the previous and next layer. Our periodic FSC method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy. This policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies, and by a new expectation maximization algorithm to yield stochastic periodic policies. Our method yields better results than earlier planning methods and can compute larger solutions than with regular FSCs.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2636–2644},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986752,
author = {Maillard, Odalric-Ambrym and Munos, R\'{e}mi and Ryabko, Daniil},
title = {Selecting the State-Representation in Reinforcement Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T2/3 where T is the horizon time.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2627–2635},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986751,
author = {Shelton, Jacquelyn A. and Bornschein, J\"{o}rg and Sheikh, Abdul-Saboor and Berkes, Pietro and L\"{u}cke, J\"{o}rg},
title = {Select and Sample — a Model of Efficient Neural Inference and Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2618–2626},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986750,
author = {Perotte, Adler and Bartlett, Nicholas and Elhadad, No\'{e}mie and Wood, Frank},
title = {Hierarchically Supervised Latent Dirichlet Allocation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce hierarchically supervised latent Dirichlet allocation (HSLDA), a model for hierarchically and multiply labeled bag-of-word data. Examples of such data include web pages and their placement in directories, product descriptions and associated categories from product hierarchies, and free-text clinical records and their assigned diagnosis codes. Out-of-sample label prediction is the primary goal of this work, but improved lower-dimensional representations of the bag-of-word data are also of interest. We demonstrate HSLDA on large-scale data from clinical document labeling and retail product categorization tasks. We show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2609–2617},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986749,
author = {Abernethy, Jacob and Frongillo, Rafael M.},
title = {A Collaborative Mechanism for Crowdsourcing Prediction Problems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of "crowdsourcing" prediction tasks. But these competitions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively "learn" a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and participants can modify this hypothesis by wagering on an update. The critical incentive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2600–2608},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986748,
author = {Khan, Omar Zia and Poupart, Pascal and Agosta, John Mark},
title = {Automated Refinement of Bayes Networks' Parameters Based on Test Ordering Constraints},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we derive a method to refine a Bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering. At each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value. We demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters. We incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods. Gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a MAP estimate that takes into account test ordering constraints and any data available. We demonstrate our approach on diagnostic sessions from a manufacturing scenario.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2591–2599},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986747,
author = {Stevenson, Ian H. and Kording, Konrad P.},
title = {Inferring Spike-Timing-Dependent Plasticity from Spike Train Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difficult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modification functions from pairs of spike trains. Then, using recursive point-process adaptive filtering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modification, we show that the true modification function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2582–2590},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986746,
author = {Wagner, Paul},
title = {A Reinterpretation of the Policy Oscillation Phenomenon in Approximate Policy Iteration},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2573–2581},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986745,
author = {Wick, Michael and McCallum, Andrew},
title = {Query-Aware MCMC},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for all the unobserved variables in a graphical model. However, in many real-world applications the user's interests are focused on a subset of the variables, specified by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-specific approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efficiency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2564–2572},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986744,
author = {Fletcher, Alyson K. and Rangan, Sundeep and Varshney, Lav R. and Bhargava, Aniruddha},
title = {Neural Reconstruction with Approximate Message Passing (NeuRAMP)},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear filtering stage that produces a low-dimensional signal that drives subsequent nonlinear stages. This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive fields in sensory neurons. The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method. The GAMP method is based on Gaussian approximations of loopy belief propagation. In the neural connectivity problem, the GAMP-based method is shown to be computational efficient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and significantly outperforms previous compressed-sensing methods. For the receptive field estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights. The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive fields of salamander retinal ganglion cells.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2555–2563},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986743,
author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
title = {Algorithms for Hyper-Parameter Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2546–2554},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986742,
author = {Lizotte, Daniel J.},
title = {Convergent Fitted Value Iteration with Linear Function Approximation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fitted value iteration (FVI) with ordinary least squares regression is known to diverge. We present a new method, "Expansion-Constrained Ordinary Least Squares" (ECOLS), that produces a linear approximation but also guarantees convergence when used with FVI. To ensure convergence, we constrain the least squares regression operator to be a non-expansion in the ∞-norm. We show that the space of function approximators that satisfy this constraint is more rich than the space of "averagers," we prove a minimax property of the ECOLS residual error, and we give an efficient algorithm for computing the coefficients of ECOLS based on constraint generation. We illustrate the algorithmic convergence of FVI with ECOLS in a suite of experiments, and discuss its properties.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2537–2545},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986741,
author = {Coates, Adam and Ng, Andrew Y.},
title = {Selecting Receptive Fields in Deep Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded "local receptive fields" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2528–2536},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986740,
author = {Sindhwani, Vikas and Lozano, Aur\'{e}lie C.},
title = {Non-Parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1-MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16].},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2519–2527},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986739,
author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
title = {Variational Gaussian Process Dynamical Systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2510–2518},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986738,
author = {Desjardins, Guillaume and Courville, Aaron and Bengio, Yoshua},
title = {On Tracking the Partition Function},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However, their use is often limited by an inability to estimate the partition function Z. In this paper, we exploit the gradient descent training procedure of restricted Boltzmann machines (a type of MRF) to track the log partition function during learning. Our method relies on two distinct sources of information: (1) estimating the change AZ incurred by each gradient update, (2) estimating the difference in Z over a small set of tempered distributions using bridge sampling. The two sources of information are then combined using an inference procedure similar to Kalman filtering. Learning MRFs through Tempered Stochastic Maximum Likelihood, we can estimate Z using no more temperatures than are required for learning. Comparing to both exact values and estimates using annealed importance sampling (AIS), we show on several datasets that our method is able to accurately track the log partition function. In contrast to AIS, our method provides this estimate at each time-step, at a computational cost similar to that required for training alone.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2501–2509},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986737,
author = {Vu, Duy Q. and Asuncion, Arthur U. and Hunter, David R. and Smyth, Padhraic},
title = {Continuous-Time Regression Models for Longitudinal Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science. Leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients. We also develop an efficient inference scheme that allows our approach to scale to large networks. On synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2492–2500},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986736,
author = {Yue, Yisong and Guestrin, Carlos},
title = {Linear Submodular Bandits and Their Application to Diversified Retrieval},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems. In this paper, we propose the linear sub-modular bandits problem, which is an online learning setting for optimizing a general class of feature-rich submodular utility models for diversified retrieval. We present an algorithm, called LSBGREEDY, and prove that it efficiently converges to a near-optimal model. As a case study, we applied our approach to the setting of personalized news recommendation, where the system must recommend small sets of news articles selected from tens of thousands of available articles each day. In a live user study, we found that LSBGREEDY significantly outperforms existing online learning approaches.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2483–2491},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986735,
author = {Rao, Vinayak and Teh, Yee Whye},
title = {Gaussian Process Modulated Renewal Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Renewal processes are generalizations of the Poisson process on the real line whose intervals are drawn i.i.d. from some distribution. Modulated renewal processes allow these interevent distributions to vary with time, allowing the introduction of nonstationarity. In this work, we take a nonparametric Bayesian approach, modelling this nonstationarity with a Gaussian process. Our approach is based on the idea of uniformization, which allows us to draw exact samples from an otherwise intractable distribution. We develop a novel and efficient MCMC sampler for posterior inference. In our experiments, we test these on a number of synthetic and real datasets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2474–2482},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986734,
author = {Mehta, Neville and Tadepalli, Prasad and Fern, Alan},
title = {Autonomous Learning of Action Models for Planning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces two new frameworks for learning action models for planning. In the mistake-bounded planning framework, the learner has access to a planner for the given model representation, a simulator, and a planning problem generator, and aims to learn a model with at most a polynomial number of faulty plans. In the planned exploration framework, the learner does not have access to a problem generator and must instead design its own problems, plan for them, and converge with at most a polynomial number of planning attempts. The paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks. A specific family of hypothesis spaces is shown to be efficiently learnable in both the frameworks.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2465–2473},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986733,
author = {Chen, Minmin and Weinberger, Kilian Q. and Blitzer, John C.},
title = {Co-Training for Domain Adaptation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most confident. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of co-training, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2456–2464},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986732,
author = {Krause, Andreas and Ong, Cheng Soon},
title = {Contextual Gaussian Process Bandit Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process defined over the joint context-action space, and develop CGP-UCB, an intuitive upper-confidence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2447–2455},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986731,
author = {Kloft, Marius and Blanchard, Gilles},
title = {The Local Rademacher Complexity of ℓ<i><sub>p</sub></i>-Norm Multiple Kernel Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive an upper bound on the local Rademacher complexity of ℓp-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p  = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order $O(n^{-frac{alpha}{1+alpha}})$, where α is the minimum eigenvalue decay rate of the individual kernels.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2438–2446},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986730,
author = {Isola, Phillip and Parikh, Devi and Torralba, Antonio and Oliva, Aude},
title = {Understanding the Intrinsic Memorability of Images},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Artists, advertisers, and photographers are routinely presented with the task of creating an image that a viewer will remember. While it may seem like image memorability is purely subjective, recent work shows that it is not an inexplicable phenomenon: variation in memorability of images is consistent across subjects, suggesting that some images are intrinsically more memorable than others, independent of a subjects' contexts and biases. In this paper, we used the publicly available memorability dataset of Isola et al. [13], and augmented the object and scene annotations with interpretable spatial, content, and aesthetic image properties. We used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image. We find that images of enclosed spaces containing people with visible faces are memorable, while images of vistas and peaceful scenes are not. Contrary to popular belief, unusual or aesthetically pleasing scenes do not tend to be highly memorable. This work represents one of the first attempts at understanding intrinsic image memorability, and opens a new domain of investigation at the interface between human cognition and computer vision.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2429–2437},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986729,
author = {Perry, Patrick O. and Mahoney, Michael W.},
title = {Regularized Laplacian Estimation and Fast Eigenvector Approximation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Definite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which ℓ2-regularized or ℓ1-regularized ℓ2-regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefficient vector of the regression problem. Our framework will imply that the solutions to the Mahoney-Orecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the first nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2420–2428},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986728,
author = {Azar, Mohammad Gheshlaghi and Munos, Remi and Ghavamzadeh, Mohammad and Kappen, Hilbert J.},
title = {Speedy Q-Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new convergent variant of Q-learning, called speedy Q-learning (SQL), to address the problem of slow convergence in the standard form of the Q-learning algorithm. We prove a PAC bound on the performance of SQL, which shows that for an MDP with n state-action pairs and the discount factor γ only T = O(log(n)/(ε2(1 – γ)4)) steps are required for the SQL algorithm to converge to an ?-optimal action-value function with high probability. This bound has a better dependency on 1/ε and 1/(1 2– γ), and thus, is tighter than the best available result for Q-learning. Our bound is also superior to the existing results for both model-free and model-based instances of batch Q-value iteration that are considered to be more efficient than the incremental methods like Q-learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2411–2419},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986727,
author = {Konidaris, George and Niekum, Scott and Thomas, Philip S.},
title = {TD<sub>γ</sub>: Re-Evaluating Complex Backups in Temporal Difference Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that the λ-return target used in the TD(λ) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. We introduce the γ-return estimator, an alternative target based on a more accurate model of variance, which defines the TDγ family of complex-backup temporal difference learning algorithms. We derive TDγ, the γ-return equivalent of the original TD(λ) algorithm, which eliminates the λ parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm, TDγ(C), with a capacity parameter C. TDγ(C) requires C times more time and memory than TD(λ) and is incremental and online. We show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains, and that TDγ(C) performs as well as or better than TDγ for intermediate settings of C.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2402–2410},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986726,
author = {Zhang, Yichuan and Sutton, Charles},
title = {Quasi-Newton Methods for Markov Chain Monte Carlo},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The performance of Markov chain Monte Carlo methods is often sensitive to the scaling and correlations between the random variables of interest. An important source of information about the local correlation and scale is given by the Hessian matrix of the target distribution, but this is often either computationally expensive or infeasible. In this paper we propose MCMC samplers that make use of quasi-Newton approximations, which approximate the Hessian of the target distribution from previous samples and gradients generated by the sampler. A key issue is that MCMC samplers that depend on the history of previous states are in general not valid. We address this problem by using limited memory quasi-Newton methods, which depend only on a fixed window of previous samples. On several real world datasets, we show that the quasi-Newton sampler is more effective than standard Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require higher-order derivatives.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2393–2401},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986725,
author = {Pacer, Michael D. and Griffiths, Thomas L.},
title = {A Rational Model of Causal Induction with Continuous Causes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Rational models of causal induction have been successful in accounting for people's judgments about causal relationships. However, these models have focused on explaining inferences from discrete data of the kind that can be summarized in a 2 \texttimes{} 2 contingency table. This severely limits the scope of these models, since the world often provides non-binary data. We develop a new rational model of causal induction using continuous dimensions, which aims to diminish the gap between empirical and theoretical approaches and real-world causal induction. This model successfully predicts human judgments from previous studies better than models of discrete causal inference, and outperforms several other plausible models of causal induction with continuous causes in accounting for people's inferences in a new experiment.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2384–2392},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986724,
author = {Shindler, Michael and Wong, Alex and Meyerson, Adam},
title = {Fast and Accurate <i>k</i>-Means for Large Datasets},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o(nk) (where n is the number of data points; note that computing the cost, given a solution, takes Θ(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2375–2383},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986723,
author = {Hein, Matthias and Setzer, Simon},
title = {Beyond Spectral Clustering - Tight Relaxations of Balanced Graph Cuts},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Spectral clustering is based on the spectral relaxation of the normalized/ratio graph cut criterion. While the spectral relaxation is known to be loose, it has been shown recently that a non-linear eigenproblem yields a tight relaxation of the Cheeger cut. In this paper, we extend this result considerably by providing a characterization of all balanced graph cuts which allow for a tight relaxation. Although the resulting optimization problems are non-convex and non-smooth, we provide an efficient first-order scheme which scales to large graphs. Moreover, our approach comes with the quality guarantee that given any partition as initialization the algorithm either outputs a better partition or it stops immediately.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2366–2374},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986722,
author = {Reichert, David P. and Seri\`{e}s, Peggy and Storkey, Amos J.},
title = {Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It has been argued that perceptual multistability reflects probabilistic inference performed by the brain when sensory input is ambiguous. Alternatively, more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation. We employ a Deep Boltzmann Machine (DBM) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework. Based on recent developments in machine learning, we show how neuronal adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference. Using the ambiguous Necker cube image, we analyze the perceptual switching exhibited by the model. We also examine the influence of spatial attention, and explore how binocular rivalry can be modeled with the same approach. Our work joins earlier studies in demonstrating how the principles underlying DBMs relate to cortical processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2357–2365},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986721,
author = {Graves, Alex},
title = {Practical Variational Inference for Neural Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2348–2356},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986720,
author = {Titsias, Michalis K. and L\'{a}zaro-Gredilla, Miguel},
title = {Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks. This model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression, multi-class classification, image processing applications and collaborative filtering.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2339–2347},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986719,
author = {Hsieh, Cho-Jui and Sustik, M\'{a}ty\'{a}s A. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
title = {Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The ℓ1 regularized Gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2330–2338},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986718,
author = {Abbott, Joshua T. and Heller, Katherine A. and Ghahramani, Zoubin and Griffiths, Thomas L.},
title = {Testing a Bayesian Measure of Representativeness Using a Large Image Database},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How do people determine which elements of a set are most representative of that set? We extend an existing Bayesian measure of representativeness, which indicates the representativeness of a sample from a distribution, to define a measure of the representativeness of an item to a set. We show that this measure is formally related to a machine learning method known as Bayesian Sets. Building on this connection, we derive an analytic expression for the representativeness of objects described by a sparse vector of binary features. We then apply this measure to a large database of images, using it to determine which images are the most representative members of different sets. Comparing the resulting predictions to human judgments of representativeness provides a test of this measure with naturalistic stimuli, and illustrates how databases that are more commonly used in computer vision and machine learning can be used to evaluate psychological theories.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2321–2329},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986717,
author = {Abbasi-Yadkori, Yasin and P\'{a}l, D\'{a}vid and Szepesv\'{a}ri, Csaba},
title = {Improved Algorithms for Linear Stochastic Bandits},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer's UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2312–2320},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986716,
author = {Susemihl, Alex and Meir, Ron and Opper, Manfred},
title = {Analytical Results for the Error in Filtering of Gaussian Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian filtering of stochastic stimuli has received a great deal of attention recently. It has been applied to describe the way in which biological systems dynamically represent and make decisions about the environment. There have been no exact results for the error in the biologically plausible setting of inference on point process, however. We present an exact analysis of the evolution of the mean-squared error in a state estimation task using Gaussian-tuned point processes as sensors. This allows us to study the dynamics of the error of an optimal Bayesian decoder, providing insights into the limits obtainable in this task. This is done for Markovian and a class of non-Markovian Gaussian processes. We find that there is an optimal tuning width for which the error is minimized. This leads to a characterization of the optimal encoding for the setting as a function of the statistics of the stimulus, providing a mathematically sound primer for an ecological theory of sensory processing.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2303–2311},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986715,
author = {Rifai, Salah and Dauphin, Yann N. and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
title = {The Manifold Tangent Classifier},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We combine three important ideas present in previous work for building classifiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concentrates near low-dimensional manifolds), and the manifold hypothesis for classification (different classes correspond to disjoint manifolds separated by low density). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2294–2302},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986714,
author = {Boutsidis, Christos and Drineas, Petros and Magdon-Ismail, Malik},
title = {Sparse Features for PCA-like Linear Regression},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Principal Components Analysis (PCA) is often used as a feature extraction procedure. Given a matrix X ∈ ℝn\texttimes{}d, whose rows represent n data points with respect to d features, the top k right singular vectors of X (the so-called eigenfeatures), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a small number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while provably achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2285–2293},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986713,
author = {Jern, Alan and Lucas, Christopher G. and Kemp, Charles},
title = {Evaluating the Inverse Decision-Making Approach to Preference Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Psychologists have recently begun to develop computational accounts of how people infer others' preferences from their behavior. The inverse decision-making approach proposes that people infer preferences by inverting a generative model of decision-making. Existing data sets, however, do not provide sufficient resolution to thoroughly evaluate this approach. We introduce a new preference learning task that provides a benchmark for evaluating computational accounts and use it to compare the inverse decision-making approach to a feature-based approach, which relies on a discriminative combination of decision features. Our data support the inverse decision-making approach to preference learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2276–2284},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986712,
author = {Dikmen, Onur and F\'{e}votte, C\'{e}dric},
title = {Nonnegative Dictionary Learning in the Exponential Noise Model for Adaptive Music Signal Representation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefficients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefficients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefficients have been integrated out (given a specific prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2267–2275},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986711,
author = {Walsh, Thomas J. and Hewlett, Daniel and Morrison, Clayton T.},
title = {Blending Autonomous Exploration and Apprenticeship Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present theoretical and empirical results for a framework that combines the benefits of apprenticeship and autonomous reinforcement learning. Our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment. The first change is replacing previously used Mistake Bound model learners with a recently proposed framework that melds the KWIK and Mistake Bound supervised learning protocols. The second change is introducing a communication of expected utility from the student to the teacher. The resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2258–2266},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986710,
author = {Chapelle, Olivier and Li, Lihong},
title = {An Empirical Evaluation of Thompson Sampling},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2249–2257},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986709,
author = {Jamieson, Kevin G. and Nowak, Robert D.},
title = {Active Ranking Using Pairwise Comparisons},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identified by standard sorting methods using n log2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Specifically, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reflect their relative distances from a common reference point in ℝd. We show that under this assumption the number of possible rankings grows like n2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2240–2248},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986708,
author = {Tschopp, Dominique and Diggavi, Suhas and Delgosha, Payam and Mohajer, Soheil},
title = {Randomized Algorithms for Comparison-Based Search},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper addresses the problem of finding the nearest neighbor (or one of the R-nearest neighbors) of a query object q in a database of n objects, when we can only use a comparison oracle. The comparison oracle, given two reference objects and a query object, returns the reference object most similar to the query object. The main problem we study is how to search the database for the nearest neighbor (NN) of a query, while minimizing the questions. The difficulty of this problem depends on properties of the underlying database. We show the importance of a characterization: combinatorial disorder D which defines approximate triangle inequalities on ranks. We present a lower bound of Ω(D log n/D + D2) average number of questions in the search phase for any randomized algorithm, which demonstrates the fundamental role of D for worst case behavior. We develop a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD3) questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD3) questions and O(n log2 n/ log(2 D)) bits to store.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2231–2239},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986707,
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Bubeck, S\'{e}bastien},
title = {Multi-Bandit Best Arm Identification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting. We first propose an algorithm called Gap-based Exploration (GapE) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit (i.e., small gap). We then introduce an algorithm, called GapE-V, which takes into account the variance of the arms in addition to their gap. We prove an upper-bound on the probability of error for both algorithms. Since GapE and GapE-V need to tune an exploration parameter that depends on the complexity of the problem, which is often unknown in advance, we also introduce variations of these algorithms that estimate this complexity online. Finally, we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2222–2230},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986706,
author = {Dethier, Julie and Nuyujukian, Paul and Eliasmith, Chris and Stewart, Terry and Elassaad, Shauki A. and Shenoy, Krishna V. and Boahen, Kwabena},
title = {A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motor prostheses aim to restore function to disabled patients. Despite compelling proof of concept systems, barriers to clinical translation remain. One challenge is to develop a low-power, fully-implantable system that dissipates only minimal power so as not to damage tissue. To this end, we implemented a Kalman-filter based decoder via a spiking neural network (SNN) and tested it in brain-machine interface (BMI) experiments with a rhesus monkey. The Kalman filter was trained to predict the arm's velocity and mapped on to the SNN using the Neural Engineering Framework (NEF). A 2,000-neuron embedded Matlab SNN implementation runs in real-time and its closed-loop performance is quite comparable to that of the standard Kalman filter. The success of this closed-loop decoder holds promise for hardware SNN implementations of statistical signal processing algorithms on neuromorphic chips, which may offer power savings necessary to overcome a major obstacle to the successful clinical translation of neural motor prostheses.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2213–2221},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986705,
author = {McAllester, David and Keshet, Joseph},
title = {Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (finite or infinite dimensional) they yield predictors approaching the infimum task loss achievable by any linear predictor over the given features. We also give finite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2205–2212},
numpages = {8},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986704,
author = {Boyles, Levi and Korattikara, Anoop and Ramanan, Deva and Welling, Max},
title = {Statistical Tests for Optimization Efficiency},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning problems, such as logistic regression, are typically formulated as pure optimization problems defined on some loss function. We argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation. By considering the statistical properties of the update variables used during the optimization (e.g. gradients), we can construct frequentist hypothesis tests to determine the reliability of these updates. We utilize subsets of the data for computing updates, and use the hypothesis tests for determining when the batch-size needs to be increased. This provides computational benefits and avoids overfitting by stopping when the batch-size has become equal to size of the full dataset. Moreover, the proposed algorithms depend on a single interpretable parameter -the probability for an update to be in the wrong direction - which is set to a single value across all algorithms and datasets. In this paper, we illustrate these ideas on three L1 regularized coordinate descent algorithms: L1-regularized L2-loss SVMs, L1-regularized logistic regression, and the Lasso, but we emphasize that the underlying methods are much more generally applicable.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2196–2204},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986703,
author = {Grave, \'{E}douard and Obozinski, Guillaume and Bach, Francis},
title = {Trace Lasso: A Trace Norm Regularization for Correlated Designs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Using the ℓ1-norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2187–2195},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986702,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
title = {Generalizing from Several Related Classification Tasks to a New Unlabeled Sample},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of assigning class labels to an unlabeled test data set, given several labeled training data sets drawn from similar distributions. This problem arises in several applications where data distributions fluctuate because of biological, technical, or other sources of variation. We develop a distribution-free, kernel-based approach to the problem. This approach involves identifying an appropriate reproducing kernel Hilbert space and optimizing a regularized empirical risk over the space. We present generalization error analysis, describe universal kernels, and establish universal consistency of the proposed methodology. Experimental results on flow cytometry data are presented.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2178–2186},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986701,
author = {Kolter, J. Zico},
title = {The Fixed Points of Off-Policy TD},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well. In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case. Furthermore, we show that we can efficiently project on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2169–2177},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986700,
author = {Dhillon, Inderjit S. and Ravikumar, Pradeep and Tewari, Ambuj},
title = {Nearest Neighbor Based Greedy Coordinate Descent},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Increasingly, optimization problems in machine learning, especially those arising from high-dimensional statistical estimation, have a large number of variables. Modern statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structure to the problem, such as sparsity. A central question is whether similar advances can be made in their computational complexity as well. In this paper, we propose strategies that indicate that such advances can indeed be made. In particular, we investigate the greedy coordinate descent algorithm, and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse. We then propose a suite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search. We also devise a more amenable form of greedy descent for composite non-smooth objectives; as well as several approximate variants of such greedy descent. We develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing. Without tuning the latter data structure, we are not only able to significantly speed up the vanilla greedy method, but also outperform cyclic descent when the problem size becomes large. Our results indicate the effectiveness of our nearest neighbor strategies, and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2160–2168},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986699,
author = {Y\i{}lmaz, Y. Kenan and Cemgil, A. Taylan and \c{S}im\c{s}ekli, Umut},
title = {Generalised Coupled Tensor Factorisation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive algorithms for generalised tensor factorisation (GTF) by building upon the well-established theory of Generalised Linear Models. Our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework, derived for a broad class of exponential family distributions including special cases such as Tweedie's distributions corresponding to β-divergences. By bounding the step size of the Fisher Scoring iteration of the GLM, we obtain general updates for real data and multiplicative updates for non-negative data. The GTF framework is, then extended easily to address the problems when multiple observed tensors are factorised simultaneously. We illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2151–2159},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986698,
author = {Feldman, Dan and Faulkner, Matthew and Krause, Andreas},
title = {Scalable Training of Mixture Models via Coresets},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset will also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3/ε2) data points suffices for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2142–2150},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986697,
author = {Foygel, Rina and Salakhutdinov, Ruslan and Shamir, Ohad and Srebro, Nathan},
title = {Learning with the Weighted Trace-Norm under Arbitrary Sampling Distributions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2133–2141},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986696,
author = {Saberian, Mohammad J. and Vasconcelos, Nuno},
title = {Multiclass Boosting: Theory and Algorithms},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of multi-class boosting is considered. A new framework, based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2124–2132},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986695,
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
title = {Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efficiently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classification problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports).},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2115–2123},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986694,
author = {Kawahara, Yoshinobu and Washio, Takashi},
title = {Prismatic Algorithm for Discrete D.C. Programming Problem},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose the first exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2106–2114},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986693,
author = {Choi, David S. and Wolfe, Patrick and Airoldi, Edoardo M.},
title = {Confidence Sets for Network Structure},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable models are frequently used to identify structure in dichotomous network data, in part because they give rise to a Bernoulli product likelihood that is both well understood and consistent with the notion of exchangeable random graphs. In this article we propose conservative confidence sets that hold with respect to these underlying Bernoulli parameters as a function of any given partition of network nodes, enabling us to assess estimates of residual network structure, that is, structure that cannot be explained by known covariates and thus cannot be easily verified by manual inspection. We demonstrate the proposed methodology by analyzing student friendship networks from the National Longitudinal Survey of Adolescent Health that include race, gender, and school year as covariates. We employ a stochastic expectation-maximization algorithm to fit a logistic regression model that includes these explanatory variables as well as a latent stochastic blockmodel component and additional node-specific effects. Although maximum-likelihood estimates do not appear consistent in this context, we are able to evaluate confidence sets as a function of different blockmodel partitions, which enables us to qualitatively assess the significance of estimated residual network structure relative to a baseline, which models covariates but lacks block structure.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2097–2105},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986692,
author = {Bergamo, Alessandro and Torresani, Lorenzo and Fitzgibbon, Andrew},
title = {PICODES: Learning a Compact Code for Novel-Category Recognition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce PICODES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of defining indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images defining the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classification performance. Optimization directly for binary features is difficult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. PICODES of 256 bytes match the accuracy of the current best known classifier for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2088–2096},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986691,
author = {Yang, Liu},
title = {Active Learning with a Drifting Distribution},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time. We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under Tsybakov noise. We further prove minimax lower bounds for this problem.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2079–2087},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986690,
author = {Wang, Zuoguan and Schalk, Gerwin and Ji, Qiang},
title = {Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Brain-computer interfaces (BCIs) use brain signals to convey a user's intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or finger flexion). The decoding approaches in these demonstrations usually employed classical classification/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern finger flexion can be exploited in this context. Specifically, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the flexion of individual fingers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full fine-grained finger articulation.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2070–2078},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986689,
author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B. and Torralba, Antonio},
title = {Learning to Learn with Compound HD Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce HD (or "Hierarchical-Deep") models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2061–2069},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986688,
author = {Saeedi, Ardavan and Bouchard-C\^{o}t\'{e}, Alexandre},
title = {Priors over Recurrent Continuous Time Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the Gamma-Exponential Process (GEP), a prior over a large family of continuous time stochastic processes. A hierarchical version of this prior (HGEP; the Hierarchical GEP) yields a useful model for analyzing complex time series. Models based on HGEPs display many attractive properties: conjugacy, exchangeability and closed-form predictive distribution for the waiting times, and exact Gibbs updates for the time scale parameters. After establishing these properties, we show how posterior inference can be carried efficiently using Particle MCMC methods [1]. This yields a MCMC algorithm that can resample entire sequences atomically while avoiding the complications of introducing slice and stick auxiliary variables of the beam sampler [2]. We applied our model to the problem of estimating the disease progression in multiple sclerosis [3], and to RNA evolutionary modeling [4]. In both domains, we found that our model outperformed the standard rate matrix estimation approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2052–2060},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986687,
author = {Park, Mijung and Horwitz, Greg and Pillow, Jonathan W.},
title = {Active Learning of Neural Response Functions with Gaussian Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A sizeable literature has focused on the problem of estimating a low-dimensional feature space for a neuron's stimulus sensitivity. However, comparatively little work has addressed the problem of estimating the nonlinear function from feature space to spike rate. Here, we use a Gaussian process (GP) prior over the infinite-dimensional space of nonlinear functions to obtain Bayesian estimates of the "non-linearity" in the linear-nonlinear-Poisson (LNP) encoding model. This approach offers increased flexibility, robustness, and computational tractability compared to traditional methods (e.g., parametric forms, histograms, cubic splines). We then develop a framework for optimal experimental design under the GP-Poisson model using uncertainty sampling. This involves adaptively selecting stimuli according to an information-theoretic criterion, with the goal of characterizing the nonlinearity with as little experimental data as possible. Our framework relies on a method for rapidly updating hyperparameters under a Gaussian approximation to the posterior. We apply these methods to neural data from a color-tuned simple cell in macaque V1, characterizing its nonlinear response function in the 3D space of cone contrasts. We find that it combines cone inputs in a highly nonlinear manner. With simulated experiments, we show that optimal design substantially reduces the amount of data required to estimate these nonlinear combination rules.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2043–2051},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986686,
author = {Macke, Jakob H. and Murray, Iain and Latham, Peter E.},
title = {How Biased Are Maximum Entropy Models?},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mutual information in biological systems. However, maximum entropy models fit to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations. However, in practice, the true distribution is usually outside the model class, and we show here that this misspecification can lead to much larger bias. We provide a perturbative approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2034–2042},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986685,
author = {Anandkumar, Animashree and Chaudhuri, Kamalika and Hsu, Daniel and Kakade, Sham M. and Song, Le and Zhang, Tong},
title = {Spectral Methods for Learning Multivariate Latent Tree Structure},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work considers the problem of learning the structure of multivariate linear tree models, which include a variety of directed tree graphical models with continuous, discrete, and mixed latent variables such as linear-Gaussian models, hidden Markov models, Gaussian mixture models, and Markov evolutionary trees. The setting is one where we only have samples from certain observed variables in the tree, and our goal is to estimate the tree structure (i.e., the graph of how the underlying hidden variables are connected to each other and to the observed variables). We propose the Spectral Recursive Grouping algorithm, an efficient and simple bottom-up procedure for recovering the tree structure from independent samples of the observed variables. Our finite sample size bounds for exact recovery of the tree structure reveal certain natural dependencies on underlying statistical and structural properties of the underlying joint distribution. Furthermore, our sample complexity guarantees have no explicit dependence on the dimensionality of the observed variables, making the algorithm applicable to many high-dimensional settings. At the heart of our algorithm is a spectral quartet test for determining the relative topology of a quartet of variables from second-order statistics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2025–2033},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986684,
author = {Wipf, David P.},
title = {Sparse Estimation with Structured Dictionaries},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2016–2024},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986683,
author = {Missura, Olana and G\"{a}rtner, Thomas},
title = {Predicting Dynamic Difficulty},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by applications in electronic games as well as teaching systems, we investigate the problem of dynamic difficulty adjustment. The task here is to repeatedly find a game difficulty setting that is neither 'too easy' and bores the player, nor 'too difficult' and overburdens the player. The contributions of this paper are (i) the formulation of difficulty adjustment as an online learning problem on partially ordered sets, (ii) an exponential update algorithm for dynamic difficulty adjustment, (iii) a bound on the number of wrong difficulty settings relative to the best static setting chosen in hindsight, and (iv) an empirical investigation of the algorithm when playing against adversaries.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2007–2015},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986682,
author = {Kar, Purushottam and Jain, Prateek},
title = {Similarity-Based Learning via Data Driven Embeddings},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of classification using similarity/distance functions over data. Specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by [1] and [2]. An attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1998–2006},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986681,
author = {Choi, Jaedeug and Kim, Kee-Eung},
title = {MAP Inference for Bayesian Inverse Reinforcement Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The difficulty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an infinite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1989–1997},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986680,
author = {Kim, Dae Il and Sudderth, Erik B.},
title = {The Doubly Correlated Nonparametric Topic Model},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Topic models are learned via a statistical model of variation within document collections, but designed to extract meaningful semantic structure. Desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions, such as manual specification of the number of topics. We propose a doubly correlated nonparametric topic (DCNT) model, the first model to simultaneously capture all three of these properties. The DCNT models metadata via a flexible, Gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction. We validate the semantic structure and predictive performance of the DCNT using a corpus of NIPS documents annotated by various metadata.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1980–1988},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986679,
author = {Saxe, Andrew and Bhand, Maneesh and Mudur, Ritvik and Suresh, Bipin and Ng, Andrew Y.},
title = {Unsupervised Learning Models of Primary Cortical Receptive Fields and Receptive Field Plasticity},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism's lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism's lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, first proposed by Mountcastle [1], that a qualitatively similar learning algorithm acts throughout primary sensory cortices.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1971–1979},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986678,
author = {Gunawardana, Asela and Meek, Christopher and Xu, Puyang},
title = {A Model for Temporal Dependencies in Event Streams},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the Piecewise-Constant Conditional Intensity Model, a model for learning temporal dependencies in event streams. We describe a closed-form Bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on Poisson superposition. We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1962–1970},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986677,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Iterative Learning for Reliable Crowdsourcing Systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Crowdsourcing systems, in which tasks are electronically distributed to numerous "information piece-workers", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such crowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm significantly outperforms majority voting and, in fact, is asymptotically optimal through comparison to an oracle that knows the reliability of every worker.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1953–1961},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986676,
author = {Thomas, Philip S.},
title = {Policy Gradient Coagent Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module's input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difficult and are also desirable to increase the biological plausibility of reinforcement learning methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1944–1952},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986675,
author = {Jalali, Ali and Johnson, Christopher C. and Ravikumar, Pradeep},
title = {On Learning Discrete Graphical Models Using Greedy Methods},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Ω(d2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of Ω(d3 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1935–1943},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986674,
author = {Slawski, Martin and Hein, Matthias},
title = {Sparse Recovery by Thresholded Non-Negative Least Squares},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting – unlike first intuition may suggest – we show that for a broad class of designs, NNLS is resistant to overriding and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming ℓ1-regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our findings suggest that NNLS may be the method of choice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1926–1934},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986673,
author = {Messias, Jo\~{a}o V. and Spaan, Matthijs T. J. and Lima, Pedro U.},
title = {Efficient Offline Communication Policies for Factored Multiagent POMDPs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Factored Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) form a powerful framework for multiagent planning under uncertainty, but optimal solutions require a rigid history-based policy representation. In this paper we allow inter-agent communication which turns the problem in a centralized Multiagent POMDP (MPOMDP). We map belief distributions over state factors to an agent's local actions by exploiting structure in the joint MPOMDP policy. The key point is that when sparse dependencies between the agents' decisions exist, often the belief over its local state factors is sufficient for an agent to unequivocally identify the optimal action, and communication can be avoided. We formalize these notions by casting the problem into convex optimization form, and present experimental results illustrating the savings in communication that we can obtain.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1917–1925},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986672,
author = {Shivaswamy, Pannagadatta K. and Jebara, Tony},
title = {Variance Penalizing AdaBoost},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1908–1916},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986671,
author = {Shaw, Blake and Huang, Bert and Jebara, Tony},
title = {Learning a Distance Metric from a Network},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many real-world networks are described by both connectivity information and features for every node. To better model and understand these networks, we present structure preserving metric learning (SPML), an algorithm for learning a Mahalanobis distance metric from a network such that the learned distances are tied to the inherent connectivity structure of the network. Like the graph embedding algorithm structure preserving embedding, SPML learns a metric which is structure preserving, meaning a connectivity algorithm such as k-nearest neighbors will yield the correct connectivity when applied using the distances from the learned metric. We show a variety of synthetic and real-world experiments where SPML predicts link patterns from node features more accurately than standard techniques. We further demonstrate a method for optimizing SPML based on stochastic gradient descent which removes the running-time dependency on the size of the network and allows the method to easily scale to networks of thousands of nodes and millions of edges.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1899–1907},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986670,
author = {Larsson, Martin and Ugander, Johan},
title = {A Concave Regularization Technique for Sparse Mixture Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary. Unfortunately concave regularization typically results in EM algorithms that must perform problematic non-concave M-step maximizations. In this work, we introduce a technique for circumventing this difficulty, using the so-called Mountain Pass Theorem to provide easily verifiable conditions under which the M-step is well-behaved despite the lacking concavity. We also develop a correspondence between logarithmic regularization and what we term the pseudo-Dirichlet distribution, a generalization of the ordinary Dirichlet distribution well-suited for inducing sparsity. We demonstrate our approach on a text corpus, inferring a sparse topic mixture model for 2,406 weblogs.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1890–1898},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986669,
author = {Nguyen, Nam H. and Nasrabadi, Nasser M. and Tran, Trac D.},
title = {Robust Lasso with Missing and Grossly Corrupted Observations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the problem of accurately recovering a sparse vector β* from highly corrupted linear measurements y = X β* + e* + w where e* is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β* and e*. Our first result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N(0, ∑), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β* and e* from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1881–1889},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986668,
author = {Hirayama, Jun-ichiro and Hyv\"{a}rinen, Aapo},
title = {Structural Equations and Divisive Normalization for Energy-Dependent Component Analysis},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Components estimated by independent component analysis and related methods are typically not independent in real data. A very common form of nonlinear dependency between the components is correlations in their variances or energies. Here, we propose a principled probabilistic model to model the energy-correlations between the latent variables. Our two-stage model includes a linear mixing of latent signals into the observed ones like in ICA. The main new feature is a model of the energy-correlations based on the structural equation model (SEM), in particular, a Linear Non-Gaussian SEM. The SEM is closely related to divisive normalization which effectively reduces energy correlation. Our new two-stage model enables estimation of both the linear mixing and the interactions related to energy-correlations, without resorting to approximations of the likelihood function or other non-principled approaches. We demonstrate the applicability of our method with synthetic dataset, natural images and brain signals.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1872–1880},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986667,
author = {Anandkumar, Anima and Tan, Vincent Y. F. and Willsky, Alan S.},
title = {High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efficient threshold-based algorithm for structure estimation based on conditional mutual information thresholding. This simple local algorithm requires only low-order statistics of the data and decides whether two nodes are neighbors in the unknown graph. We identify graph families for which the proposed algorithm has low sample and computational complexities. Under some transparent assumptions, we establish that the proposed algorithm is structurally consistent (or sparsistent) when the number of samples scales as n = Ω(J-1min log p), where p is the number of nodes and Jmin is the minimum edge potential. We also develop novel non-asymptotic techniques for obtaining necessary conditions for graphical model selection.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1863–1871},
numpages = {9},
keywords = {graphical model selection, Fano's inequality, high-dimensional learning, local-separation property, typical sets, necessary conditions},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986666,
author = {Olmos, Pablo M. and Salamanca, Luis and Murillo-Fuentes, Juan J. and P\'{e}rez-Cruz, Fernando},
title = {An Application of Tree-Structured Expectation Propagation for Channel Decoding},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show an application of a tree structure for approximate inference in graphical models using the expectation propagation algorithm. These approximations are typically used over graphs with short-range cycles. We demonstrate that these approximations also help in sparse graphs with long-range loops, as the ones used in coding theory to approach channel capacity. For asymptotically large sparse graph, the expectation propagation algorithm together with the tree structure yields a completely disconnected approximation to the graphical model but, for for finite-length practical sparse graphs, the tree structure approximation to the code graph provides accurate estimates for the marginal of each variable. Furthermore, we propose a new method for constructing the tree structure on the fly that might be more amenable for sparse graphs with general factors.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1854–1862},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986665,
author = {Yu, Chun-Nam and Greiner, Russell and Lin, Hsiu-Chin and Baracos, Vickie},
title = {Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1845–1853},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986664,
author = {Veness, Joel and Lanctot, Marc and Bowling, Michael},
title = {Variance Reduction in Monte-Carlo Tree Search},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efficacy on three different stochastic, single-agent settings: Pig, Can't Stop and Dominion.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1836–1844},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986663,
author = {Ion, Adrian and Carreira, Joao and Sminchisescu, Cristian},
title = {Probabilistic Joint Image Segmentation and Labeling},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1827–1835},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986662,
author = {Niekum, Scott and Barto, Andrew G.},
title = {Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Skill discovery algorithms in reinforcement learning typically identify single states or regions in state space that correspond to task-specific subgoals. However, such methods do not directly address the question of how many distinct skills are appropriate for solving the tasks that the agent faces. This can be highly inefficient when many identified subgoals correspond to the same underlying skill, but are all used individually as skill goals. Furthermore, skills created in this manner are often only transferable to tasks that share identical state spaces, since corresponding subgoals across tasks are not merged into a single skill goal. We show that these problems can be overcome by clustering subgoal data defined in an agent-space and using the resulting clusters as templates for skill termination conditions. Clustering via a Dirichlet process mixture model is used to discover a minimal, sufficient collection of portable skills.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1818–1826},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986661,
author = {Raykar, Vikas C. and Yu, Shipeng},
title = {Ranking Annotators for Crowdsourced Labeling Tasks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers–annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators—with the spammers having a score close to zero and the good annotators having a high score close to one.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1809–1817},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986660,
author = {Wauthier, Fabian L. and Jordan, Michael I.},
title = {Bayesian Bias Mitigation for Crowdsourcing},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Biased labelers are a systemic problem in crowdsourcing, and a comprehensive toolbox for handling their responses is still being developed. A typical crowdsourcing application can be divided into three steps: data collection, data curation, and learning. At present these steps are often treated separately. We present Bayesian Bias Mitigation for Crowdsourcing (BBMC), a Bayesian model to unify all three. Most data curation methods account for the effects of labeler bias by modeling all labels as coming from a single latent truth. Our model captures the sources of bias by describing labelers as influenced by shared random effects. This approach can account for more complex bias patterns that arise in ambiguous or hard labeling tasks and allows us to merge data curation and learning into a single computation. Active learning integrates data collection with learning, but is commonly considered infeasible with Gibbs sampling inference. We propose a general approximation strategy for Markov chains to efficiently quantify the effect of a perturbation on the stationary distribution and specialize this approach to active learning. Experiments show BBMC to outperform many common heuristics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1800–1808},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986659,
author = {Mozer, Michael C. and Link, Benjamin and Pashler, Harold},
title = {An Unsupervised Decontamination Procedure for Improving the Reliability of Human Judgments},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Psychologists have long been struck by individuals' limitations in expressing their internal sensations, impressions, and evaluations via rating scales. Instead of using an absolute scale, individuals rely on reference points from recent experience. This relativity of judgment limits the informativeness of responses on surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that map stimuli to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, the problem is to infer latent (subjective) impressions from a sequence of stimulus labels (e.g., movie names) and responses. We describe an unsupervised approach that simultaneously recovers the impressions and parameters of a contamination model that predicts how recent judgments affect the current response. We test our iterated impression inference, or II3, algorithm in three domains: rating the gap between dots, the desirability of a movie based on an advertisement, and the morality of an action. We demonstrate significant objective improvements in the quality of the recovered impressions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1791–1799},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986658,
author = {Carpentier, Alexandra and Maillard, Odalric-Ambrym and Munos, R\'{e}mi},
title = {Sparse Recovery with Brownian Sensing},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of recovering the parameter α ∈ ℝK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N, even when the features are arbitrarily non-orthogonal. Under the assumption that f is Holder continuous with exponent at least 1/2, we provide an estimate ᾶ of the parameter such that ∥α - ᾶ∥2 = O(∥η∥2/√N), where η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1782–1790},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986657,
author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert R. G.},
title = {Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of this paper is to investigate the advantages and disadvantages of learning in Banach spaces over Hilbert spaces. While many works have been carried out in generalizing Hilbert methods to Banach spaces, in this paper, we consider the simple problem of learning a Parzen window classifier in a reproducing kernel Banach space (RKBS)—which is closely related to the notion of embedding probability measures into an RKBS—in order to carefully understand its pros and cons over the Hilbert space classifier. We show that while this generalization yields richer distance measures on probabilities compared to its Hilbert space counterpart, it however suffers from serious computational drawback limiting its practical applicability, which therefore demonstrates the need for developing efficient learning algorithms in Banach spaces.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1773–1781},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986656,
author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
title = {Online Learning: Stochastic, Constrained, and Smoothed Adversaries},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a fixed distribution, and the adversarial scenario wherein, at every time step, an adversarially chosen instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We define the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we define a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite Littlestone dimension learnable.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1764–1772},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986655,
author = {Archambeau, C\'{e}dric and Guo, Shengbo and Zoeter, Onno},
title = {Sparse Bayesian Multi-Task Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new sparse Bayesian model for multi-task regression and classification. The model is able to capture correlations between tasks, or more specifically a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classification tasks it achieves competitive predictive performance compared to previously proposed methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1755–1763},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986654,
author = {Lazaric, Alessandro and Restelli, Marcello},
title = {Transfer from Multiple MDPs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1746–1754},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986653,
author = {Fukumizu, Kenji and Song, Le and Gretton, Arthur},
title = {Kernel Bayes' Rule},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample. The kernel Bayes' rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and filtering with a nonparametric state-space model. A consistency rate for the posterior estimate is established.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1737–1745},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986652,
author = {Kunapuli, Gautam and Maclin, Richard and Shavlik, Jude W.},
title = {Advice Refinement in Knowledge-Based SVMs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Knowledge-based support vector machines (KBSVMs) incorporate advice from domain experts, which can improve generalization significantly. A major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models. We propose a model that extends KBSVMs and is able to not only learn from data and advice, but also simultaneously improves the advice. The proposed approach is particularly effective for knowledge discovery in domains with few labeled examples. The proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach. Experimental results demonstrate that these algorithms yield useful refinements to expert advice, as well as improve the performance of the learning algorithm overall.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1728–1736},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986651,
author = {Kroemer, Oliver B. and Peters, Jan},
title = {A Non-Parametric Approach to Dynamic Programming},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider the problem of policy evaluation for continuous-state systems. We present a non-parametric approach to policy evaluation, which uses kernel density estimation to represent the system. The true form of the value function for this model can be determined, and can be computed using Galerkin's method. Furthermore, we also present a unified view of several well-known policy evaluation methods. In particular, we show that the same Galerkin method can be used to derive Least-Squares Temporal Difference learning, Kernelized Temporal Difference learning, and a discrete-state Dynamic Programming solution, as well as our proposed method. In a numerical evaluation of these algorithms, the proposed approach performed better than the other methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1719–1727},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986650,
author = {Li, Zhen and Ning, Huazhong and Cao, Liangliang and Zhang, Tong and Gong, Yihong and Huang, Thomas S.},
title = {Learning to Search Efficiently in High Dimensions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1710–1718},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986649,
author = {Knowles, David A. and Minka, Thomas P.},
title = {Non-Conjugate Variational Message Passing for Multinomial and Binary Regression},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the soft-max factor which is tighter than other commonly used bounds whilst maintaining computational tractability.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1701–1709},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986648,
author = {Park, Il Memming and Pillow, Jonathan W.},
title = {Bayesian Spike-Triggered Covariance Analysis},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli. Here we describe an explicit model-based interpretation of traditional estimators for a neuron's multi-dimensional feature space, which allows for several important generalizations and extensions. First, we show that traditional estimators based on the spike-triggered average (STA) and spike-triggered covariance (STC) can be formalized in terms of the "expected log-likelihood" of a Linear-Nonlinear-Poisson (LNP) model with Gaussian stimuli. This model-based formulation allows us to define maximum-likelihood and Bayesian estimators that are statistically consistent and efficient in a wider variety of settings, such as with naturalistic (non-Gaussian) stimuli. It also allows us to employ Bayesian methods for regularization, smoothing, sparsification, and model comparison, and provides Bayesian confidence intervals on model parameters. We describe an empirical Bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more flexible model for feature space inference. We validate these methods using neural data recorded extracellularly from macaque primary visual cortex.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1692–1700},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986647,
author = {Seldin, Yevgeny and Auer, Peter and Laviolette, Fran\c{c}ois and Shawe-Taylor, John and Ortner, Ronald},
title = {PAC-Bayesian Analysis of Contextual Bandits},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) N goes as $sqrt{N I_{rho_t}(S;A)}$, where Iρt(S; A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm uses all the side information, the regret bound scales as $sqrt{N ln K}$, where K is the number of actions (arms). However, if the side information Iρt (S; A) is not fully used, the regret bound is significantly tighter. In the extreme case, when Iρt(S; A)= 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1683–1691},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986646,
author = {Allahverdyan, Armen and Galstyan, Aram},
title = {Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only finite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam's razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1674–1682},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986645,
author = {El-Yaniv, Ran and Wiener, Yair},
title = {Agnostic Selective Classification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classification performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this rejected region of the domain is shown to be diminishing at rate $O(Btheta (sqrt{1/m}))^beta)$, where θ is Hanneke's disagreement coefficient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classification algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1665–1673},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986644,
author = {Erven, Tim van and Gr\"{u}nwald, Peter and Koolen, Wouter M. and de Rooij, Steven},
title = {Adaptive Hedge},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1656–1664},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986643,
author = {Cotter, Andrew and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
title = {Better Mini-Batch Algorithms via Accelerated Gradient Methods},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1647–1655},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986642,
author = {Liu, Yi-Kai},
title = {Universal Low-Rank Matrix Recovery from Pauli Measurements},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefficients.We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rank-r restricted isometry property (RIP). This implies that M can be recovered from a fixed ("universal") set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley's inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1638–1646},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986641,
author = {Zeiler, Matthew D. and Taylor, Graham W. and Sigal, Leonid and Matthews, Iain and Fergus, Rob},
title = {Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a type of Temporal Restricted Boltzmann Machine that defines a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efficient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1629–1637},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986640,
author = {Zhu, Jun and Chen, Ning and Xing, Eric P.},
title = {Infinite Latent SVM for Classification and Multi-Task Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unlike existing nonparametric Bayesian models, which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, we study nonparametric Bayesian inference with regularization on the desired posterior distributions. While priors can indirectly affect posterior distributions through Bayes' theorem, imposing posterior regularization is arguably more direct and in some cases can be much easier. We particularly focus on developing infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets. Our results appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1620–1628},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986639,
author = {Zhang, Ziming and Ladick\'{y}, L'ubor and Torr, Philip H. S. and Saffari, Amir},
title = {Learning Anchor Planes for Classification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal anchor planes, rather than anchor points. Our method needs only a few orthogonal anchor planes for coding, and it can linearize any (α, β, p)-Lipschitz smooth nonlinear function with a fixed expected value of the upper-bound approximation error on any high dimensional data. In practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classification tasks, and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate, while LCC achieves 1.90% error rate using 4096 anchor points.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1611–1619},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986638,
author = {Slivkins, Aleksandrs},
title = {Multi-Armed Bandits on Implicit Metric Spaces},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting, an online algorithm has a fixed set of alternatives ("arms"), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (infinitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs. In this paper we consider the more realistic scenario in which the metric space is implicit - it is defined by the available structure but not revealed to the algorithm directly. Specifically, we assume that an algorithm is given a tree-based classification of arms. For any given problem instance such a classification implicitly defines a similarity metric space, but the numerical similarity information is not available to the algorithm. We provide an algorithm for this setting, whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the Lipschitz MAB problem.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1602–1610},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986637,
author = {Telgarsky, Matus},
title = {The Fast Convergence of Boosting},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/∊2)). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/∊)). Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/∊)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/∊), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1593–1601},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986636,
author = {Vitale, Fabio and Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Zappella, Giovanni},
title = {See the Tree through the Lines: The Shazoo Algorithm},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains. Since graph sparsification via spanning trees retains enough information while making the task much easier, trees are an important special case of this problem. Although it is known how to predict the nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm is not available yet. We fill this hole and introduce an efficient node predictor, SHAZOO, which is nearly optimal on any weighted tree. Moreover, we show that SHAZOO can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines. Experiments on real-world datasets confirm that SHAZOO performs well in that it fully exploits the structure of the input tree, and gets very close to (and sometimes better than) less scalable energy minimization methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1584–1592},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986635,
author = {Suzuki, Taiji},
title = {Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp-MKL that imposes ℓp-mixed-norm regularization instead of ℓ1-mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp-MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1575–1583},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986634,
author = {Stemmler, Martin and Sengupta, Biswa and Laughlin, Simon and Niven, Jeremy},
title = {Energetically Optimal Action Potentials},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most action potentials in the nervous system take on the form of strong, rapid, and brief voltage deflections known as spikes, in stark contrast to other action potentials, such as in the heart, that are characterized by broad voltage plateaus. We derive the shape of the neuronal action potential from first principles, by postulating that action potential generation is strongly constrained by the brain's need to minimize energy expenditure. For a given height of an action potential, the least energy is consumed when the underlying currents obey the bang-bang principle: the currents giving rise to the spike should be intense, yet short-lived, yielding spikes with sharp onsets and offsets. Energy optimality predicts features in the biophysics that are not per se required for producing the characteristic neuronal action potential: sodium currents should be extraordinarily powerful and inactivate with voltage; both potassium and sodium currents should have kinetics that have a bell-shaped voltage-dependence; and the cooperative action of multiple 'gates' should start the flow of current.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1566–1574},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986633,
author = {Bui, Loc and Johari, Ramesh and Mannor, Shie},
title = {Committing Bandits},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a multi-armed bandit problem where there are two phases. The first phase is an experimentation phase where the decision maker is free to explore multiple options. In the second phase the decision maker has to commit to one of the arms and stick with it. Cost is incurred during both phases with a higher cost during the experimentation phase. We analyze the regret in this setup, and both propose algorithms and provide upper and lower bounds that depend on the ratio of the duration of the experimentation phase to the duration of the commitment phase. Our analysis reveals that if given the choice, it is optimal to experiment Θ(ln T) steps and then commit, where T is the time horizon.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1557–1565},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986632,
author = {Huang, Tzu-Kuo and Schneider, Jeff},
title = {Learning Auto-Regressive Models from Sequence and Non-Sequence Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vector Auto-regressive models (VAR) are useful tools for analyzing time series data. In quite a few modern time series modelling tasks, the collection of reliable time series turns out to be a major challenge, either due to the slow progression of the dynamic process of interest, or inaccessibility of repetitive measurements of the same dynamic process over time. In those situations, however, we observe that it is often easier to collect a large amount of non-sequence samples, or snapshots of the dynamic process of interest. In this work, we assume a small amount of time series data are available, and propose methods to incorporate non-sequence data into penalized least-square estimation of VAR models. We consider non-sequence data as samples drawn from the stationary distribution of the underlying VAR model, and devise a novel penalization scheme based on the Lyapunov equation concerning the covariance of the stationary distribution. Experiments on synthetic and video data demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1548–1556},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986631,
author = {Eberts, M. and Steinwart, I.},
title = {Optimal Learning Rates for Least Squares SVMs Using Gaussian Kernels},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove a new oracle inequality for support vector machines with Gaussian RBF kernels solving the regularized least squares regression problem. To this end, we apply the modulus of smoothness. With the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method. Finally, it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1539–1547},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986630,
author = {Kim, Sungwoong and Nowozin, Sebastian and Kohli, Pushmeet and Yoo, Chang D.},
title = {Higher-Order Correlation Clustering for Image Segmentation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step. As such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters. Correlation clustering, a graph-partitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms. We improve the basic correlation clustering formulation by taking into account higher-order cluster relationships. This improves clustering in the presence of local boundary ambiguities. We first apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels. Fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible. Experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1530–1538},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986629,
author = {Watanabe, Yusuke},
title = {Uniqueness of Belief Propagation on Signed Graphs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While loopy Belief Propagation (LBP) has been utilized in a wide variety of applications with empirical success, it comes with few theoretical guarantees. Especially, if the interactions of random variables in a graphical model are strong, the behaviors of the algorithm can be difficult to analyze due to underlying phase transitions. In this paper, we develop a novel approach to the uniqueness problem of the LBP fixed point; our new "necessary and sufficient" condition is stated in terms of graphs and signs, where the sign denotes the types (attractive/repulsive) of the interaction (i.e., compatibility function) on the edge. In all previous works, uniqueness is guaranteed only in the situations where the strength of the interactions are "sufficiently" small in certain senses. In contrast, our condition covers arbitrary strong interactions on the specified class of signed graphs. The result of this paper is based on the recent theoretical advance in the LBP algorithm; the connection with the graph zeta function.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1521–1529},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986628,
author = {Petterson, James and Caetano, Tiberio},
title = {Submodular Multi-Label Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we present an algorithm to learn a multi-label classifier which attempts at directly optimising the F-score. The key novelty of our formulation is that we explicitly allow for assortative (submodular) pairwise label interactions, i.e., we can leverage the co-ocurrence of pairs of labels in order to improve the quality of prediction. Prediction in this model consists of minimising a particular submodular set function, what can be accomplished exactly and efficiently via graph-cuts. Learning however is substantially more involved and requires the solution of an intractable combinatorial optimisation problem. We present an approximate algorithm for this problem and prove that it is sound in the sense that it never predicts incorrect labels. We also present a nontrivial test of a sufficient condition for our algorithm to have found an optimal solution. We present experiments on benchmark multi-label datasets, which attest the value of the proposed technique. We also make available source code that enables the reproduction of our experiments.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1512–1520},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986627,
author = {Delaitre, Vincent and Sivic, Josef and Laptev, Ivan},
title = {Learning Person-Object Interactions for Action Recognition in Still Images},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-specific body part and object interactions bypasses the difficult problem of estimating the complete human body pose configuration. Benefits of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1503–1511},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986626,
author = {Ding, Nan and Vishwanathan, S. V. N. and Qi, Yuan},
title = {<i>T</i>-Divergence Based Approximate Inference},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by defining a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student's t-prior.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1494–1502},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986625,
author = {Lempitsky, Victor and Vedaldi, Andrea and Zisserman, Andrew},
title = {A Pylon Model for Semantic Segmentation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Graph cut optimization is one of the standard workhorses of image segmentation since for binary random field representations of the image, it gives globally optimal results and there are efficient polynomial time implementations. Often, the random field is applied over a flat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a flat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding benefits of global optimality and efficiency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree.We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a flat partitioning) and demonstrate that the optimization in the pylon model is able to flexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1485–1493},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986624,
author = {Ghosh, Soumya and Ungureanu, Andrei B. and Sudderth, Erik B. and Blei, David M.},
title = {Spatial Distance Dependent Chinese Restaurant Processes for Image Segmentation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The distance dependent Chinese restaurant process (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable data [1]. The dd-CRP clusters data in a biased way: each data point is more likely to be clustered with other data that are near it in an external sense. This paper examines the dd-CRP in a spatial setting with the goal of natural image segmentation. We explore the biases of the spatial ddCRP model and propose a novel hierarchical extension better suited for producing "human-like" segmentations. We then study the sensitivity of the models to various distance and appearance hyperparameters, and provide the first rigorous comparison of nonparametric Bayesian models in the image segmentation domain. On unsupervised image segmentation, we demonstrate that similar performance to existing nonparametric Bayesian models is possible with substantially simpler models and algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1476–1484},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986623,
author = {Geiger, Andreas and Wojek, Christian and Urtasun, Raquel},
title = {Joint 3D Estimation of Objects and Scene Layout},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1467–1475},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986622,
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
title = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1458–1466},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986621,
author = {Khan, Faisal and Zhu, Xiaojin and Mutlu, Bilge},
title = {How Do Humans Teach: On Curriculum Learning and Teaching Dimension},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the empirical strategies that humans follow as they teach a target concept with a simple 1D threshold to a robot.1 Previous studies of computational teaching, particularly the teaching dimension model and the curriculum learning principle, offer contradictory predictions on what optimal strategy the teacher should follow in this teaching task. We show through behavioral studies that humans employ three distinct teaching strategies, one of which is consistent with the curriculum learning principle, and propose a novel theoretical framework as a potential explanation for this strategy. This framework, which assumes a teaching goal of minimizing the learner's expected generalization error at each iteration, extends the standard teaching dimension model and offers a theoretical justification for curriculum learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1449–1457},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986620,
author = {Ekanadham, Chaitanya and Tranchina, Daniel and Simoncelli, Eero P.},
title = {A Blind Deconvolution Method for Neural Spike Identification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and systematically mishandles temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform. Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefficients are sparse. We develop a block-coordinate descent procedure to approximate the MAP solution, based on our recently developed continuous basis pursuit method. We validate our method on simulated data as well as real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering overlapping spikes. The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1440–1448},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986619,
author = {Huang, Shuai and Li, Jing and Ye, Jieping and Chen, Kewei and Wu, Teresa and Fleisher, Adam and Reiman, Eric},
title = {Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data Using Sparse Composite Linear Discrimination Analysis},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diagnosis of Alzheimer's disease (AD) at the early stage of the disease development is of great clinical importance. Current clinical assessment that relies primarily on cognitive measures proves low sensitivity and specificity. The fast growing neuroimaging techniques hold great promise. Research so far has focused on single neuroimaging modality. However, as different modalities provide complementary measures for the same disease pathology, fusion of multi-modality data may increase the statistical power in identification of disease-related brain regions. This is especially true for early AD, at which stage the disease-related regions are most likely to be weak-effect regions that are difficult to be detected from a single modality alone. We propose a sparse composite linear discriminant analysis model (SCLDA) for identification of disease-related brain regions of early AD from multi-modality data. SCLDA uses a novel formulation that decomposes each LDA parameter into a product of a common parameter shared by all the modalities and a parameter specific to each modality, which enables joint analysis of all the modalities and borrowing strength from one another. We prove that this formulation is equivalent to a penalized likelihood with non-convex regularization, which can be solved by the DC (difference of convex functions) programming. We show that in using the DC programming, the property of the non-convex regularization in terms of preserving weak-effect features can be nicely revealed. We perform extensive simulations to show that SCLDA outperforms existing competing algorithms on feature selection, especially on the ability for identifying weak-effect features. We apply SCLDA to the Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) images of 49 AD patients and 67 normal controls (NC). Our study identifies disease-related brain regions consistent with findings in the AD literature.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1431–1439},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986618,
author = {Brea, Johanni and Senn, Walter and Pfister, Jean-Pascal},
title = {Sequence Learning with Hidden Units in Spiking Neural Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons significantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1422–1430},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986617,
author = {Kumar, Abhishek and Rai, Piyush and Daum\'{e}, Hal},
title = {Co-Regularized Multi-View Spectral Clustering},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to find a clustering that is more accurate than the ones obtained using the individual views. Often these different views admit same underlying clustering of the data, so we can approach this problem by looking for clusterings that are consistent across the views, i.e., corresponding data points in each view should have same cluster membership. We propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses, and propose two co-regularization schemes to accomplish this. Experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efficacy of our proposed approaches.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1413–1421},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986616,
author = {Dembczy\'{n}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H\"{u}llermeier, Eyke},
title = {An Exact Algorithm for F-Measure Maximization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The F-measure, originally introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure remains a statistically and computationally challenging problem, since no closed-form maximizer exists. Current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables. In this paper, we present an algorithm which is not only computationally efficient but also exact, regardless of the underlying distribution. The algorithm requires only a quadratic number of parameters of the joint distribution (with respect to the number of binary responses). We illustrate its practical performance by means of experimental results for multi-label classification.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1404–1412},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986615,
author = {Zhang, XianXing and Dunson, David B. and Carin, Lawrence},
title = {Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The nested Chinese restaurant process is extended to design a nonparametric topic-model tree for representation of human choices. Each tree path corresponds to a type of person, and each node (topic) has a corresponding probability vector over items that may be selected. The observed data are assumed to have associated temporal covariates (corresponding to the time at which choices are made), and we wish to impose that with increasing time it is more probable that topics deeper in the tree are utilized. This structure is imposed by developing a new "change point" stick-breaking model that is coupled with a Poisson and product-of-gammas construction. To share topics across the tree nodes, topic distributions are drawn from a Dirichlet process. As a demonstration of this concept, we analyze real data on course selections of undergraduate students at Duke University, with the goal of uncovering and concisely representing structure in the curriculum and in the characteristics of the student body.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1395–1403},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986614,
author = {Broeck, Guy Van den},
title = {On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning. However, this expressivity is detrimental to the tractability of inference, when done at the propositional level. To solve this problem, various lifted inference algorithms have been proposed that reason at the first-order level, about groups of objects as a whole. Despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms. The key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models. We then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1386–1394},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986613,
author = {Mensi, Skander and Naud, Richard and Gersnter, Wulfram},
title = {From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-the-art models from both schools of thought. First we find the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-and-Fire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential. We find a mathematical expression for this link-function and test the ability of the GLM to predict the firing probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we find that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1377–1385},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986612,
author = {Adametz, David and Roth, Volker},
title = {Bayesian Partitioning of Large-Scale Distance Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A Bayesian approach to partitioning distance matrices is presented. It is inspired by the Translation-invariant Wishart-Dirichlet process (TIWD) in [1] and shares a number of advantageous properties like the fully probabilistic nature of the inference model, automatic selection of the number of clusters and applicability in semi-supervised settings. In addition, our method (which we call fastTIWD) overcomes the main shortcoming of the original TIWD, namely its high computational costs. The fastTIWD reduces the workload in each iteration of a Gibbs sampler from O(n3) in the TIWD to O(n2). Our experiments show that the cost reduction does not compromise the quality of the inferred partitions. With this new method it is now possible to 'mine' large relational datasets with a probabilistic model, thereby automatically detecting new and potentially interesting clusters.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1368–1376},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986611,
author = {Yao, Angela and Gall, Juergen and Gool, Luc Van and Urtasun, Raquel},
title = {Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A common approach for handling the complexity and inherent ambiguities of 3D human pose estimation is to use pose priors learned from training data. Existing approaches however, are either too simplistic (linear), too complex to learn, or can only learn latent spaces from "simple data", i.e., single activities such as walking or running. In this paper, we present an efficient stochastic gradient descent algorithm that is able to learn probabilistic non-linear latent spaces composed of multiple activities. Furthermore, we derive an incremental algorithm for the online setting which can update the latent space without extensive relearning. We demonstrate the effectiveness of our approach on the task of monocular and multi-view tracking and show that our approach outperforms the state-of-the-art.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1359–1367},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986610,
author = {Macke, Jakob H. and B\"{u}sing, Lars and Cunningham, John P. and Yu, Byron M. and Shenoy, Krishna V. and Sahani, Maneesh},
title = {Empirical Models of Spiking in Neural Populations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing a latent dynamical model with realistic spiking observations to coupled generalised linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly better goodness-of-fit and more realistic population spike counts.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1350–1358},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986609,
author = {McHutchon, Andrew and Rasmussen, Carl Edward},
title = {Gaussian Process Training with Input Noise},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1341–1349},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986608,
author = {Dubout, Charles and Fleuret, Francois},
title = {Boosting with Maximum Adaptive Sampling},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Classical Boosting algorithms, such as AdaBoost, build a strong classifier without concern about the computational cost. Some applications, in particular in computer vision, may involve up to millions of training examples and features. In such contexts, the training time may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features, or the examples, used to train the weak learners. Even if those methods can precisely quantify the speed improvement they deliver, they offer no guarantee of being more efficient than any other, given the same amount of time.This paper aims at shading some light on this problem, i.e. given a fixed amount of time, for a particular problem, which strategy is optimal in order to reduce the training loss the most. We apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction. Experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1332–1340},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986607,
author = {Khan, Fahad Shahbaz and van de Weijer, Joost and Bagdanov, Andrew D. and Vanrell, Maria},
title = {Portmanteau Vocabularies for Multi-Cue Image Representation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a novel technique for feature combination in the bag-of-words model of image classification. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact, has the cue binding property, and supports individual weighting of cues in the final image representation. State-of-the-art results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, significantly more complex approaches to multi-cue image representation.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1323–1331},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986606,
author = {Long, Philip M. and Servedio, Rocco A.},
title = {Algorithms and Hardness Results for Parallel Large Margin Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the fundamental problem of learning an unknown large-margin half-space in the context of parallel computation.Our main positive result is a parallel algorithm for learning a large-margin half-space that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations. We show that this algorithm learns an unknown γ-margin halfspace over n dimensions using poly(n, 1/γ) processors and runs in time \~{O}(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have Ω(1/γ2) runtime dependence on γ.Our main negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We give an information-theoretic proof that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized: the ability to call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1314–1322},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986605,
author = {Savin, Cristina and Dayan, Peter and Lengyel, M\'{a}t\'{e}},
title = {Two is Better than One: Distinct Roles for Familiarity and Recollection in Retrieving Palimpsest Memories},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This finding provides a new window onto actively contentious psychological and neural aspects of recognition memory.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1305–1313},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986604,
author = {Lou, Xinghua and Hamprecht, Fred A.},
title = {Structured Learning for Cell Tracking},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1296–1304},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986603,
author = {Lim, Zhan Wei and Hsu, David and Lee, Wee Sun},
title = {Monte Carlo Value Iteration with Macro-Actions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {POMDP planning faces two major computational challenges: large state spaces and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufficient conditions for Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1287–1295},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986602,
author = {Carpentier, Alexandra and Munos, R\'{e}mi},
title = {Finite-Time Analysis of Stratified Sampling for Monte Carlo},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of stratified sampling for Monte-Carlo integration. We model this problem in a multi-armed bandit setting, where the arms represent the strata, and the goal is to estimate a weighted average of the mean values of the arms. We propose a strategy that samples the arms according to an upper bound on their standard deviations and compare its estimation quality to an ideal allocation that would know the standard deviations of the strata. We provide two regret analyses: a distribution-dependent bound \~{O}(n-3/2) that depends on a measure of the disparity of the strata, and a distribution-free bound \~{O}(n-4/3) that does not.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1278–1286},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986601,
author = {Zhao, Qibin and Caiafa, Cesar F. and Mandic, Danilo P. and Zhang, Liqing and Ball, Tonio and Schulze-Bonhage, Andreas and Cichocki, Andrzej},
title = {Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A multilinear subspace regression model based on so called latent variable decomposition is introduced. Unlike standard regression methods which typically employ matrix (2D) data representations followed by vector subspace transformations, the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data. The proposed approach aims to maximize the correlation between the so derived latent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data, where for the estimation of the latent variables we introduce an algorithm based on Multilinear Singular Value Decomposition (MSVD) on a specially defined cross-covariance tensor. It is next shown that in this way we are also able to unify the existing Partial Least Squares (PLS) and N-way PLS regression algorithms within the same framework. Simulations on benchmark synthetic data confirm the advantages of the proposed approach, in terms of its predictive ability and robustness, especially for small sample sizes. The potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroencephalograph (EEG).},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1269–1277},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986600,
author = {Juditsky, Anatoli and Karzan, Fatma K\i{}l\i{}n\c{c} and Nemirovski, Arkadi and Polyak, Boris},
title = {On the Accuracy of ℓ<sub>1</sub>-Filtering of Signals with Block-Sparse Structure},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We discuss new methods for the recovery of signals with block-sparse structure, based on ℓ1-minimization. Our emphasis is on the efficiently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1260–1268},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986599,
author = {Zhao, Bin and Fei-Fei, Li and Xing, Eric P.},
title = {Large-Scale Category Structure Aware Image Categorization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world benefits from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Specifically, a category hierarchy is utilized to properly define loss function and select common set of features for related categories. An efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1251–1259},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986598,
author = {Dai, Dong and Zhang, Tong},
title = {Greedy Model Averaging},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression. It is known that if the models are mis-specified, model averaging is superior to model selection. Specifically, let n be the sample size, then the worst case regret of the former decays at the rate of O(1/n) while the worst case regret of the latter decays at the rate of O(1/√n). In the literature, the most important and widely studied model averaging method that achieves the optimal O(1/n) average regret is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations. The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA. We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1242–1250},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986597,
author = {Hazan, Elad and Koren, Tomer and Srebro, Nathan},
title = {Beating SGD: Learning SVMs in Sublinear Time},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an optimization approach for linear SVMs based on a stochastic primal-dual approach, where the primal step is akin to an importance-weighted SGD, and the dual step is a stochastic update on the importance weights. This yields an optimization method with a sublinear dependence on the training set size, and the first method for learning linear SVMs with runtime less then the size of the training set required for learning!},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1233–1241},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986596,
author = {Vernet, Elodie and Williamson, Robert C. and Reid, Mark D.},
title = {Composite Multiclass Losses},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a "proper composite loss", which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We determine the stationarity condition, Bregman representation, order-sensitivity, existence and uniqueness of the composite representation for multiclass losses. We subsume existing results on "classification calibration" by relating it to properness and show that the simple integral representation for binary proper losses can not be extended to multiclass losses.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1224–1232},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986595,
author = {Jain, Prateek and Tewari, Ambuj and Dhillon, Inderjit S.},
title = {Orthogonal Matching Pursuit with Replacement},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number of fixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residual. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structure, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hash, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrate that for large-scale problems our proposed methods are more robust and faster than existing methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1215–1223},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986594,
author = {Lopes, Miles E. and Jacob, Laurent and Wainwright, Martin J.},
title = {A More Powerful Two-Sample Test in High Dimensions Using Random Projection},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we first derive an asymptotic power function for our test, and then provide sufficient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1206–1214},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986593,
author = {Jiang, Jiarong and Rai, Piyush and III, Hal Daum\'{e}},
title = {Message-Passing for Approximate MAP Inference with Latent Variables},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this. The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework. We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efficacy of our proposed algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1197–1205},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986592,
author = {Ujfalussy, Bal\'{a}zs B and Lengyel, M\'{a}t\'{e}},
title = {Active Dendrites: Adaptation to Spike-Based Communication},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1188–1196},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986591,
author = {Shalev-Shwartz, Shai and Wexler, Yonatan and Shashua, Amnon},
title = {ShareBoost: Efficient Multiclass Learning with Feature Sharing},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that Share-Boost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of Share-Boost and evaluate its success relatively to other state-of-the-art approaches.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1179–1187},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986590,
author = {Wang, Jun and Do, Huyen and Woznica, Adam and Kalousis, Alexandros},
title = {Metric Learning with Multiple Kernels},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Metric learning has become a very active research field. The most popular representative–Mahalanobis metric learning–can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes finding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predefined kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in the area of metric learning with multiple kernel learning. In this paper we fill this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1170–1178},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986589,
author = {Sheldon, Daniel and Dietterich, Thomas G.},
title = {Collective Graphical Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {There are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information (counts or low-dimensional contingency tables). This paper introduces Collective Graphical Models—a framework for modeling and probabilistic inference that operates directly on the sufficient statistics of the individual model. We derive a highly-efficient Gibbs sampling algorithm for sampling from the posterior distribution of the sufficient statistics conditioned on noisy aggregate observations, prove its correctness, and demonstrate its effectiveness experimentally.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1161–1169},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986588,
author = {Wingate, David and Goodman, Noah D. and Stuhlm\"{u}ller, Andreas and Siskind, Jeffrey M.},
title = {Nonstandard Interpretations of Probabilistic Programs for Efficient Inference},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic programming languages allow modelers to specify a stochastic process using syntax that resembles modern programming languages. Because the program is in machine-readable format, a variety of techniques from compiler design and program analysis can be used to examine the structure of the distribution represented by the probabilistic program. We show how nonstandard interpretations of probabilistic programs can be used to craft efficient inference algorithms: information about the structure of a distribution (such as gradients or dependencies) is generated as a monad-like side computation while executing the program. These interpretations can be easily coded using special-purpose objects and operator overloading. We implement two examples of nonstandard interpretations in two different languages, and use them as building blocks to construct inference algorithms: automatic differentiation, which enables gradient based methods, and provenance tracking, which enables efficient construction of global proposals.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1152–11600},
numpages = {10449},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986587,
author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara L},
title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset – performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1143–1151},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986586,
author = {Mackey, Lester and Talwalkar, Ameet and Jordan, Michael I.},
title = {Divide-and-Conquer Matrix Factorization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work introduces Divide-Factor-Combine (DFC), a parallel divide-and-conquer framework for noisy matrix factorization. DFC divides a large-scale matrix factorization task into smaller subproblems, solves each subproblem in parallel using an arbitrary base matrix factorization algorithm, and combines the sub-problem solutions using techniques from randomized matrix approximation. Our experiments with collaborative filtering, video background modeling, and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach. Moreover, our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1134–1142},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986585,
author = {Ngiam, Jiquan and Koh, Pang Wei and Chen, Zhenghao and Bhaskar, Sonia and Ng, Andrew Y.},
title = {Sparse Filtering},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classification. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse filtering, a simple new algorithm which is efficient and only has one hyperparameter, the number of features to learn. In contrast to most other feature learning methods, sparse filtering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function – the sparsity of ℓ2-normalized features – which can easily be implemented in a few lines of MATLAB code. Sparse filtering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse filtering on natural images, object classification (STL-10), and phone classification (TIMET), and show that our method works well on a range of different modalities.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1125–1133},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986584,
author = {Gregor, Karol and Szlam, Arthur and LeCun, Yann},
title = {Structured Sparse Coding via Lateral Inhibition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modifications of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efficient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-specified.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1116–1124},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986583,
author = {Guillory, Andrew and Bilmes, Jeff},
title = {Online Submodular Set Cover, Ranking, and Repeated Active Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an online prediction version of submodular set cover with connections to ranking and repeated active learning. In each round, the learning algorithm chooses a sequence of items. The algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint. We develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight. Our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1107–1115},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986582,
author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli Z.},
title = {Budgeted Optimization with Concurrent Stochastic-Duration Experiments},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs. Typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments, which fail to capture important aspects of many real-world problems. This paper defines a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time. We develop both offline and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks. The results show that our algorithms produce highly effective schedules compared to natural baselines.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1098–1106},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986581,
author = {Waters, Andrew E. and Sankaranarayanan, Aswin C. and Baraniuk, Richard G.},
title = {SpaRCS: Recovering Low-Rank and Sparse Matrices from Compressive Measurements},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M)= A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, affine rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efficient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1089–1097},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986580,
author = {Garber, Dan and Hazan, Elad},
title = {Approximating Semidefinite Programs in Sublinear Time},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In recent years semidefinite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the first sublinear time approximation algorithm for semidefinite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1080–1088},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986579,
author = {Xiong, Liang and P\'{o}czos, Barnab\'{a}s and Schneider, Jeff},
title = {Group Anomaly Detection Using Flexible Genre Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1071–1079},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986578,
author = {Gao, Tianshi and Koller, Daphne},
title = {Active Classification Based on Value of Classifier},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1062–1070},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986577,
author = {Sorower, Mohammad Shahed and Dietterich, Thomas G. and Doppa, Janardhan Rao and Orr, Walker and Tadepalli, Prasad and Fern, Xiaoli},
title = {Inverting Grice's Maxims to Learn Rules from Natural Language Extractions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning rules from natural language text sources. These sources, such as news articles and web texts, are created by a writer to communicate information to a reader, where the writer and reader share substantial domain knowledge. Consequently, the texts tend to be concise and mention the minimum information necessary for the reader to draw the correct conclusions. We study the problem of learning domain knowledge from such concise texts, which is an instance of the general problem of learning in the presence of missing data. However, unlike standard approaches to missing data, in this setting we know that facts are more likely to be missing from the text in cases where the reader can infer them from the facts that are mentioned combined with the domain knowledge. Hence, we can explicitly model this "missingness" process and invert it via probabilistic inference to learn the underlying domain knowledge. This paper introduces a mention model that models the probability of facts being mentioned in the text based on what other facts have already been mentioned and domain knowledge in the form of Horn clause rules. Learning must simultaneously search the space of rules and learn the parameters of the mention model. We accomplish this via an application of Expectation Maximization within a Markov Logic framework. An experimental evaluation on synthetic and natural text data shows that the method can learn accurate rules and apply them to new texts to make correct inferences. Experiments also show that the method out-performs the standard EM approach that assumes mentions are missing at random.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1053–1061},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986576,
author = {Yang, Shulin Lynn and Rahimi, Ali},
title = {Structure Learning for Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems. The solutions of these are subsequently combined with message passing algorithms. We show empirically that these methods produce better solutions with fewer function evaluations than existing global optimization methods. To develop these methods, we introduce a notion of coupling between variables of optimization. This notion of coupling generalizes the notion of independence between random variables in statistics, sparseness of the Hessian in nonlinear optimization, and the generalized distributive law. Despite its generality, this notion of coupling is easier to verify empirically, making structure estimation easy, while allowing us to migrate well-established inference methods on graphical models to the setting of global optimization.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1044–1052},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986575,
author = {Agarwal, Alekh and Foster, Dean P. and Hsu, Daniel and Kakade, Sham M. and Rakhlin, Alexander},
title = {Stochastic Convex Optimization with Bandit Feedback},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set χ under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value f(x) at any query point x ∈ χ. We demonstrate a generalization of the ellipsoid algorithm that incurs \~{O}(poly (d) √T) regret. Since any algorithm has regret at least Ω(√T) on this problem, our algorithm is optimal in terms of the scaling with T.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1035–1043},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986574,
author = {Raginsky, Maxim and Rakhlin, Alexander},
title = {Lower Bounds for Passive and Active Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander's capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of "disagreement coefficient." For passive learning, our lower bounds match the upper bounds of Gin\'{e} and Koltchinskii up to constants and generalize analogous results of Massart and N\'{e}d\'{e}lec. For active learning, we provide first known lower bounds based on the capacity function rather than the disagreement coefficient.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1026–1034},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986573,
author = {Le, Quoc V. and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew Y.},
title = {ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it difficult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1017–1025},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986572,
author = {Sontag, David and Roy, Daniel M.},
title = {Complexity of Inference in Latent Dirichlet Allocation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA). First, we study the problem of finding the maximum a posteriori (MAP) assignment of topics to words, where the document's topic distribution is integrated out. We show that, when the effective number of topics per document is small, exact inference takes polynomial time. In contrast, we show that, when a document has a large number of topics, finding the MAP assignment of topics to words in LDA is NP-hard. Next, we consider the problem of finding the MAP topic distribution for a document, where the topic-word assignments are integrated out. We show that this problem is also NP-hard. Finally, we briefly discuss the problem of sampling from the posterior, showing that this is NP-hard in one restricted setting, but leaving open the general question.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1008–1016},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986571,
author = {Karklin, Yan and Simoncelli, Eero P.},
title = {Efficient Coding of Natural Images with a Population of Noisy Linear-Nonlinear Neurons},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Efficient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented filters consistent with responses in primary visual cortex. Here we show that an efficient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the firing rate – predicts receptive fields and response nonlinearities similar to those observed in the retina. Specifically, we develop numerical methods for simultaneously learning the linear filters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields filters that are center-surround and nonlinearities that are rectifying. The filters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have filters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear "contrast" function; in this case, the optimal filters are localized and oriented.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {999–1007},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986570,
author = {Perrault-Joncas, Dominique C. and Meil\u{a}, Marina},
title = {Directed Graph Embedding: An Algorithm Based on Continuous Limits of Laplacian-Type Operators},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algorithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {990–998},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986569,
author = {Turner, Richard E. and Sahani, Maneesh},
title = {Probabilistic Amplitude and Frequency Demodulation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude- and frequency-demodulation (AFD), there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed, we approach the problem using probabilistic inference. The new approach, called probabilistic amplitude and frequency demodulation (PAFD), models instantaneous frequency using an auto-regressive generalization of the von Mises distribution, and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {981–989},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986568,
author = {Tomioka, Ryota and Suzuki, Taiji and Hayashi, Kohei and Kashima, Hisashi},
title = {Statistical Performance of Convex Tensor Decomposition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {972–980},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986567,
author = {Ren, Lu and Wang, Yingjian and Dunson, David and Carin, Lawrence},
title = {The Kernel Beta Process},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new L\'{e}vy process prior is proposed for an uncountable collection of covariate-dependent feature-learning measures; the model is called the kernel beta process (KBP). Available covariates are handled efficiently via the kernel construction, with covariates assumed observed with each data sample ("customer"), and latent covariates learned for each feature ("dish"). Each customer selects dishes from an infinite buffet, in a manner analogous to the beta process, with the added constraint that a customer first decides probabilistically whether to "consider" a dish, based on the distance in covariate space between the customer and dish. If a customer does consider a particular dish, that dish is then selected probabilistically as in the beta process. The beta process is recovered as a limiting case of the KBP. An efficient Gibbs sampler is developed for computations, and state-of-the-art results are presented for image processing and music analysis tasks.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {963–971},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986566,
author = {Balakrishnan, Sivaraman and Xu, Min and Krishnamurthy, Akshay and Singh, Aarti},
title = {Noise Thresholds for Spectral Clustering},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {954–962},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986565,
author = {Koolen, Wouter M. and Kottowski, Wojtek and Warmuth, Manfred K.},
title = {Learning Eigenvectors for Free},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We extend the classical problem of predicting a sequence of outcomes from a finite alphabet to the matrix domain. In this extension, the alphabet of n outcomes is replaced by the set of all dyads, i.e. outer products uuΤ where u is a vector in ℙn of unit length. Whereas in the classical case the goal is to learn (i.e. sequentially predict as well as) the best multinomial distribution, in the matrix case we desire to learn the density matrix that best explains the observed sequence of dyads. We show how popular online algorithms for learning a multinomial distribution can be extended to learn density matrices. Intuitively, learning the n2 parameters of a density matrix is much harder than learning the n parameters of a multinomial distribution. Completely surprisingly, we prove that the worst-case regrets of certain classical algorithms and their matrix generalizations are identical. The reason is that the worst-case sequence of dyads share a common eigensystem, i.e. the worst case regret is achieved in the classical case. So these matrix algorithms learn the eigenvectors without any regret.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {945–953},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986564,
author = {Chen, Bo and Carlson, David E. and Carin, Lawrence},
title = {On the Analysis of Multi-Channel Neural Spike Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. The feature learning and sorting are performed simultaneously across all channels. Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. The dHDP is augmented to eliminate refractory-period violations, it allows the "appearance" and "disappearance" of neurons over time, and it models smooth variation in the spike statistics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {936–944},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986563,
author = {Kakade, Sham M. and Kalai, Adam Tauman and Kanade, Varun and Shamir, Ohad},
title = {Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the first provably efficient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (fitting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We modify the isotonic regression step in Isotron to fit a Lipschitz monotonic function, and also provide an efficient O(n log(n)) algorithm for this step, improving upon the previous O(n2) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {927–935},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986562,
author = {Mahadevan, Vijay and Wong, Chi Wah and Pereira, Jose Costa and Liu, Thomas T. and Vasconcelos, Nuno and Saul, Lawrence K.},
title = {Maximum Covariance Unfolding: Manifold Learning for Bimodal Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose maximum covariance unfolding (MCU), a manifold learning algorithm for simultaneous dimensionality reduction of data from different input modalities. Given high dimensional inputs from two different but naturally aligned sources, MCU computes a common low dimensional embedding that maximizes the cross-modal (inter-source) correlations while preserving the local (intra-source) distances. In this paper, we explore two applications of MCU. First we use MCU to analyze EEG-fMRI data, where an important goal is to visualize the fMRI voxels that are most strongly correlated with changes in EEG traces. To perform this visualization, we augment MCU with an additional step for metric learning in the high dimensional voxel space. Second, we use MCU to perform cross-modal retrieval of matched image and text samples from Wikipedia. To manage large applications of MCU, we develop a fast implementation based on ideas from spectral graph theory. These ideas transform the original problem for MCU, one of semidefinite programming, into a simpler problem in semidefinite quadratic linear programming.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {918–926},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986561,
author = {Kolar, Mladen and Balakrishnan, Sivaraman and Rinaldo, Alessandro and Singh, Aarti},
title = {Minimax Localization of Structural Information in Large Noisy Matrices},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a flurry of activity. We make the following contributions1. We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest.2. We show that a combinatorial procedure based on the scan statistic achieves this optimal limit.3. We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {909–917},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986560,
author = {Xiang, Zhen James and Xu, Hao and Ramadge, Peter J.},
title = {Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efficiently.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {900–908},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986559,
author = {Hazan, Elad and Kale, Satyen},
title = {NEWTRON: An Efficient Bandit Algorithm for Online Multiclass Prediction},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an efficient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss defined in [AR09], which is parameterized by a scalar a. We prove that the regret of NEWTRON is O(log T) when α is a constant that does not vary with horizon T, and at most O(T2/3) if α is allowed to increase to infinity with T. For α = O(log T), the regret is bounded by O(√T), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {891–899},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986558,
author = {Tewari, Ambuj and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Greedy Algorithms for Structurally Constrained High Dimensional Problems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A hallmark of modern machine learning is its ability to deal with high dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying model. A deep understanding of the capabilities and limits of high dimensional learning methods under specific assumptions such as sparsity, group sparsity, and low rank has been attained. Efforts [1, 2] are now underway to distill this valuable experience by proposing general unified frameworks that can achieve the twin goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored. Inspired by these developments, we propose and analyze a general computational scheme based on a greedy strategy to solve convex optimization problems that arise when dealing with structurally constrained high-dimensional problems. Our framework not only unifies existing greedy algorithms by recovering them as special cases but also yields novel ones. Finally, we extend our results to infinite dimensional settings by using interesting connections between smoothness of norms and behavior of martingales in Banach spaces.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {882–890},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986557,
author = {Agarwal, Alekh and Duchi, John C.},
title = {Distributed Delayed Stochastic Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The main application of our results is to the development of distributed minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to asynchrony. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible. In application to distributed optimization, we show n-node architectures whose optimization error in stochastic problems—in spite of asynchronous delays—scales asymptotically as O(1/√nT), which is known to be optimal even in the absence of delays.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {873–881},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986556,
author = {Wang, Xinggang and Bai, Xiang and Yang, Xingwei and Liu, Wenyu and Latecki, Longin Jan},
title = {Maximal Cliques That Satisfy Hard Constraints with Application to Deformable Object Model Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution. The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {864–872},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986555,
author = {El-Yaniv, Ran and Pidan, Dmitry},
title = {Selective Prediction of Financial Trends with Hidden Markov Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Focusing on short term trend prediction in a financial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The first is a rejection in the spirit of Chow's well-known ambiguity principle. The second is a specialized mechanism for HMMs that identifies low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {855–863},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986554,
author = {Rad, Kamiar Rahnama and Paninski, Liam},
title = {Information Rates and Optimal Decoding in Large Neural Populations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of Shannon information rates in populations of spiking neurons. In this paper, we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities. We find that for large neural populations carrying a finite total amount of information, the full spiking population response is asymptotically as informative as a single observation from a Gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties. The Gaussian form of this asymptotic sufficient statistic allows us in certain cases to perform optimal Bayesian decoding by simple linear transformations, and to obtain closed-form expressions of the Shannon information carried by the network. One technical advantage of the theory is that it may be applied easily even to non-Poisson point process network models; for example, we find that under some conditions, neural populations with strong history-dependent (non-Poisson) effects carry exactly the same information as do simpler equivalent populations of non-interacting Poisson neurons with matched firing rates. We argue that our findings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {846–854},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986553,
author = {Ge, Xiaoyin and Safa, Issam and Belkin, Mikhail and Wang, Yusu},
title = {Data Skeletonization via Reeb Graphs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recovering hidden structure from complex and noisy non-linear data is one of the most fundamental problems in machine learning and statistical inference. While such data is often high-dimensional, it is of interest to approximate it with a low-dimensional or even one-dimensional space, since many important aspects of data are often intrinsically low-dimensional. Furthermore, there are many scenarios where the underlying structure is graph-like, e.g, river/road networks or various trajectories. In this paper, we develop a framework to extract, as well as to simplify, a one-dimensional "skeleton" from unorganized data using the Reeb graph. Our algorithm is very simple, does not require complex optimizations and can be easily applied to unorganized high-dimensional data such as point clouds or proximity graphs. It can also represent arbitrary graph structures in the data. We also give theoretical results to justify our method. We provide a number of experiments to demonstrate the effectiveness and generality of our algorithm, including comparisons to existing methods, such as principal curves. We believe that the simplicity and practicality of our algorithm will help to promote skeleton graphs as a data analysis tool for a broad range of applications.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {837–845},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986552,
author = {Kapralov, Michael and Panigrahy, Rina},
title = {Prediction Strategies without Loss},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say 'predict 0' or 'predict 1', and our payoff is +1 if the prediction is correct and –1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14∊T and loss 2√T e−∊∊T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors.Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the 'special' expert has been studied by Even-Dar et al. (COLT'07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O(√n(log N + log T)), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {828–836},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986551,
author = {Teh, Yee Whye and Blundell, Charles and Elliott, Lloyd T.},
title = {Modelling Genetic Variations with Fragmentation-Coagulation Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel class of Bayesian nonparametric models for sequential data called fragmentation-coagulation processes (FCPs). FCPs model a set of sequences using a partition-valued Markov process which evolves by splitting and merging clusters. An FCP is exchangeable, projective, stationary and reversible, and its equilibrium distributions are given by the Chinese restaurant process. As opposed to hidden Markov models, FCPs allow for flexible modelling of the number of clusters, and they avoid label switching non-identifiability problems. We develop an efficient Gibbs sampler for FCPs which uses uniformization and the forward-backward algorithm. Our development of FCPs is motivated by applications in population genetics, and we demonstrate the utility of FCPs on problems of genotype imputation with phased and unphased SNP data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {819–827},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986550,
author = {Ailon, Nir},
title = {Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a set V of n elements we wish to linearly order them using pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(n poly(log n, ε-1)) preference labels for a regret of e times the optimal loss. This is strictly better, and often significantly better than what non-adaptive sampling could achieve. Our main result helps settle an open problem posed by learning-to-rank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels?},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {810–818},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986549,
author = {Socher, Richard and Huang, Eric H. and Pennington, Jeffrey and Ng, Andrew Y. and Manning, Christopher D.},
title = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {801–809},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986548,
author = {Chierichetti, Flavio and Kleinberg, Jon and Liben-Nowell, David},
title = {Reconstructing Patterns of Information Diffusion from Incomplete Observations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by the spread of on-line information in general and on-line petitions in particular, recent research has raised the following combinatorial estimation problem. There is a tree T that we cannot observe directly (representing the structure along which the information has spread), and certain nodes randomly decide to make their copy of the information public. In the case of a petition, the list of names on each public copy of the petition also reveals a path leading back to the root of the tree. What can we conclude about the properties of the tree we observe from these revealed paths, and can we use the structure of the observed tree to estimate the size of the full unobserved tree T?Here we provide the first algorithm for this size estimation task, together with provable guarantees on its performance. We also establish structural properties of the observed tree, providing the first rigorous explanation for some of the unusual structural phenomena present in the spread of real chain-letter petitions on the Internet.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {792–800},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986547,
author = {Munos, R\'{e}mi},
title = {Optimistic Optimization of a Deterministic Function without the Knowledge of Its Smoothness},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a global optimization problem of a deterministic function f in a semi-metric space, given a finite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A first contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then define a second algorithm, SOO, which does not require the knowledge of the semi-metric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-fitted.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {783–791},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986546,
author = {Orhan, A. Emin and Jacobs, Robert A.},
title = {Probabilistic Modeling of Dependencies among Visual Short-Term Memory Representations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Extensive evidence suggests that items are not encoded independently in visual short-term memory (VSTM). However, previous research has not quantitatively considered how the encoding of an item influences the encoding of other items. Here, we model the dependencies among VSTM representations using a multivariate Gaussian distribution with a stimulus-dependent mean and covariance matrix. We report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix. We find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items' feature values, similar to a Gaussian process with a distance-dependent, stationary kernel function. We further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {774–782},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986545,
author = {Gehler, Peter Vincent and Rother, Carsten and Kiefel, Martin and Zhang, Lumin and Sch\"{o}lkopf, Bernhard},
title = {Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reflectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we are able to improve on state-of-the-art results by integrating edge information into our model. We believe that our new approach is an excellent starting point for future developments in this field.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {765–773},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986544,
author = {Petreska, Biljana and Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
title = {Dynamical Segmentation of Single Trials from Population Neural Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Simultaneous recordings of many neurons embedded within a recurrently-connected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identified by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identified without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the firing rate of an isolated neuron based on the firing rates of others, suggesting that it captures more of the "shared variance" of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reflect its computational role.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {756–764},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986543,
author = {Kayala, Matthew A. and Baldi, Pierre},
title = {A Machine Learning Approach to Predict Chemical Reactions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry. Previous approaches are not high-throughput, are not generalizable or scalable, or lack sufficient data to be effective. We describe single mechanistic reactions as concerted electron movements from an electron orbital source to an electron orbital sink. We use an existing rule-based expert system to derive a dataset consisting of 2,989 productive mechanistic steps and 6.14 million non-productive mechanistic steps. We then pose identifying productive mechanistic steps as a ranking problem: rank potential orbital interactions such that the top ranked interactions yield the major products. The machine learning implementation follows a two-stage approach, in which we first train atom level reactivity filters to prune 94.0% of non-productive reactions with less than a 0.1% false negative rate. Then, we train an ensemble of ranking models on pairs of interacting orbitals to learn a relative productivity function over single mechanistic reactions in a given system. Without the use of explicit transformation patterns, the ensemble perfectly ranks the productive mechanisms at the top 89.1% of the time, rising to 99.9% of the time when top ranked lists with at most four nonproductive reactions are considered. The final system allows multi-step reaction prediction. Furthermore, it is generalizable, making reasonable predictions over reactants and conditions which the rule-based expert system does not handle.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {747–755},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986542,
author = {Pitkow, Xaq and Ahmadian, Yashar and Miller, Ken D.},
title = {Learning Unbelievable Probabilities},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals 'unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {738–746},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986541,
author = {Kpotufe, Samory},
title = {<i>K</i>-NN Regression Adapts to Local Intrinsic Dimension},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data. These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic dimension. In particular our rates are local to a query x and depend only on the way masses of balls centered at x vary with radius.Furthermore, we show a simple way to choose k = k(x) locally at any x so as to nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension in the vicinity of x. We also establish that the minimax rate does not depend on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {729–737},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986540,
author = {Barreto, Andr\'{e} M. S. and Precup, Doina and Pineau, Joelle},
title = {Reinforcement Learning Using Kernel-Based Stochastic Factorization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Kernel-based reinforcement-learning (KBRL) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees. However, the size of the approximator grows with the number of transitions, which makes the approach impractical for large problems. In this paper we introduce a novel algorithm to improve the scalability of KBRL. We resort to a special decomposition of a transition matrix, called stochastic factorization, to fix the size of the approximator while at the same time incorporating all the information contained in the data. The resulting algorithm, kernel-based stochastic factorization (KBSF), is much faster but still converges to a unique solution. We derive a theoretical upper bound for the distance between the value functions computed by KBRL and KBSF. The effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems, including a difficult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains. We empirically demonstrate that the proposed approach is able to compress the information contained in KBRL's model. Also, on the tasks studied, KBSF outperforms two of the most prominent reinforcement-learning algorithms, namely least-squares policy iteration and fitted Q-iteration.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {720–728},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986539,
author = {Leibo, Joel Z and Mutch, Jim and Poggio, Tomaso},
title = {Why the Brain Separates Face Recognition from Object Recognition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes. Recent electro-physiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpoint-specific cells projecting to downstream viewpoint-invariant identity-specific cells [1]. A separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-specific. In particular, the 2D images evoked by a face undergoing a 3D rotation are not produced by the same image transformation (2D) that would produce the images evoked by an object of another class undergoing the same 3D rotation. However, within the class of faces, knowledge of the image transformation evoked by 3D rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint. We show, through computational simulations, that an architecture which applies this method of gaining invariance to class-specific transformations is effective when restricted to faces and fails spectacularly when applied to other object classes. We argue here that in order to accomplish viewpoint-invariant face identification from a single example view, visual cortex must separate the circuitry involved in discounting 3D rotations of faces from the generic circuitry involved in processing other objects. The resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {711–719},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986538,
author = {Zhou, Jiayu and Chen, Jianhui and Ye, Jieping},
title = {Clustered Multi-Task Learning via Alternating Structure Optimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing significant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is significantly more efficient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efficiency of the proposed algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {702–710},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986537,
author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
title = {HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {693–701},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986536,
author = {Mannor, Shie and Shamir, Ohad},
title = {From Bandits to Experts: On the Value of Side-Observations},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known "experts" setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {684–692},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986535,
author = {Kurtek, Sebastian and Srivastava, Anuj and Wu, Wei},
title = {Signal Estimation under Random Time-Warpings and Nonlinear Signal Alignment},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While signal estimation under random amplitudes, phase shifts, and additive noise is studied frequently, the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored. We present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals. First, we derive an estimator for the equivalence class of the unknown signal using the notion of Karcher mean on the quotient space of equivalence classes. This step requires the use of Fisher-Rao Riemannian metric and a square-root representation of signals to enable computations of distances and means under this metric. Then, we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal. This estimation algorithm has many applications: (1) registration/alignment of functional data, (2) separation of phase/amplitude components of functional data, (3) joint demodulation and carrier estimation, and (4) sparse modeling of functional data. Here we demonstrate only (1) and (2): Given signals are temporally aligned using nonlinear warpings and, thus, separated into their phase and amplitude components. The proposed method for signal alignment is shown to have state of the art performance using Berkeley growth, handwritten signatures, and neuroscience spike train data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {675–683},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986534,
author = {Delalleau, Olivier and Bengio, Yoshua},
title = {Shallow vs. Deep Sum-Product Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate the representational power of sum-product networks (computation networks analogous to neural networks, but whose individual units compute either products or weighted sums), through a theoretical analysis that compares deep (multiple hidden layers) vs. shallow (one hidden layer) architectures. We prove there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one, i.e. with substantially fewer hidden units. Such results were not available until now, and contribute to motivate recent research involving learning of deep sum-product networks, and more generally motivate research in Deep Learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {666–674},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986533,
author = {Wiesler, Simon and Ney, Hermann},
title = {A Convergence Analysis of Log-Linear Training},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Log-linear models are widely used probability models for statistical pattern recognition. Typically, log-linear models are trained according to a convex criterion. In recent years, the interest in log-linear models has greatly increased. The optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers. In this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. We verify our findings on two handwriting tasks. By making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {657–665},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986532,
author = {Jain, Viren and Turaga, Srinivas C. and Briggman, Kevin L. and Helmstaedter, Moritz N. and Denk, Winfried and Seung, H. Sebastian},
title = {Learning to Agglomerate Superpixel Hierarchies},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally hand-designed, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {648–656},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986531,
author = {Mooij, Joris M. and Janzing, Dominik and Heskes, Tom and Sch\"{o}lkopf, Bernhard},
title = {On Causal Discovery with Cyclic Additive Noise Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a particular class of cyclic causal models, where each variable is a (possibly nonlinear) function of its parents and additive noise. We prove that the causal graph of such models is generically identifiable in the bivariate, Gaussian-noise case. We also propose a method to learn such models from observational data. In the acyclic case, the method reduces to ordinary regression, but in the more challenging cyclic case, an additional term arises in the loss function, which makes it a special case of nonlinear independent component analysis. We illustrate the proposed method on synthetic data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {639–647},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986530,
author = {Stegle, Oliver and Lippert, Christoph and Mooij, Joris and Lawrence, Neil and Borgwardt, Karsten},
title = {Efficient Inference in Matrix-Variate Gaussian Models with Iid Observation Noise},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inference in matrix-variate Gaussian models has major applications for multi-output prediction and joint learning of row and column covariances from matrix-variate data. Here, we discuss an approach for efficient inference in such models that explicitly account for iid observation noise. Computational tractability can be retained by exploiting the Kronecker product between row and column covariance matrices. Using this framework, we show how to generalize the Graphical Lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples. We show practical utility on applications to biology, where we model covariances with more than 100,000 dimensions. We find greater accuracy in recovering biological network structures and are able to better reconstruct the confounders.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {630–638},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986529,
author = {Hwang, Sung Ju and Grauman, Kristen and Sha, Fei},
title = {Learning a Tree of Metrics with Disjoint Visual Features},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships. Given a hierarchical taxonomy that captures semantic similarity between the objects, we learn a corresponding tree of metrics (ToM). In this tree, we have one metric for each non-leaf node of the object hierarchy, and each metric is responsible for discriminating among its immediate subcategory children. Specifically, a Mahalanobis metric learned for a given node must satisfy the appropriate (dis)similarity constraints generated only among its subtree members' training instances. To further exploit the semantics, we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor (supercategory) nodes' metrics. Intuitively, this reflects that visual cues most useful to distinguish the generic classes (e.g., feline vs. canine) should be different than those cues most useful to distinguish their component fine-grained classes (e.g., Persian cat vs. Siamese cat). We validate our approach with multiple image datasets using the WordNet taxonomy, show its advantages over alternative metric learning approaches, and analyze the meaning of attribute features selected by our algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {621–629},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986528,
author = {Lin, Zhouchen and Liu, Risheng and Su, Zhixun},
title = {Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the sub-problems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3) of the original ADM based method to O(rn2), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {612–620},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986527,
author = {Mau\'{a}, Denis D. and de Campos, Cassio P.},
title = {Solving Decision Problems with Limited Information},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new algorithm for exactly solving decision-making problems represented as an influence diagram. We do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information. The algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and 1064 strategies.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {603–611},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986526,
author = {Yamada, Makoto and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Sugiyama, Masashi},
title = {Relative Density-Ratio Estimation for Robust Distribution Comparison},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high fluctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overfits even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {594–602},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986525,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
title = {The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classifiers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the first one shows that, using the unlabeled samples, the confidence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classifier.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {585–593},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986524,
author = {Takeuchi, Ichiro and Sugiyama, Masashi},
title = {Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider feature selection and weighting for nearest neighbor classifiers. A technical challenge in this scenario is how to cope with discrete update of nearest neighbors when the feature space metric is changed during the learning process. This issue, called the target neighbor change, was not properly addressed in the existing feature weighting and metric learning literature. In this paper, we propose a novel feature weighting algorithm that can exactly and efficiently keep track of the correct target neighbors via sequential quadratic programming. To the best of our knowledge, this is the first algorithm that guarantees the consistency between target neighbors and the feature space metric. We further show that the proposed algorithm can be naturally combined with regularization path tracking, allowing computationally efficient selection of the regularization parameter. We demonstrate the effectiveness of the proposed algorithm through experiments.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {576–584},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986523,
author = {Deng, Jia and Satheesh, Sanjeev and Berg, Alexander C. and Fei-Fei, Li},
title = {Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {567–575},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986522,
author = {Gomes, Ryan and Welinder, Peter and Krause, Andreas and Perona, Pietro},
title = {Crowdclustering},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Is it possible to crowdsource categorization? Amongst the challenges: (a) each worker has only a partial view of the data, (b) different workers may have different clustering criteria and may produce different numbers of categories, (c) the underlying category structure may be hierarchical. We propose a Bayesian model of how workers may approach clustering and show how one may infer clusters / categories, as well as worker parameters, using this model. Our experiments, carried out on large collections of images, suggest that Bayesian crowdclustering works well and may be superior to single-expert annotations.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {558–566},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986521,
author = {Li, Congcong and Saxena, Ashutosh and Chen, Tsuhan},
title = {θ-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For most scene understanding tasks (such as object detection or depth estimation), the classifiers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by defining a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {549–557},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986520,
author = {Pham, Trung Thanh and Chin, Tat-Jun and Yu, Jin and Suter, David},
title = {Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-structure model fitting has traditionally taken a two-stage approach: First, sample a (large) number of model hypotheses, then select the subset of hypotheses that optimise a joint fitting and model selection criterion. This disjoint two-stage approach is arguably suboptimal and inefficient — if the random sampling did not retrieve a good set of hypotheses, the optimised outcome will not represent a good fit. To overcome this weakness we propose a new multi-structure fitting approach based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our method is an adaptive hypothesis generator, whose proposal distribution is learned incrementally and online. We prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in MCMC. Our method effectively conducts hypothesis sampling and optimisation simultaneously, and yields superior computational efficiency over previous two-stage methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {540–548},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986519,
author = {Zhang, Youwei and El Ghaoui, Laurent},
title = {Large-Scale Sparse Principal Component Analysis with Application to Text Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {532–539},
numpages = {8},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986518,
author = {Armagan, Artin and Dunson, David B. and Clyde, Merlise},
title = {Generalized Beta Mixtures of Gaussians},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {523–531},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986517,
author = {Austerweil, Joseph L. and Friesen, Abram L. and Griffiths, Thomas L.},
title = {An Ideal Observer Model for Identifying the Reference Frame of Objects},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols \texttimes{} and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conflicting reference frames, the model predicts two factors should influence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We confirm people use both cues using a novel methodology that allows for easy testing of human reference frame inference.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {514–522},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986516,
author = {Sun, Qian and Chattopadhyay, Rita and Panchanathan, Sethuraman and Ye, Jieping},
title = {A Two-Stage Weighting Framework for Multi-Source Domain Adaptation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Discriminative learning when training and test data belong to different distributions is a challenging and complex task. Often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions. The difference in distributions may be both in marginal and conditional probabilities. Most of the existing domain adaptation work focuses on the marginal probability distribution difference between the domains, assuming that the conditional probabilities are similar. However in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences. In this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted Rademacher complexity measure. Empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {505–513},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986515,
author = {Newman, David and Bonilla, Edwin V. and Buntine, Wray},
title = {Improving Topic Coherence with Regularized Topic Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reflect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {496–504},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986514,
author = {Liu, Jun and Sun, Liang and Ye, Jieping},
title = {Projection onto a Nonnegative Max-Heap},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of computing the Euclidean projection of a vector of length p onto a non-negative max-heap—an ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value(s) of its child node(s). This Euclidean projection plays a building block role in the optimization problem with a non-negative max-heap constraint. Such a constraint is desirable when the features follow an ordered tree structure, that is, a given feature is selected for the given regression/classification task only if its parent node is selected. In this paper, we show that such Euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to find the so-called maximal root-tree of the subtree rooted at each node. A naive approach for finding the maximal root-tree is to enumerate all the possible root-trees, which, however, does not scale well. We reveal several important properties of the maximal root-tree, based on which we design a bottom-up algorithm with merge for efficiently finding the maximal root-tree. The proposed algorithm has a (worst-case) linear time complexity for a sequential list, and O(p2) for a general tree. We report simulation results showing the effectiveness of the max-heap for regression with an ordered tree structure. Empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list, a full binary tree, and a tree with depth 1.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {487–495},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986513,
author = {Sricharan, Kumar and Hero, Alfred O.},
title = {Efficient Anomaly Detection Using Bipartite <i>k</i>-NN Graphs},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1O-kNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {478–486},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986512,
author = {Keil, Matthias S.},
title = {Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, &amp; Fits},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many species show avoidance reactions in response to looming object approaches. In locusts, the corresponding escape behavior correlates with the activity of the lobula giant movement detector (LGMD) neuron. During an object approach, its firing rate was reported to gradually increase until a peak is reached, and then it declines quickly. The η-function predicts that the LGMD activity is a product between an exponential function of angular size exp(–Θ) and angular velocity Θ, and that peak activity is reached before time-to-contact (ttc). The η-function has become the prevailing LGMD model because it reproduces many experimental observations, and even experimental evidence for the multiplicative operation was reported. Several inconsistencies remain unresolved, though. Here we address these issues with a new model (ψ-model), which explicitly connects Θ and Θ to biophysical quantities. The ψ-model avoids biophysical problems associated with implementing exp(·), implements the multiplicative operation of η via divisive inhibition, and explains why activity peaks could occur after ttc. It consistently predicts response features of the LGMD, and provides excellent fits to published experimental data, with goodness of fit measures comparable to corresponding fits with the η-function.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {469–477},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986511,
author = {Jegelka, Stefanie and Lin, Hui and Bilmes, Jeff},
title = {On Fast Approximate Submodular Minimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact, for our application, minimum norm can have a running time of about O(n7) (O(n5) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions, the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized. We show theoretical properties, and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {460–468},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986510,
author = {Bach, Francis and Moulines, Eric},
title = {Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a. Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a. Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {451–459},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986509,
author = {Girshick, Ross B. and Felzenszwalb, Pedro F. and McAllester, David},
title = {Object Detection with Grammar Models},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {442–450},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986508,
author = {Lin, Binbin and Zhang, Chiyuan and He, Xiaofei},
title = {Semi-Supervised Regression via Parallel Field Regularization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the problem of semi-supervised learning from the vector field perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems. To achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient field of a linear function has to be a parallel vector field. Consequently, we propose to find a function which minimizes the empirical error, and simultaneously requires its gradient field to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently. The experimental results have demonstrated the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {433–441},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986507,
author = {Bogojeska, Jasmina},
title = {History Distribution Matching Method for Predicting Effectiveness of HIV Combination Therapies},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents an approach that predicts the effectiveness of HIV combination therapies by simultaneously addressing several problems affecting the available HIV clinical data sets: the different treatment backgrounds of the samples, the uneven representation of the levels of therapy experience, the missing treatment history information, the uneven therapy representation and the unbalanced therapy outcome representation. The computational validation on clinical data shows that, compared to the most commonly used approach that does not account for the issues mentioned above, our model has significantly higher predictive power. This is especially true for samples stemming from patients with longer treatment history and samples associated with rare therapies. Furthermore, our approach is at least as powerful for the remaining samples.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {424–432},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986506,
author = {Mont\'{u}far, Guido F. and Rauh, Johannes and Ay, Nihat},
title = {Expressive Power and Approximation Errors of Restricted Boltzmann Machines},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present explicit classes of probability distributions that can be learned by Restricted Boltzmann Machines (RBMs) depending on the number of units that they contain, and which are representative for the expressive power of the model. We use this to show that the maximal Kullback-Leibler divergence to the RBM model with n visible and m hidden units is bounded from above by (n – 1) – log(m + 1). In this way we can specify the number of hidden units that guarantees a sufficiently rich model containing different classes of distributions and respecting a given error tolerance.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {415–423},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986505,
author = {Boumal, Nicolas and Absil, P.-A.},
title = {RTRMC: A Riemannian Trust-Region Method for Low-Rank Matrix Completion},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider large matrices of low rank. We address the problem of recovering such matrices when most of the entries are unknown. Matrix completion finds applications in recommender systems. In this setting, the rows of the matrix may correspond to items and the columns may correspond to users. The known entries are the ratings given by users to some items. The aim is to predict the unobserved ratings. This problem is commonly stated in a constrained optimization framework. We follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the Grassmann manifold. We then apply first- and second-order Riemannian trust-region methods to solve it. The cost of each iteration is linear in the number of known entries. Our methods, RTRMC 1 and 2, outperform state-of-the-art algorithms on a wide range of problem instances.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {406–414},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986504,
author = {Jia, Yangqing and Darrell, Trevor},
title = {Heavy-Tailed Distances for Gradient Based Image Descriptors},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that fit the empirical data distribution. We instantiate this similarity measure with the Gamma-compound-Laplace distribution, and show significant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {397–405},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986503,
author = {Hwang, Yoonho and Ahn, Hee-Kap},
title = {Convergent Bounds on the Euclidean Distance},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a set V of n vectors in d-dimensional space, we provide an efficient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V. For this purpose, we define a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V. Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be refined further in such a way to converge monotonically to the exact Euclidean distance within d refinement steps. An analysis on a random sequence of refinement steps shows that the MS-distance provides very tight bounds in only a few refinement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {388–396},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986502,
author = {Alamgir, Morteza and Luxburg, Ulrike von},
title = {Phase Transition in the Family of <i>p</i>-Resistances},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the family of p-resistances on graphs for p ≥ 1. This family generalizes the standard resistance distance. We prove that for any fixed graph, for p =1 the p-resistance coincides with the shortest path distance, for p = 2 it coincides with the standard resistance distance, and for p → ∞ it converges to the inverse of the minimal s-t-cut in the graph. Secondly, we consider the special case of random geometric graphs (such as k-nearest neighbor graphs) when the number n of vertices in the graph tends to infinity. We prove that an interesting phase transition takes place. There exist two critical thresholds p* and p** such that if p&lt; p*, then the p-resistance depends on meaningful global properties of the graph, whereas if p&gt;p**, it only depends on trivial local quantities and does not convey any useful information. We can explicitly compute the critical values: p* = 1 + 1/(d − 1) and p** = 1 + 1/(d − 2) where d is the dimension of the underlying space (we believe that the fact that there is a small gap between p* and p** is an artifact of our proofs). We also relate our findings to Laplacian regularization and suggest to use q-Laplacians as regularizers, where q satisfies 1/p* + 1/q =1.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {379–387},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986501,
author = {Wnuk, Kamil and Soatto, Stefano},
title = {Multiple Instance Filtering},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a robust filtering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the effect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard finite-dimensional filtering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classification) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set).},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {370–378},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986500,
author = {Jing, Lei},
title = {Differentially Private M-Estimators},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies privacy preserving M-estimators using perturbed histograms. The proposed approach allows the release of a wide class of M-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure. The performance of the proposed method is demonstrated through a careful study of the convergence rates. A practical algorithm is given and applied on a real world data set containing both continuous and categorical variables.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {361–369},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986499,
author = {Yuan, Lei and Liu, Jun and Ye, Jieping},
title = {Efficient Methods for Overlapping Group Lasso},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {352–360},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986498,
author = {Cesa-Bianchi, Nicol\`{o} and Shamir, Ohad},
title = {Efficient Online Learning via Randomized Rounding},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely different approach, which combines "random playout" and randomized rounding of loss subgradients. As an application of our approach, we provide the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {343–351},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986497,
author = {Wang, Weiran and Carreira-Perpi\~{n}\'{a}n, Miguel \'{A}. and Lu, Zhengdong},
title = {A Denoising View of Matrix Completion},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In matrix completion, we are given a matrix where the values of only some of the entries are present, and we want to reconstruct the missing ones. Much work has focused on the assumption that the data matrix has low rank. We propose a more general assumption based on denoising, so that we expect that the value of a missing entry can be predicted from the values of neighboring points. We propose a nonparametric version of denoising based on local, iterated averaging with mean-shift, possibly constrained to preserve local low-rank manifold structure. The few user parameters required (the denoising scale, number of neighbors and local dimensionality) and the number of iterations can be estimated by cross-validating the reconstruction error. Using our algorithms as a postprocessing step on an initial reconstruction (provided by e.g. a low-rank method), we show consistent improvements with synthetic, image and motion-capture data.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {334–342},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986496,
author = {Hennig, Philipp},
title = {Optimal Reinforcement Learning for Gaussian Systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The exploration-exploitation trade-off is among the central challenges of reinforcement learning. The optimal Bayesian solution is intractable in general. This paper studies to what extent analytic statements about optimal learning are possible if all beliefs are Gaussian processes. A first order approximation of learning of both loss and dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics, is described by an infinite-dimensional partial differential equation. An approximate finite-dimensional projection gives an impression for how this result may be helpful.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {325–333},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986495,
author = {Kemp, Charles},
title = {Inductive Reasoning about Chimeric Creatures},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given one feature of a novel animal, humans readily make inferences about other features of the animal. For example, winged creatures often fly, and creatures that eat fish often live in the water. We explore the knowledge that supports these inferences and compare two approaches. The first approach proposes that humans rely on abstract representations of dependency relationships between features, and is formalized here as a graphical model. The second approach proposes that humans rely on specific knowledge of previously encountered animals, and is formalized here as a family of exemplar models. We evaluate these models using a task where participants reason about chimeras, or animals with pairs of features that have not previously been observed to co-occur. The results support the hypothesis that humans rely on explicit representations of relationships between features.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {316–324},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986494,
author = {Silva, Ricardo},
title = {Thinning Measurement Models and Questionnaire Design},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inferring key unobservable features of individuals is an important task in the applied sciences. In particular, an important source of data in fields such as marketing, social sciences and medicine is questionnaires: answers in such questionnaires are noisy measures of target unobserved features. While comprehensive surveys help to better estimate the latent variables of interest, aiming at a high number of questions comes at a price: refusal to participate in surveys can go up, as well as the rate of missing data; quality of answers can decline; costs associated with applying such questionnaires can also increase. In this paper, we cast the problem of refining existing models for questionnaire data as follows: solve a constrained optimization problem of preserving the maximum amount of information found in a latent variable model using only a subset of existing questions. The goal is to find an optimal subset of a given size. For that, we first define an information theoretical measure for quantifying the quality of a reduced questionnaire. Three different approximate inference methods are introduced to solve this problem. Comparisons against a simple but powerful heuristic are presented.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {307–315},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986493,
author = {Chen, Ke and Salman, Ahmad},
title = {Extracting Speaker-Specific Information with a Regularized Siamese Deep Network},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Speech conveys different yet mixed information ranging from linguistic to speaker-specific components, and each of them should be exclusively used in a specific task. However, it is extremely difficult to extract a specific information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper, we present a deep neural architecture to extract speaker-specific information from MFCCs. As a result, a multi-objective loss function is proposed for learning speaker-specific characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus, we demonstrate that a resultant speaker-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {298–306},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986492,
author = {Lampert, Christoph H.},
title = {Maximum Margin Multi-Label Structured Prediction},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multi-label classification techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems.In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneficial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efficient working set training, and PAC-Bayesian generalization bounds.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {289–297},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986491,
author = {Hern\'{a}ndez-Lobato, Daniel and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Dupont, Pierre},
title = {Robust Multi-Class Gaussian Process Classification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-class Gaussian Process Classifiers (MGPCs) are often affected by over-fitting problems when labeling errors occur far from the decision boundaries. To prevent this, we investigate a robust MGPC (RMGPC) which considers labeling errors independently of their distance to the decision boundaries. Expectation propagation is used for approximate inference. Experiments with several datasets in which noise is injected in the labels illustrate the benefits of RMGPC. This method performs better than other Gaussian process alternatives based on considering latent Gaussian noise or heavy-tailed processes. When no noise is injected in the labels, RMGPC still performs equal or better than the other methods. Finally, we show how RMGPC can be used for successfully identifying data instances which are difficult to classify correctly in practice.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {280–288},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986490,
author = {Gkioulekas, Ioannis and Zickler, Todd},
title = {Dimensionality Reduction Using the Sparse Linear Model},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {271–279},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986489,
author = {Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},
title = {Analysis and Improvement of Policy Gradient Estimation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We first prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {262–270},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986488,
author = {Ding, Shilin and Wahba, Grace and Zhu, Xiaojin},
title = {Learning Higher-Order Graph Structure with Features by Structure Penalty},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In discrete undirected graphical models, the conditional independence of node labels Y is specified by the graph structure. We study the case where there is another input random vector X (e.g. observed features) such that the distribution P(Y | X) is determined by functions of X that characterize the (higher-order) interactions among the Y's. The main contribution of this paper is to learn the graph structure and the functions conditioned on X at the same time. We prove that discrete undirected graphical models with feature X are equivalent to multivariate discrete models. The reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure. The functional spaces can be flexibly determined by kernels. Additionally, we impose a Structure Lasso (SLasso) penalty on groups of functions to learn the graph structure. These groups with overlaps are designed to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {253–261},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986487,
author = {Koppula, Hema Swetha and Anand, Abhishek and Joachims, Thorsten and Saxena, Ashutosh},
title = {Semantic Labeling of 3D Point Clouds for Indoor Scenes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model's parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for offices, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {244–252},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986486,
author = {Le, Hai-Son and Bar-Joseph, Ziv},
title = {Inferring Interaction Networks Using the IBP Applied to MicroRNA Target Prediction},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data. Here we extend the Indian Buffet Process (IBP), a nonparametric Bayesian model, to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks. We present an application of this method to study how microRNAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the method improves upon prior methods, correctly recovers interactions and clusters, and provides accurate biological predictions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {235–243},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986485,
author = {Duvenaud, David and Nickisch, Hannes and Rasmussen, Carl Edward},
title = {Additive Gaussian Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {226–234},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986484,
author = {Kobayashi, Ryota and Tsubo, Yasuhiro and Lansky, Petr and Shinomoto, Shigeru},
title = {Estimating Time-Varying Input Signals and Ion Channel States from a Single Voltage Trace of a Neuron},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {State-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena. Here, we develop a Bayesian method for inferring the time-varying mean and variance of the synaptic input, along with the dynamics of each ion channel from a single voltage trace of a neuron. An estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters. After optimizing the hyperparameters by maximizing the marginal likelihood, the state-space model provides the time-varying parameters of the input signals and the ion channel states. The proposed method is tested not only on the simulated data from the Hodgkin-Huxley type models but also on experimental data obtained from a cortical slice in vitro.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {217–225},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986483,
author = {Nakajima, Shinichi and Sugiyama, Masashi and Babacan, Derin},
title = {Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variational Bayesian matrix factorization (VBMF) efficiently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical finding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efficient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {208–216},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986482,
author = {Dhillon, Paramveer S. and Foster, Dean and Ungar, Lyle},
title = {Multi-View Learning of Word Embeddings via CCA},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model the context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate low dimensional context-specific word representations from unlabeled data. These representation features can then be used with any supervised learner. LR-MVL is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (NER) and chunking problems.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {199–207},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986481,
author = {Cabral, Ricardo S. and De la Torre, Fernando and Costeira, Jo\~{a}o P. and Bernardino, Alexandre},
title = {Matrix Completion for Multi-Label Image Classification},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords. This paper formulates image categorization as a multi-label classification problem using recent advances in matrix completion. Under this setting, classification of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels. We propose two convex algorithms for matrix completion based on a Rank Minimization criterion specifically tailored to visual data, and prove its convergence properties. A major advantage of our approach w.r.t. standard discriminative classification methods for image categorization is its robustness to outliers, background noise and partial occlusions both in the feature and label space. Experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms, while effectively capturing semantic concepts of classes.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {190–198},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986480,
author = {Morioka, Nobuyuki and Satoh, Shin'ichi},
title = {Generalized Lasso Based Approximation of Sparse Coding for Visual Recognition},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, ℓ1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents "Generalized Lasso based Approximation of Sparse coding" (GLAS). By representing the distribution of sparse coefficients with slice transform, we fit a piece-wise linear mapping function with the generalized lasso. We also propose an efficient post-refinement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to ℓ1 regularized sparse coding, yet achieves a significant speed up demonstrating its effectiveness for large-scale visual recognition problems.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {181–189},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986479,
author = {Farahmand, Amir-massoud},
title = {Action-Gap Phenomenon in Reinforcement Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many practitioners of reinforcement learning problems have observed that oftentimes the performance of the agent reaches very close to the optimal performance even though the estimated (action-)value function is still far from the optimal one. The goal of this paper is to explain and formalize this phenomenon by introducing the concept of the action-gap regularity. As a typical result, we prove that for an agent following the greedy policy (hat{pi}) with respect to an action-value function $(hat{Q})$, the performance loss $(E[V^*(X) - V^{hat{X}} (X)])$ is upper bounded by $(O(|| hat{Q} - Q^*||_infty^{1+zeta}))$, in which ζ ≥ = 0) is the parameter quantifying the action-gap regularity. For ζ &gt; 0, our results indicate smaller performance loss compared to what previous analyses had suggested. Finally, we show how this regularity affects the performance of the family of approximate value iteration algorithms.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {172–180},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986478,
author = {Tofigh, Ali and Sjolund, Erik and Hoglund, Mattias and Lagergren, Jens},
title = {A Global Structural EM Algorithm for a Model of Cancer Progression},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Cancer has complex patterns of progression that include converging as well as diverging progressional pathways. Vogelstein's path model of colon cancer was a pioneering contribution to cancer research. Since then, several attempts have been made at obtaining mathematical models of cancer progression, devising learning algorithms, and applying these to cross-sectional data. Beerenwinkel et al. provided, what they coined, EM-like algorithms for Oncogenetic Trees (OTs) and mixtures of such. Given the small size of current and future data sets, it is important to minimize the number of parameters of a model. For this reason, we too focus on tree-based models and introduce Hidden-variable Oncogenetic Trees (HOTs). In contrast to OTs, HOTs allow for errors in the data and thereby provide more realistic modeling. We also design global structural EM algorithms for learning HOTs and mixtures of HOTs (HOT-mixtures). The algorithms are global in the sense that, during the M-step, they find a structure that yields a global maximum of the expected complete log-likelihood rather than merely one that improves it. The algorithm for single HOTs performs very well on reasonable-sized data sets, while that for HOT-mixtures requires data sets of sizes obtainable only with tomorrow's more cost-efficient technologies.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {163–171},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986477,
author = {Shroff, Nitesh and Turaga, Pavan and Chellappa, Rama},
title = {Manifold Pr\'{e}Cis: An Annealing Technique for Diverse Sampling of Manifolds},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we consider the Pr\'{e}cis problem of sampling K representative yet diverse data points from a large dataset. This problem arises frequently in applications such as video and document summarization, exploratory data analysis, and pre-filtering. We formulate a general theory which encompasses not just traditional techniques devised for vector spaces, but also non-Euclidean manifolds, thereby enabling these techniques to shapes, human activities, textures and many other image and video based datasets. We propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power, and their diversity. We then propose efficient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm. The proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples. Experimental results show the strength and generality of the proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {154–162},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986476,
author = {Zhang, Dan and Liu, Yan and Si, Luo and Zhang, Jian and Lawrence, Richard D.},
title = {Multiple Instance Learning on Structured Data},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most existing Multiple-Instance Learning (MIL) algorithms assume data instances and/or data bags are independently and identically distributed. But there often exists rich additional dependency/structure information between instances/bags within many applications of MIL. Ignoring this structure information limits the performance of existing MIL algorithms. This paper explores the research problem as multiple instance learning on structured data (MILSD) and formulates a novel framework that considers additional structure information. In particular, an effective and efficient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of Concave-Convex Constraint Programming (CCCP) method and an adapted Cutting Plane method, which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data. Our method has the nice convergence property, with specified precision on each set of constraints. Experimental results on three different applications, i.e., webpage classification, market targeting, and protein fold identification, clearly demonstrate the advantages of the proposed method over state-of-the-art methods.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {145–153},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986475,
author = {Rezende, Danilo Jimenez and Wierstra, Daan and Gerstner, Wulfram},
title = {Variational Learning for Recurrent Spiking Networks},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation confirms the method's applicability to learning both stationary and temporal spike patterns.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {136–144},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986474,
author = {Simon, Dylan A. and Daw, Nathaniel D.},
title = {Environmental Statistics and the Trade-off between Model-Based and TD Learning in Humans},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rule-based vs. incremental learning.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {127–135},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986473,
author = {Lim, Joseph J. and Salakhutdinov, Ruslan and Torralba, Antonio},
title = {Transfer Learning by Borrowing Examples for Multiclass Object Detection},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the recent trend of increasingly large datasets for object detection, there still exist many classes with few training examples. To overcome this lack of training data for certain classes, we propose a novel way of augmenting the training data for each class by borrowing and transforming examples from other classes. Our model learns which training instances from other classes to borrow and how to transform the borrowed examples so that they become more similar to instances from the target class. Our experimental results demonstrate that our new object detector, with borrowed and transformed examples, improves upon the current state-of-the-art detector on the challenging SUN09 object detection dataset.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {118–126},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986472,
author = {Kr\"{a}henb\"{u}hl, Philipp and Koltun, Vladlen},
title = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {109–117},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986471,
author = {Gibson, Richard and Szafron, Duane},
title = {On Strategy Stitching in Large Extensive Form Multiplayer Games},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory, necessitating the use of abstraction to reduce the game size. Typically, strategies from abstract games perform better in the real game as the granularity of abstraction is increased. This paper investigates two techniques for stitching abase strategy in a coarse abstraction of the full game tree, to expert strategies in fine abstractions of smaller subtrees. We provide a general framework for creating static experts, an approach that generalizes some previous strategy stitching efforts. In addition, we show that static experts can create strong agents for both 2-player and 3-player Leduc and Limit Texas Hold'em poker, and that a specific class of static experts can be preferred among a number of alternatives. Furthermore, we describe a poker agent that used static experts and won the 3 -player events of the 2010 Annual Computer Poker Competition.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {100–108},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986470,
author = {Long, Philip M. and Servedio, Rocco A.},
title = {Learning Large-Margin Halfspaces with More Malicious Noise},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a simple algorithm that runs in time poly(n, 1/γ, 1/ε) and learns an unknown n-dimensional γ-margin halfspace to accuracy 1 – ε in the presence of malicious noise, when the noise rate is allowed to be as high as Θ(εγ√log(1/γ)). Previous efficient algorithms could only learn to accuracy e in the presence of malicious noise of rate at most Θ(εγ).Our algorithm does not work by optimizing a convex loss function. We show that no algorithm for learning γ-margin halfspaces that minimizes a convex proxy for misclassification error can tolerate malicious noise at a rate greater than Θ(εγ); this may partially explain why previous algorithms could not achieve the higher noise tolerance of our new algorithm.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {91–99},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986469,
author = {Keramati, Mehdi and Gutkin, Boris},
title = {A Reinforcement Learning Theory for Homeostatic Regulation},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Reinforcement learning models address animal's behavioral adaptation to its changing "external" environment, and are based on the assumption that Pavlovian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are concerned with behavioral adaptation in response to the "internal" state of the animal, and assume that animals' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that integrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a unified explanation of some behavioral pattern like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {82–90},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986468,
author = {Zhao, Yibiao and Zhu, Song-Chun},
title = {Image Parsing via Stochastic Scene Grammar},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classifiers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative "+" relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive "-" relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efficient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene configurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to find the most probable configuration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {73–81},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986467,
author = {Lyu, Siwei},
title = {Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When used to learn high dimensional parametric probabilistic models, the classical maximum likelihood (ML) learning often suffers from computational intractability, which motivates the active developments of non-ML learning methods. Yet, because of their divergent motivations and forms, the objective functions of many non-ML learning methods are seemingly unrelated, and there lacks a unified framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learning methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score matching [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum KL contraction framework with different choices of the KL contraction operators.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {64–72},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986466,
author = {Elhamifar, Ehsan and Vidal, Ren\'{e}},
title = {Sparse Manifold Clustering and Embedding},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an algorithm called Sparse Manifold Clustering and Embedding (SMCE) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds. Similar to most dimensionality reduction methods, SMCE finds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights. The key difference is that SMCE finds both the neighbors and the weights automatically. This is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional affine subspace. The optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding. Moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs. Experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {55–63},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986465,
author = {Lu, Zhaosong and Zhang, Yong},
title = {Penalty Decomposition Methods for Rank Minimization},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We first show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper [19]. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {46–54},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986464,
author = {Cl\'{e}men\c{c}on, St\'{e}phan},
title = {On <i>U</i>-Processes and Clustering Performance},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many clustering techniques aim at optimizing empirical criteria that are of the form of a U-statistic of degree two. Given a measure of dissimilarity between pairs of observations, the goal is to minimize the within cluster point scatter over a class of partitions of the feature space. It is the purpose of this paper to define a general statistical framework, relying on the theory of U-processes, for studying the performance of such clustering methods. In this setup, under adequate assumptions on the complexity of the subsets forming the partition candidates, the excess of clustering risk is proved to be of the order Oℙ(1/√n). Based on recent results related to the tail behavior of degenerate U-processes, it is also shown how to establish tighter rate bounds. Model selection issues, related to the number of clusters forming the data partition in particular, are also considered.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {37–45},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986463,
author = {Vondrick, Carl and Ramanan, Deva},
title = {Video Annotation and Tracking with Active Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {28–36},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986462,
author = {Levine, Sergey and Popovi\'{c}, Zoran and Koltun, Vladlen},
title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert's policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {19–27},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986461,
author = {Bach, Francis},
title = {Shaping Level Sets with Submodular Functions},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a class of sparsity-inducing regularization terms based on submodular functions. While previous work has focused on non-decreasing functions, we explore symmetric submodular functions and their Lov\'{a}sz extensions. We show that the Lov\'{a}sz extension may be seen as the convex envelope of a function that depends on level sets (i.e., the set of indices whose corresponding components of the underlying predictor are greater than a given constant): this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not only on the supports of the underlying predictors. We provide unified optimization algorithms, such as proximal operators, and theoretical guarantees (allowed level sets and recovery conditions). By selecting specific submodular functions, we give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics with application to clustering and outlier detection, and on noisy cuts in graphs with application to change point detection in the presence of outliers.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {10–18},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2986459.2986460,
author = {Wang, Hua and Huang, Heng and Kamangar, Farhad and Nie, Feiping and Ding, Chris},
title = {Maximum Margin Multi-Instance Learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reflect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3I) approach to parameterize the C2B distance by introducing the class specific distance metrics and the locally adaptive significance coefficients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multi-label) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1–9},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@proceedings{10.5555/2986459,
title = {NIPS'11: Proceedings of the 24th International Conference on Neural Information Processing Systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Granada, Spain}
}

