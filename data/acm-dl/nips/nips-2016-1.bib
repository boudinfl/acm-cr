@inproceedings{10.5555/3157096.3157380,
author = {Tenzer, Yaniv and Schwing, Alexander and Gimpel, Kevin and Hazan, Tamir},
title = {Constraints Based Convex Belief Propagation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inference in Markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications. In order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances. Unfortunately this comes at the price of a tremendous computational burden. In this paper we suggest to tackle consistency by incorporating constraints on beliefs. This permits derivation of a closed-form message-passing algorithm which we refer to as the Constraints Based Convex Belief Propagation (CBCBP). Experiments show that CBCBP outperforms the conventional consistency potential based approach, while being at least an order of magnitude faster.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2540–2548},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157379,
author = {Li, Zhe and Gong, Boqing and Yang, Tianbao},
title = {Improved Dropout for Shallow and Deep Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2531–2539},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157378,
author = {Cortes, Corinna and Kuznetsov, Vitaly and Mohrii, Mehryar and Yang, Scott},
title = {Structured Prediction Theory Based on Factor Graph Complexity},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems. Our guarantees are expressed in terms of a data-dependent complexity measure, factor graph complexity, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets along with a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest.We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs. We present new learning bounds for this advanced setting, which we use to design two new algorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2522–2530},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157377,
author = {Yurochkin, Mikhail and Nguyen, XuanLong},
title = {Geometric Dirichlet Means Algorithm for Topic Inference},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2513–2521},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157376,
author = {Zhou, Hao Henry and Ravi, Sathya N. and Ithapu, Vamsi K. and Johnson, Sterling C. and Wahba, Grace and Singh, Vikas},
title = {Hypothesis Testing in Unsupervised Domain Adaptation with Applications in Alzheimer's Disease},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Consider samples from two different data sources {xix} ~ Psource and {xit} ~ Ptarget. We only observe their transformed versions h(xis) and g(xit), for some known function class h(·) and g(·). Our goal is to perform a statistical test checking if Psource = Ptarget while removing the distortions induced by the transformations. This problem is closely related to domain adaptation, and in our case, is motivated by the need to combine clinical and imaging based biomarkers from multiple sites and/or batches - a fairly common impediment in conducting analyses with much larger sample sizes. We address this problem using ideas from hypothesis testing on the transformed measurements, wherein the distortions need to be estimated in tandem with the testing. We derive a simple algorithm and study its convergence and consistency properties in detail, and provide lower-bound strategies based on recent work in continuous optimization. On a dataset of individuals at risk for Alzheimer's disease, our framework is competitive with alternative procedures that are twice as expensive and in some cases operationally infeasible to implement.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2504–2512},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157375,
author = {Lynn, Christopher W. and Lee, Daniel D.},
title = {Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the Ising influence maximization problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) shift from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graph topologies, which are of independent interest.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2495–2503},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157374,
author = {Wan, Yali and Meil\u{a}, Marina},
title = {Graph Clustering: Block-Models and Model Free Results},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain "correctness" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2486–2494},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157373,
author = {Yang, Fan and Barber, Rina Foygel and Jain, Prateek and Lafferty, John},
title = {Selective Inference for Group-Sparse Linear Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2477–2485},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157372,
author = {Gao, Weihao and Oh, Sewoong and Viswanath, Pramod},
title = {Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of k-NN distances with a finite k, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be precomputed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2468–2466},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157371,
author = {Grosse, Roger B. and Ancha, Siddharth and Roy, Daniel M.},
title = {Measuring the Reliability of MCMC Inference with Bidirectional Monte Carlo},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL [GS] and Stan [CGHL+ p], and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2459–2467},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157370,
author = {Ho, Chien-Ju and Frongillo, Rafael and Chen, Yiling},
title = {Eliciting Categorical Data for Optimal Aggregation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Models for collecting and aggregating categorical data on crowdsourcing platforms typically fall into two broad categories: those assuming agents honest and consistent but with heterogeneous error rates, and those assuming agents strategic and seek to maximize their expected reward. The former often leads to tractable aggregation of elicited data, while the latter usually focuses on optimal elicitation and does not consider aggregation. In this paper, we develop a Bayesian model, wherein agents have differing quality of information, but also respond to incentives. Our model generalizes both categories and enables the joint exploration of optimal elicitation and aggregation. This model enables our exploration, both analytically and experimentally, of optimal aggregation of categorical data and optimal multiple-choice interface design.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2450–2458},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157369,
author = {Chaudhuri, Sougata and Tewari, Ambuj},
title = {Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O(T2/3 log T) distribution independent and O(log T) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O(T2/3 √log T) distribution independent and O(log2 T) distribution dependent regret respectively. Crucially, our framework needs only the simpler "argmax" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O(log T) regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2441–2449},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157368,
author = {Hartford, Jason and Wright, James R. and Leyton-Brown, Kevin},
title = {Deep Learning for Predicting Human Strategic Behavior},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2432–2440},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157367,
author = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael},
title = {Satisfying Real-World Goals with Dataset Constraints},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2423–2431},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157366,
author = {Choy, Christopher B. and Gwak, Jun Young and Savarese, Silvio and Chandraker, Manmohan},
title = {Universal Correspondence Network},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations. In contrast to previous CNN-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity. Our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with O(n) feed forward passes for n keypoints, instead of O(n2) for typical patch similarity methods. We propose a convolutional spatial transformer to mimic patch normalization in traditional features like SIFT, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations. Extensive experiments on KITTI, PASCAL, and CUB-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2414–2422},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157365,
author = {Beatson, Alex and Wang, Zhaoran and Liu, Han},
title = {Blind Attacks on Machine Learners},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The importance of studying the robustness of learners to malicious data is well established. While much work has been done establishing both robust estimators and effective data injection attacks when the attacker is omniscient, the ability of an attacker to provably harm learning while having access to little information is largely unstudied. We study the potential of a "blind attacker" to provably limit a learner's performance by data injection attack without observing the learner's training set or any parameter of the distribution from which it is drawn. We provide examples of simple yet effective attacks in two settings: firstly, where an "informed learner" knows the strategy chosen by the attacker, and secondly, where a "blind learner" knows only the proportion of malicious data and some family to which the malicious distribution chosen by the attacker belongs. For each attack, we analyze minimax rates of convergence and establish lower bounds on the learner's minimax risk, exhibiting limits on a learner's ability to learn under data injection attack even when the attacker is "blind".},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2405–2413},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157364,
author = {Krishnamurthy, Akshay and Agarwal, Alekh and Dud\'{\i}k, Miroslav},
title = {Contextual Semibandits via Supervised Learning Oracles},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2396–2404},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157363,
author = {Nguyen, Anh Tuan and Xu, Jian and Yang, Zhi},
title = {A Bio-Inspired Redundant Sensing Architecture},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sensing is the process of deriving signals from the environment that allows artificial systems to interact with the physical world. The Shannon theorem specifies the maximum rate at which information can be acquired [1]. However, this upper bound is hard to achieve in many man-made systems. The biological visual systems, on the other hand, have highly efficient signal representation and processing mechanisms that allow precise sensing. In this work, we argue that redundancy is one of the critical characteristics for such superior performance. We show architectural advantages by utilizing redundant sensing, including correction of mismatch error and significant precision enhancement. For a proof-of-concept demonstration, we have designed a heuristic-based analog-to-digital converter- a zero-dimensional quantizer. Through Monte Carlo simulation with the error probabilistic distribution as a priori, the performance approaching the Shannon limit is feasible. In actual measurements without knowing the error distribution, we observe at least 2-bit extra precision. The results may also help explain biological processes including the dominance of binocular vision, the functional roles of the fixational eye movements, and the structural mechanisms allowing hyperacuity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2387–2395},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157362,
author = {Liu, Qiang and Wang, Dilin},
title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2378–2386},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157361,
author = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Cohen, William W. and Salakhutdinov, Ruslan R.},
title = {Review Networks for Caption Generation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2369–2377},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157360,
author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
title = {Variational Autoencoder for Deep Learning of Images, Labels and Captions},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2360–2368},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157359,
author = {Liu, Chaoyue and Belkin, Mikhail},
title = {Clustering with Bregman Divergences: An Asymptotic Analysis},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Clustering, in particular k-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of k-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters k is large. We establish quantization rates and describe the limiting distribution of the centers as k → ∞, extending well-known results for k-means clustering.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2351–2359},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157358,
author = {Nan, Feng and Wang, Joseph and Saligrama, Venkatesh},
title = {Pruning Random Forests for Prediction on a Budget},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost &amp; accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2342–2350},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157357,
author = {Chen, Eunice Yuh-Jie and Shen, Yujia and Choi, Arthur and Darwiche, Adnan},
title = {Learning Bayesian Networks with Ancestral Constraints},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning Bayesian networks optimally, when subject to background knowledge in the form of ancestral constraints. Our approach is based on a recently proposed framework for optimal structure learning based on non-decomposable scores, which is general enough to accommodate ancestral constraints. The proposed framework exploits oracles for learning structures using decomposable scores, which cannot accommodate ancestral constraints since they are non-decomposable. We show how to empower these oracles by passing them decomposable constraints that they can handle, which are inferred from ancestral constraints that they cannot handle. Empirically, we demonstrate that our approach can be orders-of-magnitude more efficient than alternative frameworks, such as those based on integer linear programming.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2333–2341},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157356,
author = {Roy, Aurko and Pokutta, Sebastian},
title = {Hierarchical Clustering via Spreading Metrics},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the cost function for hierarchical clusterings introduced by [16] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [16] that a top-down algorithm returns a hierarchical clustering of cost at most O (αn log n) times the cost of the optimal hierarchical clustering, where αn is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O(log3/2n) times the cost of the optimal solution. We improve this by giving an O(log n)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [16]. We also give constant factor inapproximability results for this problem.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2324–2332},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157355,
author = {Eldridge, Justin and Belkin, Mikhail and Wang, Yusu},
title = {Graphons, Mergeons, and so On!},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the "correct" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2315–2323},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157354,
author = {Petrik, Marek and Ghavamzadeh, Mohammad and Chow, Yinlam},
title = {Safe Policy Improvement by Minimizing Robust Baseline Regret},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An important problem in sequential decision-making under uncertainty is to use limited data to compute a safe policy, which is guaranteed to outperform a given baseline strategy. In this paper, we develop and analyze a new model-based approach that computes a safe policy, given an inaccurate model of the system's dynamics and guarantees on the accuracy of this model. The new robust method uses this model to directly minimize the (negative) regret w.r.t. the baseline policy. Contrary to existing approaches, minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and to seamlessly fall back to the baseline policy, otherwise. We show that our formulation is NP-hard and propose a simple approximate algorithm. Our empirical results on several domains further show that even the simple approximate algorithm can outperform standard approaches.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2306–2314},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157353,
author = {Mohri, Mehryar and Yang, Scott},
title = {Optimistic Bandit Convex Optimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the general and powerful scheme of predicting information re-use in optimization algorithms. This allows us to devise a computationally efficient algorithm for bandit convex optimization with new state-of-the-art guarantees for both Lipschitz loss functions and loss functions with Lipschitz gradients. This is the first algorithm admitting both a polynomial time complexity and a regret that is polynomial in the dimension of the action space that improves upon the original regret bound for Lipschitz loss functions, achieving a regret of \~{O}(T11/16d3/8). Our algorithm further improves upon the best existing polynomial-in-dimension bound (both computationally and in terms of regret) for loss functions with Lipschitz gradients, achieving a regret of \~{O}(T8/13d5/3).},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2297–2305},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157352,
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi},
title = {Examples Are Not Enough, Learn to Criticize! Criticism for Interpretability},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2288–2296},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157351,
author = {Papaxanthos, Laetitia and Llinares-L\'{o}pez, Felipe and Bodenham, Dean and Borgwardt, Karsten},
title = {Finding Significant Combinations of Features in the Presence of Categorical Covariates},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In high-dimensional settings, where the number of features p is much larger than the number of samples n, methods that systematically examine arbitrary combinations of features have only recently begun to be explored. However, none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate. As a result, many false discoveries might occur due to unaccounted confounding effects.We propose the Fast Automatic Conditional Search (FACS) algorithm, a significant discriminative itemset mining method which conditions on categorical covariates and only scales as O(k log k), where k is the number of states of the categorical covariate. Based on the Cochran-Mantel-Haenszel Test, FACS demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art, opening the door to numerous applications in biomedicine.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2279–2287},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157350,
author = {Alvarez, Jose M. and Salzmann, Mathieu},
title = {Learning the Number of Neurons in Deep Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2270–2278},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157349,
author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
title = {Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2261–2269},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157348,
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob},
title = {Learning Multiagent Communication with Backpropagation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2252–2260},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157347,
author = {Alemi, Alexander A. and Chollet, Fran\c{c}ois and Een, Niklas and Irving, Geoffrey and Szegedy, Christian and Urban, Josef},
title = {DeepMath - Deep Sequence Models for Premise Selection},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2243–2251},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157346,
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
title = {Improved Techniques for Training GANs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2234–2242},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157345,
author = {Hayashi, Kohei and Yoshida, Yuichi},
title = {Minimizing Quadratic Functions in Constant Time},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following n-dimensional quadratic minimization problem in constant time, which is independent of n: z* = minv∈ℝn 〈v, Av〉 + n<v, diag(d)v=""> + n(b, v), where A ∈ ℝn x n is a matrix and d, b ∈ ℝn are vectors. Our theoretical analysis specifies the number of samples k(δ, ε) such that the approximated solution z satisfies |z - z*| = O(εn2) with probability 1 - δ. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2225–2233},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}</v,>

@inproceedings{10.5555/3157096.3157344,
author = {Namkoong, Hongseok and Duchi, John C.},
title = {Stochastic Gradient Methods for Distributionally Robust Optimization with <i>f</i>-Divergences},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance. Our methods apply to distributionally robust optimization problems proposed by Ben-Tal et al., which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution. Our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures. For a sample of size n, the per-iteration cost of our method scales as O(log n), which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2216–2224},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157343,
author = {Fraccaro, Marco and S\o{}nderby, S\o{}ren Kaae and Paquet, Ulrich and Winther, Ole},
title = {Sequential Neural Models with Stochastic Layers},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TTMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2207–2215},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157342,
author = {Zhong, Kai and Jain, Prateek and Dhillon, Inderjit S.},
title = {Mixed Linear Regression with Multiple Components},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study the mixed linear regression (MLR) problem, where the goal is to recover multiple underlying linear models from their unlabeled linear measurements. We propose a non-convex objective function which we show is locally strongly convex in the neighborhood of the ground truth. We use a tensor method for initialization so that the initial models are in the local strong convexity region. We then employ general convex optimization algorithms to minimize the objective function. To the best of our knowledge, our approach provides first exact recovery guarantees for the MLR problem with K ≥ 2 components. Moreover, our method has near-optimal computational complexity \~{O}(Nd) as well as near-optimal sample complexity \~{O} (d) for constant K. Furthermore, we show that our non-convex formulation can be extended to solving the subspace clustering problem as well. In particular, when initialized within a small constant distance to the true subspaces, our method converges to the global optima (and recovers true subspaces) in time linear in the number of points. Furthermore, our empirical results indicate that even with random initialization, our approach converges to the global optima in linear time, providing speed-up of up to two orders of magnitude.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2198–2206},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157341,
author = {Kale, Satyen and Lee, Chansoo and P\'{a}l, D\'{a}vid},
title = {Hardness of Online Sleeping Combinatorial Optimization Problems},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of ONLINE SHORTEST PATHS, ONLINE MINIMUM SPANNING TREE, ONLINE k-SUBSETS, ONLINE k-TRUNCATED PERMUTATIONS, ONLINE MINIMUM CUT, and ONLINE BIPARTITE MATCHING. The hardness result for the sleeping version of the Online shortest Paths problem resolves an open problem presented at COLT 2015 [Koolen et al., 2015].},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2189–2197},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157340,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2180–2188},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157339,
author = {Chen, Xi and Cheng, Yu and Tang, Bo},
title = {On the Recursive Teaching Dimension of VC Classes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recursive teaching dimension (RTD) of a concept class C ⊆ {0,1}n, introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of C in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension.Given a concept class C ⊆ {0,1}n with VCD(C) = d, we first show that RTD(C) is at most d · 2d+1. This is the first upper bound for RTD(C) that depends only on VCD(C), independent of the size of the concept class |C| and its domain size n. Before our work, the best known upper bound for RTD(C) is O(d2d log log |C|), obtained by Moran et al. [MSWY15]. We remove the log log |C| factor.We also improve the lower bound on the worst-case ratio of RTD(C) to VCD(C). We present a family of classes {Ck}k≥1 with VCD(Ck) = 3k and RTD(Ck) = 5k, which implies that the ratio of RTD(C) to VCD(C) in the worst case can be as large as 5/3. Before our work, the largest ratio known was 3/2 as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class C has been known to satisfy RTD(C) &gt; (3/2) · VCD(C).},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2171–2179},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157338,
author = {Lee, Christina E. and Li, Yihua and Shah, Devavrat and Song, Dogyoon},
title = {Blind Regression: Nonparametric Regression for Latent Variable Models via Collaborative Filtering},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the framework of blind regression motivated by matrix completion for recommendation systems: given m users, n movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user u and movie i have features x1(u) and x2 (i) respectively, and their corresponding rating y(u, i) is a noisy measurement of f (x1 (u), x2 (i)) for some unknown function f. In contrast with classical regression, the features x = (x1(u), x2(i)) are not observed, making it challenging to apply standard regression methods to predict the unobserved ratings.Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least max(m-1+δ, n-1/2+δ) with δ &gt; 0, we prove that the expected fraction of our estimates with error greater than ε is less than γ2/ε2 plus a polynomially decaying term, where γ2 is the variance of the additive entry-wise noise term.Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2163–2173},
numpages = {11},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157337,
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
title = {Value Iteration Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2154–2162},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157336,
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2145–2153},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157335,
author = {Yan, Songbai and Chaudhuri, Kamalika and Javidi, Tara},
title = {Active Learning from Imperfect Labelers},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2136–2144},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157334,
author = {Lee, Stefan and Purushwalkam, Senthil and Cogswell, Michael and Ranjan, Viresh and Crandall, David and Batra, Dhruv},
title = {Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks - introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2127–2135},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157333,
author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
title = {Learning Transferrable Representations for Unsupervised Domain Adaptation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers [11, 33] have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters [11] and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2118–2126},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157332,
author = {G\"{u}\c{c}l\"{u}, Umut and Thielen, Jordy and Hanke, Michael and van Gerven, Marcel A. J.},
title = {Brains on Beats},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We developed task-optimized deep neural networks (DNNs) that achieved state-of-the-art performance in different evaluation scenarios for automatic music tagging. These DNNs were subsequently used to probe the neural representations of music. Representational similarity analysis revealed the existence of a representational gradient across the superior temporal gyrus (STG). Anterior STG was shown to be more sensitive to low-level stimulus features encoded in shallow DNN layers whereas posterior STG was shown to be more sensitive to high-level stimulus features encoded in deep DNN layers.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2109–2117},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157331,
author = {Dutta, Sanghamitra and Cadambe, Viveck and Grover, Pulkit},
title = {"Short-Dot": Computing Large Linear Transforms Distributedly Using Coded Short Dot Products},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Faced with saturation of Moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However, distributed computing is often bottle necked by a small fraction of slow processors called "stragglers" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers. In this paper, we propose a novel technique - that we call "Short-Dot" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further, only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2100–2108},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157330,
author = {Balcan, Maria-Florina and Sandholm, Tuomas and Vitercik, Ellen},
title = {Sample Complexity of Automated Mechanism Design},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The design of revenue-maximizing combinatorial auctions, i.e. multi-item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale. In the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution. Despite this strong and of tentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown. In recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions. The most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class. In this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy. In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory. In particular, the hypothesis functions used in our contexts are defined through multi-stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2091–2099},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157329,
author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
title = {Learning Structured Sparsity in Deep Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN's evaluation. Experimental results show that SSL achieves on average 5.1 \texttimes{} and 3.1 \texttimes{} speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by ~ 1%.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2082–2090},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157328,
author = {He, Xinran and Xu, Ke and Kempe, David and Liu, Yan},
title = {Learning Influence Functions from Incomplete Observations},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model. It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2073–2081},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157327,
author = {Lei, Qi and Zhong, Kai and Dhillon, Inderjit S.},
title = {Coordinate-Wise Power Method},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose a coordinate-wise version of the power method from an optimization viewpoint. The vanilla power method simultaneously updates all the coordinates of the iterate, which is essential for its convergence analysis. However, different coordinates converge to the optimal value at different speeds. Our proposed algorithm, which we call coordinate-wise power method, is able to select and update the most important k coordinates in O(kn) time at each iteration, where n is the dimension of the matrix and k ≤ n is the size of the active set. Inspired by the "greedy" nature of our method, we further propose a greedy coordinate descent algorithm applied on a non-convex objective function specialized for symmetric matrices. We provide convergence analyses for both methods. Experimental results on both synthetic and real data show that our methods achieve up to 23 times speedup over the basic power method. Meanwhile, due to their coordinate-wise nature, our methods are very suitable for the important case when data cannot fit into memory. Finally, we introduce how the coordinate-wise mechanism could be applied to other iterative methods that are used in machine learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2064–2072},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157326,
author = {Durmus, Alain and \c{S}im\c{s}ekli, Umut and Moulines, \'{E}ric and Badeau, Roland and Richard, Ga\"{e}l},
title = {Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms have become increasingly popular for Bayesian inference in large-scale applications. Even though these methods have proved useful in several scenarios, their performance is often limited by their bias. In this study, we propose a novel sampling algorithm that aims to reduce the bias of SG-MCMC while keeping the variance at a reasonable level. Our approach is based on a numerical sequence acceleration method, namely the Richardson-Romberg extrapolation, which simply boils down to running almost the same SG-MCMC algorithm twice in parallel with different step sizes. We illustrate our framework on the popular Stochastic Gradient Langevin Dynamics (SGLD) algorithm and propose a novel SG-MCMC algorithm referred to as Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD). We provide formal theoretical analysis and show that SGRRLD is asymptotically consistent, satisfies a central limit theorem, and its non-asymptotic bias and the mean squared-error can be bounded. Our results show that SGRRLD attains higher rates of convergence than SGLD in both finite-time and asymptotically, and it achieves the theoretical accuracy of the methods that are based on higher-order integrators. We support our findings using both synthetic and real data experiments.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2055–2063},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157325,
author = {Tsai, Chuan-Yung and Saxe, Andrew and Cox, David},
title = {Tensor Switching Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2046–2054},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157324,
author = {Raju, Rajkumar V. and Pitkow, Xaq},
title = {Inference by Reparameterization in Neural Population Codes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Behavioral experiments on humans and animals suggest that the brain performs probabilistic inference to interpret its environment. Here we present a new general-purpose, biologically-plausible neural implementation of approximate inference. The neural network represents uncertainty using Probabilistic Population Codes (PPCs), which are distributed neural representations that naturally encode probability distributions, and support marginalization and evidence integration in a biologically-plausible manner. By connecting multiple PPCs together as a probabilistic graphical model, we represent multivariate probability distributions. Approximate inference in graphical models can be accomplished by message-passing algorithms that disseminate local information throughout the graph. An attractive and often accurate example of such an algorithm is Loopy Belief Propagation (LBP), which uses local marginalization and evidence integration operations to perform approximate inference efficiently even for complex models. Unfortunately, a subtle feature of LBP renders it neurally implausible. However, LBP can be elegantly reformulated as a sequence of Tree-based Reparameterizations (TRP) of the graphical model. We re-express the TRP updates as a nonlinear dynamical system with both fast and slow timescales, and show that this produces a neurally plausible solution. By combining all of these ideas, we show that a network of PPCs can represent multivariate probability distributions and implement the TRP updates to perform probabilistic inference. Simulations with Gaussian graphical models demonstrate that the neural network inference quality is comparable to the direct evaluation of LBP and robust to noise, and thus provides a promising mechanism for general probabilistic inference in the population codes of the brain.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2037–2045},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157323,
author = {Hoiles, William and van der Schaar, Mihaela},
title = {A Non-Parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating the bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference, permutation testing, and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2028–2036},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157322,
author = {Saad, Feras and Mansinghka, Vikash},
title = {A Probabilistic Programming Approach to Probabilistic Data Analysis},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include discriminative machine learning, hierarchical Bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs. We demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language. The practical value is illustrated in two ways. First, the paper describes an analysis on a database of Earth satellites, which identifies records that probably violate Kepler's Third Law by composing causal probabilistic programs with non-parametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2019–2027},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157321,
author = {Linderman, Scott W. and Adams, Ryan P. and Pillow, Jonathan W.},
title = {Bayesian Latent Structure Discovery from Multi-Neuron Recordings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via P6lya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2010–2018},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157320,
author = {Atwood, James and Towsley, Don},
title = {Diffusion-Convolutional Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on a GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2001–2009},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157319,
author = {Friedrich, Johannes and Paninski, Liam},
title = {Fast Active Set Methods for Online Spike Inference from Calcium Imaging},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of O(105) traces of whole-brain zebrafish imaging data on a laptop.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1992–2000},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157318,
author = {Yu, Felix Xinnan and Suresh, Ananda Theertha and Choromanski, Krzysztof and Holtmann-Rice, Daniel and Kumar, Sanjiv},
title = {Orthogonal Random Features},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from O(d2) to O(d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1983–1991},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157317,
author = {Maitin-Shepard, Jeremy and Jain, Viren and Januszewski, Michal and Li, Peter and Abbeel, Pieter},
title = {Combinatorial Energy Learning for Image Segmentation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1974–1982},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157316,
author = {Chalk, Matthew and Marre, Olivier and Tkacik, Gasper},
title = {Relevant Sparse Codes with Variational Information Bottleneck},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1965–1973},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157315,
author = {Kasaei, S. Hamidreza and Tom\'{e}, Ana Maria and Lopes, Lu\'{\i}s Seabra},
title = {Hierarchical Object Representation for Open-Ended Object Category Learning and Recognition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge-base that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1956–1964},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157314,
author = {Bak, Ji Hyun and Choi, Jung Yoon and Akrami, Athena and Witten, Ilana and Pillow, Jonathan W.},
title = {Adaptive Optimal Training of Animal Behavior},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neuroscience experiments often require training animals to perform tasks designed to elicit various sensory, cognitive, and motor behaviors. Training typically involves a series of gradual adjustments of stimulus conditions and rewards in order to bring about learning. However, training protocols are usually hand-designed, relying on a combination of intuition, guesswork, and trial-and-error, and often require weeks or months to achieve a desired level of task performance. Here we combine ideas from reinforcement learning and adaptive optimal experimental design to formulate methods for adaptive optimal training of animal behavior. Our work addresses two intriguing problems at once: first, it seeks to infer the learning rules underlying an animal's behavioral changes during training; second, it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a desired objective. We develop and test these methods using data collected from rats during training on a two-interval sensory discrimination task. We show that we can accurately infer the parameters of a policy-gradient-based learning algorithm that describes how the animal's internal model of the task evolves over the course of training. We then formulate a theory for optimal training, which involves selecting sequences of stimuli that will drive the animal's internal policy toward a desired location in the parameter space. Simulations show that our method can in theory provide a substantial speedup over standard training methods. We feel these results will hold considerable theoretical and practical implications both for researchers in reinforcement learning and for experimentalists seeking to train animals.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1947–1955},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157313,
author = {Tolstikhin, Ilya and Sriperumbudur, Bharath K. and Sch\"{o}lkopf, Bernhard},
title = {Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on ℝd and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its U-statistic variant, which are usually employed in applications.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1938–1946},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157312,
author = {Rogers, Ryan and Roth, Aaron and Ullman, Jonathan and Vadhan, Salil},
title = {Privacy Odometers and Filters: Pay-as-You-Go Composition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we initiate the study of adaptive composition in differential privacy when the length of the composition, and the privacy parameters themselves can be chosen adaptively, as a function of the outcome of previously run analyses. This case is much more delicate than the setting covered by existing composition theorems, in which the algorithms themselves can be chosen adaptively, but the privacy parameters must be fixed up front. Indeed, it isn't even clear how to define differential privacy in the adaptive parameter setting. We proceed by defining two objects which cover the two main use cases of composition theorems. A privacy filter is a stopping time rule that allows an analyst to halt a computation before his pre-specified privacy budget is exceeded. A privacy odometer allows the analyst to track realized privacy loss as he goes, without needing to pre-specify a privacy budget. We show that unlike the case in which privacy parameters are fixed, in the adaptive parameter setting, these two use cases are distinct. We show that there exist privacy filters with bounds comparable (up to constants) with existing privacy composition theorems. We also give a privacy odometer that nearly matches non-adaptive private composition theorems, but is sometimes worse by a small asymptotic factor. Moreover, we show that this is inherent, and that any valid privacy odometer in the adaptive parameter setting must lose this factor, which shows a formal separation between the filter and odometer use-cases.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1929–1937},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157311,
author = {S\"{u}mb\"{u}l, Uygar and Roossien, Douglas and Chen, Fei and Barry, Nicholas and Boyden, Edward S. and Cai, Dawen and Cunningham, John P. and Paninski, Liam},
title = {Automated Scalable Segmentation of Neurons from Multispectral Images},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Reconstruction of neuroanatomy is a fundamental problem in neuroscience. Stochastic expression of colors in individual cells is a promising tool, although its use in the nervous system has been limited due to various sources of variability in expression. Moreover, the intermingled anatomy of neuronal trees is challenging for existing segmentation algorithms. Here, we propose a method to automate the segmentation of neurons in such (potentially pseudo-colored) images. The method uses spatio-color relations between the voxels, generates supervoxels to reduce the problem size by four orders of magnitude before the final segmentation, and is parallelizable over the supervoxels. To quantify performance and gain insight, we generate simulated images, where the noise level and characteristics, the density of expression, and the number of fluorophore types are variable. We also present segmentations of real Brainbow images of the mouse hippocampus, which reveal many of the dendritic segments.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1920–1928},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157310,
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
title = {End-to-End Goal-Driven Web Navigation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with question-answer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1911–1919},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157309,
author = {Wei, Zijun and Adeli, Hossein and Zelinsky, Gregory and Hoai, Minh and Samaras, Dimitris},
title = {Learned Region Sparsity and Diversity Also Predict Visual Attention},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms, and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1902–1910},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157308,
author = {Li, Bo and Wang, Yining and Singh, Aarti and Vorobeychik, Yevgeniy},
title = {Data Poisoning Attacks on Factorization-Based Collaborative Filtering},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1893–1901},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157307,
author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
title = {PAC-Bayesian Theory Meets Bayesian Inference},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1884–1892},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157306,
author = {Rabusseau, Guillaume and Kadri, Hachem},
title = {Low-Rank Regression with Tensor Responses},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR computes accurate solutions while being computationally very competitive.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1875–1883},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157305,
author = {Harwath, David and Torralba, Antonio and Glass, James R.},
title = {Unsupervised Learning of Spoken Language with Visual Context},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1866–1874},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157304,
author = {Sohn, Kihyuk},
title = {Improved Deep Metric Learning with Multi-Class N-Pair Loss Objective},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N-pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N-1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N+1) x N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1857–1865},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157303,
author = {Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
title = {PAC Reinforcement Learning with Rich Observations},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1848–1856},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157302,
author = {Kim, Jisu and Chen, Yen-Chi and Balakrishnan, Sivaraman and Rinaldo, Alessandro and Wasserman, Larry},
title = {Statistical Inference for Cluster Trees},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1839–1847},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157301,
author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan and Bengio, Yoshua},
title = {Architectural Complexity Measures of Recurrent Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output non-linearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1830–1838},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157300,
author = {Liu, Yang and Chen, Yiling},
title = {A Bandit Framework for Strategic Regression},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a learner's problem of acquiring data dynamically for training a regression model, where the training data are collected from strategic data sources. A fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data, despite that the quality is not directly verifiable by the learner. In this work, we study a dynamic data acquisition process where data holders can contribute multiple times. Using a bandit framework, we leverage the long-term incentive of future job opportunities to incentivize high-quality contributions. We propose a Strategic Regression-Upper Confidence Bound (SR-UCB) framework, a UCB-style index combined with a simple payment rule, where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work. For linear regression and a certain family of non-linear regression problems, we show that SR-UCB enables an O(√log T/T) -Bayesian Nash Equilibrium (BNE) where each worker exerts a target effort level that the learner has chosen, with T being the number of data acquisition stages. The SR-UCB framework also has some other desirable properties: (1) The indexes can be updated in an online fashion (hence computation is light). (2) A slight variant, namely Private SR-UCB (PSR-UCB), is able to preserve (O(log-1 T), O(log-1 T))-differential privacy for workers' data, with only a small compromise on incentives (each worker exerting a target effort level is an O(log6 T/√T)-BNE).},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1821–1829},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157299,
author = {Li, Steven Cheng-Xian and Marlin, Benjamin},
title = {A Scalable End-to-End Gaussian Process Adapter for Irregularly Sampled Time Series Classification},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1812–1820},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157298,
author = {Han, Jun and Liu, Qiang},
title = {Bootstrap Model Aggregation for Distributed Statistical Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1803–1811},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157297,
author = {Huang, Kejun and Fu, Xiao and Sidiropoulos, Nicholas D.},
title = {Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words - i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1794–1802},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157296,
author = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
title = {The Multi-Fidelity Multi-Armed Bandit},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a variant of the classical stochastic K-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a multi-fidelity bandit, where, at each time step, the forecaster may choose to play an arm at any one of M fidelities. The highest fidelity (desired outcome) expends cost λ(M). The mth fidelity (an approximation) expends λ(m) &lt; λ(M) and returns a biased estimate of the highest fidelity. We develop MF-UCB, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, MF-UCB would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that MF-UCB is nearly optimal under certain conditions.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1785–1793},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157295,
author = {Vaswani, Namrata and Guo, Han},
title = {Correlated-PCA: Principal Components' Analysis When Data and Noise Are Correlated},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Prov-ably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as "data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1776–1784},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157294,
author = {Djolonga, Josip and Tschiatschek, Sebastian and Krause, Andreas},
title = {Variational Inference in Mixed Probabilistic Submodular Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of variational inference in probabilistic models with both log-submodular and log-supermodular higher-order potentials. These models can represent arbitrary distributions over binary variables, and thus generalize the commonly used pairwise Markov random fields and models with log-supermodular potentials only, for which efficient approximate inference algorithms are known. While inference in the considered models is #P-hard in general, we present efficient approximate algorithms exploiting recent advances in the field of discrete optimization. We demonstrate the effectiveness of our approach in a large set of experiments, where our model allows reasoning about preferences over sets of items with complements and substitutes.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1767–1775},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157293,
author = {Krummenacher, Gabriel and McWilliams, Brian and Kilcher, Yannic and Buhmann, Joachim M. and Meinshausen, Nicolai},
title = {Scalable Adaptive Stochastic Optimization Using Random Projections},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Adaptive stochastic gradient methods such as ADAGRAD have gained popularity in particular for training deep neural networks. The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of ADAGRAD is expected to attain better performance, however in high dimensions it is computationally impractical. We present ADA-LR and RADAGRAD two computationally efficient approximations to full-matrix ADAGRAD based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix ADAGRAD but at a much smaller computational cost. We show that the regret of ADA-LR is close to the regret of full-matrix ADAGRAD which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that ADA-LR and RADAGRAD perform similarly to full-matrix ADAGRAD. On the task of training convolutional neural networks as well as recurrent neural networks, RADAGRAD achieves faster convergence than diagonal ADAGRAD.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1758–1766},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157292,
author = {Zhang, Yizhe and Wang, Xiangyu and Chen, Changyou and Henao, Ricardo and Fan, Kai and Carin, Lawrence},
title = {Towards Unifying Hamiltonian Monte Carlo and Slice Sampling},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, demonstrating their connection via the Hamiltonian-Jacobi equation from Hamiltonian mechanics. This insight enables extension of HMC and slice sampling to a broader family of samplers, called Monomial Gamma Samplers (MGS). We provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the MGS draws decorrelated samples from the desired target distribution. We further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues. Our theoretical results are validated with synthetic data and real-world applications.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1749–1757},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157291,
author = {Simon-Gabriel, Carl-Johann and undefinedcibior, Adam and Tolstikhin, Ilya and Sch\"{o}lkopf, Bernhard},
title = {Consistent Kernel Mean Estimation for Functions of Random Variables},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function undefined, consistent estimators of the mean embedding of a random variable X lead to consistent estimators of the mean embedding of undefined(X). For Mat\'{e}rn kernels and sufficiently smooth functions we also provide rates of convergence.Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on i.i.d. samples as well as "reduced set" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1740–1748},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157290,
author = {Norouzi, Mohammad and Bengio, Samy and Chen, Zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
title = {Reward Augmented Maximum Likelihood for Neural Structured Prediction},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards. Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1731–1739},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157289,
author = {Wang, Mengdi and Liu, Ji and Fang, Ethan X.},
title = {Accelerating Stochastic Composition Optimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1722–1730},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157288,
author = {Chang, Kai-Wei and He, He and Daum\'{e}, Hal and Langford, John and Ross, Stephane},
title = {A Credit Assignment Compiler for Joint Prediction},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1713–1721},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157287,
author = {Yan, Xinchen and Yang, Jimei and Yumer, Ersin and Guo, Yijie and Lee, Honglak},
title = {Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1704–1712},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157286,
author = {Gautier, A. and Nguyen, Q. and Hein, M.},
title = {Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1695–1703},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157285,
author = {Schuurmans, Dale and Zinkevich, Martin},
title = {Deep Learning Games},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1686–1694},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157284,
author = {Krishnasamy, Subhashini and Sen, Rajat and Johari, Ramesh and Shakkottai, Sanjay},
title = {Regret of Queueing Bandits},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a variant of the multiarmed bandit problem where jobs queue for service, and service rates of different servers may be unknown. We study algorithms that minimize queue-regret: the (expected) difference between the queue-lengths obtained by the algorithm, and those obtained by a "genie"-aided matching algorithm that knows exact service rates. A naive view of this problem would suggest that queue-regret should grow logarithmically: since queue-regret cannot be larger than classical regret, results for the standard MAB problem give algorithms that ensure queue-regret increases no more than logarithmically in time. Our paper shows surprisingly more complex behavior. In particular, the naive intuition is correct as long as the bandit algorithm's queues have relatively long regenerative cycles: in this case queue-regret is similar to cumulative regret, and scales (essentially) logarithmically. However, we show that this "early stage" of the queueing bandit eventually gives way to a "late stage", where the optimal queue-regret scaling is O(1/t). We demonstrate an algorithm that (order-wise) achieves this asymptotic queue-regret, and also exhibits close to optimal switching time from the early stage to the late stage.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1677–1685},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157283,
author = {Cortes, Corinna and De Salvo, Giulia and Mohri, Mehryar},
title = {Boosting with Abstention},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost. At each round, our algorithm selects a pair of functions, a base predictor and a base abstention function. We define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the Bayes solution. Our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the Rademacher complexities of the corresponding function classes. We give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps. We also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1668–1676},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157282,
author = {Chen, Wei and Hu, Wei and Li, Fu and Li, Jian and Liu, Yu and Lu, Pinyan},
title = {Combinatorial Multi-Armed Bandit with General Reward Functions},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O(log T) distribution-dependent regret and \~{O}(√T) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first \~{O}(√T) bound on the (1 — ε)-approximation regret of its online problem, for any ε &gt; 0.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1659–1667},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157281,
author = {Allen-Zhu, Zeyuan and Yuan, Yang and Sridharan, Karthik},
title = {Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The amount of data available in the world is growing faster than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information. Our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of the ERM problem, and our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using clustering. Our algorithms outperform their classical counterparts ACDM and SVRG respectively.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1650–1658},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157280,
author = {Lin, Ming and Ye, Jieping},
title = {A Non-Convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(ε) recovery error after retrieving O(k3d log(1/ε)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1641–1649},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157279,
author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
title = {Robustness of Classifiers: From Adversarial to Random Noise},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1632–1640},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157278,
author = {Kriege, Nils M. and Giscard, Pierre-Louis and Wilson, Richard C.},
title = {On Valid Optimal Assignment Kernels and Applications to Graph Classification},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1623–1631},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157277,
author = {Allen-Zhu, Zeyuan and Hazan, Elad},
title = {Optimal Black-Box Reductions between Optimization Objectives},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications.Furthermore, unlike existing results, our new reductions are optimal and more practical. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1614–1622},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157276,
author = {Lagr\'{e}e, Paul and Vernade, Claire and Capp\'{e}, Olivier},
title = {Multiple-Play Bandits in the Position-Based Model},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been simply ignored by the user. The present work proposes to exploit available information regarding the display position bias under the so-called Position-based click model (PBM). We first discuss how this model differs from the Cascade model and its variants considered in several recent works on multiple-play bandits. We then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1605–1613},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157275,
author = {Mankowitz, Daniel J. and Mann, Timothy A. and Mannor, Shie},
title = {Adaptive Skills Adaptive Partitions (ASAP)},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1596–1604},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157274,
author = {Chen, Bryant},
title = {Identification and Overidentification of Linear Structural Equation Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1587–1595},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157273,
author = {Jabbari, Shahin and Rogers, Ryan and Roth, Aaron and Wu, Zhiwei Steven},
title = {Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We define and study the problem of predicting the solution to a linear program (LP) given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, that has been studied under the name "Learning from Revealed Preferences". We give mistake bound learning algorithms in two settings: in the first, the objective of the LP is known to the learner but there is an arbitrary, fixed set of constraints which are unknown. Each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the LP given the union of the known and unknown constraints. This models the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown. In the second setting, the objective of the LP is unknown, and changing in a controlled way. The constraints of the LP may also change every day, but are known. An example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known LP.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1578–1586},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157272,
author = {Silva, Ricardo},
title = {Observational-Interventional Priors for Dose-Response Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1569–1577},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157271,
author = {Garg, Vikas K. and Jaakkola, Tommi},
title = {Learning Tree Structured Potential Games},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many real phenomena, including behaviors, involve strategic interactions that can be learned from data. We focus on learning tree structured potential games where equilibria are represented by local maxima of an underlying potential function. We cast the learning problem within a max margin setting and show that the problem is NP-hard even when the strategic interactions form a tree. We develop a variant of dual decomposition to estimate the underlying game and demonstrate with synthetic and real decision/voting data that the game theoretic perspective (carving out local maxima) enables meaningful recovery.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1560–1568},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157270,
author = {Zheng, Stephan and Yue, Yisong and Lucey, Patrick},
title = {Generating Long-Term Trajectories Using Deep Hierarchical Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior. The key difficulty is that conventional models are "single-scale" and only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1551–1559},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157269,
author = {Richardson, Elad and Herskovitz, Rom and Ginsburg, Boris and Zibulevsky, Michael},
title = {SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method, and has been adapted for the stochastic learning. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating significant improvement in performance. Video presentation is given in [15]},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1542–1550},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157268,
author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
title = {Understanding Probabilistic Sparse Gaussian Process Approximations},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1533–1541},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157267,
author = {Jain, Prateek and Rao, Nikhil and Dhillon, Inderjit},
title = {Structured Sparse Regression via Greedy Hard-Thresholding},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation ≥ 0.99), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1524–1532},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157266,
author = {Bogunovic, Ilija and Scarlett, Jonathan and Krause, Andreas and Cevher, Volkan},
title = {Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new algorithm, truncated variance reduction (TRUVAR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TRUVAR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TRUVAR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1515–1523},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157265,
author = {Shrivastava, Anshumali},
title = {Simple and Efficient Weighted Minwise Hashing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Weighted minwise hashing (WMH) is one of the fundamental subroutine, required by many celebrated approximation algorithms, commonly adopted in industrial practice for large -scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. We propose a simple rejection type sampling scheme based on a carefully designed red-green map, where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method, for many practical datasets, is an order of magnitude smaller than existing methods. Experimental evaluations, on real datasets, show that for computing 500 WMH, our proposal can be 60000x faster than the Ioffe's method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient "densified" one permutation hashing schemes [26, 27]. Given the simplicity of our approach and its significant advantages, we hope that it will replace existing implementations in practice.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1506–1514},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157264,
author = {Sokolov, Artem and Kreutzer, Julia and Lo, Christopher and Riezler, Stefan},
title = {Stochastic Structured Prediction under Bandit Feedback},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1497–1505},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157263,
author = {Saberian, Mohammad and Pereira, Jose Costa and Xu, Can and Yang, Jian and Vasconcelos, Nuno},
title = {Large Margin Discriminant Dimensionality Reduction in Prediction Space},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1488–1496},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157262,
author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, R\'{e}mi},
title = {Unifying Count-Based Exploration and Intrinsic Motivation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1479–1487},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157261,
author = {Scanagatta, Mauro and Corani, Giorgio and de Campos, Cassio P. and Zaffalon, Marco},
title = {Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1470–1478},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157260,
author = {Ahn, Sungsoo and Chertkov, Michael and Shin, Jinwoo},
title = {Synthesis of MCMC and Belief Propagation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1461–1469},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157259,
author = {Bunel, Rudy and Desmaison, Alban and Kohli, Pushmeet and Torr, Philip H.S. and Kumar, M. Pawan},
title = {Adaptive Neural Compilation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1452–1460},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157258,
author = {Picheny, Victor and Gramacy, Robert B. and Wild, Stefan and Digabel, S\'{e}bastien Le},
title = {Bayesian Optimization under Mixed Constraints with a Slack-Variable Augmented Lagrangian},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show our new slack "ALBO" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1443–1451},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157257,
author = {Belousov, Boris and Neumann, Gerhard and Rothkopf, Constantin A. and Peters, Jan},
title = {Catching Heuristics Are Optimal Control Policies},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball. One theory posits that humans predict the ball trajectory to optimally plan future actions; the other claims that, instead of performing such complicated computations, humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback. In this paper, we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty. Modeling catching as a continuous partially observable Markov decision process and employing stochastic optimal control theory, we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball. Specifically, by varying model parameters such as noise, time to ground contact, and perceptual latency, we show that different strategies arise under different circumstances. The catcher's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration. Thus, we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1434–1442},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157256,
author = {Zanella, Giacomo and Betancourt, Brenda and Wallach, Hanna and Miller, Jeffrey and Zaidi, Abbas and Steorts, Rebecca C.},
title = {Flexible Models for Microclustering with Application to Entity Resolution},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman-Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1425–1433},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157255,
author = {Balamurugan, P. and Bach, Francis},
title = {Stochastic Variance Reduction Methods for Saddle-Point Problems},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the "catalyst" framework, leading to an algorithm which is always superior to accelerated batch algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1416–1424},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157254,
author = {Mairal, Julien},
title = {End-to-End Kernel Learning with Supervised Convolutional Kernel Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard "deep learning" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1407–1415},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157253,
author = {Oh, Tae-Hyun and Matsushita, Yasuyuki and Kweon, In So and Wipf, David},
title = {A Pseudo-Bayesian Algorithm for Robust PCA},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Commonly used in many applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1398–1406},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157252,
author = {Tadmor, Oren and Rosenwein, Tal and Shalev-Shwartz, Shai and Wexler, Yonatan and Shashua, Amnon},
title = {Learning a Metric Embedding for Face Recognition Using the Multibatch Method},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system. Our main technical contribution centers around a novel training method, called Multibatch, for similarity learning, i.e., for the task of generating an invariant face signature "through training pairs of same'' and not-same" face images. The Multibatch method first generates signatures for a mini-batch of k face images and then constructs an unbiased estimate of the full gradient by relying on all k2 - k pairs from the mini-batch. We prove that the variance of the Multibatch estimator is bounded by O(1/k2), under some mild conditions. In contrast, the standard gradient estimator that relies on random k/2 pairs has a variance of order 1/k. The smaller variance of the Multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent. Using the Multibatch method we train a deep convolutional neural network that achieves an accuracy of 98.2% on the LFW benchmark, while its prediction runtime takes only 30msec on a single ARM Cortex A9 core. Furthermore, the entire training process took only 12 hours on a single Titan X GPU.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1396–1397},
numpages = {2},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157251,
author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
title = {Dynamic Network Surgery for Efficient DNNs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108x and 17.7x respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1387–1395},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157250,
author = {Gunasekar, Suriya and Koyejo, Oluwasanmi and Ghosh, Joydeep},
title = {Preference Completion from Partial Rankings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to over-fitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low-rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a log factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain-regions and cognitive neuroscience terms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1378–1386},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157249,
author = {McIntosh, Lane T. and Maheswaranathan, Niru and Nayebi, Aran and Ganguli, Surya and Baccus, Stephen A.},
title = {Deep Learning Models of the Retinal Response to Natural Scenes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1369–1377},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157248,
author = {Newling, James and Fleuret, Fran\c{c}ois},
title = {Nested Mini-Batch K-Means},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t + 1.Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100 x earlier than the standard mini-batch algorithm.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1360–1368},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157247,
author = {Zhao, Shengjia and Zhou, Enze and Sabharwal, Ashish and Ermon, Stefano},
title = {Adaptive Concentration Inequalities for Sequential Decision Problems},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees. We introduce Hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples. Our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems. In particular, we apply them to sequential hypothesis testing, best arm identification, and sorting. The resulting algorithms rival or exceed the state of the art both theoretically and empirically.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1351–1359},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157246,
author = {Abbe, Emmanuel and Sandon, Colin},
title = {Achieving the KS Threshold in the General Stochastic Block Model with Linearized Acyclic Belief Propagation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the detection problem in symmetric SBMs, Decelle et al. conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open for three and more communities. We prove this conjecture here, obtaining a general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in O(n ln n) time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1342–1350},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157245,
author = {Onken, Arno and Panzeri, Stefano},
title = {Mixed Vine Copulas as Joint Models of Spike Counts and Local Field Potentials},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Concurrent measurements of neural activity at multiple scales, sometimes performed with multimodal techniques, become increasingly important for studying brain function. However, statistical methods for their concurrent analysis are currently lacking. Here we introduce such techniques in a framework based on vine copulas with mixed margins to construct multivariate stochastic models. These models can describe detailed mixed interactions between discrete variables such as neural spike counts, and continuous variables such as local field potentials. We propose efficient methods for likelihood calculation, inference, sampling and mutual information estimation within this framework. We test our methods on simulated data and demonstrate applicability on mixed data generated by a biologically realistic neural network. Our methods hold the promise to considerably improve statistical analysis of neural data recorded simultaneously at different scales.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1333–1341},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157244,
author = {Vinayak, Ramya Korlakai and Hassibi, Babak},
title = {Crowdsourced Clustering: Querying Edges vs Triangles},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1324–1332},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157243,
author = {Rosenfeld, Nir and Globerson, Amir},
title = {Optimal Tagging with Markov Chain Optimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many information systems use tags and keywords to describe and annotate content. These allow for efficient organization and categorization of items, as well as facilitate relevant search queries. As such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item. In tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable. In this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized.We formulate the problem by modeling traffic using a Markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest. The resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint.We show that the optimization problem is NP-hard, but has a (1 - 1/e) -approximation via a simple greedy algorithm due to monotonicity and submodularity. Furthermore, the structure of the problem allows for an efficient computation of the greedy step. To demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1315–1323},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157242,
author = {Sinha, Aman and Duchi, John},
title = {Learning Kernels with Random Features},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks. However, such methods require a user-defined kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Specifically, we present an efficient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1306–1314},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157241,
author = {Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {Sampling for Bayesian Program Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Towards learning programs from data, we introduce the problem of sampling programs from posterior distributions conditioned on that data. Within this setting, we propose an algorithm that uses a symbolic solver to efficiently sample programs. The proposal combines constraint-based program synthesis with sampling via random parity constraints. We give theoretical guarantees on how well the samples approximate the true posterior, and have empirical results showing the algorithm is efficient in practice, evaluating our approach on 22 program learning problems in the domains of text editing and computer-aided programming.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1297–1305},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157240,
author = {Ganapathiraman, Vignesh and Zhang, Xinhua and Yu, Yaoliang and Wen, Junfeng},
title = {Convex Two-Layer Modeling with Latent Structure},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unsupervised learning of structured predictors has been a long standing pursuit in machine learning. Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred. Aside from being nonconvex, it also requires the demanding inference of normalization. In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally. We further expand its applicability by resorting to a weaker form of inference—maximum a-posteriori. The flexibility of the model is demonstrated on two structures based on total unimodularity—graph matching and linear chain. Experimental results confirm the promise of the method.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1288–1296},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157239,
author = {Meng, Qi and Ke, Guolin and Wang, Taifeng and Chen, Wei and Ye, Qiwei and Ma, Zhi-Ming and Liu, Tie-Yan},
title = {A Communication-Efficient Parallel Algorithm for Decision Tree},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called Parallel Voting Decision Tree (PV-Tree), to tackle this challenge. After partitioning the training data onto a number of (e.g., M) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-k attributes are selected from each machine according to its local data. Then, globally top-2k attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-2k attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1279–1287},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157238,
author = {Huang, Chen and Loy, Chen Change and Tang, Xiaoou},
title = {Local Similarity-Aware Deep Feature Embedding},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1270–1278},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157237,
author = {Ramamohan, Siddartha and Rajkumar, Arun and Agarwal, Shivani},
title = {Dueling Bandits: Beyond Condorcet Winners to General Tournament Solutions},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work on deriving O(log T) anytime regret bounds for stochastic dueling bandit problems has considered mostly Condorcet winners, which do not always exist, and more recently, winners defined by the Copeland set, which do always exist. In this work, we consider a broad notion of winners defined by tournament solutions in social choice theory, which include the Copeland set as a special case but also include several other notions of winners such as the top cycle, uncovered set, and Banks set, and which, like the Copeland set, always exist. We develop a family of UCB-style dueling bandit algorithms for such general tournament solutions, and show O(log T) anytime regret bounds for them. Experiments confirm the ability of our algorithms to achieve low regret relative to the target winning set of interest.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1261–1269},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157236,
author = {Cuong, Nguyen Viet and Xu, Huan},
title = {Adaptive Maximization of Pointwise Submodular Functions with Budget Constraint},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning. We investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions. In both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively. This implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget. We discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1252–1260},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157235,
author = {Ye, Han-Jia and Zhan, De-Chuan and Si, Xue-Min and Jiang, Yuan and Zhou, Zhi-Hua},
title = {What Makes Objects Similar: A Unified Multi-Metric Learning Approach},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Unified Multi-Metric Learning (UM2L) framework to exploit multiple types of metrics. In UM2L, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce flexibilities for representing and utilizing both spatial and semantic linkages. Besides, we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classification performance and comprehensibility of UM2L. Visualization results also validate its ability on physical meanings discovery.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1243–1251},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157234,
author = {Yang, Ying and Aminoff, Elissa M. and Tarr, Michael J. and Kass, Robert E.},
title = {A State-Space Model of Cross-Region Dynamic Connectivity in MEG/EEG},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Cross-region dynamic connectivity, which describes the spatio-temporal dependence of neural activity among multiple brain regions of interest (ROIs), can provide important information for understanding cognition. For estimating such connectivity, magnetoencephalography (MEG) and electroencephalography (EEG) are well-suited tools because of their millisecond temporal resolution. However, localizing source activity in the brain requires solving an under-determined linear problem. In typical two-step approaches, researchers first solve the linear problem with generic priors assuming independence across ROIs, and secondly quantify cross-region connectivity. In this work, we propose a one-step state-space model to improve estimation of dynamic connectivity. The model treats the mean activity in individual ROIs as the state variable and describes non-stationary dynamic dependence across ROIs using time-varying auto-regression. Compared with a two-step method, which first obtains the commonly used minimum-norm estimates of source activity, and then fits the auto-regressive model, our state-space model yielded smaller estimation errors on simulated data where the model assumptions held. When applied on empirical MEG data from one participant in a scene-processing experiment, our state-space model also demonstrated intriguing preliminary results, indicating leading and lagged linear dependence between the early visual cortex and a higher-level scene-sensitive region, which could reflect feedforward and feedback information flow within the visual cortex during scene processing.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1234–1242},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157233,
author = {Singh, Shashank and P\'{o}czos, Barnab\'{a}s},
title = {Finite-Sample Analysis of Fixed-<i>k</i> Nearest Neighbor Density Functional Estimators},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density, including entropies and divergences. Rather than plugging a consistent density estimate (which requires k → ∞ as the sample size n → ∞ into the functional of interest, the estimators we consider fix k and perform a bias correction. This is more efficient computationally, and, as we show in certain cases, statistically, leading to faster convergence rates. Our framework unifies several previous estimators, for most of which ours are the first finite sample guarantees.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1225–1233},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157232,
author = {Xu, Yi and Yan, Yan and Lin, Qihang and Yang, Tianbao},
title = {Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than <i>O</i>(1/ε)},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we develop a novel homotopy smoothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is O(1/ε) without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of \~{O}(1/ε1-θ) 1with θ ∈ (0,1] capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and ℓ1 norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1216–1224},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157231,
author = {Niu, Gang and du Plessis, Marthinus C. and Sakai, Tomoya and Ma, Yao and Sugiyama, Masashi},
title = {Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1207–1215},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157230,
author = {Gerchinovitz, S\'{e}bastien and Lattimore, Tor},
title = {Refined Lower Bounds for Adversarial Bandits},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1198–1206},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157229,
author = {Lattimore, Finnian and Lattimore, Tor and Reid, Mark D.},
title = {Causal Bandits: Learning Good Interventions via Causal Inference},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1189–1197},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157228,
author = {Krotov, Dmitry and Hopfield, John J.},
title = {Dense Associative Memory for Pattern Recognition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1180–1188},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157227,
author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
title = {Regularization with Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1171–1179},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157226,
author = {Dubey, Avinava and Reddi, Sashank J. and P\'{o}czos, Barnab\'{a}s and Smola, Alexander J. and Xing, Eric P. and Williamson, Sinead A.},
title = {Variance Reduction in Stochastic Gradient Langevin Dynamics},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper, we present techniques for reducing variance in stochastic gradient Langevin dynamics, yielding novel stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1162–1170},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157225,
author = {Reddi, Sashank J. and Sra, Suvrit and P\'{o}czos, Barnab\'{a}s and Smola, Alexander J.},
title = {Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1153–1161},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157224,
author = {Iwata, Tomoharu and Yamada, Makoto},
title = {Multi-View Anomaly Detection via Robust Probabilistic Latent Variable Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose probabilistic latent variable models for multi-view anomaly detection, which is the task of finding instances that have inconsistent views given multi-view data. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1144–1152},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157223,
author = {Xue, Yexiang and Li, Zhiyuan and Ermon, Stefano and Gomes, Carla P. and Selman, Bart},
title = {Solving Marginal MAP Problems with NP Oracles and Parity Constraints},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Arising from many applications at the intersection of decision-making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP problem, by encoding it as a single optimization in a polynomial size of the original problem. We evaluate our approach in several machine learning and decision-making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1135–1143},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157222,
author = {Rubin, Timothy N. and Koyejo, Oluwasanmi and Jones, Michael N. and Yarkoni, Tal},
title = {Generalized Correspondence-LDA Models (GC-LDA) for Identifying Functional Regions in the Brain},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents Generalized Correspondence-LDA (GC-LDA), a generalization of the Correspondence-LDA model that allows for variable spatial representations to be associated with topics, and increased flexibility in terms of the strength of the correspondence between data types induced by the model. We present three variants of GC-LDA, each of which associates topics with a different spatial representation, and apply them to a corpus of neuroimaging data. In the context of this dataset, each topic corresponds to a functional brain region, where the region's spatial extent is captured by a probability distribution over neural activity, and the region's cognitive function is captured by a probability distribution over linguistic terms. We illustrate the qualitative improvements offered by GC-LDA in terms of the types of topics extracted with alternative spatial representations, as well as the model's ability to incorporate a-priori knowledge from the neuroimaging literature. We furthermore demonstrate that the novel features of GC-LDA improve predictions for missing data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1126–1134},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157221,
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
title = {VIME: Variational Information Maximizing Exploration},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as e-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1117–1125},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157220,
author = {Eberhardt, Sven and Cader, Jonah and Serre, Thomas},
title = {How Deep is the Feature Analysis Underlying Rapid Visual Categorization?},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.We have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1108–1116},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157219,
author = {Li, Dangna and Yang, Kun and Wong, Wing Hung},
title = {Density Estimation via Discrepancy Based Adaptive Sequential Partition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given iid observations from an unknown absolute continuous distribution defined on some domain Ω, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of Ω. The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has a provable convergence rate. We empirically demonstrate its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1099–1007},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157218,
author = {Zhai, Shuangfei and Cheng, Yu and Lu, Weining and Zhang, Zhongfei Mark},
title = {Doubly Convolutional Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1090–1098},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157217,
author = {Li, Yingzhen and Turner, Richard E.},
title = {R\'{e}Nyi Divergence Variational Inference},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces the variational Renyi bound (VR) that extends traditional variational inference to R\'{e}nyi's α-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of α that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative α values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1081–1089},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157216,
author = {Xu, Pan and Gu, Quanquan},
title = {Semiparametric Differential Graph Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many cases of network analysis, it is more attractive to study how a network varies under different conditions than an individual static network. We propose a novel graphical model, namely Latent Differential Graph Model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices. We propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization. We show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys the oracle property under mild conditions. Thorough experiments on both synthetic and real world data support our theory.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1072–1080},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157215,
author = {Berahas, Albert S. and Nocedal, Jorge and Tak\'{a}\v{c}, Martin},
title = {A Multi-Batch L-BFGS Method for Machine Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1063–1071},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157214,
author = {Munos, R\'{e}mi and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G.},
title = {Safe and Efficient Off-Policy Reinforcement Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(λ), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q* without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(λ), which was an open problem since 1989. We illustrate the benefits of Retrace(λ) on a standard suite of Atari 2600 games.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1054–1062},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157213,
author = {N\o{}kland, Arild},
title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1045–1053},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157212,
author = {Papamakarios, George and Murray, Iain},
title = {Fast ε-Free Inference of Simulation Models with Bayesian Conditional Density Estimation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ε-ball around the observed data, which is only correct in the limit ε → 0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ε → 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1036–1044},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157211,
author = {Gal, Yarin and Ghahramani, Zoubin},
title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1027–1035},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157210,
author = {Singh, Shashank and Du, Simon S. and P\'{o}czos, Barnab\'{a}s},
title = {Efficient Nonparametric Smoothness Estimation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, due to a lack of practical estimators. They also include, as special cases, L2 quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the finite-sample bias and variance of our estimators, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing and empirically validate our estimators on synthetic data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1018–1026},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157209,
author = {Garber, Dan and Meshi, Ofer},
title = {Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings:1. large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration2. the worst case convergence rate depends unfavorably on the dimensionIn this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular:1. both memory and computation overheads are only linear in the dimension2. in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous work, with a linear dependence on the number of non-zeros in the optimal solutionAt the heart of our method and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence which shows that our method delivers state-of-the-art performance.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1009–1017},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157208,
author = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Oliva, Junier and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
title = {Gaussian Process Bandit Optimisation with Multi-Fidelity Evaluations},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function f. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to f may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of f in a small but promising region and speedily identify the optimum. We formalise this task as a multi-fidelity bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1000–1008},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157207,
author = {Wang, Xinan and Dasgupta, Sanjoy},
title = {An Algorithm for ℓ<sub>1</sub> Nearest Neighbor Search via Monotonic Embedding},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fast algorithms for nearest neighbor (NN) search have in large part focused on ℓ2 distance. Here we develop an approach for ℓ1 distance that begins with an explicit and exactly distance-preserving embedding of the points into ℓ21. We show how this can efficiently be combined with random-projection based methods for ℓ2 NN search, such as locality-sensitive hashing (LSH) or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {991–999},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157206,
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
title = {LazySVD: Even Faster SVD Decomposition yet without Agonizing Pain},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [19] proved the first gap-free convergence result using the block Krylov method, Shamir [21] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [7] provided the fastest O(nnz(A) + poly(1/ε))-time algorithm using alternating minimization.In this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [19], and the first accelerated and stochastic method outperforming [21]. In the O(nnz(A) + poly(1/ε)) running-time regime, LazySVD outperforms [7] in certain parameter regimes without even using alternating minimization.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {982–990},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157205,
author = {Yun, Se-Young and Proutiere, Alexandre},
title = {Optimal Cluster Recovery in the Labeled Stochastic Block Model},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label ℓ appears with probability p(i,j, ℓ) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels.Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {973–981},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157204,
author = {Minami, Kentaro and Arai, Hiromi and Sato, Issei and Nakagawa, Hiroshi},
title = {Differential Privacy without Sensitivity},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The exponential mechanism is a general method to construct a randomized estimator that satisfies (ε, 0)-differential privacy. Recently, Wang et al. showed that the Gibbs posterior, which is a data-dependent probability distribution that contains the Bayesian posterior, is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function. While the exponential mechanism provides a way to build an (ε, 0)-differential private algorithm, it requires boundedness of the loss function, which is quite stringent for some learning problems. In this paper, we focus on (ε, δ)-differential privacy of Gibbs posteriors with convex and Lipschitz loss functions. Our result extends the classical exponential mechanism, allowing the loss functions to have an unbounded sensitivity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {964–972},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157203,
author = {Figurnov, Michael and Ibraimova, Aijan and Vetrov, Dmitry and Kohli, Pushmeet},
title = {Perforated CNNs: Acceleration through Elimination of Redundant Convolutions},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we show that perforation is complementary to the recently proposed acceleration method of Zhang et al. [28].},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {955–963},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157202,
author = {Tandon, Pulkit and Malviya, Yash H. and Rajendran, Bipin},
title = {Efficient and Robust Spiking Neural Circuit for Navigation Inspired by Echolocating Bats},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate a spiking neural circuit for azimuth angle detection inspired by the echolocation circuits of the Horseshoe bat Rhinolophus ferrumequinum and utilize it to devise a model for navigation and target tracking, capturing several key aspects of information transmission in biology. Our network, using only a simple local-information based sensor implementing the cardioid angular gain function, operates at biological spike rate of approximately 10 Hz. The network tracks large angular targets (60°) within 1 sec with a 10% RMS error. We study the navigational ability of our model for foraging and target localization tasks in a forest of obstacles and show that it requires less than 200X spike-triggered decisions, while suffering less than 1% loss in performance compared to a proportional-integral-derivative controller, in the presence of 50% additive noise. Superior performance can be obtained at a higher average spike rate of 100 Hz and 1000 Hz, but even the accelerated networks require 20X and 10X lesser decisions respectively, demonstrating the superior computational efficiency of bio-inspired information processing systems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {946–954},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157201,
author = {Tang, Pingfan and Phillips, Jeff M.},
title = {The Robustness of Estimator Composition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {937–945},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157200,
author = {Zhe, Shandian and Zhang, Kai and Wang, Pengyuan and Lee, Kuang-chih and Xu, Zenglin and Qi, Yuan and Gharamani, Zoubin},
title = {Distributed Flexible Nonlinear Tensor Factorization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Tensor factorization is a powerful tool to analyse multi-way data. Recently proposed nonlinear factorization methods, although capable of capturing complex relationships, are computationally quite expensive and may suffer a severe learning bias in case of extreme data sparsity. Therefore, we propose a distributed, flexible nonlinear tensor factorization model, which avoids the expensive computations and structural restrictions of the Kronecker-product in the existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected for training. Meanwhile, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed, key-value-free inference algorithm in the MAPREDUCE framework, which can fully exploit the memory cache mechanism in fast MAPREDUCE systems such as SPARK. Experiments demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {928–936},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157199,
author = {Kawahara, Yoshinobu},
title = {Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A spectral analysis of the Koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper, we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore, we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {919–927},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157198,
author = {Luo, Haipeng and Agarwal, Alekh and Cesa-Bianchi, Nicol\`{o} and Langford, John},
title = {Efficient Second Order Online Learning by Sketching},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {910–918},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157197,
author = {Salimans, Tim and Kingma, Diederik P.},
title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {901–909},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157196,
author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
title = {SoundNet: Learning Sound Representations from Unlabeled Video},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {892–900},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157195,
author = {Lam, Remi R. and Willcox, Karen E. and Wolpert, David H.},
title = {Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of optimizing an expensive objective function when a finite budget of total evaluations is prescribed. In that context, the optimal solution strategy for Bayesian optimization can be formulated as a dynamic programming instance. This results in a complex problem with uncountable, dimension-increasing state space and an uncountable control space. We show how to approximate the solution of this dynamic programming problem using rollout, and propose rollout heuristics specifically designed for the Bayesian optimization setting. We present numerical experiments showing that the resulting algorithm for optimization with a finite budget outperforms several popular Bayesian optimization algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {883–891},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157194,
author = {Garber, Dan},
title = {Faster Projection-Free Convex Optimization over the Spectrahedron},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Minimizing a convex function over the spectrahedron, i.e., the set of all d x d positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions. An alternative is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a β-smooth function after t iterations scales like β/t. This rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvector computation is required. For minimizing an α-strongly convex and β-smooth function, the expected error of the method after t iterations is: O(min{β/t, (β√rank(X*)/α1/4t)4/3, (β/√λmin(X*)t)2}), where rank(X*), λmin(X*) are the rank of the optimal solution and smallest nonzero eigenvalue, respectively. Beyond the significant improvement in convergence rate, it also follows that when the optimum is low-rank, our method provides better accuracy-rank tradeoff than the standard CG method. To the best of our knowledge, this is the first result that attains provably faster convergence rates for a CG variant for optimization over the spectrahedron. We also present encouraging preliminary empirical results.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {874–882},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157193,
author = {Wang, Shenlong and Fidler, Sanja and Urtasun, Raquel},
title = {Proximal Deep Structured Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related. In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the dependencies between continuous output variables. We show that inference in our model using proximal methods can be efficiently solved as a feedfoward pass of a special type of deep recurrent neural network. We demonstrate the effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {865–873},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157192,
author = {Kontorovich, Aryeh and Sabato, Sivan and Urner, Ruth},
title = {Active Nearest-Neighbor Learning in Metric Spaces},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {856–864},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157191,
author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S.},
title = {Temporal Regularized Matrix Factorization for High-Dimensional Time Series Prediction},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting. For example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000. In addition, the data is generally noisy and full of missing values. Thus, modern applications require methods that are highly scalable, and can deal with noisy data in terms of corruptions or missing values. However, classical time series methods usually fall short of handling these issues. In this paper, we present a temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal learning and forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values. Our proposed TRMF is highly general, and subsumes many existing approaches for time series analysis. We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular, TRMF is two orders of magnitude faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {847–855},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157190,
author = {Bluche, Th\'{e}odore},
title = {Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient Multi-Dimensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can select one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM databases yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {838–846},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157189,
author = {Weston, Jason},
title = {Dialog-Based Language Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of [23] and large-scale question answering from [3]. We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {829–837},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157188,
author = {He, Di and Xia, Yingce and Qin, Tao and Wang, Liwei and Yu, Nenghai and Liu, Tie-Yan and Ma, Wei-Ying},
title = {Dual Learning for Machine Translation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English ↔ French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {820–828},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157187,
author = {Wang, Jinzhuo and Wang, Wenmin and Chen, Xiongtao and Wang, Ronggang and Gao, Wen},
title = {Deep Alternative Neural Network: Exploring Contexts as Early as Possible for Action Recognition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Contexts are crucial for action recognition in video. Current methods often mine contexts after extracting hierarchical local features and focus on their high-order encodings. This paper instead explores contexts as early as possible and leverages their evolutions for action recognition. In particular, we introduce a novel architecture called deep alternative neural network (DANN) stacking alternative layers. Each alternative layer consists of a volumetric convolutional layer followed by a recurrent layer. The former acts as local feature learner while the latter is used to collect contexts. Compared with feed-forward neural networks, DANN learns contexts of local features from the very beginning. This setting helps to preserve hierarchical context evolutions which we show are essential to recognize similar actions. Besides, we present an adaptive method to determine the temporal size for network input based on optical flow energy, and develop a volumetric pyramid pooling layer to deal with input clips of arbitrary sizes. We demonstrate the advantages of DANN on two benchmarks HMDB51 and UCF101 and report competitive or superior results to the state-of-the-art.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {811–819},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157186,
author = {Wang, Xiangyu and Dunson, David and Leng, Chenlei},
title = {DECOrrelated Feature Space Partitioning for Distributed Sparse Regression},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fitting statistical models is computationally challenging when the sample size or the dimension of the dataset is huge. An attractive approach for down-scaling the problem size is to first partition the dataset into subsets and then fit using distributed algorithms. The dataset can be partitioned either horizontally (in the sample space) or vertically (in the feature space). While the majority of the literature focuses on sample space partitioning, feature space partitioning is more effective when p ≫ n. Existing methods for partitioning features, however, are either vulnerable to high correlations or inefficient in reducing the model dimension. In this paper, we solve these problems through a new embarrassingly parallel framework named DECO for distributed variable selection and parameter estimation. In DECO, variables are first partitioned and allocated to m distributed workers. The decorrelated subset data within each worker are then fitted via any algorithm designed for high-dimensional problems. We show that by incorporating the decorrelation step, DECO can achieve consistent variable selection and parameter estimation on each subset with (almost) no assumptions. In addition, the convergence rate is nearly minimax optimal for both sparse and weakly sparse models and does NOT depend on the partition number m. Extensive numerical experiments are provided to illustrate the performance of the new framework.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {802–810},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157185,
author = {Song, Zhao and Woodruff, David P. and Zhang, Huan},
title = {Sublinear Time Orthogonal Tensor Decomposition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A recent work (Wang et. al., NIPS 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees. Their algorithm is based on computing sketches of the input tensor, which requires reading the entire input. We show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor. Instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling. To achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases. For symmetric tensors T = ∑ki=1 λiui⊗p with λi &gt; 0 for all i, we estimate such norms in sublinear time whenever p is even. For the important case of p = 3 and small values of k, we can also estimate such norms. For asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below || T ||F then sublinear time is again possible. One of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {793–801},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157184,
author = {Garivier, Aur\'{e}lien and Kaufmann, Emilie and Lattimore, Tor},
title = {On Explore-Then-Commit Strategies},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {784–792},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157183,
author = {Jamieson, Kevin and Haas, Daniel and Recht, Ben},
title = {The Power of Adaptivity in Identifying Statistical Alternatives},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a "heavy" coin from an infinite bag containing both "heavy" coins with mean θ1 ∈ (0, 1), and "light" coins with mean θ0 ∈ (0, θ1), where heavy coins are drawn from the bag with proportion α ∈ (0,1/2). When α, θ0, θ1 are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters θ0, θ1, α, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples. In characterizing this gap between adaptive and nonadaptive strategies, we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {775–783},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157182,
author = {Wang, Weiran and Wang, Jialei and Garber, Dan and Srebro, Nathan},
title = {Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA, and the shift-and-invert preconditioning method for PCA, we propose two globally convergent meta-algorithms for CCA, both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {766–774},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157181,
author = {Carreira-Perpi\~{n}\'{a}n, Miguel \'{A}. and Raziperchikolaei, Ramin},
title = {An Ensemble Diversity Approach to Supervised Binary Hashing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {757–765},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157180,
author = {Cutkosky, Ashok and Boahen, Kwabena},
title = {Online Convex Optimization with Unconstrained Domains and Losses},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an online convex optimization algorithm (RESCALEDEXP) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RES CALEDEXP matches this lower bound asymptotically in the number of iterations. RESCALEDEXP is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {748–756},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157179,
author = {Khetan, Ashish and Oh, Sewoong},
title = {Computational and Statistical Tradeoffs in Learning to Rank},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For massive and heterogeneous modern data sets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {739–747},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157178,
author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
title = {Single-Image Depth Perception in the Wild},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {730–738},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157177,
author = {Cheng, Dehua and Peng, Richard and Perros, Ioakeim and Liu, Yan},
title = {SPALS: Fast Alternating Least Squares via Implicit Leverage Scores Sampling},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Tensor CANDECOMP/PARAFAC (CP) decomposition is a powerful but computationally challenging tool in modern data analytics. In this paper, we show ways of sampling intermediate steps of alternating minimization algorithms for computing low rank tensor CP decompositions, leading to the sparse alternating least squares (SPALS) method. Specifically, we sample the Khatri-Rao product, which arises as an intermediate object during the iterations of alternating least squares. This product captures the interactions between different tensor modes, and form the main computational bottleneck for solving many tensor related tasks. By exploiting the spectral structures of the matrix Khatri-Rao product, we provide efficient access to its statistical leverage scores. When applied to the tensor CP decomposition, our method leads to the first algorithm that runs in sublinear time per-iteration and approximates the output of deterministic alternating least squares algorithms. Empirical evaluations of this approach show significant speedups over existing randomized and deterministic routines for performing CP decomposition. On a tensor of the size 2.4m x 6.6m x 92k with over 2 billion nonzeros formed by Amazon product reviews, our routine converges in two minutes to the same error as deterministic ALS.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {721–729},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157176,
author = {Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
title = {Regularized Nonlinear Acceleration},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {712–720},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157175,
author = {Flamary, R\'{e}mi and F\'{e}votte, C\'{e}dric and Courty, Nicolas and Emiya, Valentin},
title = {Optimal Spectral Transportation with Application to Music Transcription},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timbre can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {703–711},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157174,
author = {Papa, Guillaume and Cl\'{e}men\c{c}on, St\'{e}phan and Bellet, Aur\'{e}lien},
title = {On Graph Reconstruction via Empirical Risk Minimization: Fast Learning Rates and Scalability},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of predicting connections between a set of data points finds many applications, in systems biology and social network analysis among others. This paper focuses on the graph reconstruction problem, where the prediction rule is obtained by minimizing the average error over all n(n - 1)/2 possible pairs of the n nodes of a training graph. Our first contribution is to derive learning rates of order Oℙ(log n/n) for this problem, significantly improving upon the slow rates of order Oℙ(1/√n) established in the seminal work of Biau and Bleakley (2006). Strikingly, these fast rates are universal, in contrast to similar results known for other statistical learning problems (e.g., classification, density level set estimation, ranking, clustering) which require strong assumptions on the distribution of the data. Motivated by applications to large graphs, our second contribution deals with the computational complexity of graph reconstruction. Specifically, we investigate to which extent the learning rates can be preserved when replacing the empirical reconstruction risk by a computationally cheaper Monte-Carlo version, obtained by sampling with replacement B ≪ n2 pairs of nodes. Finally, we illustrate our theoretical results by numerical experiments on synthetic and real graphs.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {694–702},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157173,
author = {Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
title = {Barzilai-Borwein Step Size for Stochastic Gradient Descent},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization methods, the common practice in SGD is either to use a diminishing step size, or to tune a step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {685–693},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157172,
author = {Defazio, Aaron},
title = {A Simple Practical Accelerated Method for Finite Sums},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {676–684},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157171,
author = {De Brabandere, Bert and Jia, Xu and Tuytelaars, Tinne and Van Gool, Luc},
title = {Dynamic Filter Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture.We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {667–675},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157170,
author = {Dosovitskiy, Alexey and Brox, Thomas},
title = {Generating Images with Perceptual Similarity Metrics Based on Deep Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from compressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss: (1) networks that invert the AlexNet convolutional network; (2) a modified version of a variational autoencoder that generates realistic high-resolution random images.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {658–666},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157169,
author = {Wu, Huasen and Liu, Xin},
title = {Double Thompson Sampling for Dueling Bandits},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As its name suggests, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison according to two sets of samples independently drawn from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. For general Copeland dueling bandits, we show that D-TS achieves O(K2 log T) regret. Moreover, using a back substitution argument, we refine the regret to O(K log T + K2 log log T) in Condorcet dueling bandits and most practical Copeland dueling bandits. In addition, we propose an enhancement of D-TS, referred to as D-TS+, to reduce the regret in practice by carefully breaking ties. Experiments based on both synthetic and real-world data demonstrate that D-TS and D-TS+ significantly improve the overall performance, in terms of regret and robustness.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {649–657},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157168,
author = {Raziperchikolaei, Ramin and Carreira-Perpi\~{n}\'{a}n, Miguel \'{A}.},
title = {Optimizing Affinity-Based Binary Hashing Using Auxiliary Coordinates},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {640–648},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157167,
author = {He, Kun and Wang, Yan and Hopcroft, John},
title = {A Powerful Generative Model Using Random Weights for the Deep Image Representation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization. It may possibly lead to a way to compare network architectures without training.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {631–639},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157166,
author = {Ritchie, Daniel and Thomas, Anna and Hanrahan, Pat and Goodman, Noah D.},
title = {Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs Using Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {622–630},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157165,
author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
title = {Generating Videos with Scene Dynamics},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {613–621},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157164,
author = {Wei, Dennis},
title = {A Constant-Factor Bi-Criteria Approximation Guarantee for <i>k</i>-Means++},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the k-means++ algorithm for clustering as well as the class of Dℓ sampling algorithms to which k-means++ belongs. It is shown that for any constant factor β &gt; 1, selecting βk cluster centers by Dℓ sampling yields a constant-factor approximation to the optimal clustering with k centers, in expectation and without conditions on the dataset. This result extends the previously known O(log k) guarantee for the case β = 1 to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {604–612},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157163,
author = {Belilovsky, Eugene and Varoquaux, Gael and Blaschko, Matthew},
title = {Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {595–603},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157162,
author = {Kawaguchi, Kenji},
title = {Deep Learning without Poor Local Minima},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {586–594},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157161,
author = {Orabona, Francesco and P\'{a}l, D\'{a}vid},
title = {Coin Betting and Parameter-Free Online Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {577–585},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157160,
author = {Wang, Gang and Giannakis, Georgios B.},
title = {Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper puts forth a novel algorithm, termed truncated generalized gradient flow (TGGF), to solve for x ∈ ℝn/ℂn a system of m quadratic equations yi = |<ai, x="">|2, i = 1,2,..., m, which even for {ai ∈ ℝn/ℂn}mi=1 random is known to be NP-hard in general. We prove that as soon as the number of equations m is on the order of the number of unknowns n, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data {(ai; yi)}mi=1. Specifically, TGGF proceeds in two stages: s1) A novel orthogonality-promoting initialization that is obtained with simple power iterations; and, s2) a refinement of the initial estimate by successive updates of scalable truncated generalized gradient iterations. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. Empirical results demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and, ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {568–576},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}</ai,>

@inproceedings{10.5555/3157096.3157159,
author = {Fathony, Rizal and Liu, Anqi and Asif, Kaiser and Ziebart, Brian D.},
title = {Adversarial Multiclass Classification: A Risk Minimization Perspective},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses. In contrast with empirical risk minimization (ERM) methods, which use convex surrogate losses to approximate the desired non-convex target loss function, adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game. Despite this difference in formulation, we recast adversarial classification under zero-one loss as an ERM method with a novel prescribed loss function. We demonstrate a number of theoretical and practical advantages over the very closely related hinge loss ERM methods. This establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification, simultaneously guaranteeing Fisher consistency and universal consistency, while also providing dual parameter sparsity and high accuracy predictions in practice.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {559–567},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157158,
author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
title = {Residual Networks Behave like Ensembles of Relatively Shallow Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {550–558},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157157,
author = {Zhang, Pan},
title = {Robust Spectral Detection of Global Structures in the Data by Learning a Regularization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {541–549},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157156,
author = {Cheng, Ting-Yu and Lin, Kuan-Hua and Gong, Xinyang and Liu, Kang-Jun and Wu, Shan-Hung},
title = {Learning User Perceived Clusters with Feature-Level Supervision},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains. However, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters. In this paper, we propose learning from the feature-level supervision. We show that this kind of supervision can be easily obtained in the form of perception vectors in many applications. Then we present novel algorithms, called Perception Embedded (PE) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user. Extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of PE empirically.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {532–540},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157155,
author = {Bertinetto, Luca and Henriques, Jo\~{a}o F. and Valmadre, Jack and Torr, Philip H. S. and Vedaldi, Andrea},
title = {Learning Feed-Forward One-Shot Learners},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {523–531},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157154,
author = {Fujii, Kaito and Kashima, Hisashi},
title = {Budgeted Stream-Based Active Learning via Adaptive Submodular Maximization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {514–522},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157153,
author = {Dinh, Vu and Ho, Lam Si Tung and Nguyen, Duy and Nguyen, Binh T.},
title = {Fast Learning Rates with Heavy-Tailed Losses},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function supundefined∈undefined|ℓ o undefined|, where ℓ is the loss function and undefined is the hypothesis class, exists and is Lr-integrable, and (ii) ℓ satisfies the multi-scale Bernstein's condition on undefined. Under these assumptions, we prove that learning rate faster than O(n-1/2) can be obtained and, depending on r and the multi-scale Bernstein's powers, can be arbitrarily close to O(n-1). We then verify these assumptions and derive fast learning rates for the problem of vector quantization by k-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {505–513},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157152,
author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
title = {Operator Variational Inference},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling—allowing inference to scale to massive data—as well as objectives that admit variational programs—a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {496–504},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157151,
author = {Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
title = {Variational Information Maximization for Feature Selection},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {487–495},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157150,
author = {Rudolph, Maja and Ruiz, Francisco J. R. and Mandt, Stephan and Blei, David M.},
title = {Exponential Family Embeddings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications—neural activity of zebrafish, users' shopping behavior, and movie ratings—we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {478–486},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157149,
author = {Liu, Ming-Yu and Tuzel, Oncel},
title = {Coupled Generative Adversarial Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {469–477},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157148,
author = {Ruiz, Francisco J. R. and Titsias, Michalis K. and Blei, David M.},
title = {The Generalized Reparameterization Gradient},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {460–468},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157147,
author = {Ying, Yiming and Wen, Longyin and Lyu, Siwei},
title = {Stochastic Online AUC Maximization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Area under ROC (AUC) is a metric which is widely used for measuring the classification performance for imbalanced data. It is of theoretical and practical interest to develop online learning algorithms that maximizes AUC for large-scale data. A specific challenge in developing online AUC maximization algorithm is that the learning objective function is usually defined over a pair of training examples of opposite classes, and existing methods achieves on-line processing with higher space and time complexity. In this work, we propose a new stochastic online algorithm for AUC maximization. In particular, we show that AUC optimization can be equivalently formulated as a convex-concave saddle point problem. From this saddle representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datum. We establish theoretical convergence of SOLAM with high probability and demonstrate its effectiveness on standard benchmark datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {451–459},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157146,
author = {Mao, Junhua and Xu, Jiajing and Jing, Yushi and Yuille, Alan},
title = {Training and Evaluating Multimodal Word Embeddings with Large-Scale Web Annotated Images},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest [2]. This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {442–450},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157145,
author = {Zhao, Han and Poupart, Pascal and Gordon, Geoff},
title = {A Unified Approach for Learning the Parameters of Sum-Product Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {433–441},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157144,
author = {Barbier, Jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov\'{a}, Lenka},
title = {Mutual Information for Symmetric Rank-One Matrix Estimation: A Proof of the Replica Formula},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {424–432},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157143,
author = {Wang, Hao and Shi, Xingjian and Yeung, Dit-Yan},
title = {Collaborative Recurrent Autoencoder: Recommend While Learning to Fill in the Blanks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {415–423},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157142,
author = {Zhu, Rong},
title = {Gradient-Based Sampling: An Adaptive Importance Sampling for Least-Squares},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In modern data analysis, random sampling is an efficient and widely-used strategy to overcome the computational difficulties brought by large sample size. In previous studies, researchers conducted random sampling which is according to the input data but independent on the response variable, however the response variable may also be informative for sampling. In this paper we propose an adaptive sampling called the gradient-based sampling which is dependent on both the input data and the output for fast solving of least-square (LS) problems. We draw the data points by random sampling from the full data according to their gradient values. This sampling is computationally saving, since the running time of computing the sampling probabilities is reduced to O(nd) where n is the full sample size and d is the dimension of the input. Theoretically, we establish an error bound analysis of the general importance sampling with respect to LS solution from full data. The result establishes an improved performance of the use of our gradient-based sampling. Synthetic and real data sets are used to empirically argue that the gradient-based sampling has an obvious advantage over existing sampling methods from two aspects of statistical efficiency and computational saving.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {406–414},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157141,
author = {De, Abir and Valera, Isabel and Ganguly, Niloy and Bhattacharya, Sourangshu and Gomez-Rodriguez, Manuel},
title = {Learning and Forecasting Opinion Dynamics in Social Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Social media and social networking sites have become a global pinboard for exposition and discussion of news, topics, and ideas, where social media users often update their opinions about a particular topic by learning from the opinions shared by their friends. In this context, can we learn a data-driven model of opinion dynamics that is able to accurately forecast users' opinions? In this paper, we introduce SLANT, a probabilistic modeling framework of opinion dynamics, which represents users' opinions over time by means of marked jump diffusion stochastic differential equations, and allows for efficient model simulation and parameter estimation from historical fine grained event data. We then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and identify conditions under which opinions converge to a steady state. Experiments on data gathered from Twitter show that our model provides a good fit to the data and our formulas achieve more accurate forecasting than alternatives.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {397–405},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157140,
author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
title = {GAP Safe Screening Rules for Sparse-Group Lasso},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For statistical learning in high dimension, sparse regularizations have proven useful to boost both computational and statistical efficiency. In some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level. We propose the first (provably) safe screening rules for Sparse-Group Lasso, i.e., rules that allow to discard early in the solver features/groups that are inactive at optimal solution. Thanks to efficient dual gap computations relying on the geometric properties of e-norm, safe screening rules for Sparse-Group Lasso lead to significant gains in term of computing time for our coordinate descent implementation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {388–396},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157139,
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
title = {R-FCN: Object Detection via Region-Based Fully Convolutional Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [7, 19] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [10], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {379–387},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157138,
author = {Krause, Oswin and Arbon\`{e}s, Didac R. and Igel, Christian},
title = {CMA-ES with Optimal Covariance Update and Storage Complexity},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The covariance matrix adaptation evolution strategy (CMA-ES) is arguably one of the most powerful real-valued derivative-free optimization algorithms, finding many applications in machine learning. The CMA-ES is a Monte Carlo method, sampling from a sequence of multi-variate Gaussian distributions. Given the function values at the sampled points, updating and storing the covariance matrix dominates the time and space complexity in each iteration of the algorithm. We propose a numerically stable quadratic-time covariance matrix update scheme with minimal memory requirements based on maintaining triangular Cholesky factors. This requires a modification of the cumulative step-size adaption (CSA) mechanism in the CMA-ES, in which we replace the inverse of the square root of the covariance matrix by the inverse of the triangular Cholesky factor. Because the triangular Cholesky factor changes smoothly with the matrix square root, this modification does not change the behavior of the CMA-ES in terms of required objective function evaluations as verified empirically. Thus, the described algorithm can and should replace the standard CMA-ES if updating and storing the covariance matrix matters.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {370–378},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157137,
author = {Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
title = {Multimodal Residual Learning for Visual QA},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {361–369},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157136,
author = {Bouchacourt, Diane and Kumar, M. Pawan and Nowozin, Sebastian},
title = {DISCO Nets: Dissimilarity Coefficient Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {352–360},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157135,
author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
title = {Domain Separation Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {343–351},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157134,
author = {Kirillov, Alexander and Shekhovtsov, Alexander and Rother, Carsten and Savchynskyy, Bogdan},
title = {Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of jointly inferring the M-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures. It was noted that the labelings are, so-called, nested. This nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter γ give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings. As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular, the joint M-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case - max-flow) solver for M different values of γ in parallel, for certain diversity measures. Importantly, the values for γ can be computed in a closed form in advance, prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint M-best diverse problem, which outperform competitors in terms of runtime and quality of results. In particular, as we show in the paper, the new methods compute the exact M-best diverse labelings faster than a popular method of Batra et al., which in some sense only obtains approximate solutions.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {334–342},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157133,
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
title = {Fairness in Learning: Classic and Contextual Bandits},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on "chained" confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {325–333},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157132,
author = {Chu, Xiao and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
title = {CRF-CNN: Modeling Structured Information in Human Pose Estimation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {316–324},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157131,
author = {Li, Yangyan and Pirk, S\"{o}ren and Su, Hao and Qi, Charles R. and Guibas, Leonidas J.},
title = {FPNN: Field Probing Neural Networks for 3D Data},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points — sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space "intelligently", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {307–315},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157130,
author = {Magdon-Ismail, Malik and Boutsidis, Christos},
title = {Optimal Sparse Linear Encoders and Sparse PCA},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Principal components analysis (PCA) is the optimal linear encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that can promote better generalization. (i) Given a level of sparsity, what is the best approximation to PCA? (ii) Are there efficient algorithms which can achieve this optimal combinatorial tradeoff? We answer both questions by providing the first polynomial-time algorithms to construct optimal sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {298–306},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157129,
author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
title = {Hierarchical Question-Image Co-Attention for Visual Question Answering},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {289–297},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157128,
author = {Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A. and Wood, Frank},
title = {Bayesian Optimization for Probabilistic Programs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {280–288},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157127,
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
title = {<i>F</i>-GAN: Training Generative Neural Samplers Using Variational Divergence Minimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {271–279},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157126,
author = {Djolonga, Josip and Jegelka, Stefanie and Tschiatschek, Sebastian and Krause, Andreas},
title = {Cooperative Graphical Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a rich family of distributions that capture variable interactions significantly more expressive than those representable with low-treewidth or pairwise graphical models, or log-supermodular models. We call these cooperative graphical models. Yet, this family retains structure, which we carefully exploit for efficient inference techniques. Our algorithms combine the polyhedral structure of submodular functions in new ways with variational inference methods to obtain both lower and upper bounds on the partition function. While our fully convex upper bound is minimized as an SDP or via tree-reweighted belief propagation, our lower bound is tightened via belief propagation or mean-field algorithms. The resulting algorithms are easy to implement and, as our experiments show, effectively obtain good bounds and marginals for synthetic and real-world examples.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {262–270},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157125,
author = {Wang, Yunhe and Xu, Chang and You, Shan and Tao, Dacheng and Xu, Chao},
title = {CNNpack: Packing Convolutional Neural Networks in the Frequency Domain},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {253–261},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157124,
author = {Wang, Yu-Xiong and Hebert, Martial},
title = {Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {244–252},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157123,
author = {Bilen, Hakan and Vedaldi, Andrea},
title = {Integrated Perception with Recurrent Multi-Task Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {235–243},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157122,
author = {Davis, Damek and Udell, Madeleine and Edmunds, Brent},
title = {The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the Stochastic Asynchronous Proximal Alternating Linearized Minimization (SAPALM) method, a block coordinate stochastic proximal-gradient method for solving nonconvex, nonsmooth optimization problems. SAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex, nonsmooth problems. We prove that SAPALM matches the best known rates of convergence — among synchronous or asynchronous methods — on this problem class. We provide upper bounds on the number of workers for which we can expect to see a linear speedup, which match the best bounds known for less complex problems, and show that in practice SAPALM achieves this linear speedup. We demonstrate state-of-the-art performance on several matrix factorization problems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {226–234},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157121,
author = {Reed, Scott and Akata, Zeynep and Mohan, Santosh and Tenka, Samuel and Schiele, Bernt and Lee, Honglak},
title = {Learning What and Where to Draw},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {217–225},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157120,
author = {Heller, Ruth and Heller, Yair},
title = {Multivariate Tests of Association Based on Univariate Tests},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For testing two vector random variables for independence, we propose testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from another arbitrary center point by a univariate test. We prove that under minimal assumptions, it is enough to have a consistent univariate independence test on the distances, to guarantee that the power to detect dependence between the random vectors increases to one with sample size. If the univariate test is distribution-free, the multivariate test will also be distribution-free. If we consider multiple center points and aggregate the center-specific univariate tests, the power may be further improved, and the resulting multivariate test may have a distribution-free critical value for specific aggregation methods (if the univariate test is distribution free). We show that certain multivariate tests recently proposed in the literature can be viewed as instances of this general approach. Moreover, we show in experiments that novel tests constructed using our approach can have better power and computational time than competing approaches.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {208–216},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157119,
author = {Anava, Oren and Karnin, Zohar},
title = {Multi-Armed Bandits: Competing with Optimal Sequences},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider sequential decision making problem in the adversarial setting, where regret is measured with respect to the optimal sequence of actions and the feedback adheres the bandit setting. It is well-known that obtaining sublinear regret in this setting is impossible in general, which arises the question of when can we do better than linear regret? Previous works show that when the environment is guaranteed to vary slowly and furthermore we are given prior knowledge regarding its variation (i.e., a limit on the amount of changes suffered by the environment), then this task is feasible. The caveat however is that such prior knowledge is not likely to be available in practice, which causes the obtained regret bounds to be somewhat irrelevant.Our main result is a regret guarantee that scales with the variation parameter of the environment, without requiring any prior knowledge about it whatsoever. By that, we also resolve an open problem posted by Gur, Zeevi and Besbes [8]. An important key component in our result is a statistical test for identifying non-stationarity in a sequence of independent random variables. This test either identifies non-stationarity or upper-bounds the absolute deviation of the corresponding sequence of mean values in terms of its total variation. This test is interesting on its own right and has the potential to be found useful in additional settings.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {199–207},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157118,
author = {Lasserre, Jean-Bernard and Pauwels, Edouard},
title = {Sorting out Typicality with the Inverse Moment Matrix SOS Polynomial},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {190–198},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157117,
author = {Jitkrittum, Wittawat and Szab\'{o}, Zolt\'{a}n and Chwialkowski, Kacper and Gretton, Arthur},
title = {Interpretable Distribution Features with Maximum Testing Power},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. We show that the empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {181–189},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157116,
author = {Wang, Peng and Shen, Xiaohui and Russell, Bryan and Cohen, Scott and Price, Brian and Yuille, Alan},
title = {SURGE: Surface Regularized Geometry Estimation from a Single Image},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a four-stream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30% relative improvement in planar consistency on the NYU v2 dataset [24].},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {172–180},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157115,
author = {Gao, Yuanjun and Archer, Evan and Paninski, Liam and Cunningham, John P.},
title = {Linear Dynamical Neural Population Models through Nonlinear Embeddings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models.We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {163–171},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157114,
author = {Balandat, Maximilian and Krichene, Walid and Tomlin, Claire and Bayen, Alexandre},
title = {Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in Continuous Zero-Sum Games},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a general adversarial online learning problem, in which we are given a decision set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X. At each iteration, we choose an action from X, based on the observed sequence of previous rewards. Our goal is to minimize regret. Using results from infinite dimensional convex analysis, we generalize the method of Dual Averaging to our setting and obtain upper bounds on the worst-case regret that generalize many previous results. Under the assumption of uniformly continuous rewards, we obtain explicit regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space S. Importantly, we make no convexity assumptions on either S or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated two-player zero-sum games on compact metric spaces. In doing so, we first prove that if both players play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that, under mild assumptions, Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {154–162},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157113,
author = {Karnin, Zohar},
title = {Verification Based Solution for Structured MAB Problems},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of finding the best arm in a stochastic Multi-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be easier than discovering the best arm. Our results are focused on the scenario where the failure probability δ must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification. We demonstrate the effectiveness of our framework by applying it, and matching or improving the state-of-the art results in the problems of: Linear bandits, Dueling bandits with the Condorcet assumption, Copeland dueling bandits, Unimodal bandits and Graphical bandits.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {145–153},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157112,
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
title = {Unsupervised Domain Adaptation with Residual Transfer Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {136–144},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157111,
author = {Jie, Zequn and Liang, Xiaodan and Feng, Jiashi and Jin, Xiaojie and Lu, Wen Feng and Yan, Shuicheng},
title = {Tree-Structured Reinforcement Learning for Sequential Object Localization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feedforward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {127–135},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157110,
author = {Wang, Hao and Shi, Xingjian and Yeung, Dit-Yan},
title = {Natural-Parameter Networks: A Class of Probabilistic Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {118–126},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157109,
author = {Han, Shizhong and Meng, Zibo and Khan, Ahmed Shehab and Tong, Yan},
title = {Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {109–117},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157108,
author = {Ortega, Pedro A. and Stocker, Alan A.},
title = {Human Decision-Making under Limited Time},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints—i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {100–108},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157107,
author = {Xue, Tianfan and Wu, Jiajun and Bouman, Katherine L. and Freeman, William T.},
title = {Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-world video frames. We also show that our model can be applied to visual analogy-making, and present an analysis of the learned network representations.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {91–99},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157106,
author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
title = {Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {82–90},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157105,
author = {Elhamifar, E.},
title = {High-Rank Matrix Completion and Clustering under Self-Expressive Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces. We cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points. Since the problem is NP-hard, we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data, subject to rank-one constraints. To solve the problem efficiently, we propose a rank pursuit algorithm and a convex relaxation. The solution of our algorithms recover missing entries and provides a similarity matrix for clustering. Our algorithms can deal with both low-rank and high-rank matrices, does not suffer from initialization, does not need to know dimensions of subspaces and can work with a small number of data points. By extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data, we show that when the data matrix is low-rank, our algorithm performs on par with or better than low-rank matrix completion methods, while for high-rank data matrices, our method significantly outperforms existing algorithms.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {73–81},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157104,
author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
title = {Unsupervised Learning for Physical Interaction through Video Prediction},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {64–72},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157103,
author = {Bachem, Olivier and Lucic, Mario and Hassani, S. Hamed and Krause, Andreas},
title = {Fast and Provably Good Seedings for <i>k</i>-Means},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-Means. However, k-means++ seeding, the state of the art algorithm, does not scale well to massive datasets as it is inherently sequential and requires k full passes through the data. It was recently shown that Markov chain Monte Carlo sampling can be used to efficiently approximate the seeding step of k-means++. However, this result requires assumptions on the data generating distribution. We propose a simple yet fast seeding algorithm that produces provably good clusterings even without assumptions on the data. Our analysis shows that the algorithm allows for a favourable trade-off between solution quality and computational cost, speeding up k-means++ seeding by up to several orders of magnitude. We validate our theoretical results in extensive experiments on a variety of real-world data sets.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {55–63},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157102,
author = {Shamir, Ohad},
title = {Without-Replacement Sampling for Stochastic Gradient Methods},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In contrast, sampling without replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {46–54},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157101,
author = {Nock, Richard},
title = {On Regularizing Rademacher Observation Losses},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It has recently been shown that supervised learning linear classifiers with two of the most popular losses, the logistic and square loss, is equivalent to optimizing an equivalent loss over sufficient statistics about the class: Rademacher observations (rados). It has also been shown that learning over rados brings solutions to two prominent problems for which the state of the art of learning from examples can be comparatively inferior and in fact less convenient: (i) protecting and learning from private examples, (ii) learning from distributed datasets without entity resolution. Bis repetita placent: the two proofs of equivalence are different and rely on specific properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind. This is our first contribution: we show how they can be fit into the same theory for the equivalence between example and rado losses. As a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (i.e. the data) in the equivalent rado loss, in such a way that an efficient algorithm for one regularized rado loss may be as efficient when changing the regularizer. This is our third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, SLOPE, ℓ∞, or elastic net regularizer, using the same master routine for all. Because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples we obtain the first efficient proxy to the minimization of the regularized logistic loss over examples using such a wide spectrum of regularizers. Experiments with a readily available code display that regularization significantly improves rado-based learning and compares favourably with example-based learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {37–45},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157100,
author = {Singh, Saurabh and Hoiem, Derek and Forsyth, David},
title = {Swapout: Learning an Ensemble of Deep Architectures},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout [20], stochastic depth [7] and residual architectures [5, 6] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {28–36},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157099,
author = {Nock, Richard and Menon, Aditya Krishna and Ong, Cheng Soon},
title = {A Scaled Bregman Theorem with Applications},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms through a handful of popular theorems. We present a new theorem which shows that "Bregman distortions" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. This property can be viewed from the standpoints of geometry (a scaled isometry with adaptive metrics) or convex optimization (relating generalized perspective transforms). Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation.Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {19–27},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157098,
author = {Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben},
title = {Deep ADMM-Net for Compressive Sensing MRI},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of under-sampled data in k-space, and accelerating the data acquisition in MRI. To improve the current MRI system in reconstruction accuracy and computational speed, in this paper, we propose a novel deep architecture, dubbed ADMM-Net. ADMM-Net is defined over a data flow graph, which is derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the training data for CS-based reconstruction task. Experiments on MRI image reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction accuracies with fast computational speed.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {10–18},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3157096.3157097,
author = {He, Bryan and Sa, Christopher De and Mitliagkas, Ioannis and R\'{e}, Christopher},
title = {Scan Order in Gibbs Sampling: Models in Which It Matters and Bounds on How Much},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1–9},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@proceedings{10.5555/3157096,
title = {NIPS'16: Proceedings of the 30th International Conference on Neural Information Processing Systems},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Barcelona, Spain}
}

