@inproceedings{10.5555/2997189.2997339,
author = {Levine, Sergey and Popovi\'{c}, Zoran and Koltun, Vladlen},
title = {Feature Construction for Inverse Reinforcement Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1342–1350},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997338,
author = {Leung, Gilbert and Quadrianto, Novi and Smola, Alexander J. and Tsioutsiouliklis, Kostas},
title = {Optimal Web-Scale Tiering as a Flow Problem},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a fast online solver for large scale parametric max-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 84 million web pages on a layered set of caches to serve an incoming query stream optimally.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1333–1341},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997337,
author = {Lempitsky, Victor and Zisserman, Andrew},
title = {Learning To Count Objects in Images},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object).Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization.The proposed framework is very flexible as it can accept any domain-specific visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1324–1332},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997336,
author = {Lefakis, Leonidas and Fleuret, Fran\c{c}ois},
title = {Joint Cascade Optimization Using a Product of Boosted Classifiers},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers, and concentrates the computation on difficult parts of the image which cannot be trivially rejected.We introduce a novel algorithm to construct jointly the classifiers of such a cascade, which interprets the response of a classifier as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set.Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1315–1323},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997335,
author = {Lee, Seunghak and Zhu, Jun and Xing, Eric P.},
title = {Adaptive Multi-Task Lasso: With Application to EQTL Detection},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To understand the relationship between genomic variations among population and complex diseases, it is essential to detect eQTLs which are associated with phenotypic effects. However, detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.In this paper, we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We first present a Bayesian network for a multi-task learning problem that includes priors on SNPs, making it possible to estimate the significance of each covariate adaptively. Then we find the maximum a posteriori (MAP) estimation of regression coefficients and estimate weights of covariates jointly. This optimization procedure is efficient since it can be achieved by using a projected gradient descent and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eQTLs.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1306–1314},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997334,
author = {Lee, Jason and Recht, Benjamin and Salakhutdinov, Ruslan and Srebro, Nathan and Tropp, Joel A.},
title = {Practical Large-Scale Optimization for Max-Norm Regularization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1297–1305},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997333,
author = {Lee, David C. and Gupta, Abhinav and Hebert, Martial and Kanade, Takeo},
title = {Estimating Spatial Layout of Rooms Using Volumetric Reasoning about Objects and Surfaces},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1288–1296},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997332,
author = {Le, Quoc V. and Ngiam, Jiquan and Chen, Zhenghao and Chia, Daniel and Koh, Pang Wei and Ng, Andrew Y.},
title = {Tiled Convolutional Neural Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular "tiled" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1279–1287},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997331,
author = {Le, Hai-Son and Bar-Joseph, Ziv},
title = {Cross Species Expression Analysis Using a Dirichlet Process Mixture Model with Latent Matchings},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent studies compare gene expression data across species to identify core and species specific genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1270–1278},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997330,
author = {Lazar, Aurel A. and Slutskiy, Yevgeniy B.},
title = {Identifying Dendritic Processing},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1261–1269},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997329,
author = {Lashkari, Danial and Sridharan, Ramesh and Golland, Polina},
title = {Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories (clusters of stimuli) and functional units (clusters of voxels). We assume that voxels within a unit respond similarly to all stimuli from the same category, and design a nonparametric hierarchical model to capture inter-subject variability among the units. The model explicitly encodes the relationship between brain activations and fMRI time courses. A variational inference algorithm derived based on the model learns categories, units, and a set of unit-category activation probabilities from data. When applied to data from an fMRI study of object recognition, the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1252–1260},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997328,
author = {Larochelle, Hugo and Hinton, Geoffrey},
title = {Learning to Combine Foveal Glimpses with a Third-Order Boltzmann Machine},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the "glimpse" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1243–1251},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997327,
author = {Lao, Ni and Zhu, Jun and Liu, Liu and Liu, Yandong and Cohen, William W.},
title = {Efficient Relational Learning with Hidden Variable Detection},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1234–1242},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997326,
author = {Langs, Georg and Golland, Polina and Tie, Yanmei and Rigolo, Laura and Golby, Alexandra J.},
title = {Functional Geometry Alignment and Localization of Brain Areas},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1225–1233},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997325,
author = {Lan, Tian and Wang, Yang and Yang, Weilong and Mori, Greg},
title = {Beyond Actions: Discriminative Models for Contextual Group Activities},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1216–1224},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997324,
author = {Su, Chang and Srihari, Sargur},
title = {Evaluation of Rarity of Fingerprints in Forensics},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A method for computing the rarity of latent fingerprints represented by minutiae is given. It allows determining the probability of finding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by finding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence. In the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a specific birthday. The generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints. The probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1207–1215},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997323,
author = {Kundu, Achintya and Tankasali, Vikram and Bhattacharyya, Chiranjib and Ben-Tal, Aharon},
title = {Efficient Algorithms for Learning Kernels from Multiple Similarity Matrices with General Convex Loss Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we consider the problem of learning an n x n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m &gt; 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m &gt; 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in O(m2 log n/ε2) iterations; in each iteration one solves an MKL involving m kernels and m eigen-decomposition of n x n matrices. By suitably defining a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1198–1206},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997322,
author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
title = {Self-Paced Learning for Latent Variable Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1189–1197},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997321,
author = {Kumar, Akshat and Zilberstein, Shlomo},
title = {MAP Estimation for Graphical Models by Likelihood Maximization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Computing a maximum a posteriori (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. Experiments on the real-world protein design dataset show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM also achieves a solution quality within 95% of optimal for most instances.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1180–1188},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997320,
author = {Kulesza, Alex and Taskar, Ben},
title = {Structured Determinantal Point Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1171–1179},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997319,
author = {Konidaris, George and Kuindersma, Scott and Barto, Andrew and Grupen, Roderic},
title = {Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a change-point detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1162–1170},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997318,
author = {Kolter, J. Zico and Batra, Siddarth and Ng, Andrew Y.},
title = {Energy Disaggregation via Discriminative Sparse Coding},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1153–1161},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997317,
author = {Kolmogorov, Vladimir},
title = {Generalized Roof Duality and Bisubmodular Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Consider a convex relaxation $hat f$ of a pseudo-boolean function f. We say that the relaxation is totally half-integral if $hat f$ (x) is a polyhedral function with half-integral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj, xi = 1 - xj, and xi = γ where γ ∈ {0, 1, 1/2} is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions.Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $hat f$ by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisub-modular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1144–1152},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997316,
author = {Kleiner, Ariel and Rahimi, Ali and Jordan, Michael I.},
title = {Random Conic Pursuit for Semidefinite Programming},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel algorithm, Random Conic Pursuit, that solves semidefinite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1135–1143},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997315,
author = {Kingma, Diederik P. and LeCun, Yann},
title = {Regularized Estimation of Image Statistics by Score Matching},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1126–1134},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997314,
author = {Kim, Taehwan and Shakhnarovich, Gregory and Urtasun, Raquel},
title = {Sparse Coding for Learning Interpretable Spatio-Temporal Primitives},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1117–1125},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997313,
author = {Khan, Mohammad Emtiyaz and Bouchard, Guillaume and Marlin, Benjamin M. and Murphy, Kevin P.},
title = {Variational Bounds for Mixed-Data Factor Analysis},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1108–1116},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997312,
author = {Kelly, Ryan C. and Smith, Matthew A. and Kass, Robert E. and Lee, Tai Sing},
title = {Accounting for Network Effects in Neuronal Responses Using L1 Regularized Point Process Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode "Utah" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron's response, in addition to the neuron's receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1099–1107},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997311,
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-Lan and Gregor, Karol and Mathieu, Micha\"{e}l and LeCun, Yann},
title = {Learning Convolutional Feature Hierarchies for Visual Recognition},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross detectors, and oriented grating detectors. We show that using these filters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1090–1098},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997310,
author = {Katahira, Kentaro and Okanoya, Kazuo and Okada, Masato},
title = {Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein's matching law. Loewenstein &amp; Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synap-tic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1081–1089},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997309,
author = {Karlinsky, Leonid and Dinerstein, Michael and Ullman, Shimon},
title = {Using Body-Anchored Priors for Identifying Actions in Single Images},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because instances of different actions may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are anchored to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1072–1080},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997308,
author = {Karampatziakis, Nikos},
title = {Static Analysis of Binary Executables Using Structural SVMs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general, inference in segmentation models, such as semi-CRFs, can be cubic in the length of the sequence. By taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long. Furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally, we present experimental results that demonstrate the advantages of our method against a strong baseline.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1063–1071},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997307,
author = {Kale, Satyen and Reyzin, Lev and Schapire, Robert E.},
title = {Non-Stochastic Bandit Slate Problems},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider bandit problems, motivated by applications in online advertising and news story selection, in which the learner must repeatedly select a slate, that is, a subset of size s from K possible actions, and then receives rewards for just the selected actions. The goal is to minimize the regret with respect to total reward of the best slate computed in hindsight. We consider unordered and ordered versions of the problem, and give efficient algorithms which have regret O(√T), where the constant depends on the specific nature of the problem. We also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round, and give algorithms with O(√T) regret for competing with the best such policy as well. We make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1054–1062},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997306,
author = {Joulin, Armand and Bach, Francis and Ponce, Jean},
title = {Efficient Optimization for Discriminative Latent Class Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efficient algorithm based on convex relaxations and low-rank representations of the data, capable of handling large-scale problems. Experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1045–1053},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997305,
author = {Jones, Peter B. and Saligrama, Venkatesh and Mitter, Sanjoy K.},
title = {Probabilistic Belief Revision with Structural Constraints},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1036–1044},
numpages = {9},
keywords = {information theory, consistency, bayesian methods},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997304,
author = {Jojic, Nebojsa and Perina, Alessandro and Murino, Vittorio},
title = {Structural Epitome: A Way to Summarize One's Visual Experience},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds. The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our first goal is to create a visual summary of the subject's two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g., Photosynth) or appearance-based clustering models (e.g., the epitome), is impractical due to either the large dataset size or the dramatic variations in the lighting conditions. As a remedy to these problems, we introduce a novel image representation, the "structural element (stel) epitome," and an associated efficient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T which, as in previous epitome models, defines a mapping between the image coordinates and the coordinates in the large "all-I-have-seen" epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap which indicates image similarity. However, the image similarity no longer depends on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination changes, that tend to uniformly affect pixels belonging to a single scene or object part.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1027–1035},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997303,
author = {Johnson, Mark and Demuth, Katherine and Frank, Michael and Jones, Bevan K.},
title = {Synergies in Learning Words and Their Referents},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that referto them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1018–1026},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997302,
author = {Johns, Jeff and Painter-Wakefield, Christopher and Parr, Ronald},
title = {Linear Complementarity for Regularized Policy Evaluation and Improvement},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overfitting. We propose formulating the L1 regularized linear fixed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efficient off-the-shelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efficiency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modified policy iteration that canbeusedto approximate a "greedy" homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1009–1017},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997301,
author = {Tang, Jie and Abbeel, Pieter},
title = {On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U(θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U(θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1000–1008},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997300,
author = {Jiang, Albert Xin and Leyton-Brown, Kevin},
title = {Bayesian Action-Graph Games},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Games of incomplete information, or Bayesian games, are an important game-theoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-specific utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve significantly on the state of the art.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {991–999},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997299,
author = {Jia, Yangqing and Salzmann, Mathieu and Darrell, Trevor},
title = {Factorized Latent Spaces with Structured Sparsity},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {982–990},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997298,
author = {Jha, Abhay and Gogate, Vibhav and Meliou, Alexandra and Suciu, Dan},
title = {Lifted Inference Seen from the Other Side: The Tractable Features},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Lifted Inference algorithms for representations that combine first-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efficiency by exploiting repeated structure in the first-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible. Our rules yield new tractable classes that could not be solved efficiently by any of the existing techniques.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {973–981},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997297,
author = {Jalali, Ali and Ravikumar, Pradeep and Sanghavi, Sujay and Ruan, Chao},
title = {A Dirty Model for Multi-Task Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider multi-task learning in the setting of multiple linear regression, and where some relevant features could be shared across the tasks. Recent research has studied the use of ℓ1/ℓq norm block-regularizations with q &gt; 1 for such block-sparse structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the extent to which the features are shared across tasks. Indeed they show [8] that if the extent of overlap is less than a threshold, or even if parameter values in the shared features are highly uneven, then block ℓ1/ℓq regularization could actually perform worse than simple separate elementwise ℓ1 regularization. Since these caveats depend on the unknown true parameters, we might not know when and which method to apply. Even otherwise, we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well.Here, we ask the question: can we leverage parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such dirty data which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). With the explosion of such dirty high-dimensional data in modern settings, it is vital to develop tools - dirty models - to perform biased statistical estimation tailored to such data. Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we estimate a superposition of two sets of parameters and regularize them differently. We show both theoretically and empirically, our method strictly and noticeably outperforms both ℓ1 or ℓ1/ℓq methods, under high-dimensional scaling and over the entire range of possible overlaps (except at boundary cases, where we match the best method).},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {964–972},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997296,
author = {Reddi, Sashank J. and Sarawagi, Sunita and Vishwanathan, Sundar},
title = {MAP Estimation in Binary MRFs via Bipartite Multi-Cuts},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efficiently solve the LP and also provide a lower bound by concurrently solving its dual to within an ε approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {955–963},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997295,
author = {Jain, Prateek and Kulis, Brian and Dhillon, Inderjit},
title = {Inductive Regularized Learning of Kernel Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we consider the problem of semi-supervised kernel function learning. We first propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. To this end, we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the k-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {946–954},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997294,
author = {Jain, Prateek and Meka, Raghu and Dhillon, Inderjit},
title = {Guaranteed Rank Minimization via Singular Value Projection},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {937–945},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997293,
author = {Jain, Prateek and Vijayanarasimhan, Sudheendra and Grauman, Kristen},
title = {Hashing Hyperplane Queries to near Points with Applications to Large-Scale Active Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyper-plane query. Both use hashing to retrieve near points in sub-linear time. Our first method's preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods' tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {928–936},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997292,
author = {Ishiguro, Katsuhiko and Iwata, Tomoharu and Ueda, Naonori and Tenenbaum, Joshua},
title = {Dynamic Infinite Relational Model for Time-Varying Relational Data Analysis},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split &amp; merge, of relation clusters like communities in social networks. Our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters. We extend the infinite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {919–927},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997291,
author = {Isely, Guy and Hillar, Christopher J. and Sommer, Friedrich T.},
title = {Deciphering Subsampled Data: Adaptive Compressive Sampling as a Principle of Brain Communication},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or over-complete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {910–918},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997290,
author = {Husmeier, Dirk and Dondelinger, Frank and L\`{e}bre, Sophie},
title = {Inter-Time Segment Information Sharing for Non-Homogeneous Dynamic Bayesian Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this flexibility leads to the risk of overfitting and inflated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of five genes in yeast.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {901–909},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997289,
author = {Huang, Sheng-Jun and Jin, Rong and Zhou, Zhi-Hua},
title = {Active Learning by Querying Informative and Representative Examples},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {892–900},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997288,
author = {Huang, Ling and Jia, Jinzhu and Yu, Bin and Chun, Byung-Gon and Maniatis, Petros and Naik, Mayur},
title = {Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program's behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {883–891},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997287,
author = {Huang, Jim C. and Jojic, Nebojsa and Meek, Christopher},
title = {Exact Inference and Learning for Cumulative Distribution Functions on Loopy Graphs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (non-loopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields significant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide significantly better fits to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {874–882},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997286,
author = {Hu, Diane J. and Maaten, Laurens van der and Cho, Youngmin and Saul, Lawrence K. and Lerner, Sorin},
title = {Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When software developers modify one or more files in a large code base, they must also identify and update other related files. Many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows. From data of this form, we show how to detect dependent files by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Fire-fox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs improve the performance of related file prediction over current leading methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {865–873},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997285,
author = {Hoffman, Matthew D. and Blei, David M. and Bach, Francis},
title = {Online Learning for Latent Dirichlet Allocation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {856–864},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997284,
author = {Hein, Matthias and B\"{u}hler, Thomas},
title = {An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {847–855},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997283,
author = {Hazan, Tamir and Urtasun, Raquel},
title = {A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efficiently. We first relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efficient message-passing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efficiently graphical models with cycles and very large number of parameters.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {838–846},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997282,
author = {Harmeling, Stefan and Hirsch, Michael and Sch\"{o}lkopf, Bernhard},
title = {Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In an effort to address these limitations, (i) we introduce a taxonomy of camera shakes, (ii) we build on a recently introduced framework for space-variant filtering by Hirsch et al. and a fast algorithm for single image blind deconvolution for space-invariant filters by Cho and Lee to construct a method for blind deconvolution in the case of space-variant blur, and (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time recording the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake, even without using additionally motion sensor information.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {829–837},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997281,
author = {Hannah, Lauren A. and Powell, Warren B. and Blei, David M.},
title = {Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial benefits over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {820–828},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997280,
author = {Han, Yanjun and Tao, Qing and Wang, Jue},
title = {Avoiding False Positive in Multi-Instance Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {811–819},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997279,
author = {Guo, Yuhong},
title = {Active Instance Sampling via Matrix Partition},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classification models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {802–810},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997278,
author = {Grangier, David and Melvin, Iain},
title = {Feature Set Embedding for Incomplete Data},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new learning strategy for classification problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace. In contrast, our method considers instances as sets of (feature, value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classification strategy for sets. Our proposal maps (feature, value) pairs into an embedding space and then non-linearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the final classification objective. This simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {793–801},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997277,
author = {Goodman, Dan F. M. and Brette, Romain},
title = {Learning to Localise Sounds with Spiking Neural Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
keywords = {auditory perception &amp; modeling (primary), neuroscience, computational neural models, supervised learning (secondary)},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997276,
author = {Gomes, Ryan and Krause, Andreas and Perona, Pietro},
title = {Discriminative Clustering by Regularized Information Maximization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classifier. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classifier complexity. The approach can flexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular, we instantiate the framework to un-supervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {775–783},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997275,
author = {Golovin, Daniel and Krause, Andreas and Ray, Debajyoti},
title = {Near-Optimal Bayesian Active Learning with Noisy Observations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non-uniform cost and their noise is correlated. We also propose EFFECX-TIVE, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {766–774},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997274,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Recht, Benjamin and Xu, Jun-Ming and Nowak, Robert},
title = {Transduction with Matrix Completion: Three Birds with One Stone},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We pose transductive classification as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspecified, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {757–765},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997273,
author = {Gogate, Vibhav and Webb, William Austin and Domingos, Pedro},
title = {Learning Efficient Markov Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-specific independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {748–756},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997272,
author = {Glasmachers, Tobias},
title = {Universal Consistency of Multi-Class Support Vector Classification},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Steinwart was the first to prove universal consistency of support vector machine classification. His proof analyzed the 'standard' support vector machine classifier, which is restricted to binary classification problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classification to more than two classes are inconsistent.Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart's techniques to the multi-class case.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {739–747},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997271,
author = {Gibson, Bryan R. and Zhu, Xiaojin and Rogers, Timothy T. and Kalish, Charles W. and Harrison, Joseph},
title = {Humans Learn Using Manifolds, Reluctantly},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {730–738},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997270,
author = {Ghavamzadeh, Mohammad and Lazaric, Alessandro and Maillard, Odalric-Ambrym and Munos, R\'{e}mi},
title = {LSTD with Random Projections},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {721–729},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997269,
author = {Gershman, Samuel J. and Wilson, Robert C.},
title = {The Neural Costs of Optimal Control},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {712–720},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997268,
author = {Gerhard, Felipe and Gerstner, Wulfram},
title = {Rescaling, Thinning or Complementing? On Goodness-of-Fit Procedures for Point Process Models and Generalized Linear Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-fit tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {703–711},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997267,
author = {Gelfand, Andrew E. and Chen, Yutian and Welling, Max and Maaten, Laurens van der},
title = {On Herding and the Perceptron Cycling Theorem},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {694–702},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997266,
author = {Gasthaus, Jan and Teh, Yee Whye},
title = {Improvements to the Sequence Memoizer},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the "mysterious" coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {685–693},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997265,
author = {Garrigues, Pierre J. and Olshausen, Bruno A.},
title = {Group Sparse Coding with a Laplacian Scale Mixture Prior},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefficients. Each coefficient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efficient inference procedures for both the coefficients and the scale parameter. When the scale parameters of a group of coefficients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefficients follows a divisive normalization rule, and that this may be efficiently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {676–684},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997264,
author = {Ganguli, Surya and Sompolinsky, Haim},
title = {Short-Term Memory in Neuronal Networks through Dynamical Compressed Sensing},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of "orthogonal" recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {667–675},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997263,
author = {Ganguli, Deep and Simoncelli, Eero P.},
title = {Implicit Encoding of Prior Probabilities in Optimal Neural Populations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Optimal coding provides a guiding principle for understanding the representation of sensory variables in neural populations. Here we consider the influence of a prior probability distribution over sensory variables on the optimal allocation of neurons and spikes in a population. We model the spikes of each cell as samples from an independent Poisson process with rate governed by an associated tuning curve. For this response model, we approximate the Fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density. We consider a family of objective functions based on the expected value, over the sensory prior, of a functional of the Fisher information. This family includes lower bounds on mutual information and perceptual discriminability as special cases. In all cases, we find a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior. This also implies a power law relationship between the prior and perceptual discriminability. We show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and firing rates), and psychophysically measured discrimination thresholds.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {658–666},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997262,
author = {Gai, Kun and Chen, Guangyun and Zhang, Changshui},
title = {Learning Kernels with Radiuses of Minimum Enclosing Balls},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classifier. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2) of norm constraints on combination coefficients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method significantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {649–657},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997261,
author = {Fung, C. C. Alan and Wong, K. Y. Michael and Wang, He and Wu, Si},
title = {Attractor Dynamics with Synaptic Depression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We find that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we find that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {640–648},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997260,
author = {Froyen, Vicky and Feldman, Jacob and Singh, Manish},
title = {A Bayesian Framework for Figure-Ground Interpretation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Figure/ground assignment, in which the visual image is divided into nearer (figural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model figure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a figure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure "draws" border ownership so that borders are owned by the skeletal hypothesis that best explains them. We also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {631–639},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997259,
author = {Fritz, Mario and Saenko, Kate and Darrell, Trevor},
title = {Size Matters: Metric Visual Search Constraints from Monocular Metadata},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information-object and feature absolute size-can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a "2.1D" local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics. We develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {622–630},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997258,
author = {Frigyik, Bela A. and Gupta, Maya R. and Chen, Yihua},
title = {Shadow Dirichlet for Restricted Probability Modeling},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {613–621},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997257,
author = {Foygel, Rina and Drton, Mathias},
title = {Extended Bayesian Information Criteria for Gaussian Graphical Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {604–612},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997256,
author = {Fisher, Nicholas and Banerjee, Arunava},
title = {A Novel Kernel for Learning a Neuron Model from Spike Train Data},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classification based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {595–603},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997255,
author = {Filippi, Sarah and Capp\'{e}, Olivier and Garivier, Aur\'{e}lien and Szepesv\'{a}ri, Csaba},
title = {Parametric Bandits: The Generalized Linear Case},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive finite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difficulty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to significantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {586–594},
numpages = {9},
keywords = {parametric bandits, multi-armed bandit, generalized linear models, regret minimization, UCB},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997254,
author = {Fern, Alan and Tadepalli, Prasad},
title = {A Computational Decision Theory for Interactive Assistants},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in finite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {577–585},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997253,
author = {Farahmand, Amir massoud and Munos, R\'{e}mi and Szepesv\'{a}ri, Csaba},
title = {Error Propagation for Approximate Policy and Value Iteration},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum - as opposed to what has been suggested by the previous results. Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {568–576},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997252,
author = {Elidan, Gal},
title = {Copula Bayesian Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present the Copula Bayesian Network model for representing multivariate continuous distributions, while taking advantage of the relative ease of estimating univariate distributions. Using a novel copula-based reparameterization of a conditional density, joined with a graph that encodes independencies, our model offers great flexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {559–567},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997251,
author = {Duchi, John C. and Agarwal, Alekh and Wainwright, Martin J.},
title = {Distributed Dual Averaging in Networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {550–558},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997250,
author = {Druckmann, Shaul and Chklovskii, Dmitri B.},
title = {Over-Complete Representations on Recurrent Neural Networks Can Support Persistent Percepts},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {541–549},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997249,
author = {Doshi-Velez, Finale and Wingate, David and Roy, Nicholas and Tenenbaum, Joshua},
title = {Nonparametric Bayesian Policy Priors for Reinforcement Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {532–540},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997248,
author = {Domke, Justin},
title = {Implicit Differentiation by Perturbation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, defined on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to fit graphical models with hidden variables, high treewidth and/or model misspecification.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {523–531},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997247,
author = {Ding, Nan and Vishwanathan, S. V. N.},
title = {<i>T</i>-Logistic Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efficient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {514–522},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997246,
author = {Dick, Uwe and Haider, Peter and Vanck, Thomas and Br\"{u}ckner, Michael and Scheffer, Tobias},
title = {Throttling Poisson Processes},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {505–513},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997245,
author = {Dhesi, Aman and Kar, Purushottam},
title = {Random Projection Trees Revisited},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Random Projection Tree (RPTREE) structures proposed in [1] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPTREE-MAX and the RPTREE-MEAN data structures. Our result for RPTREE-MAX gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s ≥ 2. We also prove a packing lemma for this data structure. Our final result shows that low-dimensional manifolds have bounded Local Covariance Dimension. As a consequence we show that RPTREE-MEAN adapts to manifold dimension as well.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {496–504},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997244,
author = {Vito, Ernesto De and Rosasco, Lorenzo and Toigo, Alessandro},
title = {Spectral Regularization for Support Estimation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call "completely regular ". Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {487–495},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997243,
author = {Daum\'{e}, Hal and Kumar, Abhishek and Saha, Avishek},
title = {Co-Regularization Based Semi-Supervised Domain Adaptation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in EASYADAPT (EA) [1]) and harnesses unlabeled data in target domain to further assist the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to EA as well as few other representative baseline approaches.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {478–486},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997242,
author = {Dahl, George E. and Ranzato, Marc'Aurelio and Mohamed, Abdel-rahman and Hinton, Geoffrey},
title = {Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {469–477},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997241,
author = {Cuingnet, R\'{e}mi and Chupin, Marie and Benali, Habib and Colliot, Olivier},
title = {Spatial and Anatomical Regularization of SVM for Brain Image Analysis},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover, when the kernel is linear, SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However, the features' spatial distribution is not taken into account. As a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difficult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classification of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer's disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {460–468},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997240,
author = {Crammer, Koby and Lee, Daniel D.},
title = {Learning via Gaussian Herding},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new family of online learning algorithms based upon constraining the velocity flow over a distribution of weight vectors. In particular, we show how to effectively herd a Gaussian weight vector distribution by trading off velocity constraints with a loss function. By uniformly bounding this loss function, we demonstrate how to solve the resulting optimization analytically. We compare the resulting algorithms on a variety of real world datasets, and demonstrate how these algorithms achieve state-of-the-art robust performance, especially with high label noise in the training data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {451–459},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997239,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
title = {Learning Bounds for Importance Weighting},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the R\'{e}nyi divergence of the traning and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally, we analyze the properties of normalized importance weights which are also commonly used.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {442–450},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997238,
author = {Corbett, Elaine A. and Perreault, Eric J. and K\"{o}rding, Konrad P.},
title = {Mixture of Time-Warped Trajectory Models for Movement Decoding},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time - a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {433–441},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997237,
author = {Cohen, Shay B. and Smith, Noah A.},
title = {Empirical Risk Minimization with Approximations of Probabilistic Grammars},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the un-supervised setting.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {424–432},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997236,
author = {Claassen, Tom and Heskes, Tom},
title = {Causal Discovery in Multiple Models from Different Experiments},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {415–423},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997235,
author = {Christmann, Andreas and Steinwart, Ingo},
title = {Universal Kernels on Non-Standard Input Spaces},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of ℝd. Examples include SVMs for the analysis of histograms or colored images, SVMs for text classification and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H ⊂ Lp(PX) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X ⊄ ℝd. We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of ℝd. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {406–414},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997234,
author = {Chiuso, Alessandro and Pillonetto, Gianluigi},
title = {Learning Sparse Dynamic Linear Systems Using Stable Spline Kernels and Exponential Hyperpriors},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced "Stable Spline kernel". Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {397–405},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997233,
author = {Chiappa, Silvia and Peters, Jan},
title = {Movement Extraction by Detecting Dynamics Switches and Repetitions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many time-series such as human movement data consist of a sequence of basic actions, e.g., forehands and backhands in tennis. Automatically extracting and characterizing such actions is an important problem for a variety of different applications. In this paper, we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions. Each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement, with possible time re-scaling. We analyze three different approximation methods for dealing with model intractability, and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {388–396},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997232,
author = {Chevallier, Sylvain and Paugam-Moisy, H\'{e}l\`{e}ne and Sebag, Mich\`{e}le},
title = {SpikeAnts, a Spiking Neuron Network Modelling the Emergence of Organization in a Complex System},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Specifically, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {379–387},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997231,
author = {Chen, Wei and Liu, Tie-Yan and Ma, Zhiming},
title = {Two-Layer Generalization Analysis for Ranking Using Rademacher Average},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR, data are hierarchically organized, i.e., consisting of queries and documents. Previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of the learned ranking model. In this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent after represented by features extracted from query-document matching. To tackle the challenge, we decompose the expected risk according to the two layers, and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performances of ranking algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {370–378},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997230,
author = {Chen, Ning and Zhu, Jun and Xing, Eric P.},
title = {Predictive Subspace Learning for Multi-View Data: A Large Margin Approach},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning from multi-view data is important in many applications, such as image classification and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efficient inference and parameter estimation methods for the latent sub-space model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification, annotation and retrieval.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {361–369},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997229,
author = {Chechetka, Anton and Guestrin, Carlos},
title = {Evidence-Specific Structures for Rich Tractable CRFs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {352–360},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997228,
author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
title = {Rates of Convergence for the Cluster Tree},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For a density f on ℝd, a high-density cluster is any connected component of {x : f(x) ≥ λ}, for some λ &lt; 0. The set of all high-density clusters form a hierarchy called the cluster tree of f. We present a procedure for estimating the cluster tree given samples from f. We give finite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {343–351},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997227,
author = {Chambers, America L. and Smyth, Padhraic and Steyvers, Mark},
title = {Learning Concept Graphs from Text with Stick-Breaking Priors},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {334–342},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997226,
author = {Bucak, Serhat S. and Jin, Rong and Jain, Anil K.},
title = {Multi-Label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is O(m1/3√lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {325–333},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997225,
author = {Br\"{o}cheler, Matthias and Getoor, Lise},
title = {Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Continuous Markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classification, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {316–324},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997224,
author = {Brendel, William and Todorovic, Sinisa},
title = {Segmentation as Maximum-Weight Independent Set},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually "meaningful" segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to an optimum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {307–315},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997223,
author = {Boutsidis, Christos and Zouzias, Anastasios and Drineas, Petros},
title = {Random Projections for <i>k</i>-Means Clustering},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ ℝn x d) can be projected into t = Ω(k/ε2) dimensions, for any ε ∈ (0, 1/3), in O(nd[ε-2k/ log(d)]) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + ε. The projection is done by post-multiplying A with a d x t random matrix R having entries + 1/√t or -1/√t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {298–306},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997222,
author = {Boularias, Abdeslam and Chaib-Draa, Brahim},
title = {Bootstrapping Apprenticeship Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features. Most IRL algorithms use a simple Monte Carlo estimation to approximate the expected feature counts under the expert's policy. In this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts. To reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known. Empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {289–297},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997221,
author = {Bouchard-C\^{o}t\'{e}, Alexandre and Jordan, Michael I.},
title = {Variational Inference over Combinatorial Spaces},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6].},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {280–288},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997220,
author = {Boots, Byron and Gordon, Geoffrey J.},
title = {Predictive State Temporal Difference Learning},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {271–279},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997219,
author = {Bonilla, Edwin V. and Guo, Shengbo and Sanner, Scott},
title = {Gaussian Process Preference Elicitation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian approaches to preference elicitation (PE) are particularly attractive due to their ability to explicitly model uncertainty in users' latent utility functions. However, previous approaches to Bayesian PE have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users. In this paper, we address this deficiency by introducing a Gaussian Process (GP) prior over users' latent utility functions on the joint space of user and item features. We learn the hyper-parameters of this GP on a set of preferences of previous users and use it to aid in the elicitation process for a new user. This approach provides a flexible model of a multi-user utility function, facilitates an efficient value of information (VOI) heuristic query selection strategy, and provides a principled way to incorporate the elicitations of multiple users back into the model. We show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {262–270},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997218,
author = {Bohte, Sander M. and Rombouts, Jaldert O.},
title = {Fractionally Predictive Spiking Neurons},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron suffices to carry out such an approximation, given a suitable refractory response. Empirically, we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {253–261},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997217,
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
title = {Kernel Descriptors for Visual Recognition},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classification benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {244–252},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997216,
author = {Blaschko, Matthew B. and Vedaldi, Andrea and Zisserman, Andrew},
title = {Simultaneous Object Detection and Ranking with Weak Supervision},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated.To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance.The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {235–243},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997215,
author = {Blanchard, Gilles and Kr\"{a}mer, Nicole},
title = {Optimal Learning Rates for Kernel Conjugate Gradient Regression},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: first, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulfilled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {226–234},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997214,
author = {Bien, Jacob and Xu, Ya and Mahoney, Michael W.},
title = {CUR from a Sparse Optimization Viewpoint},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {217–225},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997213,
author = {Bickson, Danny and Guestrin, Carlos},
title = {Inference with Multivariate Heavy-Tails in Linear Models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy-tailed distributions.In this work, we propose a novel simple linear graphical model for independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, Levy and Gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both).We provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {208–216},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997212,
author = {Beygelzimer, Alina and Hsu, Daniel and Langford, John and Zhang, Tong},
title = {Agnostic Active Learning without Constraints},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {199–207},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997211,
author = {Bernstein, Andrey and Mannor, Shie and Shimkin, Nahum},
title = {Online Classification with Specificity Constraints},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the online binary classification problem, where we are given m classifiers. At each stage, the classifiers map the input to the probability that the input belongs to the positive class. An online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classifiers. In this paper, we use sensitivity and specificity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisfies the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classifiers, we show that this algorithm takes a very simple form. To our best knowledge, this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {190–198},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997210,
author = {Bergamo, Alessandro and Torresani, Lorenzo},
title = {Exploiting Weakly-Labeled Web Images to Improve Object Classification: A Domain Adaptation Approach},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classifiers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data.In this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few strongly-labeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classifiers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces significant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {181–189},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997209,
author = {Bento, Jos\'{e} and Ibrahimi, Morteza and Montanari, Andrea},
title = {Learning Networks of Stochastic Differential Equations},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider linear models for stochastic dynamics. To any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval T.We analyze the ℓ1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high. This result substantiates the notion of a well defined 'time complexity' for the network inference problem.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {172–180},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997208,
author = {Bengio, Samy and Weston, Jason and Grangier, David},
title = {Label Embedding Trees for Large Multi-Class Tasks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {163–171},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997207,
author = {Bellala, Gowtham and Bhavnani, Suresh K. and Scott, Clayton},
title = {Extensions of Generalized Binary Search to Group Identification and Exponential Costs},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of "yes" or "no" questions posed about that object, and arises in problems such as active learning and active diagnosis. Here, we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs. Then, we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case, we present an exact formula for the objective function involving Shannon or R\'{e}nyi entropy, and develop a greedy algorithm for minimizing it.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {154–162},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997206,
author = {Bayati, Mohsen and Bento, Jos\'{e} and Montanari, Andrea},
title = {The LASSO Risk: Asymptotic Results and Real World Examples},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning a coefficient vector x0 ∈ ℝN from noisy linear observation y = Ax0 + w ∈ ℝn. In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator $hat x$. In this case, a popular approach consists in solving an ℓ1-penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN).For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical models ideas.Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {145–153},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997205,
author = {Barber, Chris and Bockhorst, Joseph and Roebber, Paul},
title = {Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes significantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {136–144},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997204,
author = {Bach, Stephen H. and Maloof, Marcus A.},
title = {A Bayesian Approach to Concept Drift},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {127–135},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997203,
author = {Bach, Francis},
title = {Structured Sparsity-Inducing Norms through Submodular Functions},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov\'{a}sz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {118–126},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997202,
author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli Z.},
title = {Batch Bayesian Optimization via Simulation Matching},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {109–117},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997201,
author = {Ayvaci, Alper and Raptis, Michalis and Soatto, Stefano},
title = {Occlusion Detection and Motion Estimation with Convex Optimization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that, under standard assumptions of Lambertian reflection and static illumination, the task can be posed as a convex minimization problem. Therefore, the solution, computed using efficient algorithms, is guaranteed to be globally optimal, for any number of independently moving objects, and any number of occlusion layers. We test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {100–108},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997200,
author = {Awasthi, Pranjal and Zadeh, Reza Bosagh},
title = {Supervised Clustering},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efficient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will find the target clustering under the strongest property.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {91–99},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997199,
author = {Austerweil, Joseph L. and Griffiths, Thomas L.},
title = {Learning Invariant Features Using the Transformed Indian Buffet Process},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to define a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {82–90},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997198,
author = {Arora, Nimar S. and Russell, Stuart and Kidwell, Paul and Sudderth, Erik},
title = {Global Seismic Monitoring as Probabilistic Inference},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the first stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {73–81},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997197,
author = {Araya-L\'{o}pez, Mauricio and Buffet, Olivier and Thomas, Vincent and Charpillet, Fran\c{c}ois},
title = {A POMDP Extension with Belief-Dependent Rewards},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce ρPOMDPs, an extension of POMDPs where the reward function ρ depends on the belief state. We show that, under the common assumption that ρ is convex, the value function is also convex, what makes it possible to (1) approximate ρ arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {64–72},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997196,
author = {\'{A}lvarez, Mauricio A. and Peters, Jan and Sch\"{o}lkopf, Bernhard and Lawrence, Neil D.},
title = {Switched Latent Force Models for Movement Segmentation},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {55–63},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997195,
author = {Agarwal, Arvind and Daum\'{e}, Hal and Gerber, Samuel},
title = {Learning Multiple Tasks Using Manifold Regularization},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {46–54},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997194,
author = {Agarwal, Alekh and Negahban, Sahand N. and Wainwright, Martin J.},
title = {Fast Global Convergence of Gradient Methods for High-Dimensional Statistical Recovery},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many statistical M-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizes We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov's first-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ* and the optimal solution ^θ. This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates. Our analysis applies to a wide range of M-estimators and statistical models, including sparse linear regression using Lasso (ℓ1-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {37–45},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997193,
author = {Agakov, Felix V. and McKeigue, Paul and Krohn, Jon and Storkey, Amos},
title = {Sparse Instrumental Variables (SPIV) for Genome-Wide Studies},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as "instruments" to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype-phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early-stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with ~ 105 genetic instruments and ~ 47 x 103 gene transcripts.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {28–36},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997192,
author = {Adams, Ryan Prescott and Ghahramani, Zoubin and Jordan, Michael I.},
title = {Tree-Structured Stick Breaking for Hierarchical Data},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {19–27},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997191,
author = {Ackerman, Margareta and Ben-David, Shai and Loker, David},
title = {Towards Property-Based Classification of Clustering Paradigms},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a "correct" clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some first steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms.In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg's famous impossibility result, while providing a simpler proof.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {10–18},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2997189.2997190,
author = {Abernethy, Jacob and Warmuth, Manfred K.},
title = {Repeated Games against Budgeted Adversaries},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player's best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the min-imax strategy is indeed efficiently computable and relies on a "random playout" technique. We give three diverse applications of this new algorithmic template: a cost-sensitive "Hedge" setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {1–9},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@proceedings{10.5555/2997189,
title = {NIPS'10: Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Vancouver, British Columbia, Canada}
}

