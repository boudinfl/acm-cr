@inproceedings{10.5555/2984093.2984355,
author = {Zoran, Daniel and Weiss, Yair},
title = {The "Tree-Dependent Components" of Natural Scenes Are Edge Filters},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new model for natural image statistics. Instead of minimizing dependency between components of natural images, we maximize a simple form of dependency in the form of tree-dependencies. By learning filters and tree structures which are best suited for natural images we observe that the resulting filters are edge filters, similar to the famous ICA on natural images results. Calculating the likelihood of an image patch using our model requires estimating the squared output of pairs of filters connected in the tree. We observe that after learning, these pairs of filters are predominantly of similar orientations but different phases, so their joint energy resembles models of complex cells.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2340–2348},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984354,
author = {Langford, John and Smola, Alexander J. and Zinkevich, Martin},
title = {Slow Learners Are Fast},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Online learning algorithms have impressive convergence properties when it comes to risk minimization and convex games on very large problems. However, they are inherently sequential in their design which prevents them from taking advantage of modern multi-core architectures. In this paper we prove that online learning with delayed updates converges well, thereby facilitating parallel online learning.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2331–2339},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984353,
author = {Zhu, Xiaojin and Rogers, Timothy T. and Gibson, Bryan R.},
title = {Human Rademacher Complexity},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity. Rademacher complexity measures a learner's ability to fit random labels, and can be used to bound the learner's true error based on the observed training sample error. We first review the definition of Rademacher complexity and its generalization bound. We then describe a "learning the noise" procedure to experimentally measure human Rademacher complexities. The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning. Finally, we discuss the potential applications of human Rademacher complexity in cognitive science.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2322–2330},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984352,
author = {Zhu, Long Leo and Chen, Yuanhao and Freeman, William and Torralba, Antonio},
title = {Nonparametric Bayesian Texture Learning and Synthesis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a nonparametric Bayesian method for texture learning and synthesis. A texture image is represented by a 2D Hidden Markov Model (2DHMM) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout (the compatibility between adjacent textons). The 2DHMM is coupled with the Hierarchical Dirichlet process (HDP) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular. The HDP makes use of Dirichlet process prior which favors regular textures by penalizing the model complexity. This framework (HDP-2DHMM) learns the texton vocabulary and their spatial layout jointly and automatically. The HDP-2DHMM results in a compact representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art patch-based rendering methods. We also show that the HDP-2DHMM can be applied to perform image segmentation and synthesis. The preliminary results suggest that HDP-2DHMM is generally useful for further applications in low-level vision problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2313–2321},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984351,
author = {Zhou, Shuheng},
title = {Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given n noisy samples with p dimensions, where n ≪ p, we show that the multi-step thresholding procedure can accurately estimate a sparse vector β ∈ ℝp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Candes-Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2304–2312},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984350,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
title = {Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2295–2303},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984349,
author = {Zhou, Feng and Torre, Fernando de la},
title = {Canonical Time Warping for Alignment of Human Behavior},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Alignment of time series is an important problem to solve in many scientific disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW's effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2286–2294},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984348,
author = {Zhou, Chunxiao and Wang, Huixia Judy and Wang, Yongmei Michelle},
title = {Efficient Moments-Based Permutation Tests},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we develop an efficient moments-based permutation test approach to improve the test's computational efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation. Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2277–2285},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984347,
author = {Zheng, Wenming and Lin, Zhouchen},
title = {Optimizing Multi-Class Spatio-Spectral Filters via Bayes Error Estimation for EEG Classification},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The method of common spatio-spectral patterns (CSSPs) is an extension of common spatial patterns (CSPs) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram (EEG) classification. Although the CSSPs method has shown to be more powerful than the CSPs method in the EEG classification, this method is only suitable for two-class EEG classification problems. In this paper, we generalize the two-class CSSPs method to multi-class cases. To this end, we first develop a novel theory of multi-class Bayes error estimation and then present the multi-class CSSPs (MC-SSPs) method based on this Bayes error theoretical framework. By minimizing the estimated closed-form Bayes error, we obtain the optimal spatio-spectral filters of MCSSPs. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on the BCI competition 2005 data set. The experimental results show that our method significantly outperforms the previous multi-class CSPs (MCSPs) methods in the EEG classification.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2268–2276},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984346,
author = {Zhao, Peilin and Hoi, Steven C. H. and Jin, Rong},
title = {DUOL: A Double Updating Approach for Online Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In most online learning algorithms, the weights assigned to the misclassified examples (or support vectors) remain unchanged during the entire learning process. This is clearly insufficient since when a new misclassified example is added to the pool of support vectors, we generally expect it to affect the weights for the existing support vectors. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short. Instead of only assigning a fixed weight to the misclassified example received in current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be significantly improved by the proposed online learning method. Encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2259–2267},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984345,
author = {Zhao, Manqi and Saligrama, Venkatesh},
title = {Anomaly Detection with Score Functions Based on Nearest Neighbor Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below α, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, α, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2250–2258},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984344,
author = {Zhang, Zhihua and Dai, Guang},
title = {Optimal Scoring for Unsupervised Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We are often interested in casting classification and clustering problems as a regression framework, because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria. In this paper we illustrate optimal scoring, which was originally proposed for performing the Fisher linear discriminant analysis by regression, in the application of unsupervised learning. In particular, we devise a novel clustering algorithm that we call optimal discriminant clustering. We associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering, discriminative clustering and sparse principal component analysis. Experimental results on a collection of benchmark datasets validate the effectiveness of the optimal discriminant clustering algorithm.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2241–2249},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984343,
author = {Yu, Yao-Liang and Li, Yuxi and Schuurmans, Dale and Szepesv\'{a}ri, Csaba},
title = {A General Projection Property for Distribution Families},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Surjectivity of linear projections between distribution families with fixed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classification, optimization, portfolio selection and Markov decision processes.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2232–2240},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984342,
author = {Yu, Kai and Zhang, Tong and Gong, Yihong},
title = {Nonlinear Learning Using Local Coordinate Coding},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2223–2231},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984341,
author = {Ying, Yiming and Huang, Kaizhu and Campbell, Colin},
title = {Sparse Metric Learning via Smooth Optimization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a non-differentiable loss function. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2214–2222},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984340,
author = {Ying, Yiming and Campbell, Colin and Girolami, Mark},
title = {Analysis of SVM with Indefinite Kernels},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recent introduction of indefinite SVM by Luss and d'Aspremont [15] has effectively demonstrated SVM classification with a non-positive semi-definite kernel (indefinite kernel). This paper studies the properties of the objective function introduced there. In particular, we show that the objective function is continuously differentiable and its gradient can be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous. The main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indefinite kernel matrix and the proxy positive semi-definite one. Our elementary result greatly facilitates the application of gradient-based algorithms. Based on our analysis, we further develop Nesterov's smooth optimization approach [17,18] for indefinite SVM which has an optimal convergence rate for smooth problems. Experiments on various benchmark datasets validate our analysis and demonstrate the efficiency of our proposed algorithms.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2205–2213},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984339,
author = {Ye, Nan and Lee, Wee Sun and Chieu, Hai Leong and Wu, Dan},
title = {Conditional Random Fields with High-Order Features for Sequence Labeling},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences used in the features is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2196–2204},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984338,
author = {Yao, Hengshuai and Bhatnagar, Shalabh and Diao, Dongcui},
title = {Multi-Step Linear Dyna-Style Planning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we introduce a multi-step linear Dyna-style planning algorithm. The key element of the multi-step linear Dyna is a multi-step linear model that enables multi-step projection of a sampled feature and multi-step planning based on the simulated multi-step transition experience. We propose two multi-step linear models. The first iterates the one-step linear model, but is generally computationally complex. The second interpolates between the one-step model and the infinite-step model (which turns out to be the LSTD solution), and can be learned efficiently online. Policy evaluation on Boyan Chain shows that multi-step linear Dyna learns a policy faster than single-step linear Dyna, and generally learns faster as the number of projection steps increases. Results on Mountain-car show that multi-step linear Dyna leads to much better online performance than single-step linear Dyna and model-free algorithms; however, the performance of multi-step linear Dyna does not always improve as the number of projection steps increases. Our results also suggest that previous attempts on extending LSTD for online control were unsuccessful because LSTD looks infinite steps into the future, and suffers from the model errors in non-stationary (control) environments.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2187–2195},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984337,
author = {Yao, Bangpeng and Walther, Dirk B. and Beck, Diane M. and Fei-Fei, Li},
title = {Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The human brain can be described as containing a number of functional regions. These regions, as well as the connections between them, play a key role in information processing in the brain. However, most existing multi-voxel pattern analysis approaches either treat multiple regions as one large uniform region or several independent regions, ignoring the connections between them. In this paper we propose to model such connections in an Hidden Conditional Random Field (HCRF) framework, where the classifier of one region of interest (ROI) makes predictions based on not only its voxels but also the predictions from ROIs that it connects to. Furthermore, we propose a structural learning method in the HCRF framework to automatically uncover the connections between ROIs. We illustrate this approach with fMRI data acquired while human subjects viewed images of different natural scene categories and show that our model can improve the top-level (the classifier combining information from all ROIs) and ROI-level prediction accuracy, as well as uncover some meaningful connections between ROIs.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2178–2186},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984336,
author = {Yang, Zhirong and King, Irwin and Xu, Zenglin and Oja, Erkki},
title = {Heavy-Tailed Symmetric Stochastic Neighbor Embedding},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization. Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties. The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heavy-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2169–2177},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984335,
author = {Yang, Zhi and Zhao, Qi and Keefer, Edward and Liu, Wentai},
title = {Noise Characterization, Modeling, and Reduction for in Vivo Neural Recording},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Studying signal and noise properties of recorded neural data is critical in developing more efficient algorithms to recover the encoded information. Important issues exist in this research including the variant spectrum spans of neural spikes that make it difficult to choose a globally optimal bandpass filter. Also, multiple sources produce aggregated noise that deviates from the conventional white Gaussian noise. In this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass filter that fits the spectrum of individual spikes is proposed. Multiple noise sources have been studied through analytical models as well as empirical measurements. The dominant noise source is identified as neuron noise followed by interface noise of the electrode. This suggests that major efforts to reduce noise from electronics are not well spent. The measured noise from in vivo experiments shows a family of 1/fx spectrum that can be reduced using noise shaping techniques. In summary, the methods of adaptive bandpass filtering and noise shaping together result in several dB signal-to-noise ratio (SNR) enhancement.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2160–2168},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984334,
author = {Yang, Xiaolin and Kim, Seyoung and Xing, Eric P.},
title = {Heterogeneous Multitask Learning with Joint Sparsity Constraints},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multitask learning addresses the problem of learning related tasks that presumably share some commonalities on their input-output mapping functions. Previous approaches to multitask learning usually deal with homogeneous tasks, such as purely regression tasks, or entirely classification tasks. In this paper, we consider the problem of learning multiple related tasks of predicting both continuous and discrete outputs from a common set of input variables that lie in a high-dimensional feature space. All of the tasks are related in the sense that they share the same set of relevant input variables, but the amount of influence of each input on different outputs may vary. We formulate this problem as a combination of linear regressions and logistic regressions, and model the joint sparsity as L1/L∞ or L1/L2 norm of the model parameters. Among several possible applications, our approach addresses an important open problem in genetic association mapping, where the goal is to discover genetic markers that influence multiple correlated traits jointly. In our experiments, we demonstrate our method in this setting, using simulated and clinical asthma datasets, and we show that our method can effectively recover the relevant inputs with respect to all of the tasks.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2151–2159},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984333,
author = {Yang, Shuang-Hong and Zha, Hongyuan and Hu, Bao-Gang},
title = {Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose Dirichlet-Bernoulli Alignment (DBA), a generative model for corpora in which each pattern (e.g., a document) contains a set of instances (e.g., paragraphs in the document) and belongs to multiple classes. By casting predefined classes as latent Dirichlet variables (i.e., instance level labels), and modeling the multi-label of each pattern as Bernoulli variables conditioned on the weighted empirical average of topic assignments, DBA automatically aligns the latent topics discovered from data to human-defined classes. DBA is useful for both pattern classification and instance disambiguation, which are tested on text classification and named entity disambiguation in web search queries respectively.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2143–2150},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984332,
author = {Yan, Feng and Xu, Ningyi and Qi, Yuan Alan},
title = {Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. This partitioning scheme also balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors. The proposed partitioning scheme and data streaming make our approach scalable with more multiprocessors. Furthermore, they can be used as general techniques to parallelize other machine learning models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2134–2142},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984331,
author = {Xu, Zenglin and Jin, Rong and Zhu, Jianke and King, Irwin and Lyu, Michael R. and Yang, Zhirong},
title = {Adaptive Regularization for Transductive Support Vector Machine},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We discuss the framework of Transductive Support Vector Machine (TSVM) from the perspective of the regularization strength induced by the unlabeled data. In this framework, SVM and TSVM can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data, respectively. Therefore, to supplement this framework of the regularization strength, it is necessary to introduce data-dependant partial regularization. To this end, we reformulate TSVM into a form with controllable regularization strength, which includes SVM and TSVM as special cases. Furthermore, we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption. Experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art TSVM algorithms.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2125–2133},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984330,
author = {Xiao, Lin},
title = {Dual Averaging Method for Regularized Stochastic Learning and Online Optimization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ1-norm for promoting sparsity. We develop a new online algorithm, the regularized dual averaging (RDA) method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. Computational experiments show that the RDA method can be very effective for sparse online learning with ℓ1-regularization.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2116–2124},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984329,
author = {Xiang, Zhen James and Xi, Yongxin Taylor and Hasson, Uri and Ramadge, Peter J.},
title = {Boosting with Spatial Regularization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {By adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting. From this regularized loss framework we derive an efficient boosting algorithm that uses additional weights/priors on the base classifiers. We prove that the proposed algorithm exhibits a "grouping effect", which encourages the selection of all spatially local, discriminative base classifiers. The algorithm's primary advantage is in applications where the trained classifier is used to identify the spatial pattern of discriminative information, e.g. the voxel selection problem in fMRI. We demonstrate the algorithm's performance on various data sets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2107–2115},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984328,
author = {Xia, Fen and Liu, Tie-Yan and Li, Hang},
title = {Statistical Consistency of Top-<i>k</i> Ranking},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2098–2106},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984327,
author = {Wu, Lei and Jin, Rong and Hoi, Steven C. H. and Zhu, Jianke and Yu, Nenghai},
title = {Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a non-parametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of fixed metric by implicitly deriving a local distance from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We also present an efficient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efficient for high dimensional data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2089–2097},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984326,
author = {Wright, John and Peng, Yigang and Ma, Yi and Ganesh, Arvind and Rao, Shankar},
title = {Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized "robust principal component analysis" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2080–2088},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984325,
author = {Wipf, David and Nagarajan, Srikantan},
title = {Sparse Estimation Using General Likelihoods and Non-Factorial Priors},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity. While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient and globally-convergent, reweighted ℓ1-norm minimization procedure. The first method under consideration arises from the sparse Bayesian learning (SBL) framework. Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better. These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions. We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in some empirical tests. For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted ℓ1-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints. As a byproduct of this development, a rigorous reformulation of sparse Bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2071–2079},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984324,
author = {Wilson, Robert C. and Finkel, Leif H.},
title = {A Neural Implementation of the Kalman Filter},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent experimental evidence suggests that the brain is capable of approximating Bayesian inference in the face of noisy input stimuli. Despite this progress, the neural underpinnings of this computation are still poorly understood. In this paper we focus on the Bayesian filtering of stochastic time series and introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the Kalman filter in the limit of small prediction error. When the prediction error is large we show that the network responds robustly to changepoints in a way that is qualitatively compatible with the optimal Bayesian model. The model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2062–2070},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984323,
author = {Wilder, Matthew H. and Jones, Matt and Mozer, Michael C.},
title = {Sequential Effects Reflect Parallel Learning of Multiple Environmental Regularities},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Across a wide range of cognitive tasks, recent experience influences behavior. For example, when individuals repeatedly perform a simple two-alternative forced-choice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g., Jones and Sieck, 2003; Mozer, Kinoshita, and Shettel, 2007; Yu and Cohen, 2008). The Dynamic Belief Model (DBM) (Yu and Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that first-order statistics (base rates) also influence sequential effects. We propose a model that learns both first- and second-order sequence properties, each according to the basic principles of the DBM but under a unified inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious fits to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Martello, Sahm, and Spillmann, 2005) and electrophysiological studies (Jentzsch and Sommer, 2002), supporting the psychological and neurobiological reality of its two components.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2053–2061},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984322,
author = {Wick, Michael and Rohanimanesh, Khashayar and Singh, Sameer and McCallum, Andrew},
title = {Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large, relational factor graphs with structure defined by first-order logic or other languages give rise to notoriously difficult inference problems. Because unrolling the structure necessary to represent distributions over all hypotheses has exponential blow-up, solutions are often derived from MCMC. However, because of limitations in the design and parameterization of the jump function, these sampling-based methods suffer from local minima—the system must transition through lower-scoring configurations before arriving at a better MAP solution. This paper presents a new method of explicitly selecting fruitful downward jumps by leveraging reinforcement learning (RL). Rather than setting parameters to maximize the likelihood of the training data, parameters of the factor graph are treated as a log-linear function approximator and learned with methods of temporal difference (TD); MAP inference is performed by executing the resulting policy on held out test data. Our method allows efficient gradient updates since only factors in the neighborhood of variables affected by an action need to be computed—we bypass the need to compute marginals entirely. Our method yields dramatic empirical success, producing new state-of-the-art results on a complex joint model of ontology alignment, with a 48% reduction in error over state-of-the-art in that domain.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2044–2052},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984321,
author = {Whitehill, Jacob and Ruvolo, Paul and Wu, Tingfan and Bergsma, Jacob and Movellan, Javier},
title = {Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector [9]). New Internet-based services allow for a large number of labelers to collaborate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used "Majority Vote" heuristic for inferring image labels, and is robust to both noisy and adversarial labelers.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2035–2043},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984320,
author = {Waugh, Kevin and Bard, Nolan and Bowling, Michael},
title = {Strategy Grafting in Extensive Games},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Extensive games are often used to model the interactions of multiple agents within an environment. Much recent work has focused on increasing the size of an extensive game that can be feasibly solved. Despite these improvements, many interesting games are still too large for such techniques. A common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size. This abstract game is then solved and the resulting strategy is played in the original game. Most top programs in recent AAAI Computer Poker Competitions use this approach. The trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games. These larger abstract games have more expressive strategy spaces and therefore contain better strategies. In this paper we present a new method for computing strategies in large games. This method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve. We demonstrate the power of the approach experimentally in both small and large games, while also providing a theoretical justification for the resulting improvement.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2026–2034},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984319,
author = {Watanabe, Yusuke and Fukumizu, Kenji},
title = {Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2017–2025},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984318,
author = {Wang, Yang and Haffari, Gholamreza and Wang, Shaojun and Mori, Greg},
title = {A Rate Distortion Approach for Semi-Supervised Conditional Random Fields},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel information theoretic approach for semi-supervised learning of conditional random fields that defines a training objective to combine the conditional likelihood on labeled data and the mutual information on unlabeled data. In contrast to previous minimum conditional entropy semi-supervised discriminative learning methods, our approach is grounded on a more solid foundation, the rate distortion theory in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations. Our experimental results show the rate distortion approach outperforms standard l2 regularization, minimum conditional entropy regularization as well as maximum conditional entropy regularization on both multi-class classification and sequence labeling problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2008–2016},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984317,
author = {Wang, Liwei},
title = {Sufficient Conditions for Agnostic Active Learnable},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study pool-based active learning in the presence of noise, i.e. the agnostic setting. Previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have advantage. In this paper, we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning. We show that under some noise condition, if the Bayesian classification boundary and the underlying distribution are smooth to a finite order, active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution are infinitely smooth, the improvement is exponential.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1999–2007},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984316,
author = {Wang, Chong and Blei, David M.},
title = {Variational Inference for the Nested Chinese Restaurant Process},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable, current inference methods have all relied on MCMC sampling. In this paper, we develop an alternative inference technique based on variational methods. To employ variational methods, we derive a tree-based stick-breaking construction of the nCRP mixture model, and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the nCRP to continuous data as well.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1990–1998},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984315,
author = {Wang, Chong and Blei, David M.},
title = {Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the "topics"). In the sparse topic model (sparseTM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efficient Gibbs sampler for the sparseTM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the sparseTM on four real-world datasets. Compared to traditional approaches, the empirical results will show that sparseTMs give better predictive performance with simpler inferred models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1982–1989},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984314,
author = {Wallach, Hanna M. and Mimno, David and McCallum, Andrew},
title = {Rethinking LDA: Why Priors Matter},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such "smoothing parameters" have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic-word distributions provides no real benefit. Approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1973–1981},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984313,
author = {Wu, Xiao-Ming and So, Anthony Man-Cho and Li, Zhenguo and Li, Shuo-Yen Robert},
title = {Fast Graph Laplacian Regularized Kernel Learning via Semidefinite-Quadratic-Linear Programming},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Kernel learning is a powerful framework for nonlinear data modeling. Using the kernel trick, a number of problems have been formulated as semidefinite programs (SDPs). These include Maximum Variance Unfolding (MVU) (Weinberger et al., 2004) in nonlinear dimensionality reduction, and Pairwise Constraint Propagation (PCP) (Li et al., 2008) in constrained clustering. Although in theory SDPs can be efficiently solved, the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the SDP approach unscalable. In this paper, we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs (SQLPs), which only contain a simple positive semidefinite constraint, a second-order cone constraint and a number of linear constraints. These constraints are much easier to process numerically, and the gain in speedup over previous approaches is at least of the order m2.5, where m is the matrix dimension. Experimental results are also presented to show the superb computational efficiency of our approach.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1964–1972},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984312,
author = {Vul, Edward and Frank, Michael C. and Tenenbaum, Joshua B. and Alvarez, George},
title = {Explaining Human Multiple Object Tracking as Resource-Constrained Approximate Inference in a Dynamic Probabilistic Model},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We find that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle filter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a flexible cognitive resource, which can be formalized as either memory or attention.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1955–1963},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984311,
author = {Venkataraman, Shobha and Blum, Avrim and Song, Dawn and Sen, Subhabrata and Spatscheck, Oliver},
title = {Tracking Dynamic Sources of Malicious Activity at Internet-Scale},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We formulate and address the problem of discovering dynamic malicious regions on the Internet. We model this problem as one of adaptively pruning a known decision tree, but with additional challenges: (1) severe space requirements, since the underlying decision tree has over 4 billion leaves, and (2) a changing target function, since malicious activity on the Internet is dynamic. We present a novel algorithm that addresses this problem, by putting together a number of different "experts" algorithms and online paging algorithms. We prove guarantees on our algorithm's performance as a function of the best possible pruning of a similar size, and our experiments show that our algorithm achieves high accuracy on large real-world data sets, with significant improvements over existing approaches.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1946–1954},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984310,
author = {Veness, Joel and Silver, David and Uther, William and Blair, Alan},
title = {Bootstrapping from Game Tree Search},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuel's checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1937–1945},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984309,
author = {Vedaldi, Andrea and Zisserman, Andrew},
title = {Structured Output Regression for Detection with Partial Truncation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efficient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert [1] to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing, right facing, front) with the same formulation; and (iv), most significantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask.We demonstrate the method by training and testing on the PASCAL VOC 2007 data set - training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with significant truncations.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1928–1936},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984308,
author = {Vanpaemel, Wolf},
title = {Measuring Model Complexity with the Prior Predictive},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the last few decades, model complexity has received a lot of press. While many methods have been proposed that jointly measure a model's descriptive adequacy and its complexity, few measures exist that measure complexity in itself. Moreover, existing measures ignore the parameter prior, which is an inherent part of the model and affects the complexity. This paper presents a stand alone measure for model complexity, that takes the number of parameters, the functional form, the range of the parameters and the parameter prior into account. This Prior Predictive Complexity (PPC) is an intuitive and easy to compute measure. It starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes. The PPC then measures how wide this range exactly is.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1919–1927},
numpages = {9},
keywords = {perception, structure learning, model comparison methods, model selection},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984307,
author = {Vanhatalo, Jarno and Jyl\"{a}nki, Pasi and Vehtari, Aki},
title = {Gaussian Process Regression with Student-<i>t</i> Likelihood},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the influence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1910–1918},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984306,
author = {van Gerven, Marcel and Cseke, Botond and Oostenveld, Robert and Heskes, Tom},
title = {Bayesian Source Localization with the Multivariate Laplace Prior},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the specification of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1901–1909},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984305,
author = {Durme, Benjamin Van and Lall, Ashwin},
title = {Streaming Pointwise Mutual Information},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work has led to the ability to perform space efficient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1892–1900},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984304,
author = {Valizadegan, Hamed and Jin, Rong and Zhang, Ruofei and Mao, Jianchang},
title = {Learning to Rank by Optimizing NDCG Measure},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using information retrieval measures, such as Normalized Discounted Cumulative Gain (NDCG) [1] and Mean Average Precision (MAP) [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1883–1891},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984303,
author = {Ullman, Tomer D. and Baker, Chris L. and Macindoe, Owen and Evans, Owain and Goodman, Noah D. and Tenenbaum, Joshua B.},
title = {Help or Hinder: Bayesian Models of Social Goal Inference},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Everyday social interactions are heavily influenced by our snap judgments about others' goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is 'helping' or 'hindering' another's attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent's behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1874–1882},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984302,
author = {Turaga, Srinivas C. and Briggman, Kevin L. and Helmstaedter, Moritz and Denk, Winfried and Seung, H. Sebastian},
title = {Maximin Affinity Learning of Image Segmentation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure.The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1865–1873},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984301,
author = {Todorov, Emanuel},
title = {Compositionality of Optimal Control Laws},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties - one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of LQG controllers) without requiring the final cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Green's function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the field of motor control, yet they have been difficult to reconcile. Our work makes this possible.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1856–1864},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984300,
author = {Tillman, Robert E. and Gretton, Arthur and Spirtes, Peter},
title = {Nonlinear Directed Acyclic Structure Learning with Weakly Additive Noise Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1847–1855},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984299,
author = {The, Yee Whye and G\"{o}r\"{u}r, Dilan},
title = {Indian Buffet Processes with Power-Law Behavior},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the stable-beta process and deriving the IBP corresponding to it. We find interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and find that our power-law IBP is a good model for word occurrences in document corpora.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1838–1846},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984298,
author = {Syed, Umar and Slivkins, Aleksandrs and Mishra, Nina},
title = {Adapting to the Shifting Intent of Search Queries},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Search engines today present results that are often oblivious to recent shifts in intent. For example, the meaning of the query 'independence day' shifts in early July to a US holiday and to a movie around the time of the box office release. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 1/2 the search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1829–1837},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984297,
author = {Sutskever, Ilya and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
title = {Modelling Relational Data Using Bayesian Clustered Tensor Factorization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us "understand" a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1821–1828},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984296,
author = {Sun, Liang and Liu, Jun and Chen, Jianhui and Ye, Jieping},
title = {Efficient Recovery of Jointly Sparse Vectors},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the reconstruction of sparse signals in the multiple measurement vector (MMV) model, in which the signal, represented as a matrix, consists of a set of jointly sparse vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard compressive sensing (CS). Recent theoretical studies focus on the convex relaxation of the MMV problem based on the (2,1)-norm minimization, which is an extension of the well-known 1-norm minimization employed in SMV. However, the resulting convex optimization problem in MMV is significantly much more difficult to solve than the one in SMV. Existing algorithms reformulate it as a second-order cone programming (SOCP) or semidefinite programming (SDP) problem, which is computationally expensive to solve for problems of moderate size. In this paper, we propose a new (dual) reformulation of the convex optimization problem in MMV and develop an efficient algorithm based on the prox-method. Interestingly, our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning. Our simulation studies demonstrate the scalability of the proposed algorithm.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1812–1820},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984295,
author = {Subramanya, Amarnag and Bilmes, Jeff},
title = {Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to outperform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1803–1811},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984294,
author = {Streeter, Matthew and Golovin, Daniel and Krause, Andreas},
title = {Online Learning of Assignments},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Which ads should we display in sponsored search in order to maximize our revenue? How should we dynamically rank information sources to maximize the value of the ranking? These applications exhibit strong diminishing returns: Redundancy decreases the marginal utility of each ad or information source. We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one. We present an efficient algorithm for this general problem and analyze it in the no-regret model. Our algorithm possesses strong theoretical guarantees, such as a performance ratio that converges to the optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1794–1802},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984293,
author = {Steyvers, Mark and Lee, Michael and Miller, Brent and Hemmer, Pernille},
title = {The Wisdom of Crowds in the Recollection of Order Information},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When individuals independently recollect events or retrieve facts from memory, how can we aggregate these retrieved memories to reconstruct the actual set of events or facts? In this research, we report the performance of individuals in a series of general knowledge tasks, where the goal is to reconstruct from memory the order of historic events, or the order of items along some physical dimension. We introduce two Bayesian models for aggregating order information based on a Thurstonian approach and Mallows model. Both models assume that each individual's reconstruction is based on either a random permutation of the unobserved ground truth, or by a pure guessing strategy. We apply MCMC to make inferences about the underlying truth and the strategies employed by individuals. The models demonstrate a "wisdom of crowds" effect, where the aggregated orderings are closer to the true ordering than the orderings of the best individual.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1785–1793},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984292,
author = {Stevenson, Ian H. and K\"{o}rding, Konrad P.},
title = {Structural Inference Affects Depth Perception in the Context of Potential Occlusion},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then fit the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We find that the model accurately predicts shifts in subject's behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1777–1784},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984291,
author = {Steinwart, Ingo and Christmann, Andreas},
title = {Fast Learning from Non-i.i.d. Observations},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from α-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1768–1776},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984290,
author = {Sriperumbudur, Bharath K. and Lanckriet, Gert R. G.},
title = {On the Convergence of the Concave-Convex Procedure},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), its proof is more specialized and technical than actually required for the specific case of CCCP. In this paper, we follow a different reasoning and show how Zangwill's global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwill's theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1759–1767},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984289,
author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Lanckriet, Gert R. G. and Sch\"{o}lkopf, Bernhard},
title = {Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The generalized MMD is shown to have consistent finite sample estimates, and its performance is demonstrated on a homogeneity testing example.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1750–1758},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984288,
author = {Sprekeler, Henning and Hennequin, Guillaume and Gerstner, Wulfram},
title = {Code-Specific Policy Gradient Rules for Spiking Neurons},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general "full spike train" code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1741–1749},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984287,
author = {Song, Le and Kolar, Mladen and Xing, Eric P.},
title = {Time-Varying Dynamic Bayesian Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Directed graphical models such as Bayesian networks are a favored formalism for modeling the dependency structures in complex multivariate systems such as those encountered in biology and neural science. When a system is undergoing dynamic transformation, temporally rewiring networks are needed for capturing the dynamic causal influences between covariates. In this paper, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series. This is a challenging problem due the non-stationarity and sample scarcity of time series data. We present a kernel reweighted ℓ1-regularized auto-regressive procedure for this problem which enjoys nice properties such as computational efficiency and provable asymptotic consistency. To our knowledge, this is the first practical and statistically sound method for structure learning of TV-DBNs. We applied TV-DBNs to time series measurements during yeast cell cycle and brain response to visual stimuli. In both cases, TV-DBNs reveal interesting dynamics underlying the respective biological systems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1732–1740},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984286,
author = {Sollich, Peter and Urry, Matthew J and Coti, Camille},
title = {Kernels and Learning Curves for Gaussian Process Regression on Random Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate how well Gaussian process regression can learn functions defined on graphs, using large regular random graphs as a paradigmatic example. Random-walk based kernels are shown to have some non-trivial properties: within the standard approximation of a locally tree-like graph structure, the kernel does not become constant, i.e. neighbouring function values do not become fully correlated, when the lengthscale σ of the kernel is made large. Instead the kernel attains a non-trivial limiting form, which we calculate. The fully correlated limit is reached only once loops become relevant, and we estimate where the crossover to this regime occurs. Our main subject are learning curves of Bayes error versus training set size. We show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input, and generically scale with n/V, the number of training examples per vertex. We also explore how this behaviour changes for kernel lengthscales that are large enough for loops to become important.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1723–1731},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984285,
author = {Socher, Richard and Gershman, Samuel J. and Perotte, Adler J. and Sederberg, Per B. and Blei, David M. and Norman, Kenneth A.},
title = {A Bayesian Analysis of Dynamics in Free Recall},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject first studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle filter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1714–1722},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984284,
author = {Smaragdis, Paris and Shashanka, Madhusudana and Raj, Bhiksha},
title = {A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce significantly better separation results as compared to similar systems based on compact statistical models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1705–1713},
numpages = {9},
keywords = {example-based representation, sparse models, signal separation},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984283,
author = {Sinz, Fabian and Simoncelli, Eero P. and Bethge, Matthias},
title = {Hierarchical Modeling of Local Image Features through <i>L</i><sub><i>p</i></sub>-Nested Symmetric Distributions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new family of distributions, called Lp-nested symmetric distributions, whose densities are expressed in terms of a hierarchical cascade of Lp-norms. This class generalizes the family of spherically and Lp-spherically symmetric distributions which have recently been successfully used for natural image modeling. Similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables. With suitable choices of the parameters and norms, this family includes the Independent Subspace Analysis (ISA) model as a special case, which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex. Lp -nested distributions are relatively easy to estimate and allow us to explore the variety of models between ISA and the Lp-spherically symmetric models. By fitting the generalized Lp-nested model to 8 x 8 image patches, we show that the subspaces obtained from ISA are in fact more dependent than the individual filter coefficients within a subspace. When first applying contrast gain control as preprocessing, however, there are no dependencies left that could be exploited by ISA. This suggests that complex cell modeling can only be useful for redundancy reduction in larger image patches.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1696–1704},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984282,
author = {Sinha, Kaushik and Belkin, Mikhail},
title = {Semi-Supervised Learning Using Sparse Eigenfunction Bases},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices. It turns out that when the data has clustered, that is, when the high density regions are sufficiently separated by low density valleys, each high density area corresponds to a unique representative eigenvector.Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions when the cluster assumption holds. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity corresponds naturally to the cluster assumption.Experimental results on a number of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1687–1695},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984281,
author = {Singh-Miller, Natasha and Collins, Michael},
title = {Learning Label Embeddings for Nearest-Neighbor Multi-Class Classification with an Application to Speech Recognition},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P(y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate significant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1678–1686},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984280,
author = {Shi, Lei and Griffiths, Thomas L.},
title = {Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1669–1677},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984279,
author = {Shervashidze, Nino and Borgwardt, Karsten M.},
title = {Fast Subtree Kernels on Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon &amp; G\"{a}rtner scales as O(n24dh). Key to this efficiency is the observation that the Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classification benchmark datasets in terms of accuracy and runtime.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1660–1668},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984278,
author = {Shen, Chunhua and Kim, Junae and Wang, Lei and van den Hengel, Anton},
title = {Positive Semidefinite Metric Learning with Boosting},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The learning of appropriate distance metrics is a critical problem in image classification and retrieval. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. BOOSTMETRIC is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1651–1659},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984277,
author = {Shani, Guy and Meek, Christopher},
title = {Improving Existing Fault Recovery Policies},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An automated recovery system is a key component in a large data center. Such a system typically employs a hand-made controller created by an expert. While such controllers capture many important aspects of the recovery process, they are often not systematically optimized to reduce costs such as server downtime. In this paper we describe a passive policy learning approach for improving existing recovery policies without exploration. We explain how to use data gathered from the interactions of the hand-made controller with the system, to create an improved controller. We suggest learning an indefinite horizon Partially Observable Markov Decision Process, a model for decision making under uncertainty, and solve it using a point-based algorithm. We describe the complete process, starting with data gathering, model learning, model checking procedures, and computing a policy.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1642–1650},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984276,
author = {Seeger, Matthias W.},
title = {Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a first study, designs are found that improve significantly on others chosen independently for each slice or drawn at random.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1633–1641},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984275,
author = {Schmidt, Mikkel N.},
title = {Linearly Constrained Bayesian Matrix Factorization for Blind Source Separation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of non-negative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1624–1632},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984274,
author = {Schlecht, Joseph and Barnard, Kobus},
title = {Learning Models of Object Structure},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g. chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances. We learn the models using joint statistical inference over category parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1615–1623},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984273,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
title = {Replicated Softmax: An Undirected Topic Model},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a two-layer undirected graphical model, called a "Replicated Softmax", that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1607–1614},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984272,
author = {Salakhutdinov, Ruslan},
title = {Learning in Markov Random Fields Using Tempered Transitions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Markov random fields (MRF's), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF's is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF's. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1598–1606},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984271,
author = {Saenko, Kate and Darrell, Trevor},
title = {Filtering Abstract Senses from Image Search Results},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to find images on the web that are associated with the object name and train a visual classifier from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classifier to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difficult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classifiers trained on concrete-sense images returned by our method for a set of ten common office objects.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1589–1597},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984270,
author = {Russell, Bryan C. and Efros, Alexei A. and Sivic, Josef and Freeman, William T. and Zisserman, Andrew},
title = {Segmenting Scenes by Matching Image Composites},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1580–1588},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984269,
author = {Bul\`{o}, Samuel Rota and Pelillo, Marcello},
title = {A Game-Theoretic Approach to Hypergraph Clustering},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order (rather than pairwise) similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. In this paper, we provide a radically different perspective to the problem. In contrast to the classical approach, we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose. Specifically, we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player "clustering game", whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept. From the computational viewpoint, we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex, and we provide a discrete-time dynamics to perform this optimization. Experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1571–1579},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984268,
author = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
title = {Lower Bounds on Minimax Rates for Nonparametric Regression with Additive Sparsity and Smoothness},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study minimax rates for estimating high-dimensional nonparametric regression models with sparse additive structure and smoothness constraints. More precisely, our goal is to estimate a function f* : ℝp → ℝ that has an additive decomposition of the form f*(X1,..., Xp) = ∑j∈s h*j(Xj), where each component function h*j lies in some class H of "smooth" functions, and S ⊂ {1,...,p} is an unknown subset with cardinality s = |S|. Given n i.i.d. observations of f*(X) corrupted with additive white Gaussian noise where the covariate vectors (X1, X2, X3, Xp) are drawn with i.i.d. components from some distribution P, we determine lower bounds on the minimax rate for estimating the regression function with respect to squared-L2 (ℙ) error. Our main result is a lower bound on the minimax rate that scales as max (s log(p/s)/n, s ∊2n(H)). The first term reflects the sample size required for performing subset selection, and is independent of the function class H. The second term s ∊2n(H) is an s-dimensional estimation term corresponding to the sample size required for estimating a sum of s univariate functions, each chosen from the function class H. It depends linearly on the sparsity index s but is independent of the global dimension p. As a special case, if H corresponds to functions that are m-times differentiable (an mth-order Sobolev space), then the s-dimensional estimation term takes the form ∊2n(H) ≍ s n-2m/(2m+1). Either of the two terms may be dominant in different regimes, depending on the relation between the sparsity and smoothness of the additive decomposition.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1563–1570},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984267,
author = {Rao, Vinayak and Teh, Yee Whye},
title = {Spatial Normalized Gamma Processes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally DP distributed. They are used in Bayesian nonparametric models when the usual exchangeability assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each associated with a point in a space such that neighbouring DPs are more dependent. We describe Markov chain Monte Carlo inference involving Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence on a synthetic dataset and demonstrate an application of the model to topic modeling through time.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1554–1562},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984266,
author = {Rangan, Sundeep and Fletcher, Alyson K. and Goyal, Vivek K},
title = {Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector "decouples" as n scalar MAP estimators. The result is a counterpart to Guo and Verd\'{u}'s replica analysis of minimum mean-squared error estimation.The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero norm-regularized estimation it reduces to a hard-threshold. Among other benefits, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1545–1553},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984265,
author = {Ram, Parikshit and Lee, Dongryeol and Ouyang, Hua and Gray, Alexander G.},
title = {Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations. As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1 + ∊) distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the first time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN. Experiments on high-dimensional datasets show that our algorithm often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1536–1544},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984264,
author = {Ram, Parikshit and Lee, Dongryeol and March, William B. and Gray, Alexander G.},
title = {Linear-Time Algorithms for Pairwise Statistical Problems},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several key computational bottlenecks in machine learning involve pairwise distance computations, including all-nearest-neighbors (finding the nearest neigh-bor(s) for each point, e.g. in manifold learning) and kernel summations (e.g. in kernel density estimation or kernel machines). We consider the general, bichromatic case for these problems, in addition to the scientific problem of N-body simulation. In this paper we show for the first time O(N) worst case runtimes for practical algorithms for these problems based on the cover tree data structure [1].},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1527–1535},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984263,
author = {Raginsky, Maxim and Lazebnik, Svetlana},
title = {Locality-Sensitive Binary Codes from Shift-Invariant Kernels},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1509–1517},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984262,
author = {Quadrianto, Novi and Petterson, James and Smola, Alex J.},
title = {Distribution Matching for Transduction},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g. by providing a large margin of separation on both sets. We use this idea to design a transduction algorithm which can be used without modification for classification, regression, and structured estimation. At its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match. This is a classical two-sample problem which can be solved efficiently in its most general form by using distance measures in Hilbert Space. It turns out that a number of existing heuristics can be viewed as special cases of our approach.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1500–1508},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984261,
author = {Quadrianto, Novi and Caetano, Tib\'{e}rio S. and Lim, John and Schuurmans, Dale},
title = {Convex Relaxation of Mixture Regression with Efficient Algorithms},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models. Although our relaxation involves a semidefinite matrix variable, we reformulate the problem to eliminate the need for general semidefinite programming. In particular, we provide two reformulations that admit fast algorithms. The first is a max-min spectral reformulation exploiting quasi-Newton descent. The second is a min-min reformulation consisting of fast alternating steps of closed-form updates. We evaluate the methods against Expectation-Maximization in a real problem of motion segmentation from video data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1491–1499},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984260,
author = {Pirsiavash, Hamed and Ramanan, Deva and Fowlkes, Charless},
title = {Bilinear Classifiers for Visual Recognition},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe an algorithm for learning bilinear SVMs. Bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classifier yields a separable filter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classifiers. We also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks. Bilinear classifiers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1482–1490},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984259,
author = {Pillow, Jonathan W.},
title = {Time-Rescaling Methods for the Estimation and Assessment of Non-Poisson Neural Encoding Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a conditional renewal (CR) model for neural spike trains. This model captures both real-time and rescaled-time history effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with shape k ≠ 1), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1473–1481},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984258,
author = {Pfister, Jean-Pascal and Dayan, Peter and Lengyel, M\'{a}t\'{e}},
title = {Know Thy Neighbour: A Normative Theory of Synaptic Depression},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Synapses exhibit an extraordinary degree of short-term malleability, with release probabilities and effective synaptic strengths changing markedly over multiple timescales. From the perspective of a fixed computational operation in a network, this seems like a most unacceptable degree of added variability. We suggest an alternative theory according to which short-term synaptic plasticity plays a normatively-justifiable role. This theory starts from the commonplace observation that the spiking of a neuron is an incomplete, digital, report of the analog quantity that contains all the critical information, namely its membrane potential. We suggest that a synapse solves the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives, acting as a recursive filter. We show that the dynamics of short-term synaptic depression closely resemble those required for optimal filtering, and that they indeed support high quality estimation. Under this account, the local postsynaptic potential and the level of synaptic resources track the (scaled) mean and variance of the estimated presynaptic membrane potential. We make experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1464–1472},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984257,
author = {Petterson, James and Caetano, Tib\'{e}rio S. and McAuley, Julian J. and Yu, Jin},
title = {Exponential Family Graph Matching and Ranking},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application-document ranking-exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning document ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is comparatively high.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1455–1463},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984256,
author = {Petrik, Marek and Zilberstein, Shlomo},
title = {Robust Value Function Approximation Using Bilinear Programming},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose approximate bilinear programming, a new formulation of value function approximation that provides strong a priori guarantees. In particular, this approach provably finds an approximate value function that minimizes the Bellman residual. Solving a bilinear program optimally is NP-hard, but this is unavoidable because the Bellman-residual minimization itself is NP-hard. We therefore employ and analyze a common approximate algorithm for bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on a simple benchmark problem.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1446–1454},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984255,
author = {Perkins, Theodore J.},
title = {Maximum Likelihood Trajectories for Continuous-Time Markov Chains},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random. Many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control. However, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved. We study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given final time, (ii) a boundary value problem, in which initial and final states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to finding maximum likelihood trajectories for hidden Markov models. We show that maximum likelihood trajectories are not always well-defined, and describe a polynomial time test for well-definedness. When well-definedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efficient dynamic programming algorithms for doing so.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1437–1445},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984254,
author = {Perina, Alessandro and Cristani, Marco and Castellani, Umberto and Murino, Vittorio and Jojic, Nebojsa},
title = {Free Energy Score-Space},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A score function induced by a generative model of the data can provide a feature vector of a fixed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a fixed-length vector in a highly informative space, typically referred to as a "score space". Discriminative classifiers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classifiers, or the discriminative classifiers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classification performance that at least matches the performance of the free energy classifier based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classifiers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1428–1436},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984253,
author = {Peng, Jian and Bo, Liefeng and Xu, Jinbo},
title = {Conditional Neural Fields},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conditional random fields (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs significantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1419–1427},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984252,
author = {Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey and Mitchell, Tom M.},
title = {Zero-Shot Learning with Semantic Output Codes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of zero-shot learning, where the goal is to learn a classifier f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes. As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1410–1418},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984251,
author = {Ouyang, Tom Y. and Davis, Randall},
title = {Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach significantly improves recognition performance.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1401–1409},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984250,
author = {Orbanz, Peter},
title = {Construction of Nonparametric Bayesian Models from Parametric Bayes Equations},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the general problem of constructing nonparametric Bayesian models on infinite-dimensional random objects, such as functions, infinite graphs or infinite permutations. The problem has generated much interest in machine learning, where it is treated heuristically, but has not been studied in full generality in non-parametric Bayesian statistics, which tends to focus on models over probability distributions. Our approach applies a standard tool of stochastic process theory, the construction of stochastic processes from their finite-dimensional marginal distributions. The main contribution of the paper is a generalization of the classic Kolmogorov extension theorem to conditional probabilities. This extension allows a rigorous construction of nonparametric Bayesian models from systems of finite-dimensional, parametric Bayes equations. Using this approach, we show (i) how existence of a conjugate posterior for the nonparametric model can be guaranteed by choosing conjugate finite-dimensional models in the construction, (ii) how the mapping to the posterior parameters of the nonparametric model can be explicitly determined, and (iii) that the construction of conjugate models in essence requires the finite-dimensional models to be in the exponential family. As an application of our constructive framework, we derive a model on infinite permutations, the nonparametric Bayesian analogue of a model recently proposed for the analysis of rank data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1392–1400},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984249,
author = {Onken, Arno and Gr\"{u}new\"{a}lder, Steffen and Obermayer, Klaus},
title = {Correlation Coefficients Are Insufficient for Analyzing Spike Count Dependencies},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The linear correlation coefficient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefficient is in general insufficient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-fire neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We find that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poisson-like marginals is well characterized by the linear correlation coefficient and verify it for different copula-based models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1383–1391},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984248,
author = {Ozakin, Arkadas and Gray, Alexander},
title = {Submanifold Density Estimation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation. However, long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades. In practice, it has been recognized that often such data have a much lower-dimensional intrinsic structure. We propose a small modification to kernel density estimation for estimating probability density functions on Riemannian submanifolds of Euclidean space. Using ideas from Riemannian geometry, we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold. We conclude with empirical results demonstrating the behavior predicted by our theory.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1375–1382},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984247,
author = {Nowak, Robert},
title = {Noisy Generalized Binary Search},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper addresses the problem of noisy Generalized Binary Search (GBS). GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries. At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search. GBS is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning. In most of these cases, the responses to queries can be noisy. Past work has provided a partial characterization of GBS, but existing noise-tolerant versions of GBS are suboptimal in terms of query complexity. This paper presents an optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1366–1374},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984246,
author = {Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},
title = {STDP Enables Spiking Neurons to Detect Hidden Causes of Their Inputs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or "causes") of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these neurons will fire after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artificial neural networks.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1357–1365},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984245,
author = {Negahban, Sahand and Ravikumar, Pradeep and Wainwright, Martin J. and Yu, Bin},
title = {A Unified Framework for High-Dimensional Analysis of <i>M</i>-Estimators with Decomposable Regularizers},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n → 0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M-estimator) which combines a loss function (measuring how well the model fits the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a unified framework for establishing consistency and convergence rates for such regularized M-estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identifies two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M-estimators have fast convergence rates.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1348–1356},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984244,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {3D Object Recognition with Deep Belief Nets},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1339–1347},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984243,
author = {Nadler, Boaz and Srebro, Nathan and Zhou, Xueyuan},
title = {Semi-Supervised Learning with the Graph Laplacian: The Limit of Infinite Unlabelled Data},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points. We show that in ℝd, d ≥ 2, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function. We also contrast the method with the Laplacian Eigenvector method, and discuss the "smoothness" assumptions associated with this alternate method.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1330–1338},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984242,
author = {Mozer, Michael C. and Pashler, Harold and Cepeda, Nicholas and Lindsey, Robert and Vul, Ed},
title = {Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a significant impact on memory retention. Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. Appropriate spacing of study can double retention on educationally relevant time scales. We introduce a Multiscale Context Model (MCM) that is able to predict the influence of a particular study schedule on retention for specific material. MCM's prediction is based on empirical data characterizing forgetting of the material following a single study session. MCM is a synthesis of two existing memory models (Staddon, Chelaru, &amp; Higa, 2002; Raaijmakers, 2003). On the surface, these models are unrelated and incompatible, but we show they share a core feature that allows them to be integrated. MCM can determine study schedules that maximize the durability of learning, and has implications for education and training. MCM can be cast either as a neural network with inputs that fluctuate over time, or as a cascade of leaky integrators. MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, &amp; Shadmehr, 2007), yet MCM is better able to account for human declarative memory.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1321–1329},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984241,
author = {Morimura, Tetsuro and Uchibe, Eiji and Yoshimoto, Junichiro and Doya, Kenji},
title = {A Generalized Natural Actor-Critic Algorithm},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Policy gradient Reinforcement Learning (RL) algorithms have received substantial attention, seeking stochastic policies that maximize the average (or discounted cumulative) reward. In addition, extensions based on the concept of the Natural Gradient (NG) show promising learning efficiency because these regard metrics for the task. Though there are two candidate metrics, Kakade's Fisher Information Matrix (FIM) for the policy (action) distribution and Morimura's FIM for the state-action joint distribution, but all RL algorithms with NG have followed Kakade's approach. In this paper, we describe a generalized Natural Gradient (gNG) that linearly interpolates the two FIMs and propose an efficient implementation for the gNG learning based on a theory of the estimating function, the generalized Natural Actor-Critic (gNAC) algorithm. The gNAC algorithm involves a near optimal auxiliary function to reduce the variance of the gNG estimates. Interestingly, the gNAC can be regarded as a natural extension of the current state-of-the-art NAC algorithm [1], as long as the interpolating parameter is appropriately selected. Numerical experiments showed that the proposed gNAC algorithm can estimate gNG efficiently and outperformed the NAC algorithm.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1312–1320},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984240,
author = {Bento, Jos\'{e} and Montanari, Andrea},
title = {Which Graphical Models Are Difficult to Learn?},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1303–1311},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984239,
author = {Doshi-Velez, Finale and Knowles, David and Mohamed, Shakir and Ghahramani, Zoubin},
title = {Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Nonparametric Bayesian models provide a framework for flexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the first parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1294–1302},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984238,
author = {Moghaddam, Baback and Marlin, Benjamin M. and Khan, Mohammad Emtiyaz and Murphy, Kevin P.},
title = {Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We make several contributions in accelerating approximate Bayesian structural inference for non-decomposable GGMs. Our first contribution is to show how to efficiently compute a BIC or Laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation. This optimization technique can be used as a fast scoring function inside standard Stochastic Local Search (SLS) for generating posterior samples. Our second contribution is a novel framework for efficiently generating large sets of high-quality graph topologies without performing local search. This graph proposal method, which we call "Neighborhood Fusion" (NF), samples candidate Markov blankets at each node using sparse regression techniques. Our third contribution is a hybrid method combining the complementary strengths of NF and SLS. Experimental results in structural recovery and prediction tasks demonstrate that NF and hybrid NF/SLS out-perform state-of-the-art local search methods, on both synthetic and real-world datasets, when realistic computational limits are imposed.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1285–1293},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984237,
author = {Miller, Kurt T. and Griffiths, Thomas L. and Jordan, Michael I.},
title = {Nonparametric Latent Feature Models for Link Prediction},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1276–1284},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984236,
author = {Meng, Yicong and Shi, Bertram E.},
title = {Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-Out},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We extend the concept of phase tuning, a ubiquitous mechanism among sensory neurons including motion and disparity selective neurons, to the motion contrast detection. We demonstrate that the motion contrast can be detected by phase shifts between motion neuronal responses in different spatial regions. By constructing the differential motion opponency in response to motions in two different spatial regions, varying motion contrasts can be detected, where similar motion is detected by zero phase shifts and differences in motion by non-zero phase shifts. The model can exhibit either enhancement or suppression of responses by either different or similar motion in the surrounding. A primary advantage of the model is that the responses are selective to relative motion instead of absolute motion, which could model neurons found in neurophysiological experiments responsible for motion pop-out detection.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1267–1275},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984235,
author = {Meka, Raghu and Jain, Prateek and Dhillon, Inderjit S.},
title = {Matrix Completion from Power-Law Distributed Samples},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The low-rank matrix completion problem is a fundamental problem with many important applications. Recently, [4],[13] and [5] obtained the first non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random. Unfortunately, most real-world datasets do not satisfy this assumption, but instead exhibit power-law distributed samples. In this paper, we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models. Our method is simpler to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs, a problem of independent interest. By analyzing the graph theoretic problem, we show that our method achieves exact recovery when the observed entries are sampled from the Chung-Lu-Vu model, which can generate power-law distributed graphs. We also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim. Furthermore, our method is easy to implement and is substantially faster than existing methods. We demonstrate the effectiveness of our method on random instances where the low-rank matrix is sampled according to the prevalent random graph models for complex networks and present promising preliminary results on the Netflix challenge dataset.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1258–1266},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984234,
author = {McCallum, Andrew and Schultz, Karl and Singh, Sameer},
title = {FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data. The power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a powerful and flexible way. Rather than using a declarative language, such as SQL or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning. By combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain significant efficiencies. We have implemented such imperatively defined factor graphs in a system we call FACTORIE, a software library for an object-oriented, strongly-typed, functional language. In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25%—achieving a new state of the art.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1249–1257},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984233,
author = {Margaritis, Dimitris},
title = {Toward Provably Correct Feature Selection in Arbitrary Domains},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we address the problem of provably correct feature selection in arbitrary domains. An optimal solution to the problem is a Markov boundary, which is a minimal set of features that make the probability distribution of a target variable conditionally invariant to the state of all other features in the domain. While numerous algorithms for this problem have been proposed, their theoretical correctness and practical behavior under arbitrary probability distributions is unclear. We address this by introducing the Markov Boundary Theorem that precisely characterizes the properties of an ideal Markov boundary, and use it to develop algorithms that learn a more general boundary that can capture complex interactions that only appear when the values of multiple features are considered together. We introduce two algorithms: an exact, provably correct one as well a more practical randomized anytime version, and show that they perform well on artificial as well as benchmark and real-world data sets. Throughout the paper we make minimal assumptions that consist of only a general set of axioms that hold for every probability distribution, which gives these algorithms universal applicability.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1240–1248},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984232,
author = {Mann, Gideon and McDonald, Ryan and Mohri, Mehryar and Silberman, Nathan and Walker, Daniel D.},
title = {Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Training conditional maximum entropy models on massive data sets requires significant computational resources. We examine three common distributed training methods for conditional maxent: a distributed gradient computation method, a majority vote method, and a mixture weight method. We analyze and compare the CPU and network time complexity of each of these methods and present a theoretical analysis of conditional maxent models, including a study of the convergence of the mixture weight method, the most resource-efficient technique. We also report the results of large-scale experiments comparing these three methods which demonstrate the benefits of the mixture weight method: this method consumes less resources, while achieving a performance comparable to that of standard approaches.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1231–1239},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984231,
author = {Malisiewicz, Tomasz and Efros, Alexei A.},
title = {Beyond Categories: The Visual Memex Model for Reasoning about Object Relationships},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the object's relationship to other elements of the scene (context). Most current approaches rely on modeling the relationships between object categories as a source of context. In this paper we seek to move beyond categories to provide a richer appearance-based model of context. We present an exemplar-based model of objects and their relationships, the Visual Memex, that encodes both local appearance and 2D spatial context between object instances. We evaluate our model on Torralba's proposed Context Challenge against a baseline category-based system. Our experiments suggest that moving beyond categories for context modeling appears to be quite beneficial, and may be the critical missing ingredient in scene understanding systems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1222–1230},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984230,
author = {Maillard, Odalric-Ambrym and Munos, R\'{e}mi},
title = {Compressed Least-Squares Regression},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of learning, from K data, a regression function in a linear space of high dimension N using projections onto a random subspace of lower dimension M. From any algorithm minimizing the (possibly penalized) empirical risk, we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain). We show that solving the problem in the compressed domain instead of the initial domain reduces the estimation error at the price of an increased (but controlled) approximation error. We apply the analysis to Least-Squares (LS) regression and discuss the excess risk and numerical complexity of the resulting "Compressed Least Squares Regression" (CLSR) in terms of N, K, and M. When we choose M = O(√K), we show that CLSR has an estimation error of order O(log K/ √K).},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1213–1221},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984229,
author = {Maei, Hamid R. and Szepesv\'{a}ri, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S.},
title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(λ), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms' effectiveness.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1204–1212},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984228,
author = {Macke, Jakob H. and Gerwinn, Sebastian and White, Leonard E. and Kaschube, Matthias and Bethge, Matthias},
title = {Bayesian Estimation of Orientation Preference Maps},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial and temporal scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data. In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data. We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reflects known properties of OPMs, and a noise covariance adjusted to the data. The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements. By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations, pinwheel locations or pinwheel counts. Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and quantitative model comparisons. We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1195–1203},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984227,
author = {Dermed, Liam Mac and Isbell, Charles},
title = {Solving Stochastic Games},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms. We provide the first approximation algorithm which solves stochastic games with cheap-talk to within ∊ absolute error of the optimal game-theoretic solution, in time polynomial in 1/∊. Our algorithm extends Murray's and Gordon's (2007) modified Bellman equation which determines the set of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1186–1194},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984226,
author = {Luttinen, Jaakko and Ilin, Alexander},
title = {Variational Gaussian-Process Factor Analysis for Modeling Spatio-Temporal Data},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1177–1185},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984225,
author = {Jie, Luo and Caputo, Barbara and Ferrari, Vittorio},
title = {Who's Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a corpus of news items consisting of images accompanied by text captions, we want to find out "who's doing what", i.e. associate names and action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint 'face and pose' model solves the correspondence problem better than earlier models covering only the face, and that it can perform recognition of new uncaptioned images.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1168–1176},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984224,
author = {Lu, Hongjing and Weiden, Matthew and Yuille, Alan},
title = {Modeling the Spacing Effect in Sequential Category Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We define conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efficiently perform model selection to decide whether observations are from one or multiple categories.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1159–1167},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984223,
author = {Lozano, Aur\'{e}lie C. and undefinedwirszcz, Grzegorz and Abe, Naoki},
title = {Group Orthogonal Matching Pursuit for Variable Selection and Prediction},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efficiently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as "forward greedy feature selection algorithm" for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the l∞ norm of the difference between the estimated regression coefficients and the true coefficients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1150–1158},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984222,
author = {Liu, Han and Chen, Xi},
title = {Nonparametric Greedy Algorithms for the Sparse Learning Problem},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-of-the-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justifications of specific versions of the additive forward regression.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1141–1149},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984221,
author = {Liang, Percy and Bach, Francis and Bouchard, Guillaume and Jordan, Michael I.},
title = {Asymptotically Optimal Regularization in Smooth Parametric Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many types of regularization schemes have been employed in statistical learning, each motivated by some assumption about the problem domain. In this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer. In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework. We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1132–1140},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984220,
author = {Li, Wu-Jun and Yeung, Dit-Yan and Zhang, Zhihua},
title = {Probabilistic Relational PCA},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One crucial assumption made by both principal component analysis (PCA) and probabilistic PCA (PPCA) is that the instances are independent and identically distributed (i.i.d.). However, this common i.i.d. assumption is unreasonable for relational data. In this paper, by explicitly modeling covariance between instances as derived from the relational information, we propose a novel probabilistic dimensionality reduction method, called probabilistic relational PCA (PRPCA), for relational data analysis. Although the i.i.d. assumption is no longer adopted in PRPCA, the learning algorithms for PRPCA can still be devised easily like those for PPCA which makes explicit use of the i.i.d. assumption. Experiments on real-world data sets show that PRPCA can effectively utilize the relational information to dramatically outperform PCA and achieve state-of-the-art performance.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1123–1131},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984219,
author = {Leordeanu, Marius and Hebert, Martial and Sukthankar, Rahul},
title = {An Integer Projected Fixed Point Method for Graph Matching and MAP Inference},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efficiently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms find approximate solutions by relaxing the original problem. They find the optimal continuous solution of the modified problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this final discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efficient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also significantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by significantly outperforming [13], ICM and Max-Product Belief Propagation.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1114–1122},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984218,
author = {Legenstein, Robert and Chase, Steven M. and Schwartz, Andrew B. and Maass, Wolfgang},
title = {Functional Network Reorganization in Motor Cortex Can Be Explained by Reward-Modulated Hebbian Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is fitted to experimental data, the model produces learning effects similar to those found in monkey experiments.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1105–1113},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984217,
author = {Lee, Honglak and Largman, Yan and Pham, Peter and Ng, Andrew Y.},
title = {Unsupervised Feature Learning for Audio Classification Using Convolutional Deep Belief Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1096–1104},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984216,
author = {L\'{a}zaro-Gredilla, Miguel and Figueiras-Vidal, An\'{\i}bal R.},
title = {Inter-Domain Gaussian Processes for Sparse Inference Using Inducing Features},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a general inference framework for inter-domain Gaussian Processes (GPs) and focus on its usefulness to build sparse GP models. The state-of-the-art sparse GP model introduced by Snelson and Ghahramani in [1] relies on finding a small, representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well, and then uses it to perform inference. This reduces inference and model selection computation time from O(n3) to O(m2n), where m ≪ n. Inter-domain GPs can be used to find a (possibly more compact) representative set of features lying in a different domain, at the same computational cost. Being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions. We will show how previously existing models fit into this framework and will use it to develop two new sparse GP models. Tests on large, representative regression data sets suggest that significant improvement can be achieved, while retaining computational efficiency.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1087–1095},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984215,
author = {Lanctot, Marc and Waugh, Kevin and Zinkevich, Martin and Bowling, Michael},
title = {Monte Carlo Sampling for Regret Minimization in Extensive Games},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game. One efficient method for computing Nash equilibria in large, zero-sum, imperfect information games is counterfactual regret minimization (CFR). In the domain of poker, CFR has proven effective, particularly when using a domain-specific augmentation involving chance outcome sampling. In this paper, we describe a general family of domain-independent CFR sample-based algorithms called Monte Carlo counterfactual regret minimization (MCCFR) of which the original and poker-specific versions are special cases. We start by showing that MCCFR performs the same regret updates as CFR on expectation. Then, we introduce two sampling schemes: outcome sampling and external sampling, showing that both have bounded overall regret with high probability. Thus, they can compute an approximate equilibrium using self-play. Finally, we prove a new tighter bound on the regret for the original CFR algorithm and relate this new bound to MCCFR's bounds. We show empirically that, although the sample-based algorithms require more iterations, their lower cost per iteration can lead to dramatically faster convergence in various games.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1078–1086},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984214,
author = {L\"{u}cke, J\"{o}rg and Turner, Richard and Sahani, Maneesh and Henniges, Marc},
title = {Occlusive Components Analysis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research field of multiple-causes component extraction methods.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1069–1077},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984213,
author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
title = {Ensemble Nystr\"{o}m Method},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices. We introduce a new family of algorithms based on mixtures of Nystr\"{o}m approximations, ensemble Nystr\"{o}m algorithms, that yield more accurate low-rank approximations than the standard Nystr\"{o}m method. We give a detailed study of variants of these algorithms based on simple averaging, an exponential weight method, or regression-based methods. We also present a theoretical analysis of these algorithms, including novel error bounds guaranteeing a better convergence rate than the standard Nystr\"{o}m method. Finally, we report results of extensive experiments with several data sets containing up to 1M points demonstrating the significant improvement over the standard Nystr\"{o}m approximation.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1060–1068},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984212,
author = {Kumar, M. Pawan and Koller, Daphne},
title = {Learning a Small Mixture of Trees},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning, for example variational inference and classification. Within this context, we consider the task of learning a mixture of tree distributions. Although mixtures of trees can be learned by minimizing the KL-divergence using an EM algorithm, its success depends heavily on the initialization. We propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the α-divergence with α = ∞. We formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration. Compared to previous methods, our approach results in a significantly smaller mixture of trees that provides similar or better accuracies. We demonstrate the usefulness of our approach by learning pictorial structures for face recognition.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1051–1059},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984211,
author = {Kulis, Brian and Darrell, Trevor},
title = {Learning to Hash with Binary Reconstructive Embeddings},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches. In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data. We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1042–1050},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984210,
author = {Krishnan, Dilip and Fergus, Rob},
title = {Fast Image Deconvolution Using Hyper-Laplacian Priors},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising, deblurring and super-resolution. These distributions are well modeled by a hyper-Laplacian (p(x) ∝ e-k|x|α ), typically with 0.5 ≤ α ≤ 0.8. However, the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images. In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-Laplacian priors. We adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels. This per-pixel sub-problem may be solved with a lookup table (LUT). Alternatively, for two specific values of α, 1/2 and 2/3 an analytic solution can be found, by finding the roots of a cubic and quartic polynomial, respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve a 1 megapixel image in less than ~3 seconds, achieving comparable quality to existing methods such as iteratively reweighted least squares (IRLS) that take ~20 minutes. Furthermore, our method is quite general and can easily be extended to related image processing problems, beyond the deconvolution application demonstrated.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1033–1041},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984209,
author = {Kpotufe, Samory},
title = {Fast, Smooth and Adaptive Regression in Metric Spaces},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It was recently shown that certain nonparametric regressors can escape the curse of dimensionality when the intrinsic dimension of data is low ([1, 2]). We prove some stronger results in more general settings. In particular, we consider a regressor which, by combining aspects of both tree-based regression and kernel regression, adapts to intrinsic dimension, operates on general metrics, yields a smooth function, and evaluates in time O(log n). We derive a tight convergence rate of the form n-2/(2+d) where d is the Assouad dimension of the input space.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1024–1032},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984208,
author = {Konidaris, George and Barto, Andrew},
title = {Skill Discovery in Continuous Reinforcement Learning Domains Using Skill Chaining},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance benefits in a challenging continuous domain.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1015–1023},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984207,
author = {Kolar, Mladen and Song, Le and Xing, Eric P.},
title = {Sparsistent Learning of Varying-Coefficient Models with Structural Changes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To estimate the changing structure of a varying-coefficient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which first identifies jump points of structural changes and then identifies relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1006–1014},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984206,
author = {Kloft, Marius and Brefeld, Ulf and Sonnenburg, S\"{o}ren and Laskov, Pavel and M\"{u}ller, Klaus-Robert and Zien, Alexander},
title = {Efficient and Accurate ℓ<sub><i>p</i></sub>-Norm Multiple Kernel Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations to support interpretability. Unfortunately, ℓ1-norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures, we generalize MKL to arbitrary ℓp-norms. We devise new insights on the connection between several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary p &gt; 1. Empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally, we apply ℓp-norm MKL to real-world problems from computational biology, showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {997–1005},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984205,
author = {Klampfl, Stefan and Maass, Wolfgang},
title = {Replacing Supervised Classification Learning by Slow Feature Analysis in Spiking Neural Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal firing patterns of presynaptic neurons. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classification capability of Fisher's Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating firing patterns within a stream of spike trains with the same firing statistics, as well as discrimination of spoken digits, in an unsupervised manner.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {988–996},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984204,
author = {Kim, Kwang In and Steinke, Florian and Hein, Matthias},
title = {Semi-Supervised Regression Using Hessian Energy with an Application to Semi-Supervised Dimensionality Reduction},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Based on these observations, we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both these problems. If the data lies on or close to a low-dimensional submanifold in feature space, the Hessian energy prefers functions whose values vary linearly with respect to geodesic distance. We first derive the Hessian energy for smooth manifolds and continue to give a stable estimation procedure for the common case where only samples of the underlying manifold are given. The preference of ''linear" functions on manifolds renders the Hessian energy particularly suited for the task of semi-supervised dimensionality reduction, where the goal is to find a user-defined embedding function given some labeled points which varies smoothly (and ideally linearly) along the manifold. The experimental results suggest superior performance of our method compared with semi-supervised regression using Laplacian regularization or standard supervised regression techniques applied to this task.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {979–987},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984203,
author = {Kim, Jong Kyoung and Choi, Seungjin},
title = {Clustering Sequence Sets for Motif Discovery},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most of existing methods for DNA motif discovery consider only a single set of sequences to find an over-represented motif. In contrast, we consider multiple sets of sequences where we group sets associated with the same motif into a cluster, assuming that each set involves a single motif. Clustering sets of sequences yields clusters of coherent motifs, improving signal-to-noise ratio or enabling us to identify multiple motifs. We present a probabilistic model for DNA motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences. Our model infers cluster-indicating latent variables and learns motifs simultaneously, where these two tasks interact with each other. We show that our model can handle various motif discovery problems, depending on how to construct multiple sets of sequences. Experiments on three different problems for discovering DNA motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only a single set of sequences is considered.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {970–978},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984202,
author = {Kim, Gunhee and Torralba, Antonio},
title = {Unsupervised Detection of Regions of Interest Using Iterative Link Analysis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a fast and scalable alternating optimization technique to detect regions of interest (ROIs) in cluttered Web images without labels. The proposed approach discovers highly probable regions of object instances by iteratively repeating the following two functions: (1) choose the exemplar set (i.e. a small number of highly ranked reference ROIs) across the dataset and (2) refine the ROIs of each image with respect to the exemplar set. These two subproblems are formulated as ranking in two different similarity networks of ROI hypotheses by link analysis. The experiments with the PASCAL 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods. Also, we test the scalability of our approach with five objects in Flickr dataset consisting of more than 200K images.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {961–969},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984201,
author = {Keshavan, Raghunandan H. and Montanari, Andrea and Oh, Sewoong},
title = {Matrix Completion from Noisy Entries},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the 'Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced in [1], based on a combination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {952–960},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984200,
author = {Kemp, Charles},
title = {Quantification and the Language of Thought},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many researchers have suggested that the psychological complexity of a concept is related to the length of its representation in a language of thought. As yet, however, there are few concrete proposals about the nature of this language. This paper makes one such proposal: the language of thought allows first order quantification (quantification over objects) more readily than second-order quantification (quantification over features). To support this proposal we present behavioral results from a concept learning study inspired by the work of Shepard, Hovland and Jenkins.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {943–951},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984199,
author = {Kemp, Charles and Jern, Alan},
title = {Abstraction and Relational Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most models of categorization learn categories defined by characteristic features but some categories are described more naturally in terms of relations. We present a generative model that helps to explain how relational categories are learned and used. Our model learns abstract schemata that specify the relational similarities shared by instances of a category, and our emphasis on abstraction departs from previous theoretical proposals that focus instead on comparison of concrete instances. Our first experiment suggests that abstraction can help to explain some of the findings that have previously been used to support comparison-based approaches. Our second experiment focuses on one-shot schema learning, a problem that raises challenges for comparison-based approaches but is handled naturally by our abstraction-based account.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {934–942},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984198,
author = {Kemp, Charles and Jern, Alan and Xu, Fei},
title = {Object Discovery and Identification},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice. We present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments. Our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered. Our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {925–933},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984197,
author = {Kawahara, Yoshinobu and Nagano, Kiyohito and Tsuda, Koji and Bilmes, Jeff A.},
title = {Submodularity Cuts and Applications},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several key problems in machine learning, such as feature selection and active learning, can be formulated as submodular set function maximization. We present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint — the algorithm is based on a cutting-plane method and is implemented as an iterative small-scale binary-integer linear programming procedure. It is well known that this problem is NP-hard, and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time. As for (non-polynomial time) exact algorithms that perform reasonably in practice, there has been very little in the literature although the problem is quite important for many applications. Our algorithm is guaranteed to find the exact solution finitely many iterations, and it converges fast in practice due to the efficiency of the cutting-plane mechanism. Moreover, we also provide a method that produces successively decreasing upper-bounds of the optimal solution, while our algorithm provides successively increasing lower-bounds. Thus, the accuracy of the current solution can be estimated at any point, and the algorithm can be stopped early once a desired degree of tolerance is met. We evaluate our algorithm on sensor placement and feature selection applications showing good performance.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {916–924},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984196,
author = {Karasuyama, Masayuki and Takeuchi, Ichiro},
title = {Multiple Incremental Decremental Learning of Support Vector Machines},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a multiple incremental decremental algorithm of Support Vector Machine (SVM). Conventional single incremental decremental SVM can update the trained model efficiently when single data point is added to or removed from the training set. When we add and/or remove multiple data points, this algorithm is time-consuming because we need to repeatedly apply it to each data point. The proposed algorithm is computationally more efficient when multiple data points are added and/or removed simultaneously. The single incremental decremental algorithm is built on an optimization technique called parametric programming. We extend the idea and introduce multi-parametric programming for developing the proposed algorithm. Experimental results on synthetic and real data sets indicate that the proposed algorithm can significantly reduce the computational cost of multiple incremental decremental operation. Our approach is especially useful for online SVM learning in which we need to remove old data points and add new data points in a short amount of time.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {907–915},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984195,
author = {Kapoor, Ashish and Horvitz, Eric},
title = {Breaking Boundaries: Active Information Acquisition across Learning and Diagnosis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To date, the processes employed for active information acquisition during periods of learning and diagnosis have been considered as separate and have been applied in distinct phases of analysis. While active learning centers on the collection of information about training cases in order to build better predictive models, diagnosis uses fixed predictive models for guiding the collection of observations about a specific test case at hand. We introduce a model and inferential methods that bridge these phases of analysis into a holistic approach to information acquisition that considers simultaneously the extension of the predictive model and the probing of a case at hand. The bridging of active learning and real-time diagnostic feature acquisition leads to a new class of policies for learning and diagnosis.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {898–906},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984194,
author = {Kao, Yi-hao and Roy, Benjamin Van and Yan, Xiang},
title = {Directed Regression},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {When used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {889–897},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984193,
author = {Kalai, Adam Tauman and Kanade, Varun},
title = {Potential-Based Agnostic Boosting},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to MadaBoost (Domingo and Watanabe, 2000) and SmoothBoost (Servedio, 2003). Our analysis is in the agnostic framework of Kearns, Schapire and Sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead. Our boosting theorem gives, as easy corollaries, alternative derivations of two recent nontrivial results in computational learning theory: agnostically learning decision trees (Gopalan et al, 2008) and agnostically learning halfspaces (Kalai et al, 2005). Experiments suggest that the algorithm performs similarly to MadaBoost.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {880–888},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984192,
author = {Jung, Kyomin and Kohli, Pushmeet and Shah, Devavrat},
title = {Local Rules for Global MAP: When Do They Work?},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the question of computing Maximum A Posteriori (MAP) assignment in an arbitrary pair-wise Markov Random Field (MRF). We present a randomized iterative algorithm based on simple local updates. The algorithm, starting with an arbitrary initial assignment, updates it in each iteration by first, picking a random node, then selecting an (appropriately chosen) random local neighborhood and optimizing over this local neighborhood. Somewhat surprisingly, we show that this algorithm finds a near optimal assignment within n log2 n iterations with high probability for any n node pair-wise MRF with geometry (i.e. MRF graph with polynomial growth) with the approximation error depending on (in a reasonable manner) the geometric growth rate of the graph and the average radius of the local neighborhood - this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error. Through extensive simulations, we show that our algorithm finds extremely good approximate solutions for various kinds of MRFs with geometry.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {871–879},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984191,
author = {Jin, Rong and Wang, Shijun and Zhou, Yang},
title = {Regularized Distance Metric Learning: Theory and Algorithm},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data. In addition, we present an efficient online learning algorithm for regularized distance metric learning. Our empirical studies with data classification and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efficient and robust for high dimensional data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {862–870},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984190,
author = {Jern, Alan and Chang, Kai-min K. and Kemp, Charles},
title = {Bayesian Belief Polarization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Empirical studies have documented cases of belief polarization, where two people with opposing prior beliefs both strengthen their beliefs after observing the same evidence. Belief polarization is frequently offered as evidence of human irrationality, but we demonstrate that this phenomenon is consistent with a fully Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the set of Bayesian models that we consider.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {853–861},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984189,
author = {Nath, J. Saketha and Dinesh, G. and Raman, S. and Bhattacharyya, Chiranjib and Ben-Tal, Aharon and Ramakrishnan, K. R.},
title = {On the Algorithmics and Applications of a Mixed-Norm Based Kernel Learning Formulation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated from real world problems, like object categorization, we study a particular mixed-norm regularization for Multiple Kernel Learning (MKL). It is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand. The formulation hence employs l∞ regularization for promoting combinations at the component level and l1 regularization for promoting sparsity among kernels in each component. While previous attempts have formulated this as a non-convex problem, the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient Mirror-Descent (MD) based procedure. The MD procedure optimizes over product of simplexes, which is not a well-studied case in literature. Results on real-world datasets show that the new MKL formulation is well-suited for object categorization tasks and that the MD based algorithm outperforms state-of-the-art MKL solvers like simpleMKL in terms of computational effort.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {844–852},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984188,
author = {Iwata, Tomoharu and Yamada, Takeshi and Ueda, Naonori},
title = {Modeling Social Annotation Data with Content Relevance Using a Topic Model},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy, i.e. not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {835–843},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984187,
author = {Ihler, Alexander T. and Frank, Andrew J. and Smyth, Padhraic},
title = {Particle-Based Variational Inference for Continuous Systems},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions defined on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated.In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the benefits of these more advanced inference techniques to the continuous domain.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {826–834},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984186,
author = {Hutter, Marcus},
title = {Discrete MDL Predicts in Total Variation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {817–825},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984185,
author = {Huang, Shuai and Li, Jing and Sun, Liang and Liu, Jun and Wu, Teresa and Chen, Kewei and Fleisher, Adam and Reiman, Eric and Ye, Jieping},
title = {Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent advances in neuroimaging techniques provide great potentials for effective diagnosis of Alzheimer's disease (AD), the most common form of dementia. Previous studies have shown that AD is closely related to the alternation in the functional brain network, i.e., the functional connectivity among different brain regions. In this paper, we consider the problem of learning functional brain connectivity from neuroimaging, which holds great promise for identifying image-based markers used to distinguish Normal Controls (NC), patients with Mild Cognitive Impairment (MCI), and patients with AD. More specifically, we study sparse inverse covariance estimation (SICE), also known as exploratory Gaussian graphical models, for brain connectivity modeling. In particular, we apply SICE to learn and analyze functional brain connectivity patterns from different subject groups, based on a key property of SICE, called the "monotone property" we established in this paper. Our experimental results on neuroimaging PET data of 42 AD, 116 MCI, and 67 NC subjects reveal several interesting connectivity patterns consistent with literature findings, and also some new patterns that can help the knowledge discovery of AD.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {808–816},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984184,
author = {Huang, Jonathan and Guestrin, Carlos},
title = {Riffled Independence for Ranked Data},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called riffled independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. In riffled independence, one draws two permutations independently, then performs the riffle shuffle, common in card games, to combine the two permutations to form a single permutation. In ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using riffled independence within Fourier-theoretic frameworks which have been explored by a number of recent papers.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {799–807},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984183,
author = {Hu, Tao and Chklovskii, Dmitri B.},
title = {Reconstruction of Sparse Circuits Using Multi-Neuronal Excitation (RESCUME)},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the central problems in neuroscience is reconstructing synaptic connectivity in neural circuits. Synapses onto a neuron can be probed by sequentially stimulating potentially pre-synaptic neurons while monitoring the membrane voltage of the post-synaptic neuron. Reconstructing a large neural circuit using such a "brute force" approach is rather time-consuming and inefficient because the connectivity in neural circuits is sparse. Instead, we propose to measure a post-synaptic neuron's voltage while stimulating sequentially random subsets of multiple potentially pre-synaptic neurons. To reconstruct these synaptic connections from the recorded voltage we apply a decoding algorithm recently developed for compressive sensing. Compared to the brute force approach, our method promises significant time savings that grow with the size of the circuit. We use computer simulations to find optimal stimulation parameters and explore the feasibility of our reconstruction method under realistic experimental conditions including noise and non-linear synaptic integration. Multi-neuronal stimulation allows reconstructing synaptic connectivity just from the spiking activity of post-synaptic neurons, even when sub-threshold voltage is unavailable. By using calcium indicators, voltage-sensitive dyes, or multi-electrode arrays one could monitor activity of multiple post-synaptic neurons simultaneously, thus mapping their synaptic inputs in parallel, potentially reconstructing a complete neural circuit.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {790–798},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984182,
author = {Hu, Chonghai and Kwok, James T. and Pan, Weike},
title = {Accelerated Gradient Methods for Stochastic Optimization and Online Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., ℓ1-regularizer). Gradient methods, though highly scalable and easy to implement, are known to converge slowly. In this paper, we develop a novel accelerated gradient method for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic composite optimization with convex or strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple algorithm but with the best regret bounds currently known for these problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {781–789},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984181,
author = {Rai, Piyush and Daum\'{e}, Hal},
title = {Multi-Label Prediction via Sparse Infinite CCA},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efficacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {772–780},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984180,
author = {Hsu, Daniel and Kakade, Sham M. and Langford, John and Zhang, Tong},
title = {Multi-Label Prediction via Compressed Sensing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider multi-label prediction problems with large output spaces under the assumption of output sparsity - that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {772–780},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984179,
author = {Hsu, Chun-Nan and Chang, Yu-Ming and Huang, Han-Shen and Lee, Yuh-Jye},
title = {Periodic Step-Size Adaptation for Single-Pass on-Line Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {763–771},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984178,
author = {Hsu, Anne S. and Griffiths, Thomas L.},
title = {Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence - the absence of a sentence - when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {754–762},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984177,
author = {Honorio, Jean and Ortiz, Luis and Samaras, Dimitris and Paragios, Nikos and Goldstein, Rita},
title = {Sparse and Locally Constant Gaussian Graphical Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes, motion trajectories, 2D and 3D images). Although these datasets are typically under-sampled and high-dimensional, they often need to be represented with low-complexity statistical models, which are comprised of only the important probabilistic dependencies in the datasets. Most methods attempt to reduce model complexity by enforcing structure sparseness. However, sparseness cannot describe inherent regularities in the structure. Hence, in this paper we first propose a new class of Gaussian graphical models which, together with sparseness, imposes local constancy through ℓ1-norm penalization. Second, we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions. Through synthetic experiments, we evaluate the closeness of the recovered models to the ground truth. We also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it captures useful structures such as the rotation and shrinking of a beating heart, motion correlations between body parts during walking and functional interactions of brain regions. Our method outperforms the state-of-the-art structure learning techniques for Gaussian graphical models both for small and large datasets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {745–753},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984176,
author = {Henao, Ricardo and Winther, Ole},
title = {Bayesian Sparse Factor Models and DAGs Inference and Comparison},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we present a novel approach to learn directed acyclic graphs (DAGs) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison. We require identifiability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {736–744},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984175,
author = {Heller, Katherine and Sanborn, Adam and Chater, Nick},
title = {Hierarchical Learning of Dimensional Biases in Human Categorization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions - and have a strong preference to generalize along the axes of these dimensions, but not "diagonally". What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. The learning behaviour of the model captures the developmental shift from roughly "isotropic" for children to the axis-aligned generalization that adults show.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {727–735},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984174,
author = {Hein, Matthias},
title = {Robust Nonparametric Regression with Metric-Space Valued Output},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including several robust versions. Depending on the choice of the output space and the metric the estimator reduces to partially well-known procedures for multi-class classification, multivariate regression in Euclidean space, regression with manifold-valued output and even some cases of structured output learning. In this paper we focus on the case of regression with manifold-valued input and output. We show pointwise and Bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimators with experiments.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {718–726},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984173,
author = {Hazan, Elad and Kale, Satyen},
title = {On Stochastic and Worst-Case Models for Investing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight.In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {709–717},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984172,
author = {Hazan, Elad and Kale, Satyen},
title = {Online Submodular Minimization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and bandit settings.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {700–708},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984171,
author = {Guillory, Andrew and Bilmes, Jeff},
title = {Label Selection on Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph. We specifically study methods which choose a single batch of labeled vertices (i.e. offline, non sequential methods). In this setting, we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees. These methods bound prediction error in terms of the smoothness of the true labels with respect to the graph. Some of these bounds give new motivations for previously proposed algorithms, and some suggest new algorithms which we evaluate. We show improved performance over baseline methods on several real world data sets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {691–699},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984170,
author = {Grzegorczyk, Marco and Husmeier, Dirk},
title = {Non-Stationary Continuous Dynamic Bayesian Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many real-world scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-flexible models that lack any information sharing among time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary among segments, and in which a common network structure provides essential information sharing across segments. Our model is based on a Bayesian multiple change-point process, where the number and location of the change-points is sampled from the posterior distribution.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {682–690},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984169,
author = {Gretton, Arthur and Fukumizu, Kenji and Harchaoui, Zaid and Sriperumbudur, Bharath K.},
title = {A Fast, Consistent Kernel Two-Sample Test},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A kernel embedding of probability distributions into reproducing kernel Hilbert spaces (RKHS) has recently been proposed, which allows the comparison of two probability measures P and Q based on the distance between their respective embeddings: for a sufficiently rich RKHS, this distance is zero if and only if P and Q coincide. In using this distance as a statistic for a test of whether two samples are from different distributions, a major difficulty arises in computing the significance threshold, since the empirical statistic has as its null distribution (where P = Q) an infinite weighted sum of χ2 random variables. Prior finite sample approximations to the null distribution include using bootstrap resampling, which yields a consistent estimate but is computationally costly; and fitting a parametric model with the low order moments of the test statistic, which can work well in practice but has no consistency or accuracy guarantees. The main result of the present work is a novel estimate of the null distribution, computed from the eigen-spectrum of the Gram matrix on the aggregate sample from P and Q, and having lower computational cost than the bootstrap. A proof of consistency of this estimate is provided. The performance of the null distribution estimate is compared with the bootstrap and parametric approaches on an artificial example, high dimensional multivariate data, and text.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {673–681},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984168,
author = {Gra\c{c}a, Jo\~{a}o V. and Ganchev, Kuzman and Taskar, Ben and Pereira, Fernando},
title = {Posterior vs. Parameter Sparsity in Latent Variable Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of learning structured unsupervised models with moment sparsity typical in many natural language induction tasks. For example, in unsupervised part-of-speech (POS) induction using hidden Markov models, we introduce a bias for words to be labeled by a small number of tags. In order to express this bias of posterior sparsity as opposed to parametric sparsity, we extend the posterior regularization framework [7]. We evaluate our methods on three languages — English, Bulgarian and Portuguese — showing consistent and significant accuracy improvement over EM-trained HMMs, and HMMs with sparsity-inducing Dirichlet priors trained by variational EM. We increase accuracy with respect to EM by 2.3%-6.5% in a purely unsupervised setting as well as in a weakly-supervised setting where the closed-class words are provided. Finally, we show improvements using our method when using the induced clusters as features of a discriminative model in a semi-supervised setting.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {664–672},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984167,
author = {Gould, Stephen and Gao, Tianshi and Koller, Daphne},
title = {Region-Based Segmentation and Object Detection},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classification of many parts of the scene ambiguous.In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach simultaneously reasons about pixels, regions and objects in a coherent probabilistic model. Pixel appearance features allow us to perform well on classifying amorphous background classes, while the explicit representation of regions facilitate the computation of more sophisticated features necessary for object detection. Importantly, our model gives a single unified description of the scene—we explain every pixel in the image and enforce global consistency between all random variables in our model.We run experiments on the challenging Street Scene dataset [2] and show significant improvement over state-of-the-art results for object detection accuracy.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {655–663},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984166,
author = {Goodfellow, Ian J. and Le, Quoc V. and Saxe, Andrew M. and Lee, Honglak and Ng, Andrew Y.},
title = {Measuring Invariances in Deep Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of "deep" vs. "shallower" representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {646–654},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984165,
author = {Goldberger, Jacob and Leshem, Amir},
title = {A Gaussian Tree Approximation for Integer Least-Squares},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a finite set. The factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph. Hence, applying the Belief Propagation (BP) algorithm yields very poor results. The algorithm described here is based on an optimal tree approximation of the Gaussian density of the unconstrained linear system. It is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the BP algorithm to the modified factor graph outperforms current methods in terms of both performance and complexity. The improved performance of the proposed algorithm is demonstrated on the problem of MIMO detection.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {638–645},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984164,
author = {Ghebreab, S. and Smeulders, A. W. M. and Scholte, H. S. and Lamme, V. A. F.},
title = {A Biologically Plausible Model for Rapid Natural Image Identification},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Contrast statistics of the majority of natural images conform to a Weibull distribution. This property of natural images may facilitate efficient and very rapid extraction of a scene's visual gist. Here we investigated whether a neural response model based on the Weibull contrast distribution captures visual information that humans use to rapidly identify natural scenes. In a learning phase, we measured EEC activity of 32 subjects viewing brief Hashes of 700 natural scenes. From these neural measurements and the contrast statistics of the natural image stimuli, we derived an across subject Weibull response model. We used this model to predict the HKG responses to 100 new natural scenes and estimated which scene the subject viewed by finding the best match between the mode) predictions and the observed EEG responses. In almost 90 percent of the cases our model accurately predicted the observed scene. Moreover, in most failed cases, the scene mistaken for the observed scene was visually similar to the observed scene itself. Similar results were obtained in a separate experiment in which 16 other subjects where presented with artificial occlusion models of natural images. Together, these results suggest that Weibull contrast statistics of natural images contain a considerable amount of visual gist information to warrant rapid image identification.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {629–637},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984163,
author = {Gerwinn, Sebastian and Berens, Philipp and Bethge, Matthias},
title = {A Joint Maximum-Entropy Model for Binary Neural Population Patterns and Continuous Signals},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint second-order statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural firing patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and fitting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the firing patterns of neural ensembles and the stimuli they are processing.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {620–628},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984162,
author = {Gershman, Samuel J. and Vul, Edward and Tenenbaum, Joshua B.},
title = {Perceptual Multistability as Markov Chain Monte Carlo Inference},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While many perceptual and cognitive phenomena are well described in terms of Bayesian inference, the necessary computations are intractable at the scale of real-world tasks, and it remains unclear how the human mind approximates Bayesian computations algorithmically. We explore the proposal that for some tasks, humans use a form of Markov Chain Monte Carlo to approximate the posterior distribution over hidden variables. As a case study, we show how several phenomena of perceptual multistability can be explained as MCMC inference in simple graphical models for low-level vision.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {611–619},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984161,
author = {Germain, Pascal and Lacasse, Alexandre and Laviolette, Francois and Marchand, Mario and Shanian, Sara},
title = {From PAC-Bayes Bounds to KL Regularization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ℓp -regularized objective functions currently used, such as ridge regression and ℓp-regularized boosting, are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and Ada-Boost.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {603–610},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984160,
author = {Garcia, Eric K. and Gupta, Maya R.},
title = {Lattice Regression},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient implementation. Rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples. Experiments show that lattice regression can reduce mean test error by as much as 25% compared to Gaussian process regression (GPR) for digital color management of printers, an application for which linearly interpolating a look-up table is standard. Simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice. Surprisingly, in some cases the proposed method — although motivated by computational efficiency — performs better than directly applying GPR with no lattice at all.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {594–602},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984159,
author = {Gao, Jing and Liang, Feng and Fan, Wei and Sun, Yizhou and Han, Jiawei},
title = {Graph-Based Consensus Maximization among Multiple Supervised and Unsupervised Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Ensemble classifiers such as bagging, boosting and model averaging are known to have improved accuracy and robustness over a single model. Their potential, however, is limited in applications which have no access to raw data but to the meta-level model output. In this paper, we study ensemble learning with output from multiple supervised and unsupervised models, a topic where little work has been done. Although unsupervised models, such as clustering, do not directly generate label prediction for each individual, they provide useful constraints for the joint prediction of a set of related objects. We propose to consolidate a classification solution by maximizing the consensus among both supervised predictions and unsupervised constraints. We cast this ensemble task as an optimization problem on a bipartite graph, where the objective function favors the smoothness of the prediction over the graph, as well as penalizing deviations from the initial labeling provided by supervised models. We solve this problem through iterative propagation of probability estimates among neighboring nodes. Our method can also be interpreted as conducting a constrained embedding in a transformed space, or a ranking on the graph. Experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives1.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {585–593},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984158,
author = {Fujiwara, Yusuke and Miyawaki, Yoichi and Kamitani, Yukiyasu},
title = {Estimating Image Bases for Visual Image Reconstruction from Human Brain Activity},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-defined visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were fixed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {576–584},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984157,
author = {Fromer, Menachem and Globerson, Amir},
title = {An LP View of the M-Best MAP Problem},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of finding the M assignments with maximum probability in a probabilistic graphical model. We show how this problem can be formulated as a linear program (LP) on a particular polytope. We prove that, for tree graphs (and junction trees in general), this polytope has a particularly simple form and differs from the marginal polytope in a single inequality constraint. We use this characterization to provide an approximation scheme for non-tree graphs, by using the set of spanning trees over such graphs. The method we present puts the M-best inference problem in the context of LP relaxations, which have recently received considerable attention and have proven useful in solving difficult inference problems. We show empirically that our method often finds the provably exact M best configurations for problems of high tree-width.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {567–575},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984156,
author = {Fritz, Mario and Black, Michael and Bradski, Gary and Karayev, Sergey and Darrell, Trevor},
title = {An Additive Latent Feature Model for Transparent Object Recognition},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {558–566},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984155,
author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
title = {Sharing Features among Dynamical Systems with Beta Processes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a Bayesian nonparametric approach to the problem of modeling related time series. Using a beta process prior, our approach is based on the discovery of a set of latent dynamical behaviors that are shared among multiple time series. The size of the set and the sharing pattern are both inferred from data. We develop an efficient Markov chain Monte Carlo inference method that is based on the Indian buffet process representation of the predictive distribution of the beta process. In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals. We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {549–557},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984154,
author = {Fletcher, Alyson K. and Rangan, Sundeep},
title = {Orthogonal Matching Pursuit from Noisy Measurements: A New Analysis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A well-known analysis of Tropp and Gilbert shows that orthogonal matching pursuit (OMP) can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free linear measurements obtained through a random Gaussian measurement matrix with a probability that approaches one as n → ∞. This work strengthens this result by showing that a lower number of measurements, m = 2k log(n - k), is in fact sufficient for asymptotic recovery. More generally, when the sparsity level satisfies kmin ≤ k ≤ kmax but is unknown, m = 2kmax log(n - kmin) measurements is sufficient. Furthermore, this number of measurements is also sufficient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (SNR) scales to infinity. The scaling m = 2k log(n - k) exactly matches the number of measurements required by the more complex lasso method for signal recovery in a similar SNR scaling.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {540–548},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984153,
author = {Fidler, Sanja and Boben, Marko and Leonardis, Ale\v{s}},
title = {Evaluating Multi-Class Learning Strategies in a Hierarchical Framework for Object Detection},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Multi-class object learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time — but are complex to train. Conveniently, sequential class learning cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the shareability of features among object classes and might depend on ordering of classes during learning. In hierarchical frameworks these issues have been little explored. In this paper, we provide a rigorous experimental analysis of various multiple object class learning strategies within a generative hierarchical framework. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, and 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned object classes on several recognition datasets. We show that sequential training achieves the best trade-off between inference and training times at a comparable detection performance and could thus be used to learn the classes on a larger scale.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {531–539},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984152,
author = {Fergus, Rob and Weiss, Yair and Torralba, Antonio},
title = {Semi-Supervised Learning in Gigantic Image Collections},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. "Clean labels" can be manually obtained on a small fraction, "noisy labels" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {522–530},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984151,
author = {Fazli, Siamac and Grozea, Cristian and Dan\"{o}czy, M\'{a}rton and Popescu, Florin and Blankertz, Benjamin and M\"{u}ller, Klaus-Robert},
title = {Subject Independent EEG-Based BCI Decoding},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the quest to make Brain Computer Interfacing (BCI) more usable, dry electrodes have emerged that get rid of the initial 30 minutes required for placing an electrode cap. Another time consuming step is the required individualized adaptation to the BCI user, which involves another 30 minutes calibration for assessing a subject's brain signature. In this paper we aim to also remove this calibration proceedure from BCI setup time by means of machine learning. In particular, we harvest a large database of EEG BCI motor imagination recordings (83 subjects) for constructing a library of subject-specific spatio-temporal filters and derive a subject independent BCI classifier. Our offline results indicate that BCI-nat\"{\i}ve users could start real-time BCI use with no prior calibration at only a very moderate performance loss.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {513–521},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984150,
author = {Farias, Vivek F. and Jagabathula, Srikanth and Shah, Devavrat},
title = {A Data-Driven Approach to Modeling Choice},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We visit the following fundamental problem: For a 'generic' model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from offering a particular assortment of choices? This problem is central to areas within operations research, marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {504–512},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984149,
author = {Duchi, John and Singer, Yoram},
title = {Efficient Learning Using Forward-Backward Splitting},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1. We derive concrete and very simple algorithms for minimization of loss functions with ℓ1, ℓ2, ℓ22, and ℓ∞ regularization. We also show how to construct efficient algorithms for mixed-norm ℓ1/ℓq regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {495–503},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984148,
author = {Du, Lan and Ren, Lu and Dunson, David B. and Carin, Lawrence},
title = {A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efficiently via variational Bayesian analysis, with example results presented on two image databases.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {486–494},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984147,
author = {Doshi-Velez, Finale},
title = {The Infinite Partially Observable Markov Decision Process},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difficult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We define an infinite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {477–485},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984146,
author = {Dietz, Laura and Dallmeier, Valentin and Zeller, Andreas and Scheffer, Tobias},
title = {Localizing Bugs in Program Executions with Graphical Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the Bernoulli graph model on data of the software projects AspectJ and Rhino.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {468–476},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984145,
author = {Desai, Vijay V. and Farias, Vivek F. and Moallemi, Ciamac C.},
title = {A Smoothed Approximate Linear Program},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program - the 'smoothed approximate linear program' - relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {459–467},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984144,
author = {Dekel, Ofer},
title = {Distribution-Calibrated Hierarchical Classification},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While many advances have already been made in hierarchical classification learning, we take a step back and examine how a hierarchical classification problem should be formally defined. We pay particular attention to the fact that many arbitrary decisions go into the design of the label taxonomy that is given with the training data. Moreover, many hand-designed taxonomies are unbalanced and misrepresent the class structure in the underlying data distribution. We attempt to correct these problems by using the data distribution itself to calibrate the hierarchical classification loss function. This distribution-based correction must be done with care, to avoid introducing unmanageable statistical dependencies into the learning problem. This leads us off the beaten path of binomial-type estimation and into the unfamiliar waters of geometric-type estimation. In this paper, we present a new calibrated definition of statistical risk for hierarchical classification, an unbiased estimator for this risk, and a new algorithmic reduction from hierarchical classification to cost-sensitive classification.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {450–458},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984143,
author = {Dalalyan, Arnak S. and Keriven, Renaud},
title = {<i>L</i><sub>1</sub>-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new approach to the problem of robust estimation in multiview geometry. Inspired by recent advances in the sparse recovery problem of statistics, we define our estimator as a Bayesian maximum a posteriori with multivariate Laplace prior on the vector describing the outliers. This leads to an estimator in which the fidelity to the data is measured by the L∞-norm while the regularization is done by the L1-norm. The proposed procedure is fairly fast since the outlier removal is done by solving one linear program (LP). An important difference compared to existing algorithms is that for our estimator it is not necessary to specify neither the number nor the proportion of the outliers. We present strong theoretical results assessing the accuracy of our procedure, as well as a numerical example illustrating its efficiency on real data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {441–449},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984142,
author = {Cuturi, Marco and Vert, Jean-Philippe and d'Aspremont, Alexandre},
title = {White Functionals for Anomaly Detection in Dynamical Systems},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose new methodologies to detect anomalies in discrete-time processes taking values in a probability space. These methods are based on the inference of functionals whose evaluations on successive states visited by the process are stationary and have low autocorrelations. Deviations from this behavior are used to flag anomalies. The candidate functionals are estimated in a subspace of a reproducing kernel Hilbert space associated with the original probability space considered. We provide experimental results on simulated datasets which show that these techniques compare favorably with other algorithms.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {432–440},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984141,
author = {Culpepper, Benjamin J. and Olshausen, Bruno A.},
title = {Learning Transport Operators for Image Manifolds},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe an unsupervised manifold learning algorithm that represents a surface through a compact description of operators that traverse it. The operators are based on matrix exponentials, which are the solution to a system of first-order linear differential equations. The matrix exponents are represented by a basis that is adapted to the statistics of the data so that the infinitesimal generator for a trajectory along the underlying manifold can be produced by linearly composing a few elements. The method is applied to recover topological structure from low dimensional synthetic data, and to model local structure in how natural images change over time and scale.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {423–431},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984140,
author = {Crammer, Koby and Kulesza, Alex and Dredze, Mark},
title = {Adaptive Regularization of Weight Vectors},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present AROW, a new online learning algorithm that combines several useful properties: large margin training, confidence weighting, and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise. We derive a mistake bound, similar in form to the second order perceptron bound, that does not assume separability. We also relate our algorithm to recent confidence-weighted online learning techniques and show empirically that AROW achieves state-of-the-art performance and notable robustness in the case of non-separable data.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {414–422},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984139,
author = {Courville, Aaron C. and Eck, Douglas and Bengio, Yoshua},
title = {An Infinite Factor Model Hierarchy via a Noisy-or Mechanism},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {405–413},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984138,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
title = {Learning Non-Linear Combinations of Kernels},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. We analyze this problem in the case of regression and the kernel ridge regression algorithm. We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {396–404},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984137,
author = {Coquelin, Pierre-Arnaud and Deguest, Romain and Munos, R\'{e}mi},
title = {Sensitivity Analysis in HMMs with Application to Likelihood Maximization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Infinitesimal Perturbation Analysis (IPA) on the filtering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the filtering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles.We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {387–395},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984136,
author = {Conroy, Bryan R. and Singer, Benjamin D. and Haxby, James V. and Ramadge, Peter J.},
title = {FMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {378–386},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984135,
author = {Coen-Cagli, Ruben and Dayan, Peter and Schwartz, Odelia},
title = {Statistical Models of Linear and Non-Linear Contextual Interactions in Early Visual Processing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor-like receptive fields and divisive gain modulation from local surrounds; these have led to influential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non-linear correlations between the responses of spatially distributed Gabor-like receptive fields, which, when trained on an ensemble of natural scenes, unifies a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Li's (2002) hypothesis that V1 builds a saliency map, and fits data on the tilt illusion.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {369–377},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984134,
author = {Ci\'{e}men\c{c}on, St\'{e}phan and Depecker, Marine and Vayatis, Nicolas},
title = {AUC Optimization and the Two-Sample Problem},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The purpose of the paper is to explore the connection between multivariate homogeneity tests and AUC optimization. The latter problem has recently received much attention in the statistical learning literature. From the elementary observation that, in the two-sample problem setup, the null assumption corresponds to the situation where the area under the optimal ROC curve is equal to 1/2, we propose a two-stage testing method based on data splitting. A nearly optimal scoring function in the AUC sense is first learnt from one of the two half-samples. Data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage. The last step amounts to performing a standard Mann-Whitney Wilcoxon test in the one-dimensional framework. We show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power, provided the ranking produced is accurate enough in the AUC sense. The results of a numerical experiment are eventually displayed in order to show the efficiency of the method.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {360–368},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984133,
author = {Choi, Arthur and Darwiche, Adnan},
title = {Relax Then Compensate: On Max-Product Belief Propagation and More},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new perspective on approximations to the maximum a posteriori (MAP) task in probabilistic graphical models, that is based on simplifying a given instance, and then tightening the approximation. First, we start with a structural relaxation of the original model. We then infer from the relaxation its deficiencies, and compensate for them. This perspective allows us to identify two distinct classes of approximations. First, we find that max-product belief propagation can be viewed as a way to compensate for a relaxation, based on a particular idealized case for exactness. We identify a second approach to compensation that is based on a more refined idealized case, resulting in a new approximation with distinct properties. We go on to propose a new class of algorithms that, starting with a relaxation, iteratively seeks tighter approximations.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {351–359},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984132,
author = {Cho, Youngmin and Saul, Lawrence K.},
title = {Kernel Methods for Deep Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {342–350},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984131,
author = {Chin, Tat-Jun and Wang, Hanzi and Suter, David},
title = {The Ordered Residual Kernel for Robust Motion Subspace Clustering},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel and highly effective approach for multi-body motion segmentation. Drawing inspiration from robust statistical model fitting, we estimate putative subspace hypotheses from the data. However, instead of ranking them we encapsulate the hypotheses in a novel Mercer kernel which elicits the potential of two point trajectories to have emerged from the same subspace. The kernel permits the application of well-established statistical learning methods for effective outlier rejection, automatic recovery of the number of motions and accurate segmentation of the point trajectories. The method operates well under severe outliers arising from spurious trajectories or mistracks. Detailed experiments on a recent benchmark dataset (Hopkins 155) show that our method is superior to other state-of-the-art approaches in terms of recovering the number of motions, segmentation accuracy, robustness against gross outliers and computational efficiency.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {333–341},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984130,
author = {Chen, Ye and Kapralov, Michael and Pavlov, Dmitry and Canny, John F.},
title = {Factor Modeling for Advertisement Targeting},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We adapt a probabilistic latent variable model, namely GaP (Gamma-Poisson) [6], to ad targeting in the contexts of sponsored search (SS) and behaviorally targeted (BT) display advertising. We also approach the important problem of ad positional bias by formulating a one-latent-dimension GaP factorization. Learning from click-through data is intrinsically large scale, even more so for ads. We scale up the algorithm to terabytes of real-world SS and BT data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality. Specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the SS and BT implementations, respectively. Finally, we report the experimental results using Yahoo's vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy. For BT in particular, the ROC area achieved by GaP is exceeding 0.95, while one prior approach using Poisson regression [11] yielded 0.83. For computational performance, we compare a single-node sparse implementation with a parallel implementation using Hadoop MapReduce, the results are counterintuitive yet quite interesting. We therefore provide insights into the underlying principles of large-scale learning.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {324–332},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984129,
author = {Chen, Wei and Liu, Tie-Yan and Lan, Yanyan and Ma, Zhiming and Li, Hang},
title = {Ranking Measures and Loss Functions in Learning to Rank},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learning-to-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measure-based ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classification tasks, and define a so-called essential loss for ranking as the weighted sum of the classification errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modifications can lead to better ranking performances, demonstrating the correctness of our theoretical analysis.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {315–323},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984128,
author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
title = {An Online Algorithm for Large Scale Image Similarity Learning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. It stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, current approaches for learning similarity do not scale to large datasets, especially when imposing metric constraints on the learned similarity. We describe OASIS, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a standard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude. On 2.7 million images collected from the web, OASIS can be trained within 3 days on a single CPU. The non-metric similarities learned by OASIS can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by orders of magnitude than was handled before.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {306–314},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984127,
author = {Chaudhuri, Kamalika and Freund, Yoav and Hsu, Daniel},
title = {A Parameter-Free Hedging Algorithm},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large.In this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for DTOL. We introduce a new notion of regret, which is more natural for applications with a large number of actions. We show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {297–305},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984126,
author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M.},
title = {Reading Tea Leaves: How Humans Interpret Topic Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {288–296},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984125,
author = {Chai, Kian Ming A.},
title = {Generalization Errors and Learning Curves for Regression with Multi-Task Gaussian Processes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide some insights into how task correlations in multi-task Gaussian process (GP) regression affect the generalization error and the learning curve. We analyze the asymmetric two-tasks case, where a secondary task is to help the learning of a primary task. Within this setting, we give bounds on the generalization error and the learning curve of the primary task. Our approach admits intuitive understandings of the multi-task GP by relating it to single-task GPs. For the case of one-dimensional input-space under optimal sampling with data only for the secondary task, the limitations of multi-task GP can be quantified explicitly.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {279–287},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984124,
author = {Chai, Barry and Walther, Dirk B. and Beck, Diane M. and Fei-Fei, Li},
title = {Exploring Functional Connectivity of the Human Brain Using Multivariate Information Analysis},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this study, we present a new method for establishing fMRI pattern-based functional connectivity between brain regions by estimating their multivariate mutual information. Recent advances in the numerical approximation of high-dimensional probability distributions allow us to successfully estimate mutual information from scarce fMRI data. We also show that selecting voxels based on the multivariate mutual information of local activity patterns with respect to ground truth labels leads to higher decoding accuracy than established voxel selection methods. We validate our approach with a 6-way scene categorization fMRI experiment. Multivariate information analysis is able to find strong information sharing between PPA and RSC, consistent with existing neuroscience studies on scenes. Furthermore, an exploratory whole-brain analysis uncovered other brain regions that share information with the PPA-RSC scene network.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {270–278},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984123,
author = {Cevher, Volkan},
title = {Learning with Compressible Priors},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a set of probability distributions, dubbed compressible priors, whose independent and identically distributed (iid) realizations result in p-compressible signals. A signal x ∈ ℝN is called p-compressible with magnitude R if its sorted coefficients exhibit a power-law decay as |x|(i) ≲ R · i-d, where the decay rate d is equal to 1/p. p-compressible signals live close to K-sparse signals (K ≪ N) in the ℓr-norm (r &gt; p) since their best K-sparse approximation error decreases with O (R · K1/r-1/p). We show that the membership of generalized Pareto, Student's t, log-normal, Fr\'{e}chet, and log-logistic distributions to the set of compressible priors depends only on the distribution parameters and is independent of N. In contrast, we demonstrate that the membership of the generalized Gaussian distribution (GGD) depends both on the signal dimension and the GGD parameters: the expected decay rate of N-sample iid realizations from the GGD with the shape parameter q is given by 1/ [qlog (N/q)]. As stylized examples, we show via experiments that the wavelet coefficients of natural images are 1.67-compressible whereas their pixel gradients are 0.95 log (N/0.95)-compressible, on the average. We also leverage the connections between compressible priors and sparse signals to develop new iterative re-weighted sparse signal recovery algorithms that outperform the standard ℓ1-norm minimization. Finally, we describe how to learn the hyperparameters of compressible priors in underdetermined regression problems by exploiting the geometry of their order statistics during signal recovery.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {261–269},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984122,
author = {Cecchi, Guillermo A. and Rish, Irina and Thyreau, Benjamin and Thirion, Bertrand and Plaze, Marion and Paillere-Martinot, Marie-Laure and Martelli, Catherine and Martinot, Jean-Luc and Poline, Jean-Baptiste},
title = {Discriminative Network Models of Schizophrenia},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, "emergent" working of the brain. We propose a novel data-driven approach to capture emergent features using functional brain networks [4] extracted from fMRI data, and demonstrate its advantage over traditional region-of-interest (ROI) and local, task-specific linear activation analyzes. Our results suggest that schizophrenia is indeed associated with disruption of global brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns. Moreover, further exploitation of interactions by sparse Markov Random Field classifiers shows clear gain over linear methods, such as Gaussian Naive Bayes and SVM, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fMRI experiment using a simple auditory task.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {252–260},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984121,
author = {Cayton, Lawrence},
title = {Efficient Bregman Range Search},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop an algorithm for efficient range search when the notion of dissimilarity is given by a Bregman divergence. The range search task is to return all points in a potentially large database that are within some specified distance of a query. It arises in many learning algorithms such as locally-weighted regression, kernel density estimation, neighborhood graph-based algorithms, and in tasks like outlier detection and information retrieval. In metric spaces, efficient range search-like algorithms based on spatial data structures have been deployed on a variety of statistical tasks. Here we describe an algorithm for range search for an arbitrary Bregman divergence. This broad class of dissimilarity measures includes the relative entropy, Mahalanobis distance, Itakura-Saito divergence, and a variety of matrix divergences. Metric methods cannot be directly applied since Bregman divergences do not in general satisfy the triangle inequality. We derive geometric properties of Bregman divergences that yield an efficient algorithm for range search based on a recently proposed space decomposition for Bregman divergences.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {243–251},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984120,
author = {Cavagnaro, Daniel R. and Pitt, Mark A. and Myung, Jay I.},
title = {Adaptive Design Optimization in Experiments with People},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efficiently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {234–242},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984119,
author = {Caron, Fran\c{c}ois and Doucet, Arnaud},
title = {Bayesian Nonparametric Models on Decomposable Graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. These models are attractive because they ensure exchangeability (over samples). We propose here extensions of these models where the dependency between samples is given by a known decomposable graph. These models have appealing properties and can be easily learned using Monte Carlo techniques.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {225–233},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984118,
author = {Carbonetto, Peter and King, Matthew and Hamze, Firas},
title = {A Stochastic Approximation Method for Inference in Probabilistic Graphical Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We describe a new algorithmic framework for inference in probabilistic models, and apply it to inference for latent Dirichlet allocation (LDA). Our framework adopts the methodology of variational inference, but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions. Our approach can also be viewed as a "population-based" sequential Monte Carlo (SMC) method, but unlike existing SMC methods there is no need to design the artificial sequence of distributions. Significantly, our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation. We conduct experiments on a difficult inference problem in population genetics, a problem that is related to inference for LDA. The results of these experiments suggest that our method can offer improvements in stability and accuracy over existing methods, and at a comparable cost.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {216–224},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984117,
author = {Campbell, W. M. and Karam, Z. N. and Sturim, D. E.},
title = {Speaker Comparison with Inner Product Discriminant Functions},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Speaker comparison, the process of finding the speaker similarity between two speech signals, occupies a central role in a variety of applications—speaker verification, clustering, and identification. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process. For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM). Speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models. We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison—support vector machines, joint factor analysis, and linear scoring. The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods. Compensation of nuisances is performed via linear transforms on GMM parameter vectors. Using the IPDF framework, we show that many current techniques are simple variations of each other. We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require significantly less computation than current techniques.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {207–215},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984116,
author = {Cai, Chenghui and Carin, Lawrence},
title = {Learning to Explore and Exploit in POMDPs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the specific problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {198–206},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984115,
author = {Bush, Keith and Pineau, Joelle},
title = {Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning. We experiment with manifold embeddings to reconstruct the observable state-space in the context of offline, model-based reinforcement learning. We demonstrate that the embedding of a system can change as a result of learning, and we argue that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system. We apply this approach to learn a neurostimulation policy that suppresses epileptic seizures on animal brain slices.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {189–197},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984114,
author = {Brasselet, Romain and Johansson, Roland S. and Arleo, Angelo},
title = {Optimal Context Separation of Spiking Haptic Signals by Second-Order Somatosensory Neurons},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order so-matosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {180–188},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984113,
author = {Br\"{u}ckner, Michael and Scheffer, Tobias},
title = {Nash Equilibria of Static Prediction Games},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The standard assumption of identically distributed training and test data is violated when an adversary can exercise some control over the generation of the test data. In a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data. We study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic. We identify conditions under which the prediction game has a unique Nash equilibrium, and derive algorithms that will find the equilibrial prediction models. In a case study, we explore properties of Nash-equilibrial prediction models for email spam filtering empirically.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {171–179},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984112,
author = {Bouvrie, Jake and Rosasco, Lorenzo and Poggio, Tomaso},
title = {On Invariance in Hierarchical Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A goal of central importance in the study of hierarchical models for object recognition - and indeed the mammalian visual cortex - is that of understanding quantitatively the trade-off between invariance and selectivity, and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data. In this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models. We show that by taking an algebraic perspective, one can provide a concise set of conditions which must be met to establish invariance, as well as a constructive prescription for meeting those conditions. Analyses in specific cases of particular relevance to computer vision and text processing are given, yielding insight into how and when invariance can be achieved. We find that the minimal intrinsic properties of a hierarchical model needed to support a particular invariance can be clearly described, thereby encouraging efficient computational implementations.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {162–170},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984111,
author = {Boutsidis, Christos and Mahoney, Michael W. and Drineas, Petros},
title = {Unsupervised Feature Selection for the <i>k</i>-Means Clustering Problem},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel feature selection algorithm for the k-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter ∊ ∈ (0,1), selects and appropriately rescales in an unsupervised manner Θ(k log(k/∊)/∊2) features from a dataset of arbitrary dimensions. We prove that, if we run any γ-approximate k-means algorithm (γ ≥ 1) on the features selected using our method, we can find a (1 + (1 + ∊) ≥)-approximate partition with high probability.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {153–161},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984110,
author = {Bouchard-C\^{o}t\'{e}, Alexandre and Petrov, Slav and Klein, Dan},
title = {Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pruning can massively accelerate the computation of feature expectations in large models. However, any single pruning mask will introduce bias. We present a novel approach which employs a randomized sequence of pruning masks. Formally, we apply auxiliary variable MCMC sampling to generate this sequence of masks, thereby gaining theoretical guarantees about convergence. Because each mask is generally able to skip large portions of an underlying dynamic program, our approach is particularly compelling for high-degree algorithms. Empirically, we demonstrate our method on bilingual parsing, showing decreasing bias as more masks are incorporated, and outperforming fixed tic-tac-toe pruning.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {144–152},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984109,
author = {Bo, Liefeng and Sminchisescu, Cristian},
title = {Efficient Match Kernels between Sets of Features for Visual Recognition},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classifiers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels for large datasets due to their significant computational cost. To address this problem, we propose efficient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a set-level feature. The local feature maps are learned so their inner products preserve, to the best possible, the values of the specified kernel function. Classifiers based on EMK are linear both in the number of images and in the number of local features. We demonstrate that EMK are extremely efficient and achieve the current state of the art in three difficult computer vision datasets: Scene-15, Caltech-101 and Caltech-256.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {135–143},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984108,
author = {Blaschko, Matthew B. and Shelton, Jacquelyn A. and Bartels, Andreas},
title = {Augmenting Feature-Driven FMRI Analyses: Semi-Supervised Learning and Resting State Activity},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Resting state activity is brain activation that arises in the absence of any task, and is usually measured in awake subjects during prolonged fMRI scanning sessions where the only instruction given is to close the eyes and do nothing. It has been recognized in recent years that resting state activity is implicated in a wide variety of brain function. While certain networks of brain areas have different levels of activation at rest and during a task, there is nevertheless significant similarity between activations in the two cases. This suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting. We evaluate this setting empirically yielding three main results: (i) regression tends to be improved by the use of Laplacian regularization even when no additional unlabeled data are available, (ii) resting state data seem to have a similar marginal distribution to that recorded during the execution of a visual processing task implying largely similar types of activation, and (iii) this source of information can be broadly exploited to improve the robustness of empirical inference in fMRI studies, an inherently data poor domain.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {126–134},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984107,
author = {Bian, Wei and Tao, Dacheng},
title = {Manifold Regularization for SIR with Rate Root-n Convergence},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {117–125},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984106,
author = {Berkes, Pietro and White, Benjamin L. and Fiser, J\'{o}zsef},
title = {No Evidence for Active Sparsification in the Visual Cortex},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {108–116},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984105,
author = {Bergstra, James and Bengio, Yoshua},
title = {Slow, Decorrelated Features for Pretraining Complex Cell-like Networks},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1. A single-hidden-layer neural network of this kind of model achieves 1.50% error on MNIST. We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models. This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells. With this pretraining, the same single-hidden-layer model achieves 1.34% error, even though the pretraining sample distribution is very different from the fine-tuning distribution. To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {99–107},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984104,
author = {Berens, Philipp and Gerwinn, Sebastian and Ecker, Alexander S. and Bethge, Matthias},
title = {Neurometric Function Analysis of Population Codes},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using Fisher Information. Here, we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the Jensen-Shannon information to study neural population codes. We first explore the relationship between minimum discrimination error, Jensen-Shannon Information and Fisher Information and show that the discrimination framework is more informative about the coding accuracy than Fisher Information as it defines an error for any pair of possible stimuli. In particular, it includes Fisher Information as a special case. Second, we use the framework to study population codes of angular variables. Specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is, for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {90–98},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984103,
author = {Bengio, Samy and Pereira, Fernando and Singer, Yoram and Strelow, Dennis},
title = {Group Sparse Coding},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {82–89},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984102,
author = {Bejan, Cosmin Adrian and Titsworth, Matthew and Hickl, Andrew and Harabagiu, Sanda},
title = {Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a sequence of unsupervised, nonparametric Bayesian models for clustering complex linguistic objects. In this approach, we consider a potentially infinite number of features and categorical outcomes. We evaluated these models for the task of within- and cross-document event coreference on two corpora. All the models we investigated show significant improvements when compared against an existing baseline for this task.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {73–81},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984101,
author = {Bai, Bing and Sadamasa, Kunihiko and Weston, Jason and Qi, Yanjun and Grangier, David and Cortes, Corinna and Collobert, Ronan and Mohri, Mehryar},
title = {Polynomial Semantic Indexing},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a class of nonlinear (polynomial) models that are discriminatively trained to directly map from the word content in a query-document or document-document pair to a ranking score. Dealing with polynomial models on word features is computationally challenging. We propose a low-rank (but diagonal preserving) representation of our polynomial models to induce feasible memory and computation requirements. We provide an empirical study on retrieval tasks based on Wikipedia documents, where we obtain state-of-the-art performance while providing realistically scalable methods.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {64–72},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984100,
author = {Arora, Raman},
title = {On Learning Rotations},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the von Neumann divergence. The multiplicative updates are exponentiated skew-symmetric matrices which comprise the Lie algebra of the rotation group. The orthonormality and unit determinant of the matrix parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to intuitive interpretation in terms of the differential geometry of the manifold associated with the rotation group. A complexity reduction result is presented that exploits the eigenstructure of the matrix updates to simplify matrix exponentiation to a quadratic form.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {55–63},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984099,
author = {Arlot, Sylvain and Bach, Francis},
title = {Data-Driven Calibration of Linear Estimators with Minimal Penalties},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows' CL penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {46–54},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984098,
author = {Anati, Roy and Daniilidis, Kostas},
title = {Constructing Topological Maps Using Markov Random Fields and Loop-Closure Detection},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a system which constructs a topological map of an environment given a sequence of images. This system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously. Additionally, an MRF is constructed to model the probability of loop-closures. A locally optimal labeling is found using Loopy-BP. Finally we outline a method to generate a topological map from loop closure data. Results, presented on four urban sequences and one indoor sequence, outperform the state of the art.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {37–45},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984097,
author = {Amini, Massih R. and Usunier, Nicolas and Goutte, Cyril},
title = {Learning from Multiple Partially Observed Views -an Application to Multilingual Text Categorization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples. We assume the existence of view generating functions which may complete the missing views in an approximate way. This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages. In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages. We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning. An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning. Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {28–36},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984096,
author = {Allen, Martin and Zilberstein, Shlomo},
title = {Complexity of Decentralized Control: Special Cases},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {19–27},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984095,
author = {Ailon, Nir and Jaiswal, Ragesh and Monteleoni, Claire},
title = {Streaming <i>k</i>-Means Approximation},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are: a derivation of an extremely simple pseudo-approximation batch algorithm for k-means (based on the recent k-means++), in which the algorithm is allowed to output more than k centers, and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data reveal the practical utility of our method.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {10–18},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.5555/2984093.2984094,
author = {Agarwal, Alekh and Bartlett, Peter and Ravikumar, Pradeep and Wainwright, Martin J.},
title = {Information-Theoretic Lower Bounds on the Oracle Complexity of Convex Optimization},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite a large literature on upper bounds on complexity of convex optimization, relatively less attention has been paid to the fundamental hardness of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining a understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for various function classes. We also discuss implications of these results for the understanding the inherent complexity of large-scale learning and estimation problems.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1–9},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@proceedings{10.5555/2984093,
title = {NIPS'09: Proceedings of the 22nd International Conference on Neural Information Processing Systems},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
location = {Vancouver, British Columbia, Canada}
}

