@inproceedings{10.5555/2969830.2969931,
author = {Cowan, J. D.},
title = {Neural Networks: The Early Days},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A short account is given of various investigations of neural network properties, beginning with the classic work of McCulloch &amp; Pitts. Early work on neurodynamics and statistical mechanics, analogies with magnetic materials, fault tolerance via parallel distributed processing, memory, learning, and pattern recognition, is described.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {828–830},
numpages = {3},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969930,
author = {Smotroff, Ira G.},
title = {Dataflow Architectures: Flexible Platforms for Neural Network Simulation},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Dataflow architectures are general computation engines optimized for the execution of fine-grain parallel algorithms. Neural networks can be simulated on these systems with certain advantages. In this paper, we review dataflow architectures, examine neural network simulation performance on a new generation dataflow machine, compare that performance to other simulation alternatives, and discuss the benefits and drawbacks of the dataflow approach.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {818–825},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969929,
author = {N\'{u}\~{n}ez, Fernando J. and Fortes, Jose A. B.},
title = {Performance of Connectionist Learning Algorithms on 2-D SIMD Processor Arrays},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The mapping of the back-propagation and mean field theory learning algorithms onto a generic 2-D SIMD computer is described. This architecture proves to be very adequate for these applications since efficiencies close to the optimum can be attained. Expressions to find the learning rates are given and then particularized to the DAP array procesor.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {810–817},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969928,
author = {Zhang, Xiru and Mckenna, Michael and Mesirov, Jill P. and Waltz, David L.},
title = {An Efficient Implementation of the Back-Propagation Algorithm on the Connection Machine CM-2},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we present a novel implementation of the widely used Back-propagation neural net learning algorithm on the Connection Machine CM-2 - a general purpose, massively parallel computer with a hypercube topology. This implementation runs at about 180 million interconnections per second (IPS) on a 64K processor CM- 2. The main interprocessor communication operation used is 2D nearest neighbor communication. The techniques developed here can be easily extended to implement other algorithms for layered neural nets on the CM-2, or on other massively parallel computers which have 2D or higher degree connections among their processors.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {801–809},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969927,
author = {Chiueh, Tzi-Dar and Goodman, Rodney M.},
title = {VLSI Implementation of a High-Capacity Neural Network Associative Memory},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we describe the VLSI design and testing of a high capacity associative memory which we call the exponential correlation associative memory (ECAM). The prototype 3µ-CMOS programmable chip is capable of storing 32 memory patterns of 24 bits each. The high capacity of the ECAM is partly due to the use of special exponentiation neurons, which are implemented via sub-threshold MOS transistors in this design. The prototype chip is capable of performing one associative recall in 3 µs.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {793–800},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969926,
author = {Brownlow, Michael and Tarassenko, Lionel and Murray, Alan F. and Hamilton, Alister and Han, II Song and Reekie, H. Martin},
title = {Pulse-Firing Neural Chips for Hundreds of Neurons},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We announce new CMOS synapse circuits using only three and four MOSFETs/synapse. Neural states are asynchronous pulse streams, upon which arithmetic is performed directly. Chips implementing over 100 fully programmable synapses are described and projections to networks of hundreds of neurons are made.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {785–792},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969925,
author = {Platt, John C.},
title = {Analog Circuits for Constrained Optimization},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper explores whether analog circuitry can adequately perform constrained optimization. Constrained optimization circuits are designed using the differential multiplier method. These circuits fulfill time-varying constraints correctly. Example circuits include a quadratic programming circuit and a constrained flip-flop.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {777–784},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969924,
author = {Moopenn, A. and Duong, T. and Thakoor, A. P.},
title = {Digital-Analog Hybrid Synapse Chips for Electronic Neural Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Cascadable, CMOS synapse chips containing a cross-bar array of 32\texttimes{}32 (1024) programmable synapses have been fabricated as "building blocks" for fully parallel implementation of neural networks. The synapses are based on a hybrid digital-analog design which utilizes on-Chip 7-bit data latches to store quantized weights and two-quadrant multiplying DAC's to compute weighted outputs. The synapses exhibit 6-bit resolution and excellent monotonicity and consistency in their transfer characteristics. A 64-neuron hardware incorporating four synapse chips has been fabricated to investigate the performance of feedback networks in optimization problem solving. In this study, a 7\texttimes{}7, one-to-one assignment net and the Hop field-Tank 8-city traveling salesman problem net have been implemented in the hardware. The network's ability to obtain optimum or near optimum solutions in real time has been demonstrated.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {769–776},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969923,
author = {Satyanarayana, Srinagesh and Tsividis, Yannis and Graf, Hans Peter},
title = {A Reconfigurable Analog VLSI Neural Network Chip},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {1024 distributed-neuron synapses have been integrated in an active area of 6.1 mm \texttimes{} 3.3 mm using a 0.9 µm, double-metal, single-poly, n-well CMOS technology. The distributed-neuron synapses are arranged in blocks of 16, which we call '4 \texttimes{} 4 tiles'. Switch matrices are interleaved between each of these tiles to provide programmability of interconnections. With a small area overhead (15 %), the 1024 units of the network can be rearranged in various configurations. Some of the possible configurations are, a 12-32-12 network, a 16-12-12-16 network, two 12-32 networks etc. (the numbers separated by dashes indicate the number of units per layer, including the input layer). Weights are stored in analog form on MOS capacitors. The synaptic weights are usable to a resolution of 1 % of their full scale value. The limitation arises due to charge injection from the access switch and charge leakage. Other parameters like gain and shape of nonlinearity are also programmable.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {758–768},
numpages = {11},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969922,
author = {Koch, Christof and Bair, Wyeth and Harris, John G. and Horiuchi, Timothy and Hsu, Andrew and Luo, Jin},
title = {Real-Time Computer Vision and Robotics Using Analog VLSI Circuits},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The long-term goal of our laboratory is the development of analog resistive network-based VLSI implementations of early and intermediate vision algorithms. We demonstrate an experimental circuit for smoothing and segmenting noisy and sparse depth data using the resistive fuse and a 1-D edge-detection circuit for computing zero-crossings using two resistive grids with different space-constants. To demonstrate the robustness of our algorithms and of the fabricated analog CMOS VLSI chips, we are mounting these circuits onto small mobile vehicles operating in a real-time, laboratory environment.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {750–755},
numpages = {6},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969921,
author = {DeWeerth, Stephen P. and Mead, Carver A.},
title = {An Analog VLSI Model of Adaptation in the Vestibulo-Ocular Reflex},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The vestibulo-ocular reflex (VOR) is the primary mechanism that controls the compensatory eye movements that stabilize retinal images during rapid head motion. The primary pathways of this system are feed-forward, with inputs from the semicircular canals and outputs to the oculomotor system. Since visual feedback is not used directly in the VOR computation, the system must exploit motor learning to perform correctly. Lisberger(1988) has proposed a model for adapting the VOR gain using image-slip information from the retina. We have designed and tested analog very largescale integrated (VLSI) circuitry that implements a simplified version of Lisberger's adaptive VOR model.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {742–749},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969920,
author = {Krogh, Anders and Thorbergsson, G. I. and Hertz, John A.},
title = {A Cost Function for Internal Representations},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a cost function for learning in feed-forward neural networks which is an explicit function of the internal representation in addition to the weights. The learning problem can then be formulated as two simple perceptrons and a search for internal representations. Back-propagation is recovered as a limit. The frequency of successful solutions is better for this algorithm than for back-propagation when weights and hidden units are updated on the same timescale i.e. once every learning step.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {733–740},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969919,
author = {Baldi, Pierre and Rinott, Yosef and Stein, Charles},
title = {On the Distribution of the Number of Local Minima of a Random Function on a Graph},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {727–732},
numpages = {6},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969918,
author = {Intrator, Nathan},
title = {A Neural Network for Feature Extraction},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The paper suggests a statistical framework for the parameter estimation problem associated with unsupervised learning in a neural network, leading to an exploratory projection pursuit network that performs feature extraction, or dimensionality reduction.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {719–726},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969917,
author = {Pineda, Fernando J.},
title = {Time Dependent Adaptive Neural Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A comparison of algorithms that minimize error functions to train the trajectories of recurrent networks, reveals how complexity is traded off for causality. These algorithms are also related to time-independent formalisms. It is suggested that causal and scalable algorithms are possible when the activation dynamics of adaptive neurons is fast compared to the behavior to be learned. Standard continuous-time recurrent backpropagation is used in an example.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {710–718},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969916,
author = {Obradovic, Zoran and Parberry, Ian},
title = {Analog Neural Networks of Limited Precision I: Computing with Multilinear Threshold Functions (Preliminary Version)},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Experimental evidence has shown analog neural networks to be extremely fault-tolerant; in particular, their performance does not appear to be significantly impaired when precision is limited. Analog neurons with limited precision essentially compute k-ary weighted multilinear threshold functions, which divide Rn into k regions with k-1 hyperplanes. The behaviour of k-ary neural networks is investigated. There is no canonical set of threshold values for k&gt;3, although they exist for binary and ternary neural networks. The weights can be made integers of only O((z+k) log (z+k)) bits, where z is the number of processors, without increasing hardware or running time. The weights can be made ±1 while increasing running time by a constant multiple and hardware by a small polynomial in z and k. Binary neurons can be used if the running time is allowed to increase by a larger constant multiple and the hardware is allowed to increase by a slightly larger polynomial in z and k. Any symmetric k-ary function can be computed in constant depth and size O(nk-1/(k-2)!), and any k-ary function can be computed in constant depth and size O(nkn). The alternating neural networks of Olafsson and Abu-Mostafa, and the quantized neural networks of Fleisher are closely related to this model.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {702–709},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969915,
author = {MacKay, David J. C. and Miller, Kenneth D.},
title = {Analysis of Linsker's Simulations of Hebbian Rules},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Linsker has reported the development of centre--surround receptive fields and oriented receptive fields in simulations of a Hebb-type equation in a linear network. The dynamics of the learning rule are analysed in terms of the eigenvectors of the covariance matrix of cell activities. Analytic and computational results for Linsker's covariance matrices, and some general theorems, lead to an explanation of the emergence of centre--surround and certain oriented structures.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {694–701},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969914,
author = {Barto, A. G. and Sutton, R. S. and Watkins, C. J. C. H.},
title = {Sequential Decision Problems and Neural Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Decision making tasks that involve delayed consequences are very common yet difficult to address with supervised learning methods. If there is an accurate model of the underlying dynamical system, then these tasks can be formulated as sequential decision problems and solved by Dynamic Programming. This paper discusses reinforcement learning in terms of the sequential decision framework and shows how a learning algorithm similar to the one implemented by the Adaptive Critic Element used in the pole-balancer of Barto, Sutton, and Anderson (1983), and further developed by Sutton (1984), fits into this framework. Adaptive neural networks can play significant roles as modules for approximating the functions required for solving sequential decision problems.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {686–693},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969913,
author = {Baum, Erice B.},
title = {The Perceptron Algorithm is Fast for Non-Malicious Distributions},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Within the context of Valiant's protocol for learning, the Perceptron algorithm is shown to learn an arbitrary half-space in time O(n2/ε3) if D, the probability distribution of examples, is taken uniform over the unit sphere Sn. Here ε is the accuracy parameter. This is surprisingly fast, as "standard" approaches involve solution of a linear programming problem involving Ω(n/ε) constraints in n dimensions. A modification of Valiant's distribution independent protocol for learning is proposed in which the distribution and the function to be learned may be chosen by adversaries, however these adversaries may not communicate. It is argued that this definition is more reasonable and applicable to real world learning than Valiant's. Under this definition, the Perceptron algorithm is shown to be a distribution independent learning algorithm. In an appendix we show that, for uniform distributions, some classes of infinite V-C dimension including convex sets and a class of nested differences of convex sets are learnable.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {676–685},
numpages = {10},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969912,
author = {Dembo, Amir and Siu, Kai-Yeung and Kailath, Thomas},
title = {Complexity of Finite Precision Neural Network Classifier},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A rigorous analysis on the finite precision computational aspects of neural network as a pattern classifier via a probabilistic approach is presented. Even though there exist negative results on the capability of perceptron, we show the following positive results: Given n pattern vectors each represented by cn bits where c &gt; 1, that are uniformly distributed, with high probability the perceptron can perform all possible binary classifications of the patterns. Moreover, the resulting neural network requires a vanishingly small proportion O(log n/n) of the memory that would be required for complete storage of the patterns. Further, the perceptron algorithm takes O(n2) arithmetic operations with high probability, whereas other methods such as linear programming takes O(n3.5) in the worst case. We also indicate some mathematical connections with VLSI circuit testing and the theory of random matrices.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {668–675},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969911,
author = {Geiger, Davi and Girosi, Federico},
title = {Coupled Markov Random Fields and Mean Field Theory},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In recent years many researchers have investigated the use of Markov Random Fields (MRFs) for computer vision. They can be applied for example to reconstruct surfaces from sparse and noisy depth data coming from the output of a visual process, or to integrate early vision processes to label physical discontinuities. In this paper we show that by applying mean field theory to those MRFs models a class of neural networks is obtained. Those networks can speed up the solution for the MRFs models. The method is not restricted to computer vision.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {660–667},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969910,
author = {Lincoln, William P. and Skrzypek, Josef},
title = {Synergy of Clustering Multiple Back Propagation Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The properties of a cluster of multiple back-propagation (BP) networks are examined and compared to the performance of a single BP network. The underlying idea is that a synergistic effect within the cluster improves the performance and fault tolerance. Five networks were initially trained to perform the same input-output mapping. Following training, a cluster was created by computing an average of the outputs generated by the individual networks. The output of the cluster can be used as the desired output during training by feeding it back to the individual networks. In comparison to a single BP network, a cluster of multiple BP's generalization and significant fault tolerance. It appear that cluster advantage follows from simple maxim "you can fool some of the single BP's in a cluster all of the time but you cannot fool all of them all of the time" {Lincoln}.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {650–657},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969909,
author = {Chauvin, Yves},
title = {Dynamic Behavior of Constrained Back-Propagation Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The learning dynamics of the back-propagation algorithm are investigated when complexity constraints are added to the standard Least Mean Square (LMS) cost function. It is shown that loss of generalization performance due to overtraining can be avoided when using such complexity constraints. Furthermore, "energy," hidden representations and weight distributions are observed and compared during learning. An attempt is made at explaining the results in terms of linear and non-linear effects in relation to the gradient descent learning algorithm.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {642–649},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969908,
author = {Zipser, David},
title = {Subgrouping Reduces Complexity and Speeds up Learning in Recurrent Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {638–641},
numpages = {4},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969907,
author = {Morgan, N. and Bourlard, H.},
title = {Generalization and Parameter Estimation in Feedforward Nets: Some Experiments},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {630–637},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969906,
author = {Atlas, Les and Cole, Ronald and Connor, Jerome and EI-Sharkawi, Mohamed and Marks, Robert J. and Muthusamy, Yeshwant and Barnard, Etienne},
title = {Performance Comparisons between Backpropagation Networks and Classification Trees on Three Real-World Applications},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {622–629},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969905,
author = {Gish, Sheri L. and Blanz, W. E.},
title = {Comparing the Performance of Connectionist and Statistical Classifiers on an Image Segmentation Problem},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the development of an image segmentation system for real time image processing applications, we apply the classical decision analysis paradigm by viewing image segmentation as a pixel classification task. We use supervised training to derive a classifier for our system from a set of examples of a particular pixel classification problem. In this study, we test the suitability of a connectionist method against two statistical methods, Gaussian maximum likelihood classifier and first, second, and third degree polynomial classifiers, for the solution of a "real world" image segmentation problem taken from combustion research. Classifiers are derived using all three methods, and the performance of all of the classifiers on the training data set as well as on 3 separate entire test images is measured.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {614–621},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969904,
author = {Ahmad, Subutai and Tesauro, Gerald and He, Yu},
title = {Asymptotic Convergence of Backpropagation: Numerical Experiments},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have calculated, both analytically and in simulations, the rate of convergence at long times in the backpropagation learning algorithm for networks with and without hidden units. Our basic finding for units using the standard sigmoid transfer function is 1/t convergence of the error for large t, with at most logarithmic corrections for networks with hidden units. Other transfer functions may lead to a slower polynomial rate of convergence. Our analytic calculations were presented in (Tesauro, He &amp; Ahamd, 1989). Here we focus in more detail on our empirical measurements of the convergence rate in numerical simulations, which confirm our analytic results.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {606–613},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969903,
author = {Le Cun, Yann and Denker, John S. and Solla, Sara A.},
title = {Optimal Brain Damage},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {598–605},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969902,
author = {Atiya, Amir and Abu-Mostafa, Yaser},
title = {A Method for the Associative Storage of Analog Vectors},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A method for storing analog vectors in Hopfield's continuous feedback model is proposed. By analog vectors we mean vectors whose components are real-valued. The vectors to be stored are set as equilibria of the network. The network model consists of one layer of visible neurons and one layer of hidden neurons. We propose a learning algorithm, which results in adjusting the positions of the equilibria, as well as guaranteeing their stability. Simulation results confirm the effectiveness of the method.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {590–595},
numpages = {6},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969901,
author = {Zak, Michail and Toornarian, Nikzad},
title = {Unsupervised Learning in Neurodynamics Using the Phase Velocity Field Approach},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new concept for unsupervised learning based upon examples introduced to the neural network is proposed. Each example is considered as an interpolation node of the velocity field in the phase space. The velocities at these nodes are selected such that all the streamlines converge to an attracting set imbedded in the subspace occupied by the cluster of examples. The synaptic interconnections are found from learning procedure providing selected field. The theory is illustrated by examples.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {583–589},
numpages = {7},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969900,
author = {Nowlan, Steven J.},
title = {Maximum Likelihood Competitive Learning},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One popular class of unsupervised algorithms are competitive algorithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view competitive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maximum likelihood fit of a model of this type suggests a "softer" form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, placement of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {574–582},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969899,
author = {Atlas, Les and Cohn, David and Ladner, Richard and El-Sharkawi, M. A. and Marks, R. J. and Aggoune, M. E. and Park, D. C.},
title = {Training Connectionist Networks with Queries and Selective Sampling},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {"Selective sampling" is a form of directed search that can greatly increase the ability of a connectionist network to generalize accurately. Based on information from previous batches of samples, a network may be trained on data selectively sampled from regions in the domain that are unknown. This is realizable in cases when the distribution is known, or when the cost of drawing points from the target distribution is negligible compared to the cost of labeling them with the proper classification. The approach is justified by its applicability to the problem of training a network for power system security analysis. The benefits of selective sampling are studied analytically, and the results are confirmed experimentally.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {566–573},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969898,
author = {Rohwer, Richard},
title = {The 'moving Targets' Training Algorithm},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {558–565},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969897,
author = {Ackley, David H. and Littman, Michael L.},
title = {Generalization and Scaling in Reinforcement Learning},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In associative reinforcement learning, an environment generates input vectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output pairs. The task is to discover and remember input-output pairs that generate rewards. Especially difficult cases occur when rewards are rare, since the expected time for any algorithm can grow exponentially with the size of the problem. Nonetheless, if a reinforcement function possesses regularities, and a learning algorithm exploits them, learning time can be reduced below that of non-generalizing algorithms. This paper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {550–557},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969896,
author = {Kassebaum, John and Tenorio, Manoel Fernando and Schaefers, Christoph},
title = {The Cocktail Party Problem: Speech/Data Signal Separation Comparison between Back Propagation and SONN},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and compares its performance with Back Propagation in a signal separation application. The problem is to separate two signals; a modem data signal and a male speech signal, added and transmitted through a 4 khz channel. The signals are sampled at 8 khz, and using supervised learning, an attempt is made to reconstruct them. The SONN is an algorithm that constructs its own network topology during training, which is shown to be much smaller than the BP network, faster to trained, and free from the trial-and-error network design that characterize BP.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {542–549},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969895,
author = {Hanson, Stephen Jos\'{e}},
title = {Meiosis Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A central problem in connectionist modelling is the control of network and architectural resources during learning. In the present approach, weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions. Weight updates are a function of both the mean and standard deviation of each connection in the network and vary as a function of the error signal ("stochastic delta rule"; Hanson, 1990). Consequently, the weights maintain information on their central tendency and their "uncertainty" in prediction. Such information is useful in establishing a policy concerning the size of the nodal complexity of the network and growth of new nodes. For example, during problem solving the present network can undergo "meiosis", producing two nodes where there was one "overtaxed" node as measured by its coefficient of variation. It is shown in a number of benchmark problems that meiosis networks can find minimal architectures, reduce computational complexity, and overall increase the efficiency of the representation learning interaction.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {533–541},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969894,
author = {Fahlman, Scott E. and Lebiere, Christian},
title = {The Cascade-Correlation Learning Architecture},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {524–532},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969893,
author = {Grossman, Tal},
title = {The CHIR Algorithm for Feed Forward Networks with Binary Weights},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new learning algorithm, Learning by Choice of Internal Represetations (CHIR), was recently introduced. Whereas many algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. The algorithm applies a search procedure in the space of internal representations, and a cooperative adaptation of the weights (e.g. by using the perceptron learning rule). Since the introduction of its basic, single output version, the CHIR algorithm was generalized to train any feed forward network of binary neurons. Here we present the generalised version of the CHIR algorithm, and further demonstrate its versatility by describing how it can be modified in order to train networks with binary (±1) weights. Preliminary tests of this binary version on the random teacher problem are also reported.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {516–523},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969892,
author = {Galland, Conrad C. and Hinton, Geoffrey E.},
title = {Discovering High Order Features with Mean Field Modules},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new form of the deterministic Boltzmann machine (DBM) learning procedure is presented which can efficiently train network modules to discriminate between input vectors according to some criterion. The new technique directly utilizes the free energy of these "mean field modules" to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learning fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field modules with the mutual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {509–515},
numpages = {7},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969891,
author = {Barhen, Jacob and Toomarian, Nikzad and Gulati, Sandeep},
title = {Adjoint Operator Algorithms for Faster Learning in Dynamical Neural Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A methodology for faster supervised learning in dynamical nonlinear neural networks is presented. It exploits the concept of adjoint operators to enable computation of changes in the network's response due to perturbations in all system parameters, using the solution of a single set of appropriately constructed linear equations. The lower bound on speedup per learning iteration over conventional methods for calculating the neuromorphic energy gradient is O(N2), where N is the number of neurons in the network.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {498–508},
numpages = {11},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969890,
author = {Bell, Tony},
title = {Learning in Higher-Order 'Artificial Dendritic Trees'},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {If neurons sum up their inputs in a non-linear way, as some simulations suggest, how is this distributed fine-grained non-linearity exploited during learning? How are all the small sigmoids in synapse, spine and dendritic tree lined up in the right areas of their respective input spaces? In this report, I show how an abstract a temporal highly nested tree structure with a quadratic transfer function associated with each branchpoint, can self organise using only a single global reinforcement scalar, to perform binary classification tasks. The procedure works well, solving the 6-multiplexer and a difficult phoneme classification task as well as back-propagation does, and faster. Furthermore, it does not calculate an error gradient, but uses a statistical scheme to build moving models of the reinforcement signal.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {490–497},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969889,
author = {Saha, Avijit and Keeler, James D.},
title = {Algorithms for Better Representation and Faster Learning in Radial Basis Function Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we present upper bounds for the learning rates for hybrid models that employ a combination of both self-organized and supervised learning, using radial basis functions to build receptive field representations in the hidden units. The learning performance in such networks with nearest neighbor heuristic can be improved upon by multiplying the individual receptive field widths by a suitable overlap factor. We present results indicating optimal values for such overlap factors. We also present a new algorithm for determining receptive field centers. This method negotiates more hidden units in the regions of the input space as a function of the output and is conducive to better learning when the number of patterns (hidden units) is small.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {482–489},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969888,
author = {Mel, Bartlett W. and Koch, Christof},
title = {Sigma-Pi Learning: On Radial Basis Functions and Cortical Associative Learning},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The goal in this work has been to identify the neuronal elements of the cortical column that are most likely to support the learning of nonlinear associative maps. We show that a particular style of network learning algorithm based on locally-tuned receptive fields maps naturally onto cortical hardware, and gives coherence to a variety of features of cortical anatomy, physiology, and biophysics whose relations to learning remain poorly understood.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {474–481},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969887,
author = {Wejchert, Jakub and Tesauro, Gerald},
title = {Neural Network Visualization},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have developed graphics to visualize static and dynamic information in layered neural network learning systems. Emphasis was placed on creating new visuals that make use of spatial arrangements, size information, animation and color. We applied these tools to the study of back-propagation learning of simple Boolean predicates, and have obtained new insights into the dynamics of the learning process.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {465–472},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969886,
author = {Rogers, David},
title = {Predicting Weather Using a Genetic Memory: A Combination of Kanerva's Sparse Distributed Memory with Holland's Genetic Algorithms},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Kanerva's sparse distributed memory (SDM) is an associative-memory model based on the mathematical properties of high-dimensional binary address spaces. Holland's genetic algorithms are a search technique for high-dimensional spaces inspired by evolutionary processes of DNA. "Genetic Memory" is a hybrid of the above two systems, in which the memory uses a genetic algorithm to dynamically reconfigure its physical storage locations to reflect correlations between the stored addresses and data. For example, when presented with raw weather station data, the Genetic Memory discovers specific features in the weather data which correlate well with upcoming rain, and reconfigures the memory to utilize this information effectively. This architecture is designed to maximize the ability of the system to scale-up to handle real-world problems.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {455–464},
numpages = {10},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969885,
author = {Harp, Steven A. and Samad, Tariq and Guha, Aloke},
title = {Designing Application-Specific Neural Networks Using the Genetic Algorithm},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a general and systematic method for neural network design based on the genetic algorithm. The technique works in conjunction with network learning rules, addressing aspects of the network's gross architecture, connectivity, and learning rule parameters. Networks can be optimized for various application-specific criteria, such as learning speed, generalilation, robustness and connectivity. The approach is model-independent. We describe a prototype system, NeuroGENESYS, that employs the backpropagation learning rule. Experiments on several small problems have been conducted. In each case, NeuroGENESYS has produced networks that perform significantly better than the randomly generated networks of its initial population. The computational feasibility of our approach is discussed.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {447–454},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969884,
author = {Mozer, Michael C. and Bachrach, Jonatban},
title = {Discovering the Structure of a Reactive Environment by Exploration},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Consider a robot wandering around an unfamiliar environment, performing actions and sensing the resulting environmental states. The robot's task is to construct an internal model of its environment, a model that will allow it to predict the consequences of its actions and to determine what sequences of actions to take to reach particular goal states. Rivest and Schapire (1987a, 1987b; Schapire, 1988) have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of "finite state" environments. The heart of this algorithm is a clever representation of the environment called an update graph. We have developed a connectionist implementation of the update graph using a highly-specialized network architecture. With back propagation learning and a trivial exploration strategy - choosing random actions - the connectionist network can outperform the Rivest and Schapire algorithm on simple problems. The network has the additional strength that it can accommodate stochastic environments. Perhaps the greatest virtue of the connectionist approach is that it suggests generalizations of the update graph representation that do not arise from a traditional, symbolic perspective.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {439–446},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969883,
author = {Touretzky, David S. and Elvgren, Gillette},
title = {Rule Representations in a Connectionist Chunker},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present two connectionist architectures for chunking of symbolic rewrite rules. One uses backpropagation learning, the other competitive learning. Although they were developed for chunking the same sorts of rules, the two differ in their representational abilities and learning behaviors.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {431–438},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969882,
author = {Bengio, Yoshua and Pouliot, Yannick and Bengio, Samy and Agin, Patrick},
title = {A Neural Network to Detect Homologies in Proteins},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In order to detect the presence and location of immunoglobulin (Ig) domains from amino acid sequences we built a system based on a neural network with one hidden layer trained with back propagation. The program was designed to efficiently identify proteins exhibiting such domains, characterized by a few localized conserved regions and a low overall homology. When the National Biomedical Research Foundation (NBRF) NEW protein sequence database was scanned to evaluate the program's performance, we obtained very low rates of false negatives coupled with a moderate rate of false positives.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {423–430},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969881,
author = {Mori, Yoshihiro and Joe, Kazuki},
title = {A Large-Scale Neural Network Which Recognizes Handwritten Kanji Characters},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new way to construct a large-scale neural network for 3.000 handwritten Kanji characters recognition. This neural network consists of 3 parts: a collection of small-scale networks which are trained individually on a small number of Kanji characters; a network which integrates the output from the small-scale networks, and a process to facilitate the integration of these neworks. The recognition rate of the total system is comparable with those of the small-scale networks. Our results indicate that the proposed method is effective for constructing a large-scale network without loss of recognition performance.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {415–422},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969880,
author = {Martin, Gale L. and Pittman, James A.},
title = {Recognizing Hand-Printed Letters and Digits},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We are developing a hand-printed character recognition system using a multi-layered neural net trained through backpropagation. We report on results of training nets with samples of hand-printed digits scanned off of bank checks and hand-printed letters interactively entered into a computer through a stylus digitizer. Given a large training set, and a net with sufficient capacity to achieve high performance on the training set, nets typically achieved error rates of 4-5% at a 0% reject rate and 1-2% at a 10% reject rate. The topology and capacity of the system, as measured by the number of connections in the net, have surprisingly little effect on generalization. For those developing practical pattern recognition systems, these results suggest that a large and representative training sample may be the single, most important factor in achieving high recognition accuracy. From a scientific standpoint, these results raise doubts about the relevance to backpropagation of learning models that estimate the likelihood of high generalization from estimates of capacity. Reducing capacity does have other benefits however, especially when the reduction is accomplished by using local receptive fields with shared weights. In this latter case, we find the net evolves feature detectors resembling those in visual cortex and Linsker's orientation-selective nodes.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {405–414},
numpages = {10},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969879,
author = {Le Cun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
title = {Handwritten Digit Recognition with a Back-Propagation Network},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {396–404},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969878,
author = {Smith, Kurt R. and Miller, Michael I.},
title = {Bayesian Inference of Regular Grammar and Markov Source Models},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we develop a Bayes criterion which includes the Rissanen complexity, for inferring regular grammar models. We develop two methods for regular grammar Bayesian inference. The first method is based on treating the regular grammar as a 1-dimensional Markov source, and the second is based on the combinatoric characteristics of the regular grammar itself. We apply the resulting Bayes criteria to a particular example in order to show the efficiency of each method.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {388–395},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969877,
author = {Giles, C. L. and Sun, G. Z. and Chen, H. H. and Lee, Y. C. and Chen, D.},
title = {Higher Order Recurrent Networks &amp; Grammatical Inference},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP, and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {380–387},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969876,
author = {Touretzky, David S. and Wheeler, Deirdre W.},
title = {A Computational Basis for Phonology},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The phonological structure of human languages is intricate, yet highly constrained. Through a combination of connectionist modeling and linguistic analysis, we are attempting to develop a computational basis for the nature of phonology. We present a connectionist architecture that performs multiple simultaneous insertion, deletion, and mutation operations on sequences of phonemes, and introduce a novel additional primitive, clustering. Clustering provides an interesting alternative to both iterative and relaxation accounts of assimilation processes such as vowel harmony. Our resulting model is efficient because it processes utterances entirely in parallel using only feed-forward circuitry.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {372–379},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969875,
author = {Jain, Ajay N. and Waibel, Alex H.},
title = {Incremental Parsing by Modular Recurrent Connectionist Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a novel, modular, recurrent connectionist network architecture which learns to robustly perform incremental parsing of complex sentences. From sequential input, one word at a time, our networks learn to do semantic role assignment, noun phrase attachment, and clause structure recognition for sentences with passive constructions and center embedded clauses. The networks make syntactic and semantic predictions at every point in time, and previous predictions are revised as expectations are affirmed or violated with the arrival of new information. Our networks induce their own "grammar rules" for dynamically transforming an input sequence of words into a syntactic/semantic interpretation. These networks generalize and display tolerance to input which has been corrupted in ways common in spoken language.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {364–371},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969874,
author = {Reklaitis, Gintaras V. and Tsirukis, Athanasios G. and Tenorio, Manoel F.},
title = {Generalized Hopfield Networks and Nonlinear Optimization},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A nonlinear neural framework, called the Generalized Hopfield network, is proposed, which is able to solve in a parallel distributed manner systems of nonlinear equations. The method is applied to the general nonlinear optimization problem. We demonstrate GHNs implementing the three most important optimization algorithms, namely the Augmented Lagrangian, Generalized Reduced Gradient and Successive Quadratic Programming methods. The study results in a dynamic view of the optimization problem and offers a straightforward model for the parallelization of the optimization computations, thus significantly extending the practical limits of problems that can be formulated as an optimization problem and which can gain from the introduction of nonlinearities in their structure (eg. pattern recognition, supervised learning, design of content-addressable memories).},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {355–362},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969873,
author = {Farotimi, O. and Dembo, A. and Kailath, T.},
title = {Neural Network Weight Matrix Synthesis Using Optimal Control Techniques},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given a set of input-output training samples, we describe a procedure for determining the time sequence of weights for a dynamic neural network to model an arbitrary input-output process. We formulate the input-output mapping problem as an optimal control problem, defining a performance index to be minimized as a function of time-varying weights. We solve the resulting nonlinear two-point-boundary-value problem, and this yields the training rule. For the performance index chosen, this rule turns out to be a continuous time generalization of the outer product rule earlier suggested heuristically by Hopfield for designing associative memories. Learning curves for the new technique are presented.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {348–354},
numpages = {7},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969872,
author = {Carter, Michael J. and Rudolph, Franklin J. and Nucci, Adam J.},
title = {Operational Fault Tolerance of CMAC Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The performance sensitivity of Albus' CMAC network was studied for the scenario in which faults are introduced into the adjustable weights after training has been accomplished. It was found that fault sensitivity was reduced with increased generalization when "loss of weight" faults were considered, but sensitivity was increased for "saturated weight" faults.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {340–347},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969871,
author = {Hormel, Michael},
title = {A Self-Organizing Associative Memory System for Control Applications},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The CMAC storage scheme has been used as a basis for a software implementation of an associative memory system AMS, which itself is a major part of the learning control loop LERNAS. A major disadvantage of this CMAC-concept is that the degree of local generalization (area of interpolation) is fixed. This paper deals with an algorithm for self-organizing variable generalization for the AKS, based on ideas of T. Kohonen.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {332–339},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969870,
author = {Jordan, Michael I. and Jacobs, Robert A.},
title = {Learning to Control an Unstable System with Forward Modeling},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The forward modeling approach is a methodology for learning control when data is available in distal coordinate systems. We extend previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {324–331},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969869,
author = {Atkeson, Christopher G.},
title = {Using Local Models to Control Movement},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper explores the use of a model neural network for motor learning. Steinbuch and Taylor presented neural network designs to do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. The network design is equivalent to local regression. This network architecture can represent smooth nonlinear functions, yet has simple training rules with a single global optimum. The network has been used for motor learning of a simulated arm and a simulated running machine.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {316–323},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969868,
author = {Donnett, Jim and Smithers, Tim},
title = {Neuronal Group Selection Theory: A Grounding in Robotics},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we discuss a current attempt at applying the organizational principle Edelman calls Neuronal Group Selection to the control of a real, two-link robotic manipulator. We begin by motivating the need for an alternative to the position-control paradigm of classical robotics, and suggest that a possible avenue is to look at the primitive animal limb 'neurologically ballistic' control mode. We have been considering a selectionist approach to coordinating a simple perception-action task.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {308–315},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969867,
author = {Okamoto, Toshiaki and Kawato, Mitsuo and Ioui, Toshio and Miyake, Sei},
title = {Model Based Image Compression and Adaptive Data Representation by Interacting Filter Banks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To achieve high-rate image data compression while maintainig a high quality reconstructed image, a good image model and an efficient way to represent the specific data of each image must be introduced. Based on the physiological knowledge of multi - channel characteristics and inhibitory interactions between them in the human visual system, a mathematically coherent parallel architecture for image data compression which utilizes the Markov random field Image model and interactions between a vast number of filter banks, is proposed.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {298–305},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969866,
author = {Viola, Paul A.},
title = {Neurally Inspired Plasticity in Oculomotor Processes},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have constructed a two axis camera positioning system which is roughly analogous to a single human eye. This Artificial-Eye (A-eye) combines the signals generated by two rate gyroscopes with motion information extracted from visual analysis to stabilize its camera. This stabilization process is similar to the vestibulo-ocular response (VOR); like the VOR, A-eye learns a system model that can be incrementally modified to adapt to changes in its structure, performance and environment. A-eye is an example of a robust sensory system that performs computations that can be of significant use to the designers of mobile robots.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {290–297},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969865,
author = {Kanerva, Pentti},
title = {Contour-Map Encoding of Shape for Early Vision},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Contour maps provide a general method for recognizing two-dimensional shapes. All but blank images give rise to such maps, and people are good at recognizing objects and shapes from them. The maps are encoded easily in long feature vectors that are suitable for recognition by an associative memory. These properties of contour maps suggest a role for them in early visual perception. The prevalence of direction-sensitive neurons in the visual cortex of mammals supports this view.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {282–289},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969864,
author = {Weinshall, Daphna and Edelman, Shimon and B\"{u}lthoff, Heinrich H.},
title = {A Self-Organizing Multiple-View Representation of 3D Objects},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We demonstrate the ability of a two-layer network of thresholded summation units to support representation of 3D objects in which several distinct 2D views are stored for each object. Using unsupervised Hebbian relaxation, the network learned to recognize ten objects from different viewpoints. The training process led to the emergence of compact representations of the specific input views. When tested on novel views of the same objects, the network exhibited a substantial generalization capability. In simulated psychophysical experiments, the network's behavior was qualitatively similar to that of human subjects.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {274–281},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969863,
author = {Zemel, Richard S. and Mozer, Michael C. and Hinton, Geoffrey E.},
title = {TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implementation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {266–273},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969862,
author = {Seibert, Michael and Waxman, Allen M.},
title = {Learning Aspect Graph Representations from View Sequences},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In our effort to develop a modular neural system for invariant learning and recognition of 3D objects, we introduce here a new module architecture called an aspect network constructed around adaptive axo-axo-dendritic synapses. This builds upon our existing system (Seibert &amp; Waxman, 1989) which processes 20 shapes and classifies them into view categories (i.e., aspects) invariant to illumination, position, orientation, scale, and projective deformations. From a sequence of views, the aspect network learns the transitions between these aspects, crystallizing a graph-like structure from an initially amorphous network. Object recognition emerges by accumulating evidence over multiple views which activate competing object hypotheses.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {258–265},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969861,
author = {Malkoff, Donald B.},
title = {A Neural Network for Real-Time Signal Processing},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes a neural network algorithm that (1) performs temporal pattern matching in real-time, (2) is trained on-line, with a single pass, (3) requires only a single template for training of each representative class, (4) is continuously adaptable to changes in background noise, (5) deals with transient signals having low signal-to-noise ratios, (6) works in the presence of non-Gaussian noise, (7) makes use of context dependencies and (8) outputs Bayesian probability estimates. The algorithm has been adapted to the problem of passive sonar signal detection and classification. It runs on a Connection Machine and correctly classifies, within 500 ms of onset, signals embedded in noise and subject to considerable uncertainty.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {248–255},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969860,
author = {Lee, Susan Ciarrocca},
title = {Using a Translation-Invariant Neural Network to Diagnose Heart Arrhythmia},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Distinctive electrocardiogram (ECG) patterns are created when the heart is beating normally and when a dangerous arrhythmia is present. Some devices which monitor the ECG and react to arrhythmias parameterize the ECG signal and make a diagnosis based on the parameters. The author discusses the use of a neural network to classify the ECG signals directly, without parameterization. The input to such a network must be translation-invariant, since the distinctive features of the ECG may appear anywhere in an arbritrarily-chosen ECG segment. The input must also be insensitive to the episode-to-episode and patient-to-patient variability in the rhythm pattern.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {240–247},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969859,
author = {Sejnowski, T. J. and Yuhas, B. P. and Goldstein, M. H. and Jenkins, R. E.},
title = {Combining Visual and Acoustic Speech Signals with a Neural Network Improves Intelligibility},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Acoustic speech recognition degrades in the presence of noise. Compensatory information is available from the visual speech signals around the speaker's mouth. Previous attempts at using these visual speech signals to improve automatic speech recognition systems have combined the acoustic and visual speech information at a symbolic level using heuristic rules. In this paper, we demonstrate an alternative approach to fusing the visual and acoustic speech information by training feedforward neural networks to map the visual signal onto the corresponding short-term spectral amplitude envelope (STSAE) of the acoustic signal. This information can be directly combined with the degraded acoustic STSAE. Significant improvements are demonstrated in vowel recognition from noise-degraded acoustic signals. These results are compared to the performance of humans, as well as other pattern matching and estimation algorithms.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {232–239},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969858,
author = {Mann, Jim},
title = {The Effects of Circuit Integration on a Feature Map Vector Quantizer},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The effects of parameter modifications imposed by hardware constraints on a self-organizing feature map algorithm were examined. Performance was measured by the error rate of a speech recognition system which included this algorithm as part of the front-end processing. System parameters which were varied included weight (connection strength) quantization, adaptation quantization, distance measures and circuit approximations which include device characteristics and process variability. Experiments using the TI isolated word database for 16 speakers demonstrated degradation in performance when weight quantization fell below 8 bits. The competitive nature of the algorithm relaxes constraints on uniformity and linearity which makes it an excellent candidate for a fully analog circuit implementation. Prototype circuits have been fabricated and characterized following the constraints established through the simulation efforts.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {226–231},
numpages = {6},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969857,
author = {Bengio, Y oshua and De Mori, Renato and Cardin, Regis},
title = {Speaker Independent Speech Recognition with Neural Networks and Speech Knowledge},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We attempt to combine neural networks with knowledge from speech science to build a speaker independent speech recognition system. This knowledge is utilized in designing the preprocessing, input coding, output coding, output supervision and architectural constraints. To handle the temporal aspect of speech we combine delays, copies of activations of hidden and output units at the input level, and Back-Propagation for Sequences (BPS), a learning algorithm for networks with local self-loops. This strategy is demonstrated in several experiments, in particular a nasal discrimination task for which the application of a speech theory hypothesis dramatically improved generalization.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {218–225},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969856,
author = {Bridle, John S.},
title = {Training Stochastic Model Recognition Algorithms as Networks Can Lead to Maximum Mutual Information Estimation of Parameters},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {211–217},
numpages = {7},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969855,
author = {Hampshire, John B. and Waibel, Alex},
title = {Connectionist Architectures for Multi-Speaker Phoneme Recognition},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a number of Time-Delay Neural Network (TDNN) based architectures for multi-speaker phoneme recognition (/b,d,g/ task). We use speech of two females and four males to compare the performance of the various architectures against a baseline recognition rate of 95.9% for a single IDNN on the six-speaker /b,d,g/ task. This series of modular designs leads to a highly modular multi-network architecture capable of performing the six-speaker recognition task at the speaker dependent rate of 98.4%. In addition to its high recognition rate, the so-called "Meta-Pi" architecture learns - without direct supervision - to recognize the speech of one particular male speaker using internal models of other male speakers exclusively.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {203–210},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969854,
author = {Huang, William Y. and Lippmann, Richard P.},
title = {HMM Speech Recognition with Neural Net Discrimination},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Two approaches were explored which integrate neural net classifiers with Hidden Markov Model (HMM) speech recognizers. Both attempt to improve speech pattern discrimination while retaining the temporal processing advantages of HMMs. One approach used neural nets to provide second-stage discrimination following an HMM recognizer. On a small vocabulary task, Radial Basis Function (RBF) and back-propagation neural nets reduced the error rate substantially (from 7.9% to 4.2% for the RBF classifier). In a larger vocabulary task, neural net classifiers did not reduce the error rate. They, however, outperformed Gaussian, Gaussian mixture, and k-nearest neighbor (KNN) classifiers. In another approach, neural nets functioned as low-level acoustic-phonetic feature extractors. When classifying phonemes based on single 10 msec. frames, discriminant RBF neural net classifiers outperformed Gaussian mixture classifiers. Performance, however, differed little when classifying phones by accumulating scores across all frames in phonetic segments using a single node HMM recognizer.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {194–202},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969853,
author = {Bourlard, Herv\'{e} and Morgan, Nelson},
title = {A Continuous Speech Recognition System Embedding MLP into HMM},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We are developing a phoneme based, speaker-dependent continuous speech recognition system embedding a Multilayer Perceptron (MLP) (i.e., a feedforward Artificial Neural Network), into a Hidden Markov Model (HMM) approach. In [Bourlard &amp; Wellekens], it was shown that MLPs were approximating Maximum a Posteriori (MAP) probabilities and could thus be embedded as an emission probability estimator in HMMs. By using contextual information from a sliding window on the input frames, we have been able to improve frame or phoneme classification performance over the corresponding performance for Simple Maximum Likelihood (ML) or even MAP probabilities that are estimated without the benefit of context. However, recognition of words in continuous speech was not so simply improved by the use of an MLP, and several modifications of the original scheme were necessary for getting acceptable performance. It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {186–193},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969852,
author = {Lang, Kevin J. and Hinton, Geoffrey E.},
title = {Dimensionality Reduction and Prior Knowledge in E-Set Recognition},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {178–185},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969851,
author = {Lee, Yuchun and Lippmann, Richard P.},
title = {Practical Characteristics of Neural Network and Conventional Pattern Classifiers on Artificial and Speech Problems},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Eight neural net and conventional pattern classifiers (Bayesian-unimodal Gaussian, k-nearest neighbor, standard back-propagation, adaptive-stepsize back-propagation, hypersphere, feature-map, learning vector quantizer, and binary decision tree) were implemented on a serial computer and compared using two speech recognition and two artificial tasks. Error rates were statistically equivalent on almost all tasks, but classifiers differed by orders of magnitude in memory requirements, training time, classification time, and ease of adaptivity. Nearest-neighbor classifiers trained rapidly but required the most memory. Tree classifiers provided rapid classification but were complex to adapt. Back-propagation classifiers typically required long training times and had intermediate memory requirements. These results suggest that classifier selection should often depend more heavily on practical considerations concerning memory and computation resources, and restrictions on training and classification times than on error rate.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {168–177},
numpages = {10},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969850,
author = {Tang, D. S.},
title = {Analytic Solutions to the Formation of Feature-Analysing Cells of a Three-Layer Feedforward Visual Information Processing Neural Net},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Analytic solutions to the information-theoretic evolution equation of the connection strength of a three-layer feedforward neural net for visual information processing are presented. The results are (1) the receptive fields of the feature-analysing cells correspond to the eigenvector of the maximum eigenvalue of the Fredholm integral equation of the first kind derived from the evolution equation of the connection strength; (2) a symmetry-breaking mechanism (parity-violation) has been identified to be responsible for the changes of the morphology of the receptive field; (3) the conditions for the formation of different morphologies are explicitly identified.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {160–165},
numpages = {6},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969849,
author = {Rhodes, Paul},
title = {A Systematic Study of the Input/Output Properties of a 2 Compartment Model Neuron with Active Membranes},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The input/output properties of a 2 compartment model neuron are systematically explored. Taken from the work of MacGregor (MacGregor, 1987), the model neuron compartments contain several active conductances, including a potassium conductance in the dendritic compartment driven by the accumulation of intradendritic calcium. Dynamics of the conductances and potentials are governed by a set of coupled first order differential equations which are integrated numerically. There are a set of 17 internal parameters to this model, specificying conductance rate constants, time constants, thresholds, etc.To study parameter sensitivity, a set of trials were run in which the input driving the neuron is kept fixed while each internal parameter is varied with all others left fixed.To study the input/output relation, the input to the dendrite (a square wave) was varied (in frequency and magnitude) while all internal parameters of the system were left fixed, and the resulting output firing rate and bursting rate was counted.The input/output relation of the model neuron studied turns out to be much more sensitive to modulation of certain dendritic potassium current parameters than to plasticity of synapse efficacy per se (the amount of current influx due to synapse activation). This would in turn suggest, as has been recently observed experimentally, that the potassium current may be as or more important a focus of neural plasticity than synaptic efficacy.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {149–159},
numpages = {11},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969848,
author = {Kenyon, G. T. and Fetz, E. E. and Puff, R. D.},
title = {Effects of Firing Synchrony on Signal Propagation in Layered Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spiking neurons which integrate to threshold and fire were used to study the transmission of frequency modulated (FM) signals through layered networks. Firing correlations between cells in the input layer were found to modulate the transmission of FM signals under certain dynamical conditions. A tonic level of activity was maintained by providing each cell with a source of Poisson-distributed synaptic input. When the average membrane depolarization produced by the synaptic input was sufficiently below threshold, the firing correlations between cells in the input layer could greatly amplify the signal present in subsequent layers. When the depolarization was sufficiently close to threshold, however, the firing synchrony between cells in the initial layers could no longer effect the propagation of FM signals. In this latter case, integrate-and-fire neurons could be effectively modeled by simpler analog elements governed by a linear input-output relation.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {141–148},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969847,
author = {Chernjavsky, Alex and Moody, John},
title = {Note on Development of Modularity in Simple Cortical Models},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The existence of modularity in the organization of nervous systems (e.g. cortical columns and olfactory glomeruli) is well known. We show that localized activity patterns in a layer of cells, collective excitations, can induce the formation of modular structures in the anatomical connections via a Hebbian learning mechanism. The networks are spatially homogeneous before learning, but the spontaneous emergence of localized collective excitations and subsequently modularity in the connection patterns breaks translational symmetry. This spontaneous symmetry breaking phenomenon is similar to those which drive pattern formation in reaction-diffusion systems. We have identified requirements on the patterns of lateral connections and on the gains of internal units which are essential for the development of modularity. These essential requirements will most likely remain operative when more complicated (and biologically realistic) models are considered.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {133–140},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969846,
author = {Softky, William R. and Kammen, Daniel M.},
title = {Can Simple Cells Learn Curves? A Hebbian Model in a Structured Environment},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the mammalian visual cortex, orientation-selective 'simple cells' which detect straight lines may be adapted to detect curved lines instead. We test a biologically plausible, Hebbian, single-neuron model, which learns oriented receptive fields upon exposure to unstructured (noise) input and maintains orientation selectivity upon exposure to edges or bars of all orientations and positions. This model can also learn arc-shaped receptive fields upon exposure to an environment of only circular rings. Thus, new experiments which try to induce an abnormal (curved) receptive field may provide insight into the plasticity of simple cells. The model suggests that exposing cells to only a single spatial frequency may induce more striking spatial frequency and orientation dependent effects than heretofore observed.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {125–132},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969845,
author = {Lee, Maurice and Bower, James M.},
title = {A Computer Modeling Approach to Understanding the Inferior Olive and Its Relationship to the Cerebellar Cortex in Rats},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents the results of a simulation of the spatial relationship between the inferior olivary nucleus and folium crus IIA of the lateral hemisphere of the rat cerebellum. The principal objective of this modeling effort was to resolve an apparent conflict between a proposed zonal organization of olivary projections to cerebellar cortex suggested by anatomical tract-tracing experiments (Brodal &amp; Kawamura 1980; Campbell &amp; Armstrong 1983) and a more patchy organization apparent with physiological mapping (Robertson 1987). The results suggest that several unique features of the olivocerebellar circuit may contribute to the appearance of zonal organization using anatomical techniques, but that the detailed patterns of patchy tactile projections seen with physiological techniques are a more accurate representation of the afferent organization of this region of cortex.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {117–124},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969844,
author = {Crair, Michael C. and Bialek, William},
title = {Non-Boltzmann Dynamics in Networks of Spiking Neurons},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study networks of spiking neurons in which spikes are fired as a Poisson process. The state of a cell is determined by the instantaneous firing rate, and in the limit of high firing rates our model reduces to that studied by Hopfield. We find that the inclusion of spiking results in several new features, such as a noise-induced asymmetry between "on" and "off" states of the cells and probability currents which destroy the usual description of network dynamics in terms of energy surfaces. Taking account of spikes also allows us to calibrate network parameters such as "synaptic weights" against experiments on real synapses. Realistic forms of the post synaptic response alters the network dynamics, which suggests a novel dynamical learning mechanism.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {109–116},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969843,
author = {Servan-Schreiber, David and Printz, Harry and Cohen, Jonathan D.},
title = {The Effect of Catecholamines on Performance: From Unit to System Behavior},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {At the level of individual neurons, catecholamine release increases the responsivity of cells to excitatory and inhibitory inputs. We present a model of catecholamine effects in a network of neural-like elements. We argue that changes in the responsivity of individual elements do not affect their ability to detect a signal and ignore noise. However, the same changes in cell responsivity in a network of such elements do improve the signal detection performance of the network as a whole. We show how this result can be used in a computer simulation of behavior to account for the effect of CNS stimulants on the signal detection performance of human subjects.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {100–108},
numpages = {9},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969842,
author = {Cowan, J. D. and Friedman, A. E.},
title = {Development and Regeneration of Eye-Brain Maps: A Computational Model},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We outline a computational model of the development and regeneration of specific eye-brain circuits. The model comprises a self-organizing map-forming network which uses local Hebb rules. constrained by molecular markers. Various simulations of the development of eye-brain maps in fish and frogs are described.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {92–99},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969841,
author = {Wilson, Matthew A. and Bower, James M.},
title = {Computer Simulation of Oscillatory Behavior in Cerebral Cortical Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It has been known for many years that specific regions of the working cerebral cortex display periodic variations in correlated cellular activity. While the olfactory system has been the focus of much of this work, similar behavior has recently been observed in primary visual cortex. We have developed models of both the olfactory and visual cortex which replicate the observed oscillatory properties of these networks. Using these models we have examined the dependence of oscillatory behavior on single cell properties and network architectures. We discuss the idea that the oscillatory events recorded from cerebral cortex may be intrinsic to the architecture of cerebral cortex as a whole, and that these rhythmic patterns may be important in coordinating neuronal activity during sensory processing.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {84–91},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969840,
author = {Kammen, Daniel and Koch, Christof and Holmes, Philip J.},
title = {Collective Oscillations in the Visual Cortex},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The firing patterns of populations of cells in the cat visual cortex can exhibit oscillatory responses in the range of 35 - 85 Hz. Furthermore, groups of neurons many mm's apart can be highly synchronized as long as the cells have similar orientation tuning. We investigate two basic network architectures that incorporate either nearest-neighbor or global feedback interactions and conclude that non-local feedback plays a fundamental role in the initial synchronization and dynamic stability of the oscillations.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {76–83},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969839,
author = {Baird, Bill},
title = {Associative Memory in a Simple Model of Oscillating Cortex},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A generic model of oscillating cortex, which assumes "minimal" coupling justified by known anatomy, is shown to function as an associative memory, using previously developed theory. The network has explicit excitatory neurons with local inhibitory interneuron feedback that forms a set of nonlinear oscillators coupled only by long range excitatory connections. Using a local Hebb-like learning rule for primary and higher order synapses at the ends of the long range connections, the system learns to store the kinds of oscillation amplitude patterns observed in olfactory and visual cortex. This rule is derived from a more general "projection algorithm" for recurrent analog networks, that analytically guarantees content addressable memory storage of continuous periodic sequences - capacity: N/2 Fourier components for an N node network - no "spurious" attractors.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {68–75},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969838,
author = {Nelson, Mark E. and Bower, James M.},
title = {Computational Efficiency: A Common Organizing Principle for Parallel Computer Maps and Brain Maps?},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It is well-known that neural responses in particular brain regions are spatially organized, but no general principles have been developed that relate the structure of a brain map to the nature of the associated computation. On parallel computers, maps of a sort quite similar to brain maps arise when a computation is distributed across multiple processors. In this paper we will discuss the relationship between maps and computations on these computers and suggest how similar considerations might also apply to maps in the brain.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {60–67},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969837,
author = {Grajski, Kamil A. and Merzenich, Michael M.},
title = {Neural Network Simulation of Somatosensory Representational Plasticity},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The brain represents the skin surface as a topographic map in the somatosensory cortex. This map has been shown experimentally to be modifiable in a use-dependent fashion throughout life. We present a neural network simulation of the competitive dynamics underlying this cortical plasticity by detailed analysis of receptive field properties of model neurons during simulations of skin coactivation, cortical lesion, digit amputation and nerve section.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {52–59},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969836,
author = {Beer, Randall D. and Chiel, Hillel J.},
title = {Neural Implementation of Motivated Behavior: Feeding in an Artificial Insect},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Most complex behaviors appear to be governed by internal motivational states or drives that modify an animal's responses to its environment. It is therefore of considerable interest to understand the neural basis of these motivational states. Drawing upon work on the neural basis of feeding in the marine mollusc Aplysia, we have developed a heterogeneous artificial neural network for controlling the feeding behavior of a simulated insect. We demonstrate that feeding in this artificial insect shares many characteristics with the motivated behavior of natural animals.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {44–51},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969835,
author = {Bialek, William and Rieke, Fred and De Ruyter Van Steveninck, R. R. and Warland, David},
title = {Reading a Neural Code},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Traditional methods of studying neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task - decoding short segments of a spike train to extract information about an unknown, time-varying stimulus. Here we present strategies for characterizing the neural code from the point of view of the organism, culminating in algorithms for real-time stimulus reconstruction based on a single sample of the spike train. These methods are applied to the design and analysis of experiments on an identified movement-sensitive neuron in the fly visual system. As far as we know this is the first instance in which a direct "reading" of the neural code has been accomplished.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {36–43},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969834,
author = {Lockery, Shawn R. and Fang, Yan and Sejnowski, Terrence J.},
title = {Neural Network Analysis of Distributed Representations of Dynamical Sensory-Motor Transformations in the Leech},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These "hidden" units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamical networks were trained on experimentally derived input-output patterns using recurrent back-propagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {28–35},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969833,
author = {Harris-Warrick, Ronald M.},
title = {Mechanisms for Neuro Modulation of Biological Neural Networks},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The pyloric Central Pattern Generator of the crustacean stomatogastric ganglion is a well-defined biological neural network. This 14-neuron network is modulated by many inputs. These inputs reconfigure the network to produce multiple output patterns by three simple mechanisms: 1) determining which cells are active; 2) modulating the synaptic efficacy; 3) changing the intrinsic response properties of individual neurons. The importance of modifiable intrinsic response properties of neurons for network function and modulation is discussed.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {18–27},
numpages = {10},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969832,
author = {Spence, Clay D. and Pearson, John C.},
title = {The Computation of Sound Source Elevation in the Barn Owl},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The midbrain of the barn owl contains a map-like representation of sound source direction which is used to precisely orient the head toward targets of interest. Elevation is computed from the interaural difference in sound level. We present models and computer simulations of two stages of level difference processing which qualitatively agree with known anatomy and physiology, and make several striking predictions.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {10–17},
numpages = {8},
series = {NIPS'89}
}

@inproceedings{10.5555/2969830.2969831,
author = {Simmons, James A.},
title = {Acoustic-Imaging Computations by Echolocating Bats: Unification of Diversely-Represented Stimulus Features into Whole Images},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The echolocating bat, Eptesicus fuscus, perceives the distance to sonar targets from the delay of echoes and the shape of targets from the spectrum of echoes. However, shape is perceived in terms of the target's range profile. The time separation of echo components from parts of the target located at different distances is reconstructed from the echo spectrum and added to the estimate of absolute delay already derived from the arrival-time of echoes. The bat thus perceives the distance to targets and depth within targets along the same psychological range dimension, which is computed. The image corresponds to the crosscorrelation function of echoes. Fusion of physiologically distinct time- and frequency-domain representations into a final, common time-domain image illustrates the binding of within-modality features into a unified, whole image. To support the structure of images along the dimension of range, bats can perceive echo delay with a hyperacuity of 10 nanoseconds.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {2–9},
numpages = {8},
series = {NIPS'89}
}

@proceedings{10.5555/2969830,
title = {NIPS'89: Proceedings of the 2nd International Conference on Neural Information Processing Systems},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

