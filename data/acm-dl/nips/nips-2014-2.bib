@inproceedings{10.5555/2969033.2969238,
author = {Hsieh, Cho-Jui and Si, Si and Dhillon, Inderjit S.},
title = {Fast Prediction for Large-Scale Kernel Machines},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding "pseudo landmark points" to the classical Nystrom kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystr\"{o}m kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3689–3697},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969237,
author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},
title = {Variational Gaussian Process State-Space Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3680–3688},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969236,
author = {Nguyen, Viet-An and Boyd-Graber, Jordan and Resnik, Philip and Chang, Jonathan},
title = {Learning a Concept Hierarchy from Multi-Labeled Documents},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {While topic models can discover patterns of word usage in large corpora, it is difficult to meld this unsupervised structure with noisy, human-provided labels, especially when the label space is large. In this paper, we present a model—Label to Hierarchy (L2H)—that can induce a hierarchy of user-generated labels and the topics associated with those labels from a set of multi-labeled documents. The model is robust enough to account for missing labels from untrained, disparate annotators and provide an interpretable summary of an otherwise unwieldy label set. We show empirically the effectiveness of L2H in predicting held-out words and labels for unseen documents.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3671–3679},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969235,
author = {Zhu, Yuancheng and Lafferty, John},
title = {Quantized Estimation of Gaussian Sequence Models in Euclidean Balls},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A central result in statistical theory is Pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3662–3670},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969234,
author = {Vertechi, Pietro and Brendel, Wieland and Machens, Christian K.},
title = {Unsupervised Learning of an Efficient Short-Term Memory Network},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning in recurrent neural networks has been a topic fraught with difficulties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Specifically, we show how these networks can learn to efficiently represent their present and past inputs, based on local learning rules only. Our results are based on several key insights. First, we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where, on average, feedforward signal inputs are canceled by recurrent inputs. We show that this learning rule minimizes a cost function. Second, we develop a local learning rule for the feedforward weights that, based on networks in which recurrent inputs already predict feedforward inputs, further minimizes the cost. Third, we show how the learning rules can be modified such that the network can directly encode non-whitened inputs. Fourth, we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself. As a consequence, the network starts to efficiently represent both its signal inputs and their history. We develop our main theory for linear networks, but then sketch how the learning rules could be transferred to balanced, spiking networks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3653–3661},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969233,
author = {McDonald, Andrew M. and Pontil, Massimiliano and Stamos, Dimitris},
title = {Spectral <i>k</i>-Support Norm Regularization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The k-support norm has successfully been applied to sparse vector prediction problems. We observe that it belongs to a wider class of norms, which we call the box-norms. Within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the k-support norm. We extend the norms from the vector to the matrix setting and we introduce the spectral k-support norm. We study its properties and show that it is closely related to the multitask learning cluster norm. We apply the norms to real and synthetic matrix completion datasets. Our findings indicate that spectral k-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3644–3652},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969232,
author = {Rakesh, S. and Bhattacharyya, Chiranjib},
title = {Learning on Graphs Using Orthonormal Representation is Statistically Consistent},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Existing research [4] suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph. However the choice of optimal embedding remains an open issue. Orthonormal representation of graphs, a class of embeddings over the unit sphere, was introduced by Lov\'{a}sz [2]. In this paper, we show that there exists orthonormal representations which are statistically consistent over a large class of graphs, including power law and random graphs. This result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction. As part of the analysis, we explicitly derive relationships between the Rademacher complexity measure and structural properties of graphs, such as the chromatic number. We further show the fraction of vertices of a graph G, on n nodes, that need to be labelled for the learning algorithm to be consistent, also known as labelled sample complexity, is Ω (ϑ(G)/n)1/4 where ϑ(G) is the famous Lov\"{y}sz ϑ function of the graph. This, for the first time, relates labelled sample complexity to graph connectivity properties, such as the density of graphs. In the multiview setting, whenever individual views are expressed by a graph, it is a well known heuristic that a convex combination of Laplacians [7] tend to improve accuracy. The analysis presented here easily extends to Multiple graph transduction, and helps develop a sound statistical understanding of the heuristic, previously unavailable.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3635–3643},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969231,
author = {Wilson, Andrew Gordon and Gilboa, Elad and Nehorai, Arye and Cunningham, John P.},
title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems. Kernel methods, such as Gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods. However, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing Gaussian process models for this purpose involves several challenges. A vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation. This difficulty is compounded by the fact that Gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard Gaussian process model. One faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset. In this paper, we propose a Gaussian process approach for large scale multidimensional pattern extrapolation. We recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points. The proposed method significantly outperforms alternative scalable and flexible Gaussian process methods, in speed and accuracy. Moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3626–3634},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969230,
author = {O'Connor, Luke and Feizi, Soheil},
title = {Biclustering Using Message Passing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice. Moreover, Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations, we find that our method outperforms two of the best existing biclustering algorithms, ISA and LAS, when the planted clusters overlap. Applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3617–3625},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969229,
author = {Chwialkowski, Kacper and Sejdinovic, Dino and Gretton, Arthur},
title = {A Wild Bootstrap for Degenerate Kernel Tests},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3608–3616},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969228,
author = {Foti, Nicholas J. and Xu, Jason and Laird, Dillon and Fox, Emily B.},
title = {Stochastic Variational Inference for Hidden Markov Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3599–3607},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969227,
author = {Zhong, Mingjun and Goddard, Nigel and Sutton, Charles},
title = {Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the-art approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3590–3598},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969226,
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
title = {Semi-Supervised Learning with Deep Generative Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3581–3589},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969225,
author = {Pasa, Luca and Sperduti, Alessandro},
title = {Pre-Training of Recurrent Neural Networks via Linear Autoencoders},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3572–3580},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969224,
author = {F\'{e}votte, C\'{e}dric and Kowalski, Matthieu},
title = {Low-Rank Time-Frequency Synthesis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram - the (power) magnitude of the short-time Fourier transform (STFT) - has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3563–3571},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969223,
author = {Zhou, Yingbo and Porwal, Utkarsh and Zhang, Ce and Ngo, Hung and Nguyen, XuanLong and R\'{e}, Christopher and Govindaraju, Venu},
title = {Parallel Feature Selection Inspired by Group Testing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3554–3562},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969222,
author = {Stollenga, Marijn F. and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
title = {Deep Networks with Internal Selective Attention through Feedback Connections},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3545–3553},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969221,
author = {Hoffman, Judy and Guadarrama, Sergio and Tzeng, Eric and Hu, Ronghang and Donahue, Jeff and Girshick, Ross and Darrell, Trevor and Saenko, Kate},
title = {LSDA: Large Scale Detection through Adaptation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a &gt;7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at lsda.berkeleyvision.org.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3536–3544},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969220,
author = {Oiwa, Hidekazu and Fujimaki, Ryohei},
title = {Partition-Wise Linear Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3527–3535},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969219,
author = {Oord, Aaron van den and Schrauwen, Benjamin},
title = {Factoring Variations in Natural Images with Deep Gaussian Mixture Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3518–3526},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969218,
author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
title = {Algorithms for CVaR Optimization in MDPs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3509–3517},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969217,
author = {Buesing, Lars and Machado, Timothy A. and Cunningham, John P. and Paninski, Liam},
title = {Clustered Factor Analysis of Multineuronal Spike Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3500–3508},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969216,
author = {Bahadori, Mohammad Taha and Yu, Qi (Rose) and Liu, Yan},
title = {Fast Multivariate Spatio-Temporal Analysis via Low Rank Tensor Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3491–3499},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969215,
author = {Rajkumar, Arun and Agarwal, Shivani},
title = {Online Decision-Making in General Combinatorial Spaces},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010), and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3482–3490},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969214,
author = {Loh, Po-Ling and Wibisono, Andre},
title = {Concavity of Reweighted Kikuchi Approximation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3473–3481},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969213,
author = {Jayaraman, Dinesh and Grauman, Kristen},
title = {Zero-Shot Recognition with Unreliable Attributes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses—even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3464–3472},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969212,
author = {Zhou, Mingyuan},
title = {Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3455–3463},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969211,
author = {Berend, Daniel and Kontorovich, Aryeh},
title = {Consistency of Weighted Majority Votes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3446–3454},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969210,
author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
title = {Rates of Convergence for Nearest Neighbor Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find, for instance, that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3437–3445},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969209,
author = {Wager, Stefan and Chamandy, Nick and Muralidharan, Omkar and Najmi, Amir},
title = {Feedback Detection for Live Predictors},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3428–3436},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969208,
author = {Liu, Wei and Mu, Cun and Kumar, Sanjiv and Chang, Shih-Fu},
title = {Discrete Graph Hashing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art un-supervised hashing methods, especially for longer codes.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3419–3427},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969207,
author = {Paige, Brooks and Wood, Frank and Doucet, Arnaud and Teh, Yee Whye},
title = {Asynchronous Anytime Sequential Monte Carlo},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional sequential Monte Carlo algorithms that is amenable to parallel and distributed implementations. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3410–3418},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969206,
author = {Qu, Qing and Sun, Ju and Wright, John},
title = {Finding a Sparse Vector in a Subspace: Linear Sparsity Using Alternating Directions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of recovering the sparsest vector in a subspace S ∈ ℝp with dim (S) = n. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds 1/√n. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is Ω(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3401–3409},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969205,
author = {Trivedi, Shubhendu and McAllester, David and Shakhnarovich, Gregory},
title = {Discriminative Metric Learning by Neighborhood Gerrymandering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error. We describe an efficient algorithm for exact loss augmented inference, and a fast gradient descent algorithm for learning in this model. The objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points. Our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed. In experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3392–3400},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969204,
author = {Wang, Zhaoran and Lu, Huanran and Liu, Han},
title = {Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees.In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping.Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of 1/√t within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level s*, dimension d and sample size n. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3383–3391},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969203,
author = {Du, Nan and Liang, Yingyu and Balcan, Maria-Florina and Song, Le},
title = {Learning Time-Varying Coverage Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Coverage functions are an important class of discrete functions that capture the law of diminishing returns arising naturally from applications in social network analysis, machine learning, and algorithmic game theory. In this paper, we propose a new problem of learning time-varying coverage functions, and develop a novel parametrization of these functions using random features. Based on the connection between time-varying coverage functions and counting processes, we also propose an efficient parameter learning algorithm based on likelihood maximization, and provide a sample complexity analysis. We applied our algorithm to the influence function estimation problem in information diffusion in social networks, and show that with few assumptions about the diffusion processes, our algorithm is able to estimate influence significantly more accurately than existing approaches on both synthetic and real world data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3374–3382},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969202,
author = {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
title = {Learning with Pseudo-Ensembles},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3365–3373},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969201,
author = {Xu, Minjie and Lakshminarayanan, Balaji and Teh, Yee Whye and Zhu, Jun and Zhang, Bo},
title = {Distributed Bayesian Posterior Sampling via Moment Sharing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a distributed Markov chain Monte Carlo (MCMC) inference algorithm for large scale Bayesian posterior simulation. We assume that the dataset is partitioned and stored across nodes of a cluster. Our procedure involves an independent MCMC posterior sampler at each node based on its local partition of the data. Moment statistics of the local posteriors are collected from each sampler and propagated across the cluster using expectation propagation message passing with low communication costs. The moment sharing scheme improves posterior estimation quality by enforcing agreement among the samplers. We demonstrate the speed and inference quality of our method with empirical studies on Bayesian logistic regression and sparse linear regression with a spike-and-slab prior.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3356–3364},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969200,
author = {Bresler, Guy and Chen, George H. and Shah, Devavrat},
title = {A Latent Source Model for Online Collaborative Filtering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the "online" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of n users either likes or dislikes each of m items. We assume there to be k types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly log(km) initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing k. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3347–3355},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969199,
author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard and Wang, Xiaoshi},
title = {Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3338–3346},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969198,
author = {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
title = {Accelerated Mini-Batch Randomized Block Coordinate Descent Method},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a "batch" setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3329–3337},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969197,
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
title = {How Transferable Are Features in Deep Neural Networks?},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3320–3328},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969196,
author = {Ammar, Waleed and Dyer, Chris and Smith, Noah A.},
title = {Conditional Random Field Autoencoders for Unsupervised Structured Prediction},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate connections to traditional autoencoders, posterior regularization, and multi-view learning. We then show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training the proposed model can be substantially more efficient than a comparable feature-rich baseline.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3311–3319},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969195,
author = {Sarkhel, Somdeb and Venugopal, Deepak and Singla, Parag and Gogate, Vibhav},
title = {An Integer Polynomial Programming Based Framework for Lifted MAP Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we present a new approach for lifted MAP inference in Markov logic networks (MLNs). The key idea in our approach is to compactly encode the MAP inference problem as an Integer Polynomial Program (IPP) by schematically applying three lifted inference steps to the MLN: lifted decomposition, lifted conditioning, and partial grounding. Our IPP encoding is lifted in the sense that an integer assignment to a variable in the IPP may represent a truth-assignment to multiple indistinguishable ground atoms in the MLN. We show how to solve the IPP by first converting it to an Integer Linear Program (ILP) and then solving the latter using state-of-the-art ILP techniques. Experiments on several benchmark MLNs show that our new algorithm is substantially superior to ground inference and existing methods in terms of computational efficiency and solution quality.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3302–3310},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969194,
author = {He, He and Daum\'{e}, Hal and Eisner, Jason},
title = {Learning to Search in Branch-and-Bound Algorithms},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3293–3301},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969193,
author = {Richard, Emile and Obozinski, Guillaume and Vert, Jean-Philippe},
title = {Tight Convex Relaxations for Sparse Matrix Factorization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of non-zero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension [1] of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual ℓ1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3284–3292},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969192,
author = {Aslan, \"{O}zlem and Zhang, Xinhua and Schuurmans, Dale},
title = {Convex Deep Learning via Normalized Kernels},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3275–3283},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969191,
author = {Srikumar, Vivek and Manning, Christopher D.},
title = {Learning Distributed Representations for Structured Output Prediction},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs. However, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning. In this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters. We extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches. We define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning. We show that our formulation outperforms structural SVM baselines in two tasks: multiclass document classification and part-of-speech tagging.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3266–3274},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969190,
author = {Gal, Yarin and Wilk, Mark van der and Rasmussen, Carl E.},
title = {Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research.We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting.We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3257–3265},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969189,
author = {Kantchelian, Alex and Tschantz, Michael Carl and Huang, Ling and Bartlett, Peter L. and Joseph, Anthony D. and Tygar, J. D.},
title = {Large-Margin Convex Polytope Machine},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3248–3256},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969188,
author = {Lopes, Miles E.},
title = {A Residual Bootstrap for High-Dimensional Regression with near Low-Rank Designs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts $c^{top}(hat{beta}_{rho}-beta)$, where $hat{beta}_{rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that p ≪ n, where the design matrix is of size n \texttimes{} p. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where p/n ≍ 1. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank — in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values X⊤i β, where X⊤i is the ith row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB simultaneously approximates all of the laws $X_i^{top}(hat{beta}_{rho}-beta)$, i = 1,..., n. This result is also notable as it imposes no sparsity assumptions on β. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3239–3247},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969187,
author = {Koolen, Wouter M. and Malek, Alan and Bartlett, Peter L.},
title = {Efficient Minimax Strategies for Square Loss Games},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider online prediction problems where the loss between the prediction and the outcome is measured by the squared Euclidean distance and its generalization, the squared Mahalanobis distance. We derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the Brier game) and the ℓ2 ball (this setup is related to Gaussian density estimation). We show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state, with coefficients that can be efficiently computed using an explicit recurrence relation. The resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3230–3238},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969186,
author = {Negrinho, Renato and Martins, Andr\'{e} F. T.},
title = {Orbit Regularization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a general framework for regularization based on group-induced ma-jorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to the convex hull of this orbit (the orbitope). We recover several well-known regularizes as particular cases, and reveal a connection between the hyperoctahedral group and the recently proposed sorted ℓ1-norm. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3221–3229},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969185,
author = {McCarter, Calvin and Kim, Seyoung},
title = {On Sparse Gaussian Chain Graph Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3212–3220},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969184,
author = {Ding, Nan and Fang, Youhan and Babbush, Ryan and Chen, Changyou and Skeel, Robert D. and Neven, Hartmut},
title = {Bayesian Sampling Using Stochastic Gradient Thermostats},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3203–3211},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969183,
author = {Yang, Jianbo and Liao, Xuejun and Chen, Minhua and Carin, Lawrence},
title = {Compressive Sensing of Signals from a GMM with Sparse Precision Matrices},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper is concerned with compressive sensing of signals drawn from a Gaussian mixture model (GMM) with sparse precision matrices. Previous work has shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse Gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso. This paper addresses a problem more challenging than both (i) and (ii), by assuming that the GMM is unknown and each signal is only observed through incomplete linear measurements. Under these challenging assumptions, we develop a hierarchical Bayesian method to simultaneously estimate the GMM and recover the signals using solely the incomplete measurements and a Bayesian shrinkage prior that promotes sparsity of the Gaussian precision matrices. In addition, we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available, the sparsity level of precision matrices, and the "incompleteness" of measurements. The proposed method is demonstrated extensively on compressive sensing of imagery and video, and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3194–3202},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969182,
author = {Soufiani, Hossein Azari and Parkes, David C. and Xia, Lirong},
title = {A Statistical Decision-Theoretic Framework for Social Choice},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3185–3193},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969181,
author = {Gopalan, Prem and Charlin, Laurent and Blei, David M.},
title = {Content-Based Recommendations with Poisson Factorization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop collaborative topic Poisson factorization (CTPF), a generative model of articles and reader preferences. CTPF can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest. In detail, CTPF models both reader behavior and article texts with Poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers. This provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership. Further, we exploit stochastic variational inference to model massive real-world datasets. For example, we can fit CPTF to the full arXiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day. We demonstrate empirically that our model outperforms several baselines, including the previous state-of-the art approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3176–3184},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969180,
author = {Yun, Se-Young and Lelarge, Marc and Proutiere, Alexandre},
title = {Streaming, Memory Limited Algorithms for Community Detection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is offline, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is online, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3167–3175},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969179,
author = {Inouye, David I. and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model [1] and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. [1] is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models [2, 3, 4, 5] and measures of model fitness [6] provide strong support that explicitly modeling word dependencies—as in APM—could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because O(p2) parameters must be estimated where p is the number of words ([1] could only provide results for datasets with p = 200). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle p = 104 as an important step towards scaling to large datasets. In addition, Inouye et al. [1] only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word "brings to mind" another word [7]). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3158–3166},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969178,
author = {Gillenwater, Jennifer and Kulesza, Alex and Fox, Emily and Taskar, Ben},
title = {Expectation-Maximization for Learning Determinantal Point Processes},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard [1]. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix [2], or learning weights for a linear combination of DPPs with fixed kernel matrices [3]. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3149–3157},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969177,
author = {Lakshminarayanan, Balaji and Roy, Daniel M. and Teh, Yee Whye},
title = {Mondrian Forests: Efficient Online Random Forests},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically retrained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3140–3148},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969176,
author = {Rangapuram, Syama Sundar and Mudrakarta, Pramod Kaushik and Hein, Matthias},
title = {Tight Continuous Relaxation of the Balanced <i>k</i>-Cut Problem},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the difficult sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method outperforms all existing approaches for ratio cut and other balanced k-cut criteria.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3131–3139},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969175,
author = {Kocaoglu, Murat and Shanmugam, Karthikeyan and Dimakis, Alexandros G. and Klivans, Adam},
title = {Sparse Polynomial Learning and Graph Sketching},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Let f : {-1, 1}n - ℝ be a polynomial with at most s non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples from the uniform distribution on {- 1,1}n that runs in time polynomial in n and 2s and succeeds if the function satisfies the unique sign property: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of f is perturbed by a small random noise, or satisfied with high probability when s parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in n and 2s is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials.Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3122–3130},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969174,
author = {Balcan, Maria-Florina and Kanchanapally, Vandana and Liang, Yingyu and Woodruff, David},
title = {Improved Distributed Principal Component Analysis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as k-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for k-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3113–3121},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969173,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969172,
author = {Putzky, Patrick and Franzen, Florian and Bassetto, Giacomo and Macke, Jakob H.},
title = {A Bayesian Model for Identifying Hierarchically Organised States in Neural Population Activity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales. To understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs. Here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity. Population states are modelled using a hidden Markov decision tree with state-dependent tuning parameters and a generalised linear observation model. We present a variational Bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings. On simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters. Using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model. Finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3095–3103},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969171,
author = {Maddison, Chris J. and Tarlow, Daniel and Minka, Tom},
title = {A* Sampling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem [1, 2, 3, 4]. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* Sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* Sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3086–3094},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969170,
author = {Frostig, Roy and Wang, Sida I. and Liang, Percy and Manning, Christopher D.},
title = {Simple MAP Inference via Low-Rank Relaxations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3077–3085},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969169,
author = {Jaggi, Martin and Smith, Virginia and Tak\'{a}\v{c}, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I.},
title = {Communication-Efficient Distributed Dual Coordinate Ascent},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25 \texttimes{} as quickly.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3068–3076},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969168,
author = {Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
title = {An Accelerated Proximal Coordinate Gradient Method},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop an accelerated randomized proximal coordinate gradient (APCG) method, for solving a broad class of composite convex optimization problems. In particular, our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods. We show how to apply the APCG method to solve the dual of the regularized empirical risk minimization (ERM) problem, and devise efficient implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (SDCA) method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3059–3067},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969167,
author = {Henriques, Jo\~{a}o F. and Martins, Pedro and Caseiro, Rui and Batista, Jorge},
title = {Fast Training of Pose Detectors in the Fourier Domain},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3050–3058},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969166,
author = {Dai, Bo and Xie, Bo and He, Niao and Liang, Yingyu and Raj, Anant and Balcan, Maria-Florina and Song, Le},
title = {Scalable Kernel Methods via Doubly Stochastic Gradients},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper, we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient—one using random training points and another using random features associated with the kernel—and performing descent steps with this noisy functional gradient. Our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization bound of O(1/√t). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3041–3049},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969165,
author = {Singh, Shashank and P\'{o}czos, Barnab\'{a}s},
title = {Exponential Concentration of a Density Functional Estimator},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the d-dimensional unit cube [0,1]d that lie in a β-H\"{o}lder smoothness class, we prove our estimator converges at the rate O(n-β/ β+d). Furthermore, we prove the estimator is exponentially concentrated about its mean, whereas most previous related results have proven only expected error bounds on estimators.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3032–3040},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969164,
author = {Mobin, Shariq A. and Arnemann, James A. and Sommer, Friedrich T.},
title = {Information-Based Learning by Agents in Unbounded State Spaces},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3023–3031},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969163,
author = {Mahmood, A. Rupam and Hasselt, Hado van and Sutton, Richard S.},
title = {Weighted Importance Sampling for Off-Policy Learning with Linear Function Approximation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, weighted importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(λ). We show empirically that our new WIS-LSTD(λ) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(λ) (Yu 2010, Bertsekas &amp; Yu 2009).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3014–3022},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969162,
author = {Zhang, Haichao and Yang, Jianchao},
title = {Scale Adaptive Blind Deblurring},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3005–3013},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969161,
author = {Vinayak, Ramya Korlakai and Oymak, Samet and Hassibi, Babak},
title = {Graph Clustering with Missing Data: Convex Algorithms and Analysis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain explicit bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2996–3004},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969160,
author = {Susemihl, Alex and Opper, Manfred and Meir, Ron},
title = {Optimal Neural Codes for Control and Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory, one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves. However, this sensory adaptation is geared towards control rather than perception, as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for simple control setups. This implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2987–2995},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969159,
author = {Venugopal, Deepak and Gogate, Vibhav},
title = {Scaling-up Importance Sampling for Markov Logic Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2978–2986},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969158,
author = {Grinberg, Yuri and Precup, Doina and Gendreau, Michel},
title = {Optimizing Energy Production Using Policy Search and Predictive State Representations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2969–2977},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969157,
author = {Titsias, Michalis K. and Yau, Christopher},
title = {Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2960–2968},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969156,
author = {Que, Qichao and Belkin, Mikhail and Wang, Yusu},
title = {Learning with Fredholm Kernels},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the "noise assumption" for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2951–2959},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969155,
author = {Semedo, Jo\~{a}o D. and Zandvakili, Amin and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
title = {Extracting Latent Structure from Multiple Interacting Neural Populations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how these latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2942–2950},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969154,
author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2933–2941},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969153,
author = {Mont\'{u}far, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
title = {On the Number of Linear Regions of Deep Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2924–2932},
numpages = {9},
keywords = {neural network, deep learning, maxout, rectifier, input space partition},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969152,
author = {McMahan, H. Brendan and Streeter, Matthew},
title = {Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision, an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2915–2923},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969151,
author = {Saxena, Shreya and Dahleh, Munther},
title = {Real-Time Decoding of an Integrate and Fire Encoder},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neuronal encoding models range from the detailed biophysically-based Hodgkin Huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal. Decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system. For use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to act on this signal, for example to drive a prosthetic device. Here, we introduce a causal, real-time decoder of the biophysically-based Integrate and Fire encoding neuron model. We show that the upper bound of the real-time reconstruction error decreases polyno-mially in time, and that the L2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal. We numerically validate the effect of these parameters on the reconstruction error.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2906–2914},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969150,
author = {Montanari, Andrea and Richard, Emile},
title = {A Statistical Model for Tensor PCA},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio β becomes larger than C√k log k (and in particular β can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the unfolded tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2897–2905},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969149,
author = {Paulus, Romain and Socher, Richard and Manning, Christopher D.},
title = {Global Belief Recursive Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2888–2896},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969148,
author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
title = {Extremal Mechanisms for Local Differential Privacy},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where the data providers and data analysts want to maximize the utility of statistical inferences performed on the released data, we study the fundamental tradeoff between local differential privacy and information theoretic utility functions. We introduce a family of extremal privatization mechanisms, which we call staircase mechanisms, and prove that it contains the optimal privatization mechanism that maximizes utility. We further show that for all information theoretic utility functions studied in this paper, maximizing utility is equivalent to solving a linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the data size. To account for this, we show that two simple staircase mechanisms, the binary and randomized response mechanisms, are universally optimal in the high and low privacy regimes, respectively, and well approximate the intermediate regime.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2879–2887},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969147,
author = {Lieder, Falk and Plunkett, Dillon and Hamrick, Jessica B. and Russell, Stuart J. and Hay, Nicholas J. and Griffiths, Thomas L.},
title = {Algorithm Selection by Rational Metareasoning as a Model of Human Strategy Selection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2870–2878},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969146,
author = {Hardt, Moritz and Price, Eric},
title = {The Noisy Power Method: A Meta Algorithm with Applications},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications:Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound.Private PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2861–2869},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969145,
author = {Bresler, Guy and Gamarnik, David and Shah, Devavrat},
title = {Structure Learning of Antiferromagnetic Ising Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of Ω(pd/2) for learning general graphical models on p nodes of maximum degree d, for the class of so-called statistical algorithms recently introduced by Feldman et al. [1]. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound suggests that the \~{O}(pd+2) runtime required by Bresler, Mossel, and Sly's [2] exhaustive-search algorithm cannot be significantly improved without restricting the class of models.Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari [3] showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time O(p2). We provide an algorithm whose performance interpolates between \~{O}(p2) and \~{O}(pd+2) depending on the strength of the repulsion.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2852–2860},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969144,
author = {Fletcher, Alyson K. and Rangan, Sundeep},
title = {Scalable Inference for Neuronal Connectivity from Calcium Imaging},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons. However, a key challenge in using calcium imaging for connectivity detection is that current systems often have a temporal response and frame rate that can be orders of magnitude slower than the underlying neural spiking process. Bayesian inference methods based on expectation-maximization (EM) have been proposed to overcome these limitations, but are often computationally demanding since the E-step in the EM procedure typically involves state estimation for a high-dimensional nonlinear dynamical system. In this work, we propose a computationally fast method for the state estimation based on a hybrid of loopy belief propagation and approximate message passing (AMP). The key insight is that a neural system as viewed through calcium imaging can be factorized into simple scalar dynamical systems for each neuron with linear interconnections between the neurons. Using the structure, the updates in the proposed hybrid AMP methodology can be computed by a set of one-dimensional state estimation procedures and linear transforms with the connectivity matrix. This yields a computationally scalable method for inferring connectivity of large neural circuits. Simulations of the method on realistic neural networks demonstrate good accuracy with computation times that are potentially significantly faster than current approaches based on Markov Chain Monte Carlo methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2843–2851},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969143,
author = {Lee, Seunghak and Kim, Jin Kyu and Zheng, Xun and Ho, Qirong and Gibson, Garth A. and Xing, Eric P.},
title = {On Model Parallelization and Scheduling Strategies for Distributed Machine Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2834–2842},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969142,
author = {Wimalawarne, Kishan and Sugiyama, Masashi and Tomioka, Ryota},
title = {Multitask Learning Meets Tensor Factorization: Task Imputation via Convex Optimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2825–2833},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969141,
author = {Wang, Huahua and Banerjee, Arindam},
title = {Bregman Alternating Direction Method of Multipliers},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the O(1/T) iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of O(n/ln n) where n is the dimensionality. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2816–2824},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969140,
author = {Derezi\'{n}ski, Michat and Warmuth, Manfred K.},
title = {The Limits of Squared Euclidean Distance Regularization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Some of the simplest loss functions considered in Machine Learning are the square loss, the logistic loss and the hinge loss. The most common family of algorithms, including Gradient Descent (GD) with and without Weight Decay, always predict with a linear combination of the past instances. We give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal. Our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function.This type of result was known for the square loss. However, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above). We also show that algorithms that regularize with the squared Euclidean distance are easily confused by random features. Finally, we conclude by discussing related open problems regarding feed forward neural networks. We conjecture that our hardness results hold for any training algorithm that is based on the squared Euclidean distance regularization (i.e. Back-propagation with the Weight Decay heuristic).},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2807–2815},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969139,
author = {Si, Si and Shin, Donghyuk and Dhillon, Inderjit S. and Parlett, Beresford N.},
title = {Multi-Scale Spectral Decomposition of Massive Graphs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Computing the k dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when k is reasonably large. In this paper, we propose and analyze a novel multi-scale spectral decomposition method (MSEIGS), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently. We show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately. Thus, eigenvectors of the clusters serve as good initializations to a block Lanczos algorithm that is used to compute spectral decomposition of the original graph. We further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations. Our method outperforms widely used solvers in terms of convergence speed and approximation quality. Furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings. For example, on a graph with more than 82 million nodes and 3.6 billion edges, MSEIGS takes less than 3 hours on a single-core machine while Randomized SVD takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors. Using 16 cores, we can reduce this time to less than 40 minutes.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2798–2806},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969138,
author = {Gunter, Tom and Osborne, Michael A. and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J.},
title = {Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2789–2797},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969137,
author = {Neu, Gergely and Valko, Michal},
title = {Online Combinatorial Optimization with Stochastic Decision Sets and Adversarial Losses},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2780–2788},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969136,
author = {Sedghi, Hanie and Anandkumar, Anima and Jonckheere, Edmond},
title = {Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of O(s log d/T) for s-sparse problems in d dimensions in T steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish O(1/T) rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2771–2779},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969135,
author = {Bishop, William E. and Yu, Byron M.},
title = {Deterministic Symmetric Positive Semidefinite Matrix Completion},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of recovering a symmetric, positive semidefinite (SPSD) matrix from a subset of its entries, possibly corrupted by noise. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix. We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met. The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. Finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2762–2770},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969134,
author = {Park, Dohyung and Caramanis, Constantine and Sanghavi, Sujay},
title = {Greedy Subspace Clustering},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2753–2761},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969133,
author = {Koyejo, Oluwasanmi and Natarajan, Nagarajan and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Consistent Binary Classification with Generalized Performance Metrics},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives. Despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy, AM measure, F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2744–2752},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969132,
author = {Chan, Hau and Ortiz, Luis E.},
title = {Computing Nash Equilibria in Generalized Interdependent Security Games},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents' voluntary investment decisions when facing potential direct risk and transfer-risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP-complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2735–2743},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969131,
author = {Garg, Ankit and Ma, Tengyu and Nguy\^{e}n, Huy L.},
title = {On Communication Cost of Distributed Statistical Estimation and Dimensionality},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean θ→ of an unknown d dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean θ→ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting [1] and to our improved bounds for the simultaneous setting, we prove new lower bounds of Ω(md/ log(m)) and Ω(md) for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the mini-max squared loss with O(md) bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be s-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a d/s factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2726–2734},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969130,
author = {Deshpande, Yash and Montanari, Andrea and Richard, Emile},
title = {Cone-Constrained Principal Component Analysis},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However, solving these problems is typically NP-hard.We consider a simple model for noisy quadratic observation of an unknown vector v0. The unknown vector is constrained to belong to a cone C ∋ v0. While optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when C is a convex cone with an efficient projection. This is surprising, since the corresponding optimization problem is non-convex and -from a worst case perspective- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone δ(C). This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2717–2725},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969129,
author = {Qian, Jing and Saligrama, Venkatesh},
title = {Efficient Minimax Signal Detection on Graphs},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph. In these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph. These problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult. We overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (LMI). Computationally efficient tests are then realized by optimizing convex objective functions subject to these LMI constraints. We prove, by means of a novel Euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-D and 2-D lattices. We show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2708–2716},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969128,
author = {Zheng, Charles Y. and Pestilli, Franco and Rokem, Ariel},
title = {Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization [1,2]. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. [3] proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost [4], together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI) [5]. We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2699–2707},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969127,
author = {Bock, Jasper De and Campos, Cassio P. de and Antonucci, Alessandro},
title = {Global Sensitivity Analysis for MAP Inference in Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2690–2698},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969126,
author = {Soh, De Wen and Tatikonda, Sekhar},
title = {Testing Unfaithful Gaussian Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set S graph separates nodes u and v then Xu is conditionally independent of Xv given XS. The opposite direction need not be true, that is, Xu ⊥ Xv | XS need not imply S is a node separator of u and v. When it does, the relation Xu ⊥ Xv | XS is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form Xi ⊥ Xj | XS.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2681–2689},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969125,
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Nets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969124,
author = {Han, Shaobo and Du, Lin and Salazar, Esther and Carin, Lawrence},
title = {Dynamic Rank Factor Model for Text Streams},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (i) discovering topic prevalence over time, and (ii) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2663–2671},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969123,
author = {Ba, Lei Jimmy and Caruana, Rich},
title = {Do Deep Nets Really Need to Be Deep?},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2654–2662},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969122,
author = {Prasad, Adarsh and Jegelka, Stefanie and Batra, Dhruv},
title = {Submodular Meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2645–2653},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969121,
author = {Zhang, Chongjie and Shah, Julie A.},
title = {Fairness in Multi-Agent Sequential Decision-Making},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with a consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2636–2644},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969120,
author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
title = {Convolutional Kernel Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data.Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2627–2635},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969119,
author = {Xin, Yu and Jaakkola, Tommi},
title = {Controlling Privacy in Recommender Systems},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of "public" users who are willing to share their preferences openly, and a large set of "private" users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2618–2626},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969118,
author = {Awasthi, Pranjal and Blum, Avrim and Sheffet, Or and Vijayaraghavan, Aravindan},
title = {Learning Mixtures of Ranking Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2609–2617},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969117,
author = {Tristan, Jean-Baptiste and Huang, Daniel and Tassarotti, Joseph and Pocock, Adam and Green, Stephen J. and Steele, Guy L.},
title = {Augur: Data-Parallel Probabilistic Modeling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Implementing inference procedures for each new probabilistic model is time-consuming and error-prone. Probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure. To make this practical it is important to generate high performance inference code. In turn, on modern architectures, high performance requires parallel execution. In this paper we present Augur, a probabilistic modeling language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2600–2608},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969116,
author = {Bateni, MohammadHossein and Bhaskara, Aditya and Lattanzi, Silvio and Mirrokni, Vahab},
title = {Distributed Balanced Clustering via Mapping Coresets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the "balanced clustering" problem. Although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem. In this paper we develop a new framework based on "mapping coresets" to tackle this issue. Our technique results in first distributed approximation algorithms for balanced clustering problems for a wide range of clustering objective functions such as k-center, k-median, and k-means.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2591–2599},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969115,
author = {Yun, Hyokun and Raman, Parameswaran and Vishwanathan, S. V. N.},
title = {Ranking via Robust Binary Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. It shows competitive performance on standard benchmark datasets against a number of other representative algorithms in the literature. We also discuss extensions of RoBiRank to large scale problems where explicit feature vectors and scores are not given. We show that RoBiRank can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824, 519 pairwise interactions between items to be ranked, RoBiRank finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2582–2590},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969114,
author = {Jiang, Albert Xin and Marcolino, Leandro Soriano and Procaccia, Ariel D. and Sandholm, Tuomas and Shah, Nisarg and Tambe, Milind},
title = {Diverse Randomized Agents Vote to Win},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate the power of voting among diverse, randomized software agents. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning. This model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models). We analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows. Our experiments, which pit teams of computer Go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2573–2581},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969113,
author = {Lawlor, Matthew and Zucker, Steven W.},
title = {Feedforward Learning of Mixture Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2564–2572},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969112,
author = {Dasgupta, Sanjoy and Kpotufe, Samory},
title = {Optimal Rates for K-NN Density and Mode Estimation},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present two related contributions of independent interest: (1) high-probability finite sample rates for k-NN density estimation, and (2) practical mode estimators - based on k-NN - which attain minimax-optimal rates under surprisingly general distributional conditions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2555–2563},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969111,
author = {Vega-Brown, William and Doniec, Marek and Roy, Nicholas},
title = {Nonparametric Bayesian Inference on Multivariate Exponential Families},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a model by choosing the maximum entropy distribution from the set of models satisfying certain smoothness and independence criteria; we show that inference on this model generalizes local kernel estimation to the context of Bayesian inference on stochastic processes. Our model enables Bayesian inference in contexts when standard techniques like Gaussian process inference are too expensive to apply. Exact inference on our model is possible for any likelihood function from the exponential family. Inference is then highly efficient, requiring only O (log N) time and O (N) space at run time. We demonstrate our algorithm on several problems and show quantifiable improvement in both speed and performance relative to models based on the Gaussian process.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2546–2554},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969110,
author = {Gens, Robert and Domingos, Pedro},
title = {Deep Symmetry Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2537–2545},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969109,
author = {Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},
title = {Design Principles of the Hippocampal Cognitive Map},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2528–2536},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969108,
author = {Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},
title = {Difference of Convex Functions Programming for Reinforcement Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Large Markov Decision Processes are usually solved using Approximate Dynamic Programming methods such as Approximate Value Iteration or Approximate Policy Iteration. The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) T*Q — Q, where T* is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning problem.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2519–2527},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969107,
author = {Su, Weijie and Boyd, Stephen and Cand\`{e}s, Emmanuel J.},
title = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2510–2518},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969106,
author = {Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},
title = {Multi-Class Deep Boosting},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble's mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2501–2509},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969105,
author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},
title = {Reputation-Based Worker Filtering in Crowdsourcing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of adversarial workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowd-sourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of sophisticated adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2492–2500},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969104,
author = {Ulrich, Kyle and Carlson, David E. and Lian, Wenzhao and Borg, Jana Schaich and Dzirasa, Kafui and Carin, Lawrence},
title = {Analysis of Brain States from Multi-Region LFP Time-Series},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a "brain state," relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2483–2491},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969103,
author = {Farajtabar, Mehrdad and Du, Nan and Gomez-Rodriguez, Manuel and Valera, Isabel and Zha, Hongyuan and Song, Le},
title = {Shaping Social Activity by Incentivizing Users},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Events in an online social network can be categorized roughly into endogenous events, where users just respond to the actions of their neighbors within the network, or exogenous events, where users take actions due to drives external to the network. How much external drive should be provided to each user, such that the network activity can be steered towards a target state? In this paper, we model social events using multivariate Hawkes processes, which can capture both endogenous and exogenous event intensities, and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity. Exploiting this connection, we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a desired activity level. We experimented with event data gathered from Twitter, and show that our method can steer the activity of the network more accurately than alternatives.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2474–2482},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969102,
author = {Patil, Kaustubh Raosaheb and Zhu, Xiaojin and Kope\'{c}, undefinedukasz and Love, Bradley C.},
title = {Optimal Teaching for Limited-Capacity Human Learners},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people's category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2465–2473},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969101,
author = {Yen, Ian E. H. and Lin, Ting-Wei and Lin, Shou-De and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Sparse Random Features Algorithm as Coordinate Descent in Hilbert Space},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a Sparse Random Features algorithm, which learns a sparse non-linear predictor by minimizing an ℓ1-regularized objective function over the Hilbert Space induced from a kernel function. By interpreting the algorithm as Randomized Coordinate Descent in an infinite-dimensional space, we show the proposed approach converges to a solution within ∊-precision of that using an exact kernel method, by drawing O(1/∊) random features, in contrast to the O(1/∊2) convergence achieved by current Monte-Carlo analyses of Random Features. In our experiments, the Sparse Random Feature algorithm obtains a sparse solution that requires less memory and prediction time, while maintaining comparable performance on regression and classification tasks. Moreover, as an approximate solver for the infinite-dimensional ℓ1-regularized problem, the randomized approach also enjoys better convergence guarantees than a Boosting approach in the setting where the greedy Boosting step cannot be performed exactly.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2456–2464},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969100,
author = {Sharma, Abhishek and Tuzel, Oncel and Liu, Ming-Yu},
title = {Recursive Context Propagation Network for Semantic Scene Labeling},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256 x 256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2447–2455},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969099,
author = {Mandt, Stephan and Blei, David},
title = {Smoothed Gradients for Stochastic Variational Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2438–2446},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969098,
author = {Cui, Zhen and Chang, Hong and Shan, Shiguang and Chen, Xilin},
title = {Generalized Unsupervised Manifold Alignment},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a Generalized Unsupervised Manifold Alignment (GU-MA) method to build the connections between different but correlated datasets without any known correspondences. Based on the assumption that datasets of the same theme usually have similar manifold structures, GUMA is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space. The main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsuper-vised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases. Experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2429–2437},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969097,
author = {Moon, Kevin R. and III, Alfred O. Hero},
title = {Multivariate <i>f</i>-Divergence Estimation with Confidence},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several nonparametric divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O (1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2420–2428},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969096,
author = {Wang, Xin and Bi, Jinbo and Yu, Shipeng and Sun, Jiangwen},
title = {On Multiplicative Multitask Feature Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2411–2419},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969095,
author = {Sun, Xu},
title = {Structure Regularization for Structured Prediction},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via structure decomposition, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2402–2410},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969094,
author = {Sadowski, Peter and Baldi, Pierre and Whiteson, Daniel},
title = {Searching for Higgs Boson Decay Modes with Deep Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Particle colliders enable us to probe the fundamental nature of matter by observing exotic particles produced by high-energy collisions. Because the experimental measurements from these collisions are necessarily incomplete and imprecise, machine learning algorithms play a major role in the analysis of experimental data. The high-energy physics community typically relies on standardized machine learning software packages for this analysis, and devotes substantial effort towards improving statistical power by hand-crafting high-level features derived from the raw collider measurements. In this paper, we train artificial neural networks to detect the decay of the Higgs boson to tau leptons on a dataset of 82 million simulated collision events. We demonstrate that deep neural network architectures are particularly well-suited for this task with the ability to automatically discover high-level features from the data and increase discovery significance.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2393–2401},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969093,
author = {Ozdemir, Bahadir and Davis, Larry S.},
title = {A Probabilistic Framework for Multimodal Retrieval Using Integrative Indian Buffet Process},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a multimodal retrieval procedure based on latent feature models. The procedure consists of a Bayesian nonparametric framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback. Experiments on two multimodal datasets, PASCAL-Sentence and SUN-Attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2384–2392},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969092,
author = {Zhong, Kai and Yen, Ian E. H. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
title = {Proximal Quasi-Newton for Computationally Intensive ℓ<sub>1</sub>-Regularized <i>M</i>-Estimators},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the class of optimization problems arising from computationally intensive ℓ1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the ℓ1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. ℓ1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that the proximal quasi-Newton method is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2375–2383},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969091,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
title = {Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2366–2374},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969090,
author = {Kangas, Kustaa and Niinim\"{a}ki, Teppo and Koivisto, Mikko},
title = {Learning Chordal Markov Networks by Dynamic Programming},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees, and it runs in O(4n) time for n vertices. On an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (Corander et al., NIPS 2013). Within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens, UAI 2013). Our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2357–2365},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969089,
author = {Kiros, Ryan and Zemel, Richard S. and Salakhutdinov, Ruslan},
title = {A Multiplicative Model for Learning Distributed Text-Based Attribute Representations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2348–2356},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969088,
author = {Hu, Huining and Li, Zhentao and Vetta, Adrian},
title = {Randomized Experimental Design for Causal Graph Discovery},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann [1] showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization - in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs.We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erd\"{o}s-R\'{e}nyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results.Our work exploits a structural characterization of essential graphs by Andersson et al. [2]. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2339–2347},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969087,
author = {Linderman, Scott W. and Stock, Christopher H. and Adams, Ryan P.},
title = {A Framework for Studying Synaptic Plasticity with Neural Spike Train Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2330–2338},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969086,
author = {Shrivastava, Anshumali and Li, Ping},
title = {Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2321–2329},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969085,
author = {Mohapatra, Pritish and Jawahar, C. V. and Kumar, M. Pawan},
title = {Efficient Optimization for Average Precision SVM},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The accuracy of information retrieval systems is often measured using average precision (AP). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using the AP-SVM framework, which minimizes a regularized convex upper bound on the empirical AP loss. However, the high computational complexity of loss-augmented inference, which is required for learning an AP-SVM, prohibits its use with large training datasets. To alleviate this deficiency, we propose three complementary approaches. The first approach guarantees an asymptotic decrease in the computational complexity of loss-augmented inference by exploiting the problem structure. The second approach takes advantage of the fact that we do not require a full ranking during loss-augmented inference. This helps us to avoid the expensive step of sorting the negative samples according to their individual scores. The third approach approximates the AP loss over all samples by the AP loss over difficult samples (for example, those that are incorrectly classified by a binary SVM), while ensuring the correct classification of the remaining samples. Using the PASCAL VOC action classification and object detection datasets, we show that our approaches provide significant speed-ups during training without degrading the test accuracy of AP-SVM.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2312–2320},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969084,
author = {Shanmugam, Karthikeyan and Tandon, Rashish and Dimakis, Alexandros G. and Ravikumar, Pradeep},
title = {On the Information Theoretic Limits of Learning Ising Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2303–2311},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969083,
author = {Koolen, Wouter M. and Erven, Tim van and Gr\"{u}nwald, Peter D.},
title = {Learning the Learning Rate for Prediction with Expert Advice},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2294–2302},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969082,
author = {Nie, Siqi and Mau\'{a}, Denis D. and Campos, Cassio P. de and Ji, Qiang},
title = {Advances in Learning Bayesian Networks of Bounded Treewidth},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work presents novel algorithms for learning Bayesian networks of bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. The approaches are empirically compared to each other and to state-of-the-art methods on a collection of public data sets with up to 100 variables.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2285–2293},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969081,
author = {Tootoonian, Sina and Lengyel, M\'{a}t\'{e}},
title = {A Dual Algorithm for Olfactory Computation in the Locust Brain},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2276–2284},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969080,
author = {Naghibi, Tofigh and Pfister, Beat},
title = {A Boosting Framework on Grounds of Online Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2267–2275},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969079,
author = {Avron, Haim and Nguyundefinedn, Huy L. and Woodruff, David P.},
title = {Subspace Embeddings for the Polynomial Kernel},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first fast oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel without explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2258–2266},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969078,
author = {S\"{u}bakan, Y. Cem and Traa, Johannes and Smaragdis, Paris},
title = {Spectral Learning of Mixture of Hidden Markov Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2249–2257},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969077,
author = {Hennequin, Guillaume and Aitchison, Laurence and Lengyel, M\'{a}t\'{e}},
title = {Fast Sampling-Based Inference in Balanced Neuronal Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong - but transient - selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2240–2248},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969076,
author = {Festa, Dylan and Hennequin, Guillaume and Lengyel, M\'{a}t\ae{}},
title = {Analog Memories in a Balanced Rate-Based Network of E-I Neurons},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2231–2239},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969075,
author = {Balcan, Maria-Florina and Berlind, Christopher and Blum, Avrim and Cohen, Emma and Patnaik, Kaushik and Song, Le},
title = {Active Learning and Best-Response Dynamics},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2222–2230},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969074,
author = {Bui, Thang and Turner, Richard},
title = {Tree-Structured Gaussian Process Approximations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gaussian process regression can be accelerated by constructing a small pseudo-dataset to summarize the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimization. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2213–2221},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969073,
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
title = {Recurrent Models of Visual Attention},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2204–2212},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969072,
author = {Wang, Xiangyu and Peng, Peichao and Dunson, David B.},
title = {Median Selection Subset Aggregation for Parallel Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the 'median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2195–2203},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969071,
author = {Saberian, Mohammad and Vasconcelos, Nuno},
title = {Multi-Resolution Cascades for Multiclass Object Detection},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An algorithm for learning fast multiclass object detection cascades is introduced. It produces multi-resolution (MRes) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass classifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner. This MRes structure is achieved with a new structurally biased boosting algorithm (SBBoost). SBBost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes. It is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired MRes structure. This is accomplished through a generalized definition of multiclass margin, which includes a set of bias parameters. SBBoost is a boosting algorithm for maximization of this margin. It can also be interpreted as standard multiclass boosting algorithm augmented with margin thresholds or a cost-sensitive boosting algorithm with costs defined by the bias parameters. A stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner. This is shown to produce MRes cascades that have high detection rate and are computationally efficient. Experiments on multiclass object detection show improved performance over previous solutions.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2186–2194},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969070,
author = {Levy, Omer and Goldberg, Yoav},
title = {Neural Word Embedding as Implicit Matrix Factorization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2177–2185},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969069,
author = {Lim, Cong Han and Wright, Stephen J.},
title = {Beyond the Birkhoff Polytope: Convex Relaxations for Vector Permutation Problems},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Birkhoff polytope (the convex hull of the set of permutation matrices), which is represented using Θ(n2) variables and constraints, is frequently invoked in formulating relaxations of optimization problems over permutations. Using a recent construction of Goemans [1], we show that when optimizing over the convex hull of the permutation vectors (the permutahedron), we can reduce the number of variables and constraints to Θ(n log n) in theory and Θ(n log2 n) in practice. We modify the recent convex formulation of the 2-SUM problem introduced by Fogel et al. [2] to use this polytope, and demonstrate how we can attain results of similar quality in significantly less computational time for large n. To our knowledge, this is the first usage of Goemans' compact formulation of the permutahedron in a convex optimization problem. We also introduce a simpler regularization scheme for this convex formulation of the 2-SUM problem that yields good empirical results.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2168–2176},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969068,
author = {Yang, Eunho and Lozano, Aur\'{e}iie C. and Ravikumar, Pradeep},
title = {Elementary Estimators for Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE "breaks down" under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the ℓ1-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2159–2167},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969067,
author = {Kervrann, Charles},
title = {PEWA: Patch-Based Exponentially Weighted Aggregation for Image Denoising},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Patch-based methods have been widely used for noise reduction in recent years. In this paper, we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms. We show that weakly denoised versions of the input image obtained with standard methods, can serve to compute an efficient patch-based aggregated estimator. In our approach, we evaluate the Stein's Unbiased Risk Estimator (SURE) of each denoised candidate image patch and use this information to compute the exponential weighted aggregation (EWA) estimator. The aggregation method is flexible enough to combine any standard denoising algorithm and has an interpretation with Gibbs distribution. The denoising algorithm (PEWA) is based on a MCMC sampling and is able to produce results that are comparable to the current state-of-the-art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2150–2158},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969066,
author = {Sohn, Kihyuk and Shang, Wenling and Lee, Honglak},
title = {Improved Multimodal Deep Learning with Variation of Information},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks. Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training. In addition, we extend to deep networks with recurrent encoding structure to finetune the whole network. In experiments, we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr database and PASCAL VOC 2007 database with and without text features.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2141–2149},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969065,
author = {Wang, Jie and Ye, Jieping},
title = {Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the ℓ1 and ℓ2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2132–2140},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969064,
author = {Parambath, Shameem A. Puthiya and Usunier, Nicolas and Grandvalet, Yves},
title = {Optimizing F-Measures by Cost-Sensitive Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2123–2131},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969063,
author = {Houlsby, Neil M. T. and Blei, David M.},
title = {A Filtering Approach to Stochastic Variational Inference},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2114–2122},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969062,
author = {Conejo, Bruno and Komodakis, Nikos and Leprince, Sebastien and Avouac, Jean Philippe},
title = {Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or in short as IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line [4].},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2105–2113},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969061,
author = {undefinedrsoy, Ozan and Cardie, Claire},
title = {Deep Recursive Neural Networks for Compositionality in Language},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model com-positionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture — a deep recursive neural network (deep RNN) — constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2096–2104},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969060,
author = {Li, Bo and Vorobeychik, Yevgeniy},
title = {Feature Cross-Substitution in Adversarial Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2087–2095},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969059,
author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-Paced Learning with Diversity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969058,
author = {Gong, Boqing and Chao, Wei-Lun and Grauman, Kristen and Sha, Fei},
title = {Diverse Sequential Subset Selection for Supervised Video Summarization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a probabilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Meanwhile, seqDPP retains the power of modeling diverse subsets, essential for summarization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard DPP model.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2069–2077},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969057,
author = {Carlson, David E. and Borg, Jana Schaich and Dzirasa, Kafui and Carin, Lawrence},
title = {On the Relationship between LFP &amp; Spiking Data},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2060–2068},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969056,
author = {Agarwal, Alekh and Beygelzimer, Alina and Hsu, Daniel and Langford, John and Telgarsky, Matus},
title = {Scalable Nonlinear Learning with Adaptive Polynomial Expansions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2051–2059},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969055,
author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
title = {Convolutional Neural Network Architectures for Matching Natural Language Sentences},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Semantic matching is of central importance to many natural language tasks [2,28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2042–2050},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969054,
author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},
title = {Attentional Neural Network: Feature Selection Using Cognitive Feedback},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2033–2041},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969053,
author = {Savin, Cristina and Deneve, Sophie},
title = {Spatio-Temporal Representations of Uncertainty in Spiking Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2024–2032},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969052,
author = {Z\"{o}hrer, Matthias and Pernkopf, Franz},
title = {General Stochastic Networks for Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter λ. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as ℓ1, ℓ2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2015–2023},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969051,
author = {Hsieh, Cho-Jui and Dhillon, Inderjit S. and Ravikumar, Pradeep and Becker, Stephen and Olsen, Peder A.},
title = {QUIC &amp; DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we develop a family of algorithms for optimizing "superposition-structured" or "dirty" statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid reg-ularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2006–2014},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969050,
author = {Bansal, Trapit and Bhattacharyya, C. and Kannan, Ravindran},
title = {A Provable SVD-Based Algorithm for Learning Topics in Dominant Admixture Corpus},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures, is NP-hard. Making a strong assumption called separability, [4] gave the first provable algorithm for inference. For the widely used LDA model, [6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn topic vectors with bounded l1 error (a natural measure for probability vectors).Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded l1 error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on w0, the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1997–2005},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969049,
author = {Sun, Yi and Chen, Yuheng and Wang, Xiaogang and Tang, Xiaoou},
title = {Deep Learning Face Representation by Joint Identification-Verification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1988–1996},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969048,
author = {Petrik, Marek and Subramanian, Dharmashankar},
title = {RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the γ-discounted infinite horizon performance loss by a factor of 1/(1-γ) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1979–1987},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969047,
author = {Liang, Jingwei and Fadili, Jalal M. and Peyr\'{e}, Gabriel},
title = {Local Linear Convergence of Forward–Backward under Partial Smoothness},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we consider the Forward-Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relative to an active manifold M. We propose a generic framework under which we show that the Forward-Backward (i) correctly identifies the active manifold M in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1970–1978},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969046,
author = {Yoshikawa, Yuya and Iwata, Tomoharu and Sawada, Hiroshi},
title = {Latent Support Measure Machines for Bag-of-Words Data Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many classification problems, the input is represented as a set of features, e.g., the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be defined properly. However, SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness, we propose a kernel-based discriminative classifier for BoW data, which we call the latent support measure machine (latent SMM). With the latent SMM, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments, we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1961–1969},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969045,
author = {Kim, Been and Rudin, Cynthia and Shah, Julie},
title = {The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the "quintessential" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1952–1960},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969044,
author = {Huang, Yanping and Rao, Rajesh P. N.},
title = {Neurons as Monte Carlo Samplers: Bayesian Inference and Learning in Spiking Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a spiking network model capable of performing both approximate inference and learning for any hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how such networks can sequentially learn hidden Markov models using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1943–1951},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969043,
author = {Chatterjee, Soumyadeep and Chen, Sheng and Banerjee, Arindam},
title = {Generalized Dantzig Selector: Application to the <i>k</i>-Support Norm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1934–1942},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969042,
author = {Michalski, Vincent and Memisevic, Roland and Konda, Kishore},
title = {Modeling Deep Temporal Dependencies with Recurrent "Grammar Cells"},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the "syntax" of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1925–1933},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969041,
author = {Yu, Aron and Grauman, Kristen},
title = {Predicting Useful Neighborhoods for Lazy Local Learning},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Lazy local learning methods train a classifier "on the fly" at test time, using only a subset of the training instances that are most relevant to the novel test example. The goal is to tailor the classifier to the properties of the data surrounding the test example. Existing methods assume that the instances most useful for building the local model are strictly those closest to the test example. However, this fails to account for the fact that the success of the resulting classifier depends on the full distribution of selected training instances. Rather than simply gathering the test example's nearest neighbors, we propose to predict the subset of training data that is jointly relevant to training its local model. We develop an approach to discover patterns between queries and their "good" neighborhoods using large-scale multi-label classification with compressed sensing. Given a novel test point, we estimate both the composition and size of the training subset likely to yield an accurate local model. We demonstrate the approach on image classification tasks on SUN and aPascal and show its advantages over traditional global and local approaches.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1916–1924},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969040,
author = {Pan, Yunpeng and Theodorou, Evangelos A.},
title = {Probabilistic Differential Dynamic Programming},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1907–1915},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969039,
author = {Wang, Xuezhi and Schneider, Jeff},
title = {Flexible Transfer Learning under Support and Model Shift},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations X across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels Y and adjusting conditional distributions P(X|Y ), such that P(X) can be matched across domains. However, covariate shift assumes that the support of test P(X) is contained in the support of training P(X), i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for P(Y). Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both X and Y by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1898–1906},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969038,
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
title = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1889–1897},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969037,
author = {Grabska-Barwi\'{n}ska, Agnieszka and Pillow, Jonathan W.},
title = {Optimal Prior-Dependent Neural Population Codes under Shared Input Noise},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent "input noise" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1880–1888},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969036,
author = {Mohri, Mehryar and Medina, Andres Mu\~{n}oz},
title = {Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previously best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than Ω(√T). We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in O(log T), an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several different scenarios.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1871–1879},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969035,
author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch\"{o}n, Thomas B.},
title = {Sequential Monte Carlo for Graphical Models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1862–1870},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2969033.2969034,
author = {Chandar, A P Sarath and Lauly, Stanislas and Larochelle, Hugo and Khapra, Mitesh M and Ravindran, Balaraman and Raykar, Vikas and Saha, Amrita},
title = {An Autoencoder Approach to Learning Bilingual Word Representations},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Cross-language learning allows one to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are coherent between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1853–1861},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@proceedings{10.5555/2969033,
title = {NIPS'14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
location = {Montreal, Canada}
}

