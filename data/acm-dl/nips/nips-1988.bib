@inproceedings{10.5555/2969735.2969829,
author = {Miller, John P.},
title = {Cricket Wind Detection},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A great deal of interest has recently been focused on theories concerning parallel distributed processing in central nervous systems. In particular, many researchers have become very interested in the structure and function of "computational maps" in sensory systems. As defined in a recent review (Knudsen et al, 1987), a "map" is an array of nerve cells, within which there is a systematic variation in the "tuning" of neighboring cells for a particular parameter. For example, the projection from retina to visual cortex is a relatively simple topographic map; each cortical hypercolumn itself contains a more complex "computational" map of preferred line orientation representing the angle of tilt of a simple line stimulus.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {802–807},
numpages = {6},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969828,
author = {Bridle, John S.},
title = {Speech Recognition: Statistical and Neural Information Processing Approaches},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Automatic Speech Recognition (ASR) is an artificial perception problem: the input is raw, continuous patterns (no symbols!) and the desired output, which may be words, phonemes, meaning or text, is symbolic. The most successful approach to automatic speech recognition is based on stochastic models. A stochastic model is a theoretical system whose internal state and output undergo a series of transformations governed by probabilistic laws [1]. In the application to speech recognition the unknown patterns of sound are treated as if they were outputs of a stochastic system [18,2]. Information about the classes of patterns is encoded as the structure of these "laws" and the probabilities that govern their operation. The most popular type of SM for ASR is also known as a "hidden Markov model."},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {796–801},
numpages = {6},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969827,
author = {Konishi, M.},
title = {Song Learning in Birds},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Birds sing to communicate. Male birds use song to advertise their territories and attract females. Each bird species has a unique song or set of songs. Song conveys both species and individual identity. In most species, young birds learn some features of adult song. Song develops gradually from amorphous to fixed patterns of vocalization as if crystals form out of liquid. Learning of a song proceeds in two steps; birds commit the song to memory in the first stage and then they vocally reproduce it in the second stage. The two stages overlap each other in some species, while they are separated by several months in other species. The ability of a bird to commit a song to memory is restricted to a period known as the sensitive phase. Vocal reproduction of the memorized song requires auditory feedback. Birds deafened before the second stage cannot reproduce the memorized song. Birds change vocal output until it matches with the memorized song, which thus serves as a template. Birds use a built-in template when a tutor model is not available. Exposure to a tutor model modifies this innate template.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {795},
numpages = {1},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969826,
author = {Braitenberg, Valentino},
title = {Neural Architecture},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {794},
numpages = {1},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969825,
author = {Andreou, Andreas G.},
title = {Electronic Receptors for Tactile/Haptic Sensing},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We discuss synthetic receptors for haptic sensing. These are based on magnetic field sensors (Hall effect structures) fabricated using standard CMOS technologies. These receptors, biased with a small permanent magnet can detect the presence of ferro or ferri-magnetic objects in the vicinity of the sensor. They can also detect the magnitude and direction of the magnetic field.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {785–792},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969824,
author = {Walker, M. R. and Haghighi, S. and Afghan, A. and Akers, L. A.},
title = {Training a Limited-Interconnect, Synthetic Neural IC},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hardware implementation of neuromorphic algorithms is hampered by high degrees of connectivity. Functionally equivalent feedforward networks may be formed by using limited fan-in nodes and additional layers, but this complicates procedures for determining weight magnitudes. No direct mapping of weights exists between fully and limited-interconnect nets. Low-level nonlinearities prevent the formation of internal representations of widely separated spatial features and the use of gradient descent methods to minimize output error is hampered by error magnitude dissipation. The judicious use of linear summations or collection units is proposed as a solution.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {777–784},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969823,
author = {Hartstein, A. and Koch, R. H.},
title = {A Self-Learning Neural Network},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new neural network structure that is compatible with silicon technology and has built-in learning capability. The thrust of this network work is a new synapse function. The synapses have the feature that the learning parameter is embodied in the thresholds of MOSFET devices and is local in character. The network is shown to be capable of learning by example as well as exhibiting the desirable features of the Hopfield type networks.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {769–776},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969822,
author = {Schwartz, D. B. and Howard, R. E. and Hubbard, W. E.},
title = {Adaptive Neural Networks Using MOS Charge Storage},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {MOS charge storage has been demonstrated as an effective method to store the weights in VLSI implementations of neural network models by several workers. However, to achieve the full power of a VLSI implementation of an adaptive algorithm, the learning operation must built into the circuit. We have fabricated and tested a circuit ideal for this purpose by connecting a pair of capacitors with a CCD like structure, allowing for variable size weight changes as well as a weight decay operation. A 2.5µ CMOS version achieves better than 10 bits of dynamic range in a 140µ \texttimes{} 350µ area. A 1.25µ chip based upon the same cell has 1104 weights on a 3.5mm \texttimes{} 6.0mm die and is capable of peak learning rates of at least 2 \texttimes{} 109 weight changes per second.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {761–768},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969821,
author = {Alspector, Joshua and Gupta, Bhusan and Allen, Robert B.},
title = {Performance of a Stochastic Learning Microchip},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have fabricated a test chip in 2 micron CMOS that can perform supervised learning in a manner similar to the Boltzmann machine. Patterns can be presented to it at 100,000 per second. The chip learns to solve the XOR problem in a few milliseconds. We also have demonstrated the capability to do unsupervised competitive learning with it. The functions of the chip components are examined and the performance is assessed.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {748–760},
numpages = {13},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969820,
author = {Mann, James R. and Gilbert, Sheldon},
title = {An Analog Self-Organizing Neural Network Chip},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A design for a fully analog version of a self-organizing feature map neural network has been completed. Several parts of this design are in fabrication. The feature map algorithm was modified to accommodate circuit solutions to the various computations required. Performance effects were measured by simulating the design as part of a frontend for a speech recognition system. Circuits are included to implement both activation computations and weight adaption or learning. External access to the analog weight values is provided to facilitate weight initialization, testing and static storage. This fully analog implementation requires an order of magnitude less area than a comparable digital/analog hybrid version developed earlier.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {739–747},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969819,
author = {Allinson, Nigel M. and Johnson, Martin J. and Moon, Kevin J.},
title = {Digital Realisation of Self-Organising Maps},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A digital realisation of two-dimensional self-organising feature maps is presented. The method is based on subspace classification using an n-tuple technique. Weight vector approximation and orthogonal projections to produce a winner-takes-all network are also discussed. Over one million effective binary weights can be applied in 25ms using a conventional microcomputer. Details of a number of image recognition tasks, including character recognition and object centring, are described.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {728–738},
numpages = {11},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969818,
author = {Delbr\"{u}ck, T. and Mead, C. A.},
title = {An Electronic Photoreceptor Sensitive to Small Changes in Intensity},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe an electronic photoreceptor circuit that is sensitive to small changes in incident light intensity. The sensitivity to changes in the intensity is achieved by feeding back to the input a filtered version of the output. The feedback loop includes a hysteretic element. The circuit behaves in a manner reminiscent of the gain control properties and temporal responses of a variety of retinal cells, particularly retinal bipolar cells. We compare the thresholds for detection of intensity increments by a human and by the circuit. Both obey Weber's law and for both the temporal contrast sensitivities are nearly identical.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {720–727},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969817,
author = {Mueller, Paul and Van Der Spiegel, Jan and Blackman, David and Chiu, Timothy and Clare, Thomas and Dao, Joseph and Donham, Christopher and Hsieh, Tzu-Pu and Loinaz, Marc},
title = {A Programmable Analog Neural Computer and Simulator},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This report describes the design of a programmable general purpose analog neural computer and simulator. It is intended primarily for real-world real-time computations such as analysis of visual or acoustical patterns, robotics and the development of special purpose neural nets. The machine is scalable and composed of interconnected modules containing arrays of neurons, modifiable synapses and switches. It runs entirely in analog mode but connection architecture, synaptic gains and time constants as well as neuron parameters are set digitally. Each neuron has a limited number of inputs and can be connected to any but not all other neurons. For the determination of synaptic gains and the implementation of learning algorithms the neuron outputs are multiplexed, A/D converted and stored in digital memory. Even at moderate size of 103 to 105 neurons computational speed is expected to exceed that of any current digital computer.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {712–719},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969816,
author = {Lazzaro, J. and Ryckebusch, S. and Mahowald, M. A. and Mead, C. A.},
title = {Winner-Take-All Networks of O(N) Complexity},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have designed, fabricated, and tested a series of compact CMOS integrated circuits that realize the winner-take-all function. These analog, continuous-time circuits use only O(n) of interconnect to perform this function. We have also modified the winner-take-all circuit, realizing a circuit that computes local nonlinear inhibition.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {703–711},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969815,
author = {Nabet, Bahram and Darling, Robert B. and Pinter, Robert B.},
title = {Analog Implementation of Shunting Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An extremely compact, all analog and fully parallel implementation of a class of shunting recurrent neural networks that is applicable to a wide variety of FET-based integration technologies is proposed. While the contrast enhancement, data compression, and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extraction for a content addressable memory (CAM) system, the network also admits a global Liapunov function and can thus achieve stable CAM storage itself. In addition the model can readily function as a front-end processor to an analog adaptive resonance circuit.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {695–702},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969814,
author = {Harris, John G.},
title = {An Analog VLSI Chip for Thin-Plate Surface Interpolation},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Reconstructing a surface from sparse sensory data is a well-known problem in computer vision. This paper describes an experimental analog VLSI chip for smooth surface interpolation from sparse depth data. An eight-node ID network was designed in 3µm CMOS and successfully tested. The network minimizes a second-order or "thin-plate" energy of the surface. The circuit directly implements the coupled depth/slope model of surface reconstruction (Harris, 1987). In addition, this chip can provide Gaussian-like smoothing of images.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {687–694},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969813,
author = {Meador, Jack L. and Cole, Clint S.},
title = {A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes a CMOS artificial neuron. The circuit is directly derived from the voltage-gated channel model of neural membrane, has low power dissipation, and small layout geometry. The principal motivations behind this work include a desire for high performance, more accurate neuron emulation, and the need for higher density in practical neural network implementations.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {678–686},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969812,
author = {Murray, Alan F. and Hamilton, Alister and Tarassenko, Lionel},
title = {Programmable Analog Pulse-Firing Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe pulse - stream firing integrated circuits that implement asynchronous analog neural networks. Synaptic weights are stored dynamically, and weighting uses time-division of the neural pulses from a signalling neuron to a receiving neuron. MOS transistors in their "ON" state act as variable resistors to control a capacitive discharge, and time-division is thus achieved by a small synapse circuit cell. The VLSI chip set design uses 2.5 µm CMOS technology.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {671–677},
numpages = {7},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969811,
author = {Eisenberg, Joe and Feld, David and Lewis, Edwin},
title = {A Passive Shared Element Analog Electrical Cochlea},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a simplified model of the micromechanics of the human cochlea, realized with electrical elements. Simulation of the model shows that it retains four signal processing features whose importance we argue on the basis of engineering logic and evolutionary evidence. Furthermore, just as the cochlea does, the model achieves massively parallel signal processing in a structurally economic way, by means of shared elements. By extracting what we believe are the five essential features of the cochlea, we hope to design a useful front-end filter to process acoustic images and to obtain a better understanding of the auditory system.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {662–670},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969810,
author = {Winter, C. L.},
title = {An Adaptive Network That Learns Sequences of Transitions},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe an adaptive network, TIN2, that learns the transition function of a sequential system from observations of its behavior. It integrates two subnets, TIN-1 (Winter, Ryan and Turner, 1987) and TIN-2. TIN-2 constructs state representations from examples of system behavior, and its dynamics are the main topics of the paper. TIN-1 abstracts transition functions from noisy state representations and environmental data during training, while in operation it produces sequences of transitions in response to variations in input. Dynamics of both nets are based on the Adaptive Resonance Theory of Carpenter and Grossberg (1987). We give results from an experiment in which TIN2 learned the behavior of a system that recognizes strings with an even number of 1's.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {653–660},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969809,
author = {Servan-Schreiber, David and Cleeremans, Axel and McClelland, James L.},
title = {Learning Sequential Structure in Simple Recurrent Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-1, together with element t, to predict element t+1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses of the hidden-layer patterns of activation showed that they encode prediction-relevant information about the entire path traversed through the network. We illustrate the phases of learning with cluster analyses performed at different points during training.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {643–652},
numpages = {10},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969808,
author = {Majani, E. and Erlanson, R. and Abu-Mostafa, Y.},
title = {On the K-Winners-Take-All Network},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present and rigorously analyze a generalization of the Winner-Take-All Network: the K-Winners-Take-All Network. This network identifies the K largest of a set of N real numbers. The network model used is the continuous Hopfield model.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {634–642},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969807,
author = {Touretzky, David S.},
title = {Analyzing the Energy Landscapes of Distributed Winner-Take-All Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {DCPS (the Distributed Connectionist Production System) is a neural network with complex dynamical properties. Visualizing the energy landscapes of some of its component modules leads to a better intuitive understanding of the model, and suggests ways in which its dynamics can be controlled in order to improve performance on difficult cases.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {626–633},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969806,
author = {Gindi, Gene and Mjolsness, Eric and Anandan, P.},
title = {Neural Networks for Model Matching and Perceptual Organization},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce an optimization approach for solving problems in computer vision that involve multiple levels of abstraction. Our objective functions include compositional and specialization hierarchies. We cast vision problems as inexact graph matching problems, formulate graph matching in terms of constrained optimization, and use analog neural networks to perform the optimization. The method is applicable to perceptual grouping and model matching. Preliminary experimental results are shown.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {618–625},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969805,
author = {Leinbach, Jared},
title = {Automatic Local Annealing},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This research involves a method for finding global maxima in constraint satisfaction networks. It is an annealing process but, unlike most others, requires no annealing schedule. Temperature is instead determined locally by units at each update, and thus all processing is done at the unit level. There are two major practical benefits to processing this way: 1) processing can continue in 'bad' areas of the network, while 'good' areas remain stable, and 2) processing continues in the 'bad' areas, as long as the constraints remain poorly satisfied (i.e. it does not stop after some predetermined number of cycles). As a result, this method not only avoids the kludge of requiring an externally determined annealing schedule, but it also finds global maxima more quickly and consistently than externally scheduled systems (a comparison to the Boltzmann machine (Ackley et al, 1985) is made). Finally, implementation of this method is computationally trivial.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {602–609},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969804,
author = {Bilbro, Griff L. and Snyder, Wesley E.},
title = {Range Image Restoration Using Mean Field Annealing},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new optimization strategy, Mean Field Annealing, is presented. Its application to MAP restoration of noisy range images is derived and experimentally verified.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {594–601},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969803,
author = {Rogers, David},
title = {Statistical Prediction with Kanerva's Sparse Distributed Memory},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new viewpoint of the processing performed by Kanerva's sparse distributed memory (SDM) is presented. In conditions of near- or over- capacity, where the associative-memory behavior of the model breaks down, the processing performed by the model can be interpreted as that of a statistical predictor. Mathematical results are presented which serve as the framework for a new statistical viewpoint of sparse distributed memory and for which the standard formulation of SDM is a special case. This viewpoint suggests possible enhancements to the SDM model, including a procedure for improving the predictiveness of the system based on Holland's work with 'Genetic Algorithms', and a method for improving the capacity of SDM even when used as an associative memory.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {586–593},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969802,
author = {Chiel, Hillel J. and Beer, Randall D. and Sterling, Leon S.},
title = {Heterogeneous Neural Networks for Adaptive Behavior in Dynamic Environments},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Research in artificial neural networks has generally emphasized homogeneous architectures. In contrast, the nervous systems of natural animals exhibit great heterogeneity in both their elements and patterns of interconnection. This heterogeneity is crucial to the flexible generation of behavior which is essential for survival in a complex, dynamic environment. It may also provide powerful insights into the design of artificial neural networks. In this paper, we describe a heterogeneous neural network for controlling the walking of a simulated insect. This controller is inspired by the neuroethological and neurobiological literature on insect locomotion. It exhibits a variety of statically stable gaits at different speeds simply by varying the tonic activity of a single cell. It can also adapt to perturbations as a natural consequence of its design.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {577–585},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969801,
author = {Marcus, C. M. and Westervelt, R. M.},
title = {Dynamics of Analog Neural Networks with Time Delay},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A time delay in the response of the neurons in a network can induce sustained oscillation and chaos. We present a stability criterion based on local stability analysis to prevent sustained oscillation in symmetric delay networks, and show an example of chaotic dynamics in a non-symmetric delay network.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {568–576},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969800,
author = {Kirillov, A. B. and Borisyuk, G. N. and Borisyuk, R. M. and Kovalenko, Ye. I. and Makarenko, V. I. and Chulaevsky, V. A. and Kryukov, V. I.},
title = {A Model of Neural Oscillator for a Unified Submodule},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new model of a controlled neuron oscillator, proposed earlier {Kryukov et al, 1986} for the interpretation of the neural activity in various parts of the central nervous system, may have important applications in engineering and in the theory of brain functions. The oscillator has a good stability of the oscillation period, its frequency is regulated linearly in a wide range and it can exhibit arbitrarily long oscillation periods without changing the time constants of its elements. The latter is achieved by using the critical slowdown in the dynamics arising in a network of nonformal excitatory neurons {Kovalenko et al, 1984, Kryukov, 1984}. By changing the parameters of the oscillator one can obtain various functional modes which are necessary to develop a model of higher brain function.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {560–567},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969799,
author = {Hendler, James},
title = {Spreading Activation over Distributed Microfeatures},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One attempt at explaining human inferencing is that of spreading activation, particularly in the structured connectionist paradigm. This has resulted in the building of systems with semantically nameable nodes which perform inferencing by examining the patterns of activation spread. In this paper we demonstrate that simple structured network inferencing can be performed by passing activation over the weights learned by a distributed algorithm. Thus, an account, is provided which explains a well-behaved relationship between structured and distributed connectionist approaches.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {553–559},
numpages = {7},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969798,
author = {Lange, Trent E. and Dyer, Michael G.},
title = {Dynamic, Non-Local Role Bindings and Inferencing in a Localist Network for Natural Language Understanding},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper introduces a means to handle the critical problem of nonlocal role-bindings in localist spreading-activation networks. Every conceptual node in the network broadcasts a stable, uniquely-identifying activation pattern, called its signature. A dynamic role-binding is created when a role's binding node has an activation that matches the bound concept's signature. Most importantly, signatures are propagated across long paths of nodes to handle the non-local role-bindings necessary for inferencing. Our localist network model, ROBIN (ROle Binding and Inferencing Network), uses signature activations to robustly represent schemata role-bindings and thus perform the inferencing, plan/goal analysis, schema instantiation, word-sense disambiguation, and dynamic re-interpretation portions of the natural language understanding process.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {545–552},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969797,
author = {Santos, Eugene},
title = {A Massively Parallel Self-Tuning Context-Free Parser},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Parsing and Learning System (PALS) is a massively parallel self-tuning context-free parser. It is capable of parsing sentences of unbounded length mainly due to its parse-tree representation scheme. The system is capable of improving its parsing performance through the presentation of training examples.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {537–544},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969796,
author = {Pollack, Jordan B.},
title = {Implications of Recursive Distributed Representations},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {I will describe my recent results on the automatic development of fixed-width recursive distributed representations of variable-sized hierarchal data structures. One implication of this work is that certain types of AI-style data-structures can now be represented in fixed-width analog vectors. Simple inferences can be performed using the type of pattern associations that neural networks excel at Another implication arises from noting that these representations become self-similar in the limit. Once this door to chaos is opened, many interesting new questions about the representational basis of intelligence emerge, and can (and will) be discussed.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {527–536},
numpages = {10},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969795,
author = {Chauvin, Yves},
title = {A Back-Propagation Algorithm with Optimal Use of Hidden Units},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a variation of the back-propagation algorithm that makes optimal use of a network hidden units by decrasing an "energy" term written as a function of the squared activations of these hidden units. The algorithm can automatically find optimal or nearly optimal architectures necessary to solve known Boolean functions, facilitate the interpretation of the activation of the remaining hidden units and automatically estimate the complexity of architectures appropriate for phonetic labeling problems. The general principle of the algorithm can also be adapted to different tasks: for example, it can be used to eliminate the [0, 0] local minimum of the [-1. +1] logistic activation function while preserving a much faster convergence and forcing binary activations over the set of hidden units.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {519–526},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969794,
author = {Kam, Moshe and Cheng, Roger},
title = {Convergence and Pattern Stabilization in the Boltzmann Machine},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Boltzmann Machine has been introduced as a means to perform global optimization for multimodal objective functions using the principles of simulated annealing. In this paper we consider its utility as a spurious-free content-addressable memory, and provide bounds on its performance in this context. We show how to exploit the machine's ability to escape local minima, in order to use it, at a constant temperature, for unambiguous associative pattern-retrieval in noisy environments. An association rule, which creates a sphere of influence around each stored pattern, is used along with the Machine's dynamics to match the machine's noisy input with one of the pre-stored patterns. Spurious fixed points, whose regions of attraction are not recognized by the rule, are skipped, due to the Machine's finite probability to escape from any state. The results apply to the Boltzmann machine and to the asynchronous net of binary threshold elements ('Hopfield model'). They provide the network designer with worst-case and best-case bounds for the network's performance, and allow polynomial-time tradeoff studies of design parameters.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {511–518},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969793,
author = {Bourlard, H. and Wellekens, C. J.},
title = {Links between Markov Models and Multilayer Perceptrons},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hidden Markov models are widely used for automatic speech recognition. They inherently incorporate the sequential character of the speech signal and are statistically trained. However, the apriori choice of the model topology limits their flexibility. Another drawback of these models is their weak discriminating power. Multilayer perceptrons are now promising tools in the connectionist approach for classification problems and have already been successfully tested on speech recognition problems. However, the sequential nature of the speech signal remains difficult to handle in that kind of machine. In this paper, a discriminant hidden Markov model is defined and it is shown how a particular multilayer perceptron with contextual and extra feedback input units can be considered as a general form of such Markov models.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {502–510},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969792,
author = {Blum, Avrim and Rivest, Ronald L.},
title = {Training a 3-Node Neural Network is NP-Complete},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output consistent with a given set of training examples. We extend the result to other simple networks. This result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. It also suggests the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {494–501},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969791,
author = {Wilson, Matthew A. and Bhalla, Upinder S. and Uhley, John D. and Bower, James M.},
title = {GENESIS: A System for Simulating Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have developed a graphically oriented, general purpose simulation system to facilitate the modeling of neural networks. The simulator is implemented under UNIX and X-windows and is designed to support simulations at many levels of detail. Specifically, it is intended for use in both applied network modeling and in the simulation of detailed, realistic, biologically-based models. Examples of current models developed under this system include mammalian olfactory bulb and cortex, invertebrate central pattern generators, as well as more abstract connectionist simulations.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {485–492},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969790,
author = {Grzywacz, Norberto M. and Amthor, Franklin R.},
title = {A Computationally Robust Anatomical Model for Retinal Directional Selectivity},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We analyze a mathematical model for retinal directionally selective cells based on recent electrophysiological data, and show that its computation of motion direction is robust against noise and speed.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {477–484},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969789,
author = {Sereno, Martin I.},
title = {Learning the Solution to the Aperture Problem for Pattern Motion with a HEBB Rule},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The primate visual system learns to recognize the true direction of pattern motion using local detectors only capable of detecting the component of motion perpendicular to the orientation of the moving edge. A multilayer feedforward network model similar to Linsker's model was presented with input patterns each consisting of randomly oriented contours moving in a particular direction. Input layer units are granted component direction and speed tuning curves similar to those recorded from neurons in primate visual area VI that project to area MT. The network is trained on many such patterns until most weights saturate. A proportion of the units in the second layer solve the aperture problem (e.g., show the same direction-tuning curve peak to plaids as to gratings), resembling pattern-direction selective neurons, which first appear in area MT.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {468–476},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969788,
author = {Baird, Bill},
title = {A Bifurcation Theory Approach to the Programming of Periodic Attractors in Network Models of Olfactory Cortex},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new learning algorithm for the storage of static and periodic attractors in biologically inspired recurrent analog neural networks is introduced. For a network of n nodes, n static or n/2 periodic attractors may be stored. The algorithm allows programming of the network vector field independent of the patterns to be stored. Stability of patterns, basin geometry, and rates of convergence may be controlled. For orthonormal patterns, the learning operation reduces to a kind of periodic outer product rule that allows local, additive, commutative, incremental learning. Standing or traveling wave cycles may be stored to mimic the kind of oscillating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration and suffice, in the bulb, to predict the pattern recognition behavior of rabbits in classical conditioning experiments. These attractors arise, during simulated inspiration, through a multiple Hopf bifurcation, which can act as a critical "decision point" for their selection by a very small input pattern.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {459–467},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969787,
author = {Tanaka, Shigeru},
title = {Theory of Self-Organization of Cortical Maps},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have mathematically shown that cortical maps in the primary sensory cortices can be reproduced by using three hypotheses which have physiological basis and meaning. Here, our main focus is on ocular dominance column formation in the primary visual cortex. Monte Carlo simulations on the segregation of ipsilateral and contralateral afferent terminals are carried out. Based on these, we show that almost all the physiological experimental results concerning the ocular dominance patterns of cats and monkeys reared under normal or various abnormal visual conditions can be explained from a viewpoint of the phase transition phenomena.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {451–458},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969786,
author = {Zhang, Jun and Miller, John P.},
title = {A Model for Resolution Enhancement (Hyperacuity) in Sensory Representation},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Heiligenberg (1987) recently proposed a model to explain how sensory maps could enhance resolution through orderly arrangement of broadly tuned receptors. We have extended this model to the general case of polynomial weighting schemes and proved that the response function is also a polynomial of the same order. We further demonstrated that the Hermitian polynomials are eigenfunctions of the system. Finally we suggested a biologically plausible mechanism for sensory representation of external stimuli with resolution far exceeding the inter-receptor separation.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {444–450},
numpages = {7},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969785,
author = {Rasnow, Brian and Assad, Christopher and Nelson, Mark E. and Bower, James M.},
title = {Simulation and Measurement of the Electric Fields Generated by Weakly Electric Fish},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The weakly electric fish, Gnathonemus petersii, explores its environment by generating pulsed electric fields and detecting small perturbations in the fields resulting from nearby objects. Accordingly, the fish detects and discriminates objects on the basis of a sequence of electric "images" whose temporal and spatial properties depend on the timing of the fish's electric organ discharge and its body position relative to objects in its environment. We are interested in investigating how these fish utilize timing and body-position during exploration to aid in object discrimination. We have developed a finite-element simulation of the fish's self-generated electric fields so as to reconstruct the electrosensory consequences of body position and electric organ discharge timing in the fish. This paper describes this finite-element simulation system and presents preliminary electric field measurements which are being used to tune the simulation.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {436–443},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969784,
author = {Alkon, Daniel L. and Quek, Francis and Vogl, Thomas P.},
title = {Computer Modeling of Associative Learning},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {419–435},
numpages = {17},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969783,
author = {Paulin, Michael G. and Nelson, Mark E. and Bower, James M.},
title = {Neural Control of Sensory Acquisition: The Vestibulo-Ocular Reflex},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a new hypothesis that the cerebellum plays a key role in actively controlling the acquisition of sensory information by the nervous system. In this paper we explore this idea by examining the function of a simple cerebellar-related behavior, the vestibulo-ocular reflex or VOR, in which eye movements are generated to minimize image slip on the retina during rapid head movements. Considering this system from the point of view of statistical estimation theory, our results suggest that the transfer function of the VOR, often regarded as a static or slowly modifiable feature of the system, should actually be continuously and rapidly changed during head movements. We further suggest that these changes are under the direct control of the cerebellar cortex and propose experiments to test this hypothesis.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {410–418},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969782,
author = {Li, Zhaoping and Hopfield, J. J.},
title = {Modeling the Olfactory Bulb: Coupled Nonlinear Oscillators},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The olfactory bulb of mammals aids in the discrimination of odors. A mathematical model based on the bulbar anatomy and electrophysiology is described. Simulations produce a 35-60 Hz modulated activity coherent across the bulb, mimicing the observed field potentials. The decision states (for the odor information) here can be thought of as stable cycles, rather than point stable states typical of simpler neuro-computing models. Analysis and simulations show that a group of coupled non-linear oscillators are responsible for the oscillatory activities determined by the odor input, and that the bulb, with appropriate inputs from higher centers, can enhance or suppress the sensitivity to particular odors. The model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {402–409},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969781,
author = {Stanton, Patric K. and Sejnowski, Terrence J.},
title = {Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In modeling studies or memory based on neural networks, both the selective enhancement and depression or synaptic strengths are required ror efficient storage or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al, 1982; Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus, a cortical structure or the brain that is involved in long-term memory. A brief, high-frequency activation or excitatory synapses in the hippocampus produces an increase in synaptic strength known as long-term potentiation, or LTP (Bliss and Lomo, 1973), that can last for many days. LTP is known to be Hebbian since it requires the simultaneous release or neurotransmitter from presynaptic terminals coupled with postsynaptic depolarization (Kelso et al., 1986; Malinow and Miller, 1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction of synaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic trees or hippocampal pyramidal cells or field CA1, and round that a low-frequency input which, by itself, does not persistently change synaptic strength, can either increase (associative LTP) or decrease in strength (associative long-term depression or LTD) depending upon whether it is positively or negatively correlated in time with a second, high-frequency bursting input. LTP of synaptic strength is Hebbian, and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTD are capable or storing information contained in the covariance between separate, converging hippocampal inputs.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {394–401},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969780,
author = {Ryckebusch, Sylvie and Bower, James M. and Mead, Carver},
title = {Modeling Small Oscillating Biological Networks in Analog VLSI},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have used analog VLSI technology to model a class of small oscillating biological neural circuits known as central pattern generators (CPG). These circuits generate rhythmic patterns of activity which drive locomotor behaviour in the animal. We have designed, fabricated, and tested a model neuron circuit which relies on many of the same mechanisms as a biological central pattern generator neuron, such as delays and internal feedback. We show that this neuron can be used to build several small circuits based on known biological CPG circuits, and that these circuits produce patterns of output which are very similar to the observed biological patterns.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {384–393},
numpages = {10},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969779,
author = {Keller, Joseph B. and Miller, Kenneth D. and Stryker, Michael P.},
title = {Models of Ocular Dominance Column Formation: Analytical and Computational Results},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have previously developed a simple mathematical model for formation of ocular dominance columns in mammalian visual cortex. The model provides a common framework in which a variety of activity-dependent biological machanisms can be studied. Analytic and computational results together now reveal the following: if inputs specific to each eye are locally correlated in their firing, and are not anticorrelated within an arbor radius, monocular cells will robustly form and be organized by intra-cortical interactions into columns. Broader correlations within each eye, or anti-correlations between the eyes, create a more purely monocular cortex; positive correlation over an arbor radius yields an almost perfectly monocular cortex. Most features of the model can be understood analytically through decomposition into eigenfunctions and linear stability analysis. This allows prediction of the widths of the columns and other features from measurable biological parameters.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {375–383},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969778,
author = {Spence, C. D. and Pearson, J. C. and Gelfand, J. J. and Peterson, R. M. and Sullivan, W. E.},
title = {Neuronal Maps for Sensory-Motor Control in the Barn Owl},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The barn owl has fused visual/auditory/motor representations of space in its midbrain which are used to orient the head so that visual or auditory stimuli are centered in the visual field of view. We present models and computer simulations of these structures which address various problems, including the construction of a map of space from auditory sensory information, and the problem of driving the motor system from these maps. We compare the results with biological data.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {366–374},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969777,
author = {Goldberg, K. Y. and Pearlmutter, B. A.},
title = {Using Backpropagation with Temporal Windows to Learn the Dynamics of the CMU Direct-Drive Arm II},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Computing the inverse dynamics of a robot ann is an active area of research in the control literature. We hope to learn the inverse dynamics by training a neural network on the measured response of a physical arm. The input to the network is a temporal window of measured positions; output is a vector of torques. We train the network on data measured from the first two joints of the CMU Direct-Drive Arm II as it moves through a randomly-generated sample of "pick-and-place" trajectories. We then test generalization with a new trajectory and compare its output with the torque measured at the physical arm. The network is shown to generalize with a root mean square error/standard deviation (RMSS) of 0.10. We interpreted the weights of the network in terms of the velocity and acceleration filters used in conventional control theory.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {356},
numpages = {1},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969776,
author = {Mel, Bartlett W.},
title = {Further Explorations in Visually-Guided Reaching: Making Murphy Smarter},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {MURPHY is a vision-based kinematic controller and path planner based on a connectionist architecture, and implemented with a video camera and Rhino XR-series robot arm. Imitative of the layout of sensory and motor maps in cerebral cortex, MURPHY'S internal representations consist of four coarse-coded populations of simple units representing both static and dynamic aspects of the sensory-motor environment. In previously reported work [4], MURPHY first learned a direct kinematic model of his camera-arm system during a period of extended practice, and then used this "mental model" to heuristically guide his hand to unobstructed visual targets. MURPHY has since been extended in two ways: First, he now learns the inverse differential-kinematics of his arm in addition to ordinary direct kinematics, which allows him to push his hand directly towards a visual target without the need for search. Secondly, he now deals with the much more difficult problem of reaching in the presence of obstacles.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {348–355},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969775,
author = {Mighell, Dorothy A. and Wilkinson, Timothy S. and Goodman, Joseph W.},
title = {Backpropagation and Its Application to Handwritten Signature Verification},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a forgery. The network is a feedforward net, with a binary image as input. There is a hidden layer, with a single unit output layer. The weights are adjusted according to the backpropagation algorithm. The signatures are entered into a C software program through the use of a Datacopy Electronic Digitizing Camera. The binary signatures are normalized and centered. The performance is examined as a function of the training set and network structure. The best scores are on the order of 2% true signature rejection with 2-4% false signature acceptance.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {340–347},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969774,
author = {Mori, Yoshihiro and Yokosawa, Kazuhiko},
title = {Neural Networks That Learn to Discriminate Similar Kanji Characters},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A neural network is applied to the problem of recognizing Kanji characters. Using a back propagation network learning algorithm, a three-layered, feed-forward network is trained to recognize similar handwritten Kanji characters. In addition, two new methods are utilized to make training effective. The recognition accuracy was higher than that of conventional methods. An analysis of connection weights showed that trained networks can discern the hierarchical structure of Kanji characters. This strategy of trained networks makes high recognition accuracy possible. Our results suggest that neural networks are very effective for Kanji character recognition.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {332–339},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969773,
author = {Denker, J. S. and Gardner, W. R. and Graf, H. P. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D. and Baird, H. S. and Guyon, I.},
title = {Neural Network Recognizer for Hand-Written Zip Code Digits},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {323–331},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969772,
author = {Alvelda, Phillip and Martin, A. Miguel San},
title = {Neural Network Star Pattern Recognition for Spacecraft Attitude},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Currently, the most complex spacecraft attitude determination and control tasks are ultimately governed by ground-based systems and personnel. Conventional on-board systems face severe computational bottlenecks introduced by serial microprocessors operating on inherently parallel problems. New computer architectures based on the anatomy of the human brain seem to promise high speed and fault-tolerant solutions to the limitations of serial processing. This paper discusses the latest applications of artificial neural networks to the problem of star pattern recognition for spacecraft attitude determination.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {314–322},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969771,
author = {Pomerleau, Dean A.},
title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {305–313},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969770,
author = {Hurlbert, Anya and Poggio, Tomaso},
title = {A Network for Image Segmentation Using Color},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumination, and to spread uniform colors within marked regions.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {297–304},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969769,
author = {Waxman, Allen M. and Seibert, Michael and Cunningham, Robert and Wu, Jian},
title = {Neural Analog Diffusion-Enhancement Layer and Spatio-Temporal Grouping in Early Vision},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new class of neural network aimed at early visual processing is described; we call it a Neural Analog Diffusion-Enhancement Layer or "NADEL." The network consists of two levels which are coupled through feedfoward and shunted feedback connections. The lower level is a two-dimensional diffusion map which accepts visual features as input, and spreads activity over larger scales as a function of time. The upper layer is periodically fed the activity from the diffusion layer and locates local maxima in it (an extreme form of contrast enhancement) using a network of local comparators. These local maxima are fed back to the diffusion layer using an on-center/off-surround shunting anatomy. The maxima are also available as output of the network. The network dynamics serves to cluster features on multiple scales as a function of time, and can be used in a variety of early visual processing tasks such as: extraction of corners and high curvature points along edge contours, line end detection, gap filling in contours, generation of fixation points, perceptual grouping on multiple scales, correspondence and path impletion in long-range apparent motion, and building 2-D shape representations that are invariant to location, orientation, scale, and small deformation on the visual field.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {289–296},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969768,
author = {Ahalt, S. C. and Garber, F. D. and Jouny, I. and Krishnamurthy, A. K.},
title = {Performance of Synthetic Neural Network Classification of Noisy Radar Signals},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This study evaluates the performance of the multilayer-perceptron and the frequency-sensitive competitive learning network in identifying five commercial aircraft from radar backscatter measurements. The performance of the neural network classifiers is compared with that of the nearest-neighbor and maximum-likelihood classifiers. Our results indicate that for this problem, the neural network classifiers are relatively insensitive to changes in the network topology, and to the noise level in the training data. While, for this problem, the traditional algorithms outperform these simple neural classifiers, we feel that neural networks show the potential for improved performance.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {281–288},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969767,
author = {Paris, Bernd-Peter and Orsak, Geoffrey and Varanasi, Mahesh and Aazhang, Behnaam},
title = {Neural Net Receivers in Multiple-Access Communications},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The application of neural networks to the demodulation of spread-spectrum signals in a multiple-access environment is considered. This study is motivated in large part by the fact that, in a multiuser system, the conventional (matched filter) receiver suffers severe performance degradation as the relative powers of the interfering signals become large (the "near-far" problem). Furthermore, the optimum receiver, which alleviates the near-far problem, is too complex to be of practical use. Receivers based on multi-layer perceptrons are considered as a simple and robust alternative to the optimum solution. The optimum receiver is used to benchmark the performance of the neural net receiver; in particular, it is proven to be instrumental in identifying the decision regions of the neural networks. The back-propagation algorithm and a modified version of it are used to train the neural net. An importance sampling technique is introduced to reduce the number of simulations necessary to evaluate the performance of neural nets. In all examples considered the proposed neural net receiver significantly outperforms the conventional receiver.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {272–280},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969766,
author = {Naillon, Martine and Theeten, Jean-Bernard},
title = {Neural Approach for TV Image Compression Using a Hopfield Type Network},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A self-organizing Hopfield network has been developed in the context of Vector Quantization, aiming at compression of television images. The metastable states of the spin glass-like network are used as an extra storage resource using the Minimal Overlap learning rule (Krauth and Mezard 1987) to optimize the organization of the attractors. The self-organizing scheme that we have devised results in the generation of an adaptive codebook for any qiven TV image.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {264–271},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969765,
author = {Goodman, Rodney M. and Miller, John W. and Smyth, Padhraic},
title = {An Information Theoretic Approach to Rule-Based Connectionist Expert Systems},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We discuss in this paper architectures for executing probabilistic rule-bases in a parallel manner, using as a theoretical basis recently introduced information-theoretic models. We will begin by describing our (non-neural) learning algorithm and theory of quantitative rule modelling, followed by a discussion on the exact nature of two particular models. Finally we work through an example of our approach, going from database to rules to inference network, and compare the network's performance with the theoretical limits for specific problems.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {256–263},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969764,
author = {Bradshaw, Gary and Fozzard, Richard and Ceci, Louis},
title = {A Connectionist Expert System That Actually Works},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Space Environment Laboratory in Boulder has collaborated with the University of Colorado to construct a small expert system for solar flare forecasting, called THEO. It performed as well as a skilled human forecaster. We have constructed TheoNet, a three-layer back-propagation connectionist network that learns to forecast flares as well as THEO does. TheoNet's success suggests that a connectionist network can perform the task of knowledge engineering automatically. A study of the internal representations constructed by the network may give insights to the "microstructure" of reasoning processes in the human brain.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {248–255},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969763,
author = {Smythe, Erich J.},
title = {Temporal Representations in a Connectionist Speech System},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {SYREN is a connectionist model that uses temporal information in a speech signal for syllable recognition. It classifies the rates and directions of formant center transitions, and uses an adaptive method to associate transition events with each syllable. The system uses explicit spatial temporal representations through delay lines. SYREN uses implicit parametric temporal representations in formant transition classification through node activation onset, decay, and transition delays in sub-networks analogous to visual motion detector cells. SYREN recognizes 79% of six repetitions of 24 consonant-vowel syllables when tested on unseen data, and recognizes 100% of its training syllables.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {240–247},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969762,
author = {Komura, Mitsuo and Tanaka, Akio},
title = {Speech Production Using a Neural Network with a Cooperative Learning Mechanism},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new neural network model and its learning algorithm. The proposed neural network consists of four layers - input, hidden, output and final output layers. The hidden and output layers are multiple. Using the proposed SICL(Spread Pattern Information and Cooperative Learning) algorithm, it is possible to learn analog data accurately and to obtain smooth outputs. Using this neural network, we have developed a speech production system consisting of a phonemic symbol production subsystem and a speech parameter production subsystem. We have succeeded in producing natural speech waves with high accuracy.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {232–239},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969761,
author = {Bengio, Yoshua and Cardin, Regis and De Mori, Renato and Cosi, Piero},
title = {Use of Multi-Layered Networks for Coding Speech with Phonetic Features},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Preliminary results on speaker-independant speech recognition are reported. A method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems. For transient sounds, event-driven property extractors with variable resolution in the time and frequency domains are used. For sonorant speech, a model of the human auditory system is preferred to FFT as a front-end module.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {224–231},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969760,
author = {Waibel, Alex},
title = {Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we show that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {215–223},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969759,
author = {Leung, Hong C. and Zue, Victor W.},
title = {Applications of Error Back-Propagation to Phonetic Classification},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper is concerced with the use of error back-propagation in phonetic classification. Our objective is to investigate the basic characteristics of back-propagation, and study how the framework of multi-layer perceptrons can be exploited in phonetic recognition. We explore issues such as integration of heterogeneous sources of information, conditions that can affect performance of phonetic classification, internal representations, comparisons with traditional pattern classification techniques, comparisons of different error metrics, and initialization of the network. Our investigation is performed within a set of experiments that attempts to recognize the 16 vowels in American English independent of speaker. Our results are comparable to human performance.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {206–214},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969758,
author = {Gluck, Mark and Parker, David B. and Reifsnider, Eric S.},
title = {Learning with Temporal Derivatives in Pulse-Coded Neuronal Systems},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A number of learning models have recently been proposed which involve calculations of temporal differences (or derivatives in continuous-time models). These models, like most adaptive network models, are formulated in terms of frequency (or activation), a useful abstraction of neuronal firing rates. To more precisely evaluate the implications of a neuronal model, it may be preferable to develop a model which transmits discrete pulse-coded information. We point out that many functions and properties of neuronal processing and learning may depend, in subtle ways, on the pulse-coded nature of the information coding and transmission properties of neuron systems. When compared to formulations in terms of activation, computing with temporal derivatives (or differences) as proposed by Kosko (1986), Klopf (1988), and Sutton (1988), is both more stable and easier when reformulated for a more neuronally realistic pulse-coded system. In reformulating these models in terms of pulse-coding, our motivation has been to enable us to draw further parallels and connections between real-time behavioral models of learning and biological circuit models of the substrates underlying learning and memory.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {195–203},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969757,
author = {Linsker, Ralph},
title = {An Application of the Principle of Maximum Information Preservation to Linear Systems},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper addresses the problem of determining the weights for a set of linear filters (model "cells") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {186–194},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969756,
author = {Hanson, Stephen Jos\'{e} and Pratt, Lorien Y.},
title = {Comparing Biases for Minimal Network Construction with Back-Propagation},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Rumelhart (1987), has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units, (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart's minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general, the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {177–185},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969755,
author = {Tawel, Raoul},
title = {Does the Neuron "Learn" like the Synapse?},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An improved learning paradigm that offers a significant reduction in computation time during the supervised learning phase is described. It is based on extending the role that the neuron plays in artificial neural systems. Prior work has regarded the neuron as a strictly passive, non-linear processing element, and the synapse on the other hand as the primary source of information processing and knowledge retention. In this work, the role of the neuron is extended insofar as allowing its parameters to adaptively participate in the learning phase. The temperature of the sigmoid function is an example of such a parameter. During learning, both the synaptic interconnection weights wijm and the neuronal temperatures Tim are optimized so as to capture the knowledge contained within the training set. The method allows each neuron to possess and update its own characteristic local temperature. This algorithm has been applied to logic type of problems such as the XOR or parity problem, resulting in a significant decrease in the required number of training cycles.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {169–176},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969754,
author = {Ahmad, Subutai and Tesauro, Gerald},
title = {Scaling and Generalization in Neural Networks: A Case Study},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The issues of scaling and generalization have emerged as key issues in current studies of supervised learning from examples in neural networks. Questions such as how many training patterns and training cycles are needed for a problem of a given size and difficulty, how to represent the input and how to choose useful training exemplars, are of considerable theoretical and practical importance. Several intuitive rules of thumb have been obtained from empirical studies, but as yet there are few rigorous results. In this paper we summarize a study of generalization in the simplest possible case-perceptron networks learning linearly separable functions. The task chosen was the majority function (i.e. return a 1 if a majority of the input units are on), a predicate with a number of useful properties. We find that many aspects of generalization in multilayer networks learning large, difficult tasks are reproduced in this simple domain, in which concrete numerical results and even some analytic understanding can be achieved.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {160–168},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969753,
author = {Ottaway, Mary B. and Simard, Patrice Y. and Ballard, Dana H.},
title = {Fixed Point Analysis for Recurrent Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper provides a systematic analysis of the recurrent backpropagation (RBP) algorithm, introducing a number of new results. The main limitation of the RBP algorithm is that it assumes the convergence of the network to a stable fixed point in order to backpropagate the error signals. We show by experiment and eigenvalue analysis that this condition can be violated and that chaotic behavior can be avoided. Next we examine the advantages of RBP over the standard backpropagation algorithm. RBP is shown to build stable fixed points corresponding to the input patterns. This makes it an appropriate tool for content addressable memories, one-to-many function learning, and inverse problems.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {149–159},
numpages = {11},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969752,
author = {Le Cun, Yann and Galland, Conrad C. and Hinton, Geoffrey E.},
title = {Gemini: Gradient Estimation through Matrix Inversion after Noise Injection},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing unknown non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {141–148},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969751,
author = {Singhal, Sharad and Wu, Lance},
title = {Training Multilayer Perceptrons with the Extended Kalman Algorithm},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {133–140},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969750,
author = {Lippmann, Richard P. and Beckman, Paul},
title = {Adaptive Neural Net Preprocessing for Signal Detection in Non-Gaussian Noise},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A nonlinearity is required before matched filtering in minimum error receivers when additive noise is present which is impulsive and highly non-Gaussian. Experiments were performed to determine whether the correct clipping nonlinearity could be provided by a single-input single-output multi-layer perceptron trained with back propagation. It was found that a multi-layer perceptron with one input and output node, 20 nodes in the first hidden layer, and 5 nodes in the second hidden layer could be trained to provide a clipping nonlinearity with fewer than 5,000 presentations of noiseless and corrupted waveform samples. A network trained at a relatively high signal-to-noise (S/N) ratio and then used as a front end for a linear matched filter detector greatly reduced the probability of error. The clipping nonlinearity formed by this network was similar to that used in current receivers designed for impulsive noise and provided similar substantial improvements in performance.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {124–132},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969749,
author = {Yair, Eyal and Gersho, Allen},
title = {The Boltzmann Perceptron Network: A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The concept of the stochastic Boltzmann machine (BM) is attractive for decision making and pattern classification purposes since the probability of attaining the network states is a function of the network energy. Hence, the probability of attaining particular energy minima may be associated with the probabilities of making certain decisions (or classifications). However, because of its stochastic nature, the complexity of the BM is fairly high and therefore such networks are not very likely to be used in practice. In this paper we suggest a way to alleviate this drawback by converting the stochastic BM into a deterministic network which we call the Boltzmann Perceptron Network (BPN). The BPN is functionally equivalent to the BM but has a feed-forward structure and low complexity. No annealing is required. The conditions under which such a conversion is feasible are given. A learning algorithm for the BPN based on the conjugate gradient method is also provided which is somewhat akin to the backpropagation algorithm.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {116–123},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969748,
author = {Mozer, Michael C. and Smolensky, Paul},
title = {Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal "rules."},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {107–115},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969747,
author = {Tesauro, Gerald},
title = {Connectionist Learning of Expert Preferences by Comparison Training},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new training paradigm, called the "comparison paradigm," is introduced for tasks in which a network must learn to choose a preferred pattern from a set of n alternatives, based on examples of human expert preferences. In this paradigm, the input to the network consists of two of the n alternatives, and the trained output is the expert's judgement of which pattern is better. This paradigm is applied to the learning of backgammon, a difficult board game in which the expert selects a move from a set of legal moves. With comparison training, much higher levels of performance can be achieved, with networks that are much smaller, and with coding schemes that are much simpler and easier to understand. Furthermore, it is possible to set up the network so that it always produces consistent rank-orderings.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {99–106},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969746,
author = {Bilbro, Griff and Mann, Reinhold and Miller, Thomas K. and Snyder, Wesley E. and Van den Bout, David E. and White, Mark},
title = {Optimization by Mean Field Annealing},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Nearly optimal solutions to many combinatorial problems can be found using stochastic simulated annealing. This paper extends the concept of simulated annealing from its original formulation as a Markov process to a new formulation based on mean field theory. Mean field annealing essentially replaces the discrete degrees of freedom in simulated annealing with their average values as computed by the mean field approximation. The net result is that equilibrium at a given temperature is achieved 1-2 orders of magnitude faster than with simulated annealing. A general framework for the mean field annealing algorithm is derived, and its relationship to Hopfield networks is shown. The behavior of MFA is examined both analytically and experimentally for a generic combinatorial optimization problem: graph bipartitioning. This analysis indicates the presence of critical temperatures which could be important in improving the performance of neural networks.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {91–98},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969745,
author = {Baum, Eric B. and Haussler, David},
title = {What Size Net Gives Valid Generalization?},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 &lt; ε ≤ 1/8. We show that if m ≥ O(W/ε log N/ε) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 - ε/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 - ε of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than Ω(W/ε) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 - ε fraction of the future test examples.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {81–90},
numpages = {10},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969744,
author = {Grossman, Tal and Meir, Ronny and Domany, Eytan},
title = {Learning by Choice of Internal Representations},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements. Whereas existing algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local and biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {73–80},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969743,
author = {Baldi, Pierre},
title = {Linear Learning: Landscapes and Algorithms},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {65–72},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969742,
author = {Tenorio, Manoel Fernando and Lee, VVei-Tsih},
title = {Self Organizing Neural Networks for the Identification Problem},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and demonstrates its use in a system identification task. The algorithm constructs the network, chooses the neuron functions, and adjusts the weights. It is compared to the Back-Propagation algorithm in the identification of the chaotic time series. The results shows that SONN constructs a simpler, more accurate model, requiring less training data and epochs. The algorithm can be applied and generalized to appilications as a classifier.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {57–64},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969741,
author = {Davis, Lawrence},
title = {Mapping Classifier Systems into Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Classifier systems are machine learning systems incotporating a genetic algorithm as the learning mechanism. Although they respond to inputs that neural networks can respond to, their internal structure, representation formalisms, and learning mechanisms differ markedly from those employed by neural network researchers in the same sorts of domains. As a result, one might conclude that these two types of machine learning formalisms are intrinsically different. This is one of two papers that, taken together, prove instead that classifier systems and neural networks are equivalent. In this paper, half of the equivalence is demonstrated through the description of a transformation procedure that will map classifier systems into neural networks that are isomorphic in behavior. Several alterations on the commonly-used paradigms employed by neural network researchers are required in order to make the transformation work. These alterations are noted and their appropriateness is discussed. The paper concludes with a discussion of the practical import of these results, and with comments on their extensibility.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {49–56},
numpages = {8},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969740,
author = {Kramer, Alan H. and Sangiovanni-Vincentelli, A.},
title = {Efficient Parallel Learning Algorithms for Neural Networks},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Parallelizable optimization techniques are applied to the problem of learning in feedforward neural networks. In addition to having superior convergence properties, optimization techniques such as the Polak-Ribiere method are also significantly more efficient than the Backpropagation algorithm. These results are based on experiments performed on small boolean learning problems and the noisy real-valued learning problem of hand-written character recognition.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {40–48},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969739,
author = {Moody, John},
title = {"Fast Learning in Multi-Resolution Hierarchies"},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A class of fast, supervised learning algorithms is presented. They use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive coding, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {29–39},
numpages = {11},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969738,
author = {Ackley, David H.},
title = {Associative Learning VIA Inhibitory Search},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {ALVIS is a reinforcement-based connectionist architecture that learns associative maps in continuous multidimensional environments. The discovered locations of positive and negative reinforcements are recorded in "do be" and "don't be" subnetworks, respectively. The outputs of the subnetworks relevant to the current goal are combined and compared with the current location to produce an error vector. This vector is backpropagated through a motor-perceptual mapping network to produce an action vector that leads the system towards do-be locations and away from don't-be locations. ALVIS is demonstrated with a simulated robot posed a target-seeking task.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {20–28},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969737,
author = {Sanger, Terence D.},
title = {An Optimality Principle for Unsupervised Learning},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random-dot stereograms are presented.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {11–19},
numpages = {9},
series = {NIPS'88}
}

@inproceedings{10.5555/2969735.2969736,
author = {Pavel, M. and Gluck, Mark A. and Henkle, Van},
title = {Constraints on Adaptive Networks for Modeling Human Generalization},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs, i.e., how they generalize. Like humans, networks can learn a deterministic categorization task by a variety of alternative individual solutions. An analysis of the constraints imposed by using networks with the minimal number of hidden units shows that this "minimal configuration" constraint is not sufficient to explain and predict human performance; only a few solutions were found to be shared by both humans and minimal adaptive networks. A further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization. A new technique, which we call "reversed learning", is described for finding appropriate initial conditions.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {2–10},
numpages = {9},
series = {NIPS'88}
}

@proceedings{10.5555/2969735,
title = {NIPS'88: Proceedings of the 1st International Conference on Neural Information Processing Systems},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

