@inproceedings{10.5555/2969442.2969643,
author = {Zhong, Mingjun and Goddard, Nigel and Sutton, Charles},
title = {Latent Bayesian Melding for Integrating Individual and Population Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3618–3626},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969642,
author = {Lienart, Thibaut and Teh, Yee Whye and Doucet, Arnaud},
title = {Expectation Particle Belief Propagation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3609–3617},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969641,
author = {Liu, Yu-Ying and Li, Shuang and Li, Fuxin and Song, Le and Rehg, James M.},
title = {Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models. We demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three approaches from the continuous time Markov chain literature to the CT-HMM domain. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3600–3608},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969640,
author = {Acharya, Jayadev and Daskalakis, Constantinos and Kamath, Gautam},
title = {Optimal Testing for Properties of Distributions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given samples from an unknown discrete distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, as well as in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of discrete distributions such as monotonicity, independence, log-concavity, unimodality, and monotone-hazard rate, the optimal sample complexity is unknown.We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in X2 -distance, or far in total variation distance?The optimality of our testers is established by providing matching lower bounds, up to constant factors. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave, monotone hazard rate distributions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3591–3599},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969639,
author = {Yen, Ian E. H. and Lin, Shan-Wei and Lin, Shou-De},
title = {A Dual-Augmented Block Minimization Framework for Learning with Limited Memory},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In past few years, several techniques have been proposed for training of linear Support Vector Machine (SVM) in limited-memory setting, where a dual block-coordinate descent (dual-BCD) method was used to balance cost spent on I/O and computation. In this paper, we consider the more general setting of regularized Empirical Risk Minimization (ERM) when data cannot fit into memory. In particular, we generalize the existing block minimization framework based on strong duality and Augmented Lagrangian technique to achieve global convergence for general convex ERM. The block minimization framework is flexible in the sense that, given a solver working under sufficient memory, one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition. We conduct experiments on L1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3582–3590},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969638,
author = {Lee, Kisuk and Zlateski, Aleksandar and Vishwanathan, Ashwin and Seung, H. Sebastian},
title = {Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Back-propagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3573–3581},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969637,
author = {Tran, Dustin and Blei, David M. and Airoldi, Edoardo M.},
title = {Copula Variational Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3564–3572},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969636,
author = {Mirzazadeh, Farzaneh and Ravanbakhsh, Siamak and Ding, Nan and Schuurmans, Dale},
title = {Embedding Inference for Structured Multilabel Prediction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A key bottleneck in structured output prediction is the need for inference during training and testing, usually requiring some form of dynamic programming. Rather than using approximate inference or tailoring a specialized inference method for a particular structure—standard responses to the scaling challenge— we propose to embed prediction constraints directly into the learned representation. By eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved, particularly at test time. We demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints, where a relationship to maximum margin structured output prediction can be established. Experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3555–3563},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969635,
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
title = {Semi-Supervised Learning with Ladder Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3546–3554},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969634,
author = {Abdolmaleki, Abbas and Lioutikov, Rudolf and Lau, Nuno and Reis, Luis Paulo and Peters, Jan and Neumann, Gerhard},
title = {Model-Based Relative Entropy Stochastic Search},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the 'distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task. The proposed method considerably outperforms the existing approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3537–3545},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969633,
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
title = {Gradient Estimation Using Stochastic Computation Graphs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs—directed acyclic graphs that include both deterministic functions and conditional probability distributions—and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3528–3536},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969632,
author = {Mittal, Happy and Mahajan, Anuj and Gogate, Vibhav and Singla, Parag},
title = {Lifted Inference Rules with Constraints},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Lifted inference rules exploit symmetries for fast reasoning in statistical relational models. Computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference. In this paper, we propose a new constraint language, called setineq, which allows subset, equality and inequality constraints, to represent substitutions over the variables in the theory. Our constraint formulation is strictly more expressive than existing representations, yet easy to operate on. We reformulate the three main lifting rules: decomposer, generalized binomial and the recently proposed single occurrence for MAP inference, to work with our constraint representation. Experiments on benchmark MLNs for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3519–3527},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969631,
author = {Waggoner, Bo and Frongillo, Rafael and Abernethy, Jacob},
title = {A Market Framework for Eliciting Private Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a mechanism for purchasing information from a sequence of participants. The participants may simply hold data points they wish to sell, or may have more sophisticated information; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's future prediction on a test set. The mechanism, which draws on the principles of prediction markets, has a bounded budget and minimizes generalization error for Bregman divergence loss functions. We then show how to modify this mechanism to preserve the privacy of participants' information: At any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately aggregated.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3510–3518},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969630,
author = {Tobar, Felipe and Bui, Thang D. and Turner, Richard E.},
title = {Learning Stationary Time Series Using Gaussian Processes with Nonparametric Kernels},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce the Gaussian Process Convolution Model (GPCM), a two-stage non-parametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from Gaussian process. The GPCM is a continuous-time nonparametric-window moving average process and, conditionally, is itself a Gaussian process with a nonparametric kernel defined in a probabilistic fashion. The generative model can be equivalently considered in the frequency domain, where the power spectral density of the signal is specified using a Gaussian process. One of the main contributions of the paper is to develop a novel variational free-energy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process. In turn, this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios. Additionally, the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data, leading to new Bayesian nonparametric approaches to spectrum estimation. The proposed GPCM is validated using synthetic and real-world signals.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3501–3509},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969629,
author = {Du, Nan and Wang, Yichen and He, Niao and Song, Le},
title = {Time-Sensitive Recommendation from Recurrent User Activities},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item at the right moment, and how to predict the next returning time of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains O(1/∊) convergence rate, scales up to problems with millions of user-item pairs and hundreds of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation tasks. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3492–3500},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969628,
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
title = {Learning Structured Output Representation Using Deep Conditional Generative Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3483–3491},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969627,
author = {Kuleshov, Volodymyr and Liang, Percy},
title = {Calibrated Structured Prediction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In user-facing applications, displaying calibrated confidence measures— probabilities that correspond to true frequency—can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting, and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3474–3482},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969626,
author = {Werling, Keenon and Chaganty, Arun and Liang, Percy and Manning, Christopher D.},
title = {On-the-Job Learning with Bayesian Decision Theory},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an on-the-job setting, where as inputs arrive, we use real-time crowd-sourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets—named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3465–3473},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969625,
author = {Linderman, Scott W. and Johnson, Matthew J. and Adams, Ryan P.},
title = {Dependent Multinomial Models Made Easy: Stick Breaking with the P\'{o}Lya-Gamma Augmentation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in P\'{o}lya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3456–3464},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969624,
author = {Chiang, Kai-Yang and Hsieh, Cho-Jui and Dhillon, Inderjit S.},
title = {Matrix Completion with Noisy Side Information},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the matrix completion problem with side information. Side information has been considered in several matrix completion applications, and has been empirically shown to be useful in many cases. Recently, researchers studied the effect of side information for matrix completion from a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features. However, since in reality most given features are noisy or only weakly informative, the development of a model to handle a general feature set, and investigation of how much noisy features can help matrix recovery, remains an important issue. In this paper, we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise. Moreover, we study the effect of general features in theory and show that by using our model, the sample complexity can be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight into the usefulness of general side information. Finally, we consider synthetic data and two applications — relationship prediction and semi-supervised clustering — and show that our model outperforms other methods for matrix completion that use features both in theory and practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3447–3455},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969623,
author = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},
title = {Bayesian Dark Knowledge},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x, D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3438–3446},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969622,
author = {Hashimoto, Tatsunori B. and Sun, Yi and Jaakkola, Tommi S.},
title = {From Random Walks to Distances on Unweighted Graphs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited.We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3429–3437},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969621,
author = {Thrampoulidis, Christos and Abbasi, Ehsan and Hassibi, Babak},
title = {LASSO with Non-Linear Measurements is Equivalent to One with Linear Measurements},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Consider estimating an unknown, but structured (e.g. sparse, low-rank, etc.), signal x0 ∈ ℝn from a vector y ∈ ℝm of measurements of the form yi = gi(aiT x0), where the ai's are the rows of a known measurement matrix A, and, g(·) is a (potentially unknown) nonlinear and random link-function. Such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties. It could also arise by design, e.g., gi (x) = sign(x + zi), corresponds to noisy 1-bit quantized measurements. Motivated by the classical work of Brillinger, and more recent work of Plan and Vershynin, we estimate x0 via solving the Generalized-LASSO, i.e., x^ := arg minx ‖y - Ax0‖2 + λf(x) for some regularization parameter λ &gt; 0 and some (typically non-smooth) convex regularizer f(·) that promotes the structure of x0, e.g. ℓ1-norm, nuclear-norm, etc. While this approach seems to naively ignore the nonlinear function g(·), both Brillinger (in the non-constrained case) and Plan and Vershynin have shown that, when the entries of A are iid standard normal, this is a good estimator of x0 up to a constant of proportionality μ, which only depends on g(·). In this work, we considerably strengthen these results by obtaining explicit expressions for‖x^—μx0‖2, for the regularized Generalized-LASSO, that are asymptotically precise when m and n grow large. A main result is that the estimation performance of the Generalized LASSO with non-linear measurements is asymptotically the same as one whose measurements are linear yi = μaiTx0 + σzi, with μ = Eγg(γ) and σ2 = E(g(γ) - μγ)2, and, γ standard normal. To the best of our knowledge, the derived expressions on the estimation performance are the first-known precise results in this context. One interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the Generalized LASSO is the celebrated Lloyd-Max quantizer.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3420–3428},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969620,
author = {He, Niao and Harchaoui, Zaid},
title = {Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems. Typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty. The proposed algorithm, called Semi-Proximal Mirror-Prox, leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain. The algorithm stands in contrast with more classical proximal gradient algorithms with smoothing, which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal complexity bounds, i.e. O(1/∊2), for the number of calls to linear minimization oracle. We present promising experimental results showing the interest of the approach in comparison to competing methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3411–3419},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969619,
author = {Khan, Mohammad Emtiyaz and Baqu\'{e}, Pierre and Fleuret, Fran\c{c}ois and Fua, Pascal},
title = {Kullback-Leibler Proximal Variational Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new variational inference method based on a proximal framework that uses the Kullback-Leibler (KL) divergence as the proximal term. We make two contributions towards exploiting the geometry and structure of the variational bound. First, we propose a KL proximal-point algorithm and show its equivalence to variational inference with natural gradients (e.g., stochastic variational inference). Second, we use the proximal framework to derive efficient variational algorithms for non-conjugate models. We propose a splitting procedure to separate non-conjugate terms from conjugate ones. We linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution. Overall, our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models. We show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms. Applications to real-world datasets show comparable performances to existing methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3402–3410},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969618,
author = {Bardenet, R\'{e}mi and Titsias, Michalis K.},
title = {Inference for Determinantal Point Processes without Spectral Knowledge},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Determinantal point processes (DPPs) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel K. DPPs possess desirable properties, such as exact sampling or analyticity of the moments, but learning the parameters of kernel K through likelihood-based inference is not straightforward. First, the kernel that appears in the likelihood is not K, but another kernel L related to K through an often intractable spectral decomposition. This issue is typically bypassed in machine learning by directly parametrizing the kernel L, at the price of some interpretability of the model parameters. We follow this approach here. Second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a DPP over a finite set of objects, and the form of a Fredholm determinant in the case of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood of a DPP, both for finite and continuous domains. Unlike previous work, our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variational inference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3393–3401},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969617,
author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
title = {A Universal Catalyst for First-Order Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3384–3392},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969616,
author = {Foster, Dylan J. and Rakhlin, Alexander and Sridharan, Karthik},
title = {Adaptive Online Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a general framework for studying adaptive regret bounds in the online learning setting, subsuming model selection and data-dependent bounds. Given a data- or model-dependent bound we ask, "Does there exist some algorithm achieving this bound?" We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes.Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3375–3383},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969615,
author = {Li, Shuang and Xie, Yao and Dai, Hanjun and Song, Le},
title = {<i>M</i>-Statistic for Kernel Change-Point Detection},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the average run length in the online case. In this paper we propose two related computationally efficient M-statistics for kernel-based change-point detection when the amount of background data is large. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-of-measure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. We show that our methods perform well in both synthetic and real world data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3366–3374},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969614,
author = {Moore, David A. and Russell, Stuart J.},
title = {Gaussian Process Random Fields},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3357–3365},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969613,
author = {Bzdok, Danilo and Eickenberg, Michael and Grisel, Olivier and Thirion, Bertrand and Varoquaux, Ga\"{e}l},
title = {Semi-Supervised Factored Logistic Regression for High-Dimensional Neuroimaging Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets. Existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks. However, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations. We therefore propose to blend representation modelling and task classification into a unified statistical learning problem. A multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder. We show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3348–3356},
numpages = {9},
keywords = {semi-supervised learning, systems biology, cognitive science, brain imaging},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969612,
author = {Audiffren, Julien and Ralaivola, Liva},
title = {Cornering Stationary and Restless Mixing Bandits with Remix-UCB},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the restless bandit problem where arms are associated with stationary φ-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of carefully recovering some independence by 'ignoring' the values of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off, which we do by considering the idea of a waiting arm in the new Remix-UCB algorithm, a generalization of Improved-UCB for the problem at hand, that we introduce. We provide a regret analysis for this bandit strategy; two noticeable features of Remix-UCB are that i) it reduces to the regular Improved-UCB when the φ-mixing coefficients are all 0, i.e. when the i.i.d scenario is recovered, and ii) when φ(n) = O(n-α), it is able to ensure a controlled regret of order Θ~ (Δ*(α-2)/ α log1/α T), where Δ* encodes the distance between the best arm and the best suboptimal arm, even in the case when α &lt; 1, i.e. the case when the φ-mixing coefficients are not summable.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3339–3347},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969611,
author = {Shah, Amar and Ghahramani, Zoubin},
title = {Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3330–3338},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969610,
author = {Koyejo, Oluwasanmi and Ravikumar, Pradeep and Natarajan, Nagarajan and Dhillon, Inderjit S.},
title = {Consistent Multilabel Classification},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro- and macro-averages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Hamming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3321–3329},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969609,
author = {Seguy, Vivien and Cuturi, Marco},
title = {Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given a family of probability measures in P(X), the space of probability measures on a Hilbert space X, our goal in this paper is to highlight one ore more curves in P(X) that summarize efficiently that family. We propose to study this problem under the optimal transport (Wasserstein) geometry, using curves that are restricted to be geodesic segments under that metric. We show that concepts that play a key role in Euclidean PCA, such as data centering or orthogonality of principal directions, find a natural equivalent in the optimal transport geometry, using Wasserstein means and differential geometry. The implementation of these ideas is, however, computationally challenging. To achieve scalable algorithms that can handle thousands of measures, we propose to use a relaxed definition for geodesics and regularized optimal transport distances. The interest of our approach is demonstrated on images seen either as shapes or color histograms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3312–3320},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969608,
author = {Plis, Sergey and Danks, David and Freeman, Cynthia and Calhoun, Vince},
title = {Rate-Agnostic (Causal) Structure Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Causal structure learning from time series data is a major scientific challenge. Extant algorithms assume that measurements occur sufficiently quickly; more precisely, they assume approximately equal system and measurement timescales. In many domains, however, measurements occur at a significantly slower rate than the underlying system changes, but the size of the timescale mismatch is often unknown. This paper develops three causal structure learning algorithms, each of which discovers all dynamic causal graphs that explain the observed measurement data, perhaps given undersampling. That is, these algorithms all learn causal structure in a "rate-agnostic" manner: they do not assume any particular relation between the measurement and system timescales. We apply these algorithms to data from simulations to gain insight into the challenge of undersampling.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3303–3311},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969607,
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
title = {Skip-Thought Vectors},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3294–3302},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969606,
author = {Wan, Yali and Meil\u{a}, Marina},
title = {A Class of Network Models Recoverable by Spectral Clustering},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with "communities" for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectral clustering using the Normalized Laplacian of the graph can recover the communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3285–3293},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969605,
author = {Razaviyayn, Meisam and Farnia, Farzan and Tse, David},
title = {Discrete R\'{e}Nyi Classifiers},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Consider the binary classification problem of predicting a target variable Y from a discrete feature vector X = (X1,..., Xd). When the probability distribution ℙ(X, Y) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability (MAP) decision rule. However, in practice, estimating the complete joint distribution ℙ(X, Y) is computationally and statistically impossible for large values of d. Therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution ℙ(X, Y) and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns.In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of (X, Y). We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, it is shown that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various UCI data repository datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3276–3284},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969604,
author = {Ping, Wei and Liu, Qiang and Ihler, Alexander},
title = {Decomposition Bounds for Marginal MAP},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3267–3275},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969603,
author = {Frongillo, Rafael and Kash, Ian A.},
title = {On Elicitation Complexity},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question—all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable. Specifically, what is the minimum number of regression parameters needed to compute the property? Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation. We establish several general results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3258–3266},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969602,
author = {Bachman, Philip and Precup, Doina},
title = {Data Generation as Sequential Decision Making},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the models using neural networks and train them using a form of guided policy search [9]. Our models generate predictions through an iterative process of feedback and refinement. We show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3249–3257},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969601,
author = {Sarkhel, Somdeb and Singla, Parag and Gogate, Vibhav},
title = {Fast Lifted MAP Inference via Partitioning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3240–3248},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969600,
author = {Swaminathan, Adith and Joachims, Thorsten},
title = {The Self-Normalized Estimator for Counterfactual Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm - Normalized Policy Optimizer for Exponential Models (Norm-POEM) - for structured output prediction using linear rules. We evaluate the empirical effectiveness of Norm-POEM on several multi-label classification problems, finding that it consistently outperforms the conventional estimator.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3231–3239},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969599,
author = {Rai, Piyush and Hu, Changwei and Henao, Ricardo and Carin, Lawrence},
title = {Large-Scale Bayesian Multi-Label Learning via Topic-Based Label Embeddings},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a scalable Bayesian multi-label learning model based on learning low-dimensional label embeddings. Our model assumes that each label vector is generated as a weighted combination of a set of topics (each topic being a distribution over labels), where the combination weights (i.e., the embeddings) for each label vector are conditioned on the observed feature vector. This construction, coupled with a Bernoulli-Poisson link function for each label of the binary label vector, leads to a model with a computational cost that scales in the number of positive labels in the label matrix. This makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse. Using a data-augmentation strategy leads to full local conjugacy in our model, facilitating simple and very efficient Gibbs sampling, as well as an Expectation Maximization algorithm for inference. Also, predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form. We report results on several benchmark data sets, comparing our model with various state-of-the art methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3222–3230},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969598,
author = {Inouye, David I. and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a novel distribution that generalizes the Multinomial distribution to enable dependencies between dimensions. Our novel distribution is based on the parametric form of the Poisson MRF model [1] but is fundamentally different because of the domain restriction to a fixed-length vector like in a Multinomial where the number of trials is fixed or known. Thus, we propose the Fixed-Length Poisson MRF (LPMRF) distribution. We develop AIS sampling methods to estimate the likelihood and log partition function (i.e. the log normalizing constant), which was not developed for the Poisson MRF model. In addition, we propose novel mixture and topic models that use LPMRF as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed Admixture of Poisson MRFs [2]. We show the effectiveness of our LPMRF distribution over Multinomial models by evaluating the test set perplexity on a dataset of abstracts and Wikipedia. Qualitatively, we show that the positive dependencies discovered by LPMRF are interesting and intuitive. Finally, we show that our algorithms are fast and have good scaling (code available online).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3213–3221},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969597,
author = {Singer, Yaron and Vondr\'{a}k, Jan},
title = {Information-Theoretic Lower Bounds for Convex Optimization with Erroneous Oracles},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function x → f (x) we consider optimization when one is given access to absolute error oracles that return values in [f (x) - ∊, f (x) + ∊] or relative error oracles that return value in [(1 - ∊)f (x), (1 + ∊)f (x)], for some ∊ &gt; 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3204–3212},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969596,
author = {Shanmugam, Karthikeyan and Kocaoglu, Murat and Dimakis, Alexandros G. and Vishwanath, Sriram},
title = {Learning Causal Graphs with Small Interventions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms.For general chordal graphs, we derive worst case lower bounds on the number of interventions. Building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely. In the worst case, our achievable scheme is an α-approximation algorithm where α is the independence number of the graph. We also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound. In the other extreme, there are graph classes for which the required number of experiments is multiplicatively α away from our lower bound.In simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3195–3203},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969595,
author = {Narasimhan, Harikrishna and Parkes, David C. and Singer, Yaron},
title = {Learnability of Influence in Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show PAC learnability of influence functions for three common influence models, namely, the Linear Threshold (LT), Independent Cascade (IC) and Voter models, and present concrete sample complexity results in each case. Our results for the LT model are based on interesting connections with neural networks; those for the IC model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments; and those for the Voter model are based on a reduction to linear regression. We show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced. We also provide efficient polynomial time learning algorithms for a setting with full observation, i.e. where the cascades also contain the time steps in which nodes are influenced.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3186–3194},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969594,
author = {Yun, Se-Young and Lelarge, Marc and Proutiere, Alexandre},
title = {Fast and Memory Optimal Low-Rank Matrix Approximation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we revisit the problem of constructing a near-optimal rank k approximation of a matrix M ∈ [0, l]mxn under the streaming data model where the columns of M are revealed sequentially. We present SLA (Streaming Low-rank Approximation), an algorithm that is asymptotically accurate, when ksk+1(M) = o(√mn) where sk+1(M) is the (k + 1)-th largest singular value of M. This means that its average mean-square error converges to 0 as m and n grow large (i.e., ‖M(k) - M(k)‖2F = o(mn) with high probability, where M(k) and M(k) denote the output of SLA and the optimal rank k approximation of M, respectively). Our algorithm makes one pass on the data if the columns of M are revealed in a random order, and two passes if the columns of M arrive in an arbitrary order. To reduce its memory footprint and complexity, SLA uses random sparsification, and samples each entry of M with a small probability δ. In turn, SLA is memory optimal as its required memory space scales as k(m + n), the dimension of its output. Furthermore, SLA is computationally efficient as it runs in O(δkmn) time (a constant number of operations is made for each observed entry of M), which can be as small as O(k log(m)4n) for an appropriate choice of 5 and if n ≥ m.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3177–3185},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969593,
author = {Neu, Gergely},
title = {Explore No More: Improved High-Probability Regret Bounds for Non-Stochastic Bandits},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample arms from the uniform distribution at least Ω( √T) times over T rounds, which can adversely affect performance if many of the arms are suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit exploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3168–3176},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969592,
author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Buckmann, Marcus},
title = {Learning from Small Samples: An Analysis of Simple Decision Heuristics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Simple decision heuristics are models of human and animal behavior that use few pieces of information—perhaps only a single piece of information—and integrate the pieces in simple ways, for example, by considering them sequentially, one at a time, or by giving them equal weight. We focus on three families of heuristics: single-cue decision making, lexicographic decision making, and tallying. It is unknown how quickly these heuristics can be learned from experience. We show, analytically and empirically, that substantial progress in learning can be made with just a few training samples. When training samples are very few, tallying performs substantially better than the alternative methods tested. Our empirical analysis is the most extensive to date, employing 63 natural data sets on diverse subjects.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3159–3167},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969591,
author = {Yurtsever, Alp and Tran-Dinh, Quoc and Cevher, Volkan},
title = {A Universal Primal-Dual Convex Optimization Framework},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown H\"{o}lder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3150–3158},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969590,
author = {Gillenwater, Jennifer and Iyer, Rishabh and Lusch, Bethany and Kidambi, Rahul and Bilmes, Jeff},
title = {Submodular Hamming Metrics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3141–3149},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969589,
author = {Mordatch, Igor and Lowrey, Kendall and Andrew, Galen and Popovic, Zoran and Todorov, Emanuel},
title = {Interactive Control of Diverse Complex Characters with Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3132–3140},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969588,
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
title = {BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3123–3131},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969587,
author = {Kobilarov, Marin},
title = {Sample Complexity Bounds for Iterative Stochastic Policy Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3114–3122},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969586,
author = {Zheng, Qinqing and Tomioka, Ryota},
title = {Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio O(n[K/2]/2) for recovering a Kth order rank one tensor of size n x · · · x n by recursive unfolding. In this paper, we first improve this bound to O(nK/4) by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal O(√n + √HK-1) bound, in which the parameter H controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with H = O(1).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3106–3113},
numpages = {8},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969585,
author = {Sa, Christopher De and Zhang, Ce and Olukotun, Kunle and R\'{e}, Christopher},
title = {Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width—regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3097–3105},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969584,
author = {Sindhwani, Vikas and Sainath, Tara N. and Kumar, Sanjiv},
title = {Structured Transforms for Small-Footprint Deep Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3088–3096},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969583,
author = {Dai, Andrew M. and Le, Quoc V.},
title = {Semi-Supervised Sequence Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3079–3087},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969582,
author = {Kwitt, Roland and Huber, Stefan and Niethammer, Marc and Lin, Weili and Bauer, Ulrich},
title = {Statistical Topological Data Analysis - a Kernel Perspective},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of statistical computations with persistence diagrams, a summary representation of topological features in data. These diagrams encode persistent homology, a widely used invariant in topological data analysis. While several avenues towards a statistical treatment of the diagrams have been explored recently, we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel Hilbert spaces. In fact, a positive definite kernel on persistence diagrams has recently been proposed, connecting persistent homology to popular kernel-based learning techniques such as support vector machines. However, important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored. Our contribution is to close this gap by proving universality of a variant of the original kernel, and to demonstrate its effective use in two-sample hypothesis testing on synthetic as well as real-world data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3070–3078},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969581,
author = {Hartline, Jason and Syrgkanis, Vasilis and Tardos, \'{E}va},
title = {No-Regret Learning in Bayesian Games},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3061–3069},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969580,
author = {Erdogdu, Murat A. and Montanari, Andrea},
title = {Convergence Rates of Sub-Sampled Newton Methods},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set C ⊂ ℝp, where n ≫ p ≫ 1. In this regime, algorithms which utilize sub-sampling techniques are known to be effective. In this paper, we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to Newton's method, yet has much smaller per-iteration cost. The proposed algorithm is robust in terms of starting point and step size, and enjoys a composite convergence rate, namely, quadratic convergence at start and linear convergence when the iterate is close to the minimizer. We develop its theoretical analysis which also allows us to select near-optimal algorithm parameters. Our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well. We demonstrate how our results apply to well-known machine learning problems. Lastly, we evaluate the performance of our algorithm on several datasets under various scenarios.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3052–3060},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969579,
author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
title = {The Poisson Gamma Belief Network},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3043–3051},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969578,
author = {Frongillo, Rafael and Reid, Mark D.},
title = {Convergence Analysis of Prediction Markets via Randomized Subspace Descent},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders. The pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market. However, little is known about rates and guarantees for the convergence of these sequential mechanisms, and two recent papers cite this as an important open question.In this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent (RSD). We establish convergence rates for RSD and leverage them to prove rates for the two prediction market models above, answering the open questions. Our results extend beyond standard centralized markets to arbitrary trade networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3034–3042},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969577,
author = {Talwar, Kunal and Thakurta, Abhradeep and Zhang, Li},
title = {Nearly-Optimal Private LASSO},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a nearly optimal differentially private version of the well known LASSO estimator. Our algorithm provides privacy protection with respect to each training example. The excess risk of our algorithm, compared to the non-private version, is \~{O}(1/n2/3), assuming all the input data has bounded ℓ∞ norm. This is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix. In addition, we show that this error bound is nearly optimal amongst all differentially private algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3025–3033},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969576,
author = {Sarkar, Purnamrita and Chakrabarti, Deepayan and Bickel, Peter},
title = {The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Link prediction and clustering are key problems for network-structured data. While spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks, it can be expensive for large graphs. On the other hand, the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast, and works very well in practice. We show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough, and can do so even in sparser graphs with the addition of a "cleaning" step. Empirical results on simulated and real-world data support our conclusions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3016–3024},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969575,
author = {Afshar, Hadi Mohasel and Domke, Justin},
title = {Reflection, Refraction, and Hamiltonian Monte Carlo},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3007–3015},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969574,
author = {Stollenga, Marijn F. and Byeon, Wonmin and Liwicki, Marcus and Schmidhuber, Juergen},
title = {Parallel Multi-Dimensional LSTM, with Application to Fast Biomedical Volumetric Image Segmentation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelise on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelise, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2998–3006},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969573,
author = {Syrgkanis, Vasilis and Agarwal, Alekh and Luo, Haipeng and Schapire, Robert E.},
title = {Fast Convergence of Regularized Learning in Games},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T-3/4), while the sum of utilities converges to an approximate optimum at O(T-1)-an improvement upon the worst case O(T-1/2) rates. We show a black-box reduction for any algorithm in the class to achieve \~{O}(T-1/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [17] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2989–2997},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969572,
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
title = {A Recurrent Latent Variable Model for Sequential Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2980–2988},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969571,
author = {Carlson, David E. and Collins, Edo and Hsieh, Ya-Ping and Carin, Lawrence and Cevher, Volkan},
title = {Preconditioned Spectral Descent for Deep Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that exploits the so far unused "geometry" in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA-grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2971–2979},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969570,
author = {Ren, Mengye and Kiros, Ryan and Zemel, Richard S.},
title = {Exploring Models and Data for Image Question Answering},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2953–2961},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969569,
author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom},
title = {Learning Continuous Control Policies by Stochastic Value Gradients},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment instead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2944–2952},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969568,
author = {Herbster, Mark and Pasteris, Stephen and Ghosh, Shaona},
title = {Online Prediction at the Limit of Zero Temperature},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We design an online algorithm to classify the vertices of a graph. Underpinning the algorithm is the probability distribution of an Ising model isomorphic to the graph. Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far. Computing these classifications is unfortunately based on a #P-complete problem. This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework. Our algorithm is optimal when the graph is a tree matching the prior results in [1]. For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient, as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2935–2943},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969567,
author = {Dekel, Ofer and Eldan, Ronen and Koren, Tomer},
title = {Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of \~{O}(T5/6), while the best known lower bound is Ω(T1/2). Many attempts have been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of \~{O}(T2/3). We present an efficient algorithm for the bandit smooth convex optimization problem that guarantees a regret of \~{O}(T5/8). Our result rules out an Ω(T2/3) lower bound and takes a significant step towards the resolution of this open problem.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2926–2934},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969566,
author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
title = {A Complete Recipe for Stochastic Gradient MCMC},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers—including stochastic gradient versions—based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2917–2925},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969565,
author = {Chen, Sheng and Banerjee, Arindam},
title = {Structured Estimation with Atomic Norms: General Bounds and Applications},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular Gaussian width of the unit norm ball, Gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric measures can be difficult. In this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2908–2916},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969564,
author = {Comanici, Gheorghe and Precup, Doina and Panangaden, Prakash},
title = {Basis Refinement Strategies for Linear Value Function Approximation in MDPs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a theoretical framework for analyzing basis function construction for linear value function approximation in Markov Decision Processes (MDPs). We show that important existing methods, such as Krylov bases and Bellman-error-based methods are a special case of the general framework we develop. We provide a general algorithmic framework for computing basis function refinements which "respect" the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework. We also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding "prototypes" that are diverse enough to represent the given MDP.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2899–2907},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969563,
author = {Kozdoba, Mark and Mannor, Shie},
title = {Community Detection via Measure Space Embedding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of k-means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks, and find its performance to be better or at least as good as previously known algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a p, q-stochastic block model with where p ≥ c · N- ½+ ∊ and p - q ≥ c′ √pN- ½+ ∊ log N.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2890–2898},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969562,
author = {Mirzasoleiman, Baharan and Karbasi, Amin and Badanidiyuru, Ashwinkumar and Krause, Andreas},
title = {Distributed Submodular Cover: Succinctly Summarizing Massive Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition prevalent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution. However, this sequential, centralized approach is impractical for truly large-scale problems. In this work, we develop the first distributed algorithm - DISCOVER - for submodular set cover that is easily implementable using MapReduce-style computations. We theoretically analyze our approach, and present approximation guarantees for the solutions returned by DISCOVER. We also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including active set selection, exemplar based clustering, and vertex cover on tens of millions of data points using Spark.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2881–2889},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969561,
author = {Voss, James and Belkin, Mikhail and Rademacher, Luis},
title = {A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product") space. The use of this indefinite "inner product" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing.Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2872–2880},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969560,
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
title = {Action-Conditional Video Prediction Using Deep Networks in Atari Games},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2863–2871},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969559,
author = {Wilson, Andrew Gordon and Dann, Christoph and Lucas, Christopher G. and Xing, Eric P.},
title = {The Human Kernel},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2854–2862},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969558,
author = {Krichene, Walid and Bayen, Alexandre M. and Bartlett, Peter L.},
title = {Accelerated Mirror Descent in Continuous and Discrete Time},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study accelerated mirror descent dynamics in continuous and discrete time. Combining the original continuous-time motivation of mirror descent with a recent ODE interpretation of Nesterov's accelerated method, we propose a family of continuous-time descent dynamics for convex functions with Lipschitz gradients, such that the solution trajectories converge to the optimum at a O(1/t2) rate. We then show that a large family of first-order accelerated methods can be obtained as a discretization of the ODE, and these methods converge at a O(1/k2) rate. This connection between accelerated mirror descent and the ODE provides an intuitive approach to the design and analysis of accelerated first-order algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2845–2853},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969557,
author = {Feldman, Vitaly and Perkins, Will and Vempala, Santosh},
title = {Subsampled Power Iteration: A Unified Algorithm for Block Models and Planted CSP's},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (CSP), via a common generalization in terms of random bipartite graphs. Our algorithm matches up to a constant factor the best-known bounds for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches.The main contribution of the algorithm is in the case of unequal sizes in the bi-partition that arises in our reduction from the planted CSP. Here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix.Other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2836–2844},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969556,
author = {Steinhardt, Jacob and Liang, Percy},
title = {Learning with Relaxed Supervision},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2827–2835},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969555,
author = {Dann, Christoph and Brunskill, Emma},
title = {Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound \~{O}(|S|2|A|H2/∊2 ln 1/δ) and a lower PAC bound Ω(|S||A|H2/∊2 ln 1/δ+c) that match up to log-terms and an additional linear dependency on the number of states |S|. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least H3.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2818–2826},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969554,
author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
title = {Bayesian Optimization with Exponential Convergence},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the δ-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence [1] requires access to the δ-cover sampling, which was considered to be impractical [1, 2]. Our approach eliminates both requirements and achieves an exponential convergence rate.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2809–2817},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969553,
author = {Henao, Ricardo and Gan, Zhe and Lu, James and Carin, Lawrence},
title = {Deep Poisson Factor Modeling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new deep architecture for topic modeling, based on Poisson Factor Analysis (PFA) modules. The model is composed of a Poisson distribution to model observed vectors of counts, as well as a deep hierarchy of hidden binary units. Rather than using logistic functions to characterize the probability that a latent binary unit is on, we employ a Bernoulli-Poisson link, which allows PFA modules to be used repeatedly in the deep architecture. We also describe an approach to build discriminative topic models, by adapting PFA modules. We derive efficient inference via MCMC and stochastic variational methods, that scale with the number of non-zeros in the data and binary units, yielding significant efficiency, relative to models based on logistic links. Experiments on several corpora demonstrate the advantages of our model when compared to related deep models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2800–2808},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969552,
author = {Makhzani, Alireza and Frey, Brendan},
title = {Winner-Take-All Autoencoders},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion. We first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units. We then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions. We will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets, and achieve competitive classification performance.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2791–2799},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969551,
author = {Slawski, Martin and Li, Ping and Hein, Matthias},
title = {Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2782–2790},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969550,
author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
title = {Grammar as a Foreign Language},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2773–2781},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969549,
author = {Monfort, Mathew and Lake, Brenden M. and Ziebart, Brian D. and Lucey, Patrick and Tenenbaum, Joshua B.},
title = {Softstar: Heuristic-Guided Probabilistic Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself. This higher-level abstraction improves generalization in different prediction settings, but computing predictions often becomes intractable in large decision spaces. We propose the Softstar algorithm, a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior. This approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods. We present the algorithm, analyze approximation guarantees, and compare performance with simulation-based inference on two distinct complex decision tasks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2764–2772},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969548,
author = {Huang, Tzu-Kuo and Agarwal, Alekh and Hsu, Daniel and Langford, John and Schapire, Robert E.},
title = {Efficient and Parsimonious Agnostic Active Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We develop a new active learning algorithm for the streaming setting satisfying three important properties: 1) It provably works for any classifier representation and classification problem including those with severe noise. 2) It is efficiently implementable with an ERM oracle. 3) It is more aggressive than all previous approaches satisfying 1 and 2. To do this, we create an algorithm based on a newly defined optimization problem and analyze it. We also conduct the first experimental analysis of all efficient agnostic active learning algorithms, evaluating their strengths and weaknesses in different settings.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2755–2763},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969547,
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
title = {Efficient and Robust Automated Machine Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2755–2763},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969546,
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
title = {Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2746–2754},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969545,
author = {Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
title = {Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Asynchronous parallel implementations of stochastic gradient (SG) have been broadly used in solving deep neural network and received many successes in practice recently. However, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism. To fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of SG: one is over a computer network and the other is on a shared memory system. We establish an ergodic convergence rate O(1/ √K) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by √K (K is the total number of iterations). Our results generalize and improve existing analysis for convex minimization.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2737–2745},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969544,
author = {Wang, Hong and Xing, Wei and Asif, Kaiser and Ziebart, Brian D.},
title = {Adversarial Prediction Games for Multivariate Losses},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2728–2736},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969543,
author = {Kyng, Rasmus and Rao, Anup and Sachdeva, Sushant},
title = {Fast, Provable Algorithms for Isotonic Regression in All ℓ-<sub><i>p</i></sub>-Norms},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Given a directed acyclic graph G, and a set of values y on the vertices, the Isotonic Regression of y is a vector x that respects the partial order described by G, and minimizes ‖x - y ‖, for a specified norm. This paper gives improved algorithms for computing the Isotonic Regression for all weighted ℓp-norms with rigorous performance guarantees. Our algorithms are quite practical, and variants of them can be implemented to run fast in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2719–2727},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969542,
author = {Lee, Moontae and Bindel, David and Mimno, David},
title = {Robust Spectral Inference for Joint Stochastic Matrix Factorization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2710–2718},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969541,
author = {Mazumdar, Arya and Rawat, Ankit Singh},
title = {Associative Memory via a Sparse Recovery Model},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An associative memory is a structure learned from a dataset M of vectors (signals) in a way such that, given a noisy version of one of the vectors as input, the nearest valid vector from M (nearest neighbor) is provided as output, preferably via a fast iterative algorithm. Traditionally, binary (or q-ary) Hopfield neural networks are used to model the above structure. In this paper, for the first time, we propose a model of associative memory based on sparse recovery of signals. Our basic premise is simple. For a dataset, we learn a set of linear constraints that every vector in the dataset must satisfy. Provided these linear constraints possess some special properties, it is possible to cast the task of finding nearest neighbor as a sparse recovery problem. Assuming generic random models for the dataset, we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size O(n). Furthermore, given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates, the vector can be correctly recalled using a neurally feasible algorithm.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2701–2709},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969540,
author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
title = {Pointer Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2692–2700},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969539,
author = {Rosenbaum, Dan and Weiss, Yair},
title = {The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In recent years, approaches based on machine learning have achieved state-of-the-art performance on image restoration problems. Successful approaches include both generative models of natural images as well as discriminative training of deep neural networks. Discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time. In contrast, generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of Bayes' rule.In this paper we show how to combine the strengths of both approaches by training a discriminative, feed-forward architecture to predict the state of latent variables in a generative model of natural images. We apply this idea to the very successful Gaussian Mixture Model (GMM) of natural images. We show that it is possible to achieve comparable performance as the original GMM but with two orders of magnitude improvement in run time while maintaining the advantage of generative models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2683–2691},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969538,
author = {Sa, Christopher De and Zhang, Ce and Olukotun, Kunle and R\'{e}, Christopher},
title = {Taming the Wild: A Unified Analysis of HOG WILD! -Style Algorithms},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (HOGWILD!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called BUCKWILD!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2674–2682},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969537,
author = {Huang, Qingqing and Kakade, Sham M.},
title = {Super-Resolution off the Grid},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly.Suppose we have k point sources in d dimensions, where the points are separated by at least Δ from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees:• The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/Δ) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as Ω(√d/Δ).• The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation Δ. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities.Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition).},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2665–2673},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969536,
author = {Jamieson, Kevin and Jain, Lalit and Fernandez, Chris and Glattard, Nick and Nowak, Robert},
title = {NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2656–2664},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969535,
author = {Reddi, Sashank J. and Hefny, Ahmed and Sra, Suvrit and P\"{o}czos, Barnab\'{a}s and Smola, Alex},
title = {On Variance Reduction in Stochastic Gradient Descent and Its Asynchronous Variants},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms—a crucial requirement for modern large-scale applications—have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2647–2655},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969534,
author = {Titsias, Michalis K. and L\'{a}zaro-Gredilla, Miguel},
title = {Local Expectation Gradients for Black Box Variational Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance. Our method works efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, can be trivially parallelized.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2638–2646},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969533,
author = {Gu, Shixiang and Ghahramani, Zoubin and Turner, Richard E.},
title = {Neural Adaptive Sequential Monte Carlo},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2629–2637},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969532,
author = {Li, Wenye},
title = {Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data, our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints, through a simple alternating projection algorithm. Compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the un-calibrated matrix (except in special cases they are identical). We carried out a series of empirical experiments and the results confirmed our theoretical justification. The evaluation also reported significantly improved results in real learning tasks on benchmark datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2620–2628},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969531,
author = {Bhattacharya, Bhaswar B. and Valiant, Gregory},
title = {Testing Closeness with Unequal Sized Samples},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of testing whether two unequal-sized samples were drawn from identical distributions, versus distributions that differ significantly. Specifically, given a target error parameter ε &gt; 0, m1 independent draws from an unknown distribution p with discrete support, and m2 draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case that ‖p - q‖1 ≥ ε. If p and q are supported on at most n elements, then our test is successful with high probability provided m1 ≥ n2/3/ε4/3 and m2 = Ω (max{n/√m1 ε2, √n/ε2}). We show that this tradeoff is information theoretically optimal throughout this range in the dependencies on all parameters, n, m1, and ε, to constant factors for worst-case distributions. As a consequence, we obtain an algorithm for estimating the mixing time of a Markov chain on n states up to a log n factor that uses \~{O}(n3/2τmix) queries to a "next node" oracle. The core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic and on natural language data. We believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2611–2619},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969530,
author = {Yang, Eunho and Lozano, Aur\'{e}lie C.},
title = {Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Gaussian Graphical Models (GGMs) are popular tools for studying network structures. However, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the Gaussian distribution. In this paper, we propose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Our method guards against outliers by an implicit trimming mechanism akin to the popular Least Trimmed Squares method used for linear regression. We provide a rigorous statistical analysis of our estimator in the high-dimensional setting. In contrast, existing approaches for robust sparse GGMs estimation lack statistical guarantees. Our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2602–2610},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969529,
author = {Ba, Jimmy and Grosse, Roger and Salakhutdinov, Ruslan and Frey, Brendan},
title = {Learning Wake-Sleep Recurrent Attention Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2593–2601},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969528,
author = {Verma, Nakul and Branson, Kristin},
title = {Sample Complexity of Learning Mahalanobis Distance Metrics},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Metric learning seeks a transformation of the feature space that enhances prediction quality for a given task. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. In addition, by leveraging the structure of the data distribution, we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset, allowing us to relax the dependence on representation dimension. We show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset's intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported in previous works.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2584–2592},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969527,
author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
title = {Variational Dropout and the Local Reparameterization Trick},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the mini-batch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2575–2583},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969526,
author = {Diakonikolas, Ilias and Hardt, Moritz and Schmidt, Ludwig},
title = {Differentially Private Learning of Structured Discrete Distributions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate the problem of learning an unknown probability distribution over a discrete population from random samples. Our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing Differential Privacy to the individuals of the population.We describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families. Our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient—both in terms of sample size and running time—as their non-private counterparts. We complement our theoretical guarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2566–2574},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969525,
author = {Koolen, Wouter M. and Malek, Alan and Bartlett, Peter L. and Abbasi-Yadkori, Yasin},
title = {Minimax Time Series Prediction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider an adversarial formulation of the problem of predicting a time series with square loss. The aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect. Our approach allows natural measures of smoothness such as the squared norm of increments. More generally, we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms. We derive the minimax strategy for all problems of this type and show that it can be implemented efficiently. The optimal predictions are linear in the previous observations. We obtain an explicit expression for the regret in terms of the parameters defining the problem. For typical, simple definitions of smoothness, the computation of the optimal predictions involves only sparse matrices. In the case of norm-constrained data, where the smoothness is defined in terms of the squared norm of the comparator's increments, we show that the regret grows as T/√ λT, where T is the length of the game and λT is an increasing limit on comparator smoothness.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2557–2565},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969524,
author = {Shah, Parikshit and Rao, Nikhil and Tang, Gongguo},
title = {Sparse and Low-Rank Tensor Decomposition},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algorithm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2548–2556},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969523,
author = {Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Joshua B.},
title = {Deep Convolutional Inverse Graphics Network},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2539–2547},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969522,
author = {Mohri, Mehryar and Medina, Andr\'{e}s Mu\~{n}oz},
title = {Revenue Optimization against Strategic Buyers},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his γ-discounted surplus. In order to analyze this problem we introduce the notion of e-strategic buyer, a more natural notion of strategic behavior than what has been considered in the past. We improve upon the previous state-of-the-art and achieve an optimal regret bound in O(log T + 1/log(1/γ)) when the seller selects prices from a finite set and provide a regret bound in \~{O}(√T + T1/4/ log(1/γ)) when the prices offered are selected out of the interval [0,1].},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2530–2538},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969521,
author = {Wang, Zhaoran and Gu, Quanquan and Ning, Yang and Liu, Han},
title = {High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2521–2529},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969520,
author = {Gao, Tian and Ji, Qiang},
title = {Local Causal Discovery of Direct Causes and Effects},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs (CPDAG) in order to identify direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2512–2520},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969519,
author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
title = {Hidden Technical Debt in Machine Learning Systems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2503–2511},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969518,
author = {Miller, Andrew and Wu, Albert and Regier, Jeffrey and McAuliffe, Jon and Lang, Dustin and Prabhat and Schlegel, David and Adams, Ryan},
title = {A Gaussian Process Model of Quasar Spectral Energy Distributions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a method for combining two sources of astronomical data, spectroscopy and photometry, that carry information about sources of light (e.g., stars, galaxies, and quasars) at extremely different spectral resolutions. Our model treats the spectral energy distribution (SED) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations. We place a flexible, nonparametric prior over the SED of a light source that admits a physically interpretable decomposition, and allows us to tractably perform inference. We use our model to predict the distribution of the redshift of a quasar from five-band (low spectral resolution) photometric data, the so called "photoz" problem. Our method shows that tools from machine learning and Bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2494–2502},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969517,
author = {Shvartsman, Michael and Srivastava, Vaibhav and Cohen, Jonathan D.},
title = {A Theory of Decision Making under Dynamic Context},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The dynamics of simple decisions are well understood and modeled as a class of random walk models [e.g. 1-4]. However, most real-life decisions include a dynamically-changing influence of additional information we call context. In this work, we describe a computational theory of decision making under dynamically shifting context. We show how the model generalizes the dominant existing model of fixed-context decision making [2] and can be built up from a weighted combination of fixed-context decisions evolving simultaneously. We also show how the model generalizes recent work on the control of attention in the Flanker task [5]. Finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the AX Continuous Performance Test [6], using the same model parameters.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2485–2493},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969516,
author = {Richard, Emile and Goetz, Georges and Chichilnisky, E. J.},
title = {Recognizing Retinal Ganglion Cells in the Dark},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets. Therefore, a key step in understanding neural systems is to reliably distinguish cell types. An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive. Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array. We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF). We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm. This can result in accurate, fully automated methods for cell type classification.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2476–2484},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969515,
author = {Gan, Zhe and Li, Chunyuan and Henao, Ricardo and Carlson, David and Carin, Lawrence},
title = {Deep Temporal Sigmoid Belief Networks for Sequence Modeling},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2467–2475},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969514,
author = {Beygelzimer, Alina and Hazan, Elad and Kale, Satyen and Luo, Haipeng},
title = {Online Gradient Boosting},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2458–2466},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969513,
author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P.},
title = {Spectral Representations for Convolutional Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2449–2457},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969512,
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
title = {End-to-End Memory Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2440–2448},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969511,
author = {Wang, Xiangyu and Leng, Chenlei and Dunson, David B.},
title = {On the Consistency Theory of High Dimensional Variable Screening},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS and HOLP are both strong screening consistent (subject to additional constraints) with large probability if n &gt; O((ρs+σ/τ)2 log p) under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of SIS.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2431–2439},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969510,
author = {Neyshabur, Behnam and Salakhutdinov, Ruslan and Srebro, Nathan},
title = {Path-SGD: Path-Normalized Optimization in Deep Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and Ada-Grad.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2422–2430},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969509,
author = {Khalvati, Koosha and Rao, Rajesh P. N.},
title = {A Bayesian Framework for Modeling Confidence in Perceptual Decision Making},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The degree of confidence in one's choice or decision is a critical aspect of perceptual decision making. Attempts to quantify a decision maker's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal. In this paper, we introduce a Bayesian framework to model confidence in perceptual decision making. We show that this model, based on partially observable Markov decision processes (POMDPs), is able to predict confidence of a decision maker based only on the data available to the experimenter. We test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task. In both experiments, we show that our model's predictions closely match experimental data. Additionally, our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2413–2421},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969508,
author = {Schiratti, Jean-Baptiste and Allassonni\`{e}re, St\'{e}phanie and Colliot, Olivier and Durrleman, Stanley},
title = {Learning Spatiotemporal Trajectories from Manifold-Valued Longitudinal Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a Bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data, namely repeated measurements of the same objects or individuals at several points in time. The model allows to estimate a group-average trajectory in the space of measurements. Random variations of this trajectory result from spatiotemporal transformations, which allow changes in the direction of the trajectory and in the pace at which trajectories are followed. The use of the tools of Riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints, which lie therefore on a Riemannian manifold. Stochastic approximations of the Expectation-Maximization algorithm is used to estimate the model parameters in this highly non-linear setting. The method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of Alzheimer's disease. Experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease, thus validating the fact that it effectively estimated a normative scenario of disease progression. Random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2404–2412},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969507,
author = {Tripuraneni, Nilesh and Gu, Shixiang and Ge, Hong and Ghahramani, Zoubin},
title = {Particle Gibbs for Infinite Hidden Markov Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametric generalization of the classical Hidden Markov Model which can automatically infer the number of hidden states in the system. However, due to the infinite-dimensional nature of the transition dynamics, performing inference in the iHMM is difficult. In this paper, we present an infinite-state Particle Gibbs (PG) algorithm to re-sample state trajectories for the iHMM. The proposed algorithm uses an efficient proposal optimized for iHMMs and leverages ancestor sampling to improve the mixing of the standard PG algorithm. Our algorithm demonstrates significant convergence improvements on synthetic and real world data sets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2395–2403},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969506,
author = {Gardner, Jacob R. and Malkomes, Gustavo and Garnett, Roman and Weinberger, Kilian Q. and Barbour, Dennis and Cunningham, John P.},
title = {Bayesian Active Model Selection with an Application to Automated Audiometry},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2386–2394},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969505,
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J\"{u}rgen},
title = {Training Very Deep Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2377–2385},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969504,
author = {Yen, Ian E. H. and Zhong, Kai and Hsieh, Cho-Jui and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Sparse Linear Programming via Primal and Dual Augmented Coordinate Descent},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Over the past decades, Linear Programming (LP) has been widely used in different areas and considered as one of the mature technologies in numerical optimization. However, the complexity offered by state-of-the-art algorithms (i.e. interior-point method and primal, dual simplex methods) is still unsatisfactory for problems in machine learning with huge number of variables and constraints. In this paper, we investigate a general LP algorithm based on the combination of Augmented Lagrangian and Coordinate Descent (AL-CD), giving an iteration complexity of O((log(1/∊))2) with O(nnz(A)) cost per iteration, where nnz(A) is the number of non-zeros in the m x n constraint matrix A, and in practice, one can further reduce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy. The algorithm thus yields a tractable alternative to standard LP methods for large-scale problems of sparse solutions and nnz(A) ≪ mn. We conduct experiments on large-scale LP instances from ℓ1 -regularized multi-class SVM, Sparse Inverse Covariance Estimation, and Nonnegative Matrix Factorization, where the proposed approach finds solutions of 10-3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods.1},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2368–2376},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969503,
author = {Chakraborty, Mithun and Das, Sanmay},
title = {Market Scoring Rules Act as Opinion Pools for Risk-Averse Agents},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A market scoring rule (MSR) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents. In this paper, we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a MSR incorporates private information from agents who deviate from the assumption of risk-neutrality. We first establish that, for a myopic trading agent with a risk-averse utility function, a MSR satisfying mild regularity conditions elicits the agent's risk-neutral probability conditional on the latest market state rather than her true subjective probability. Hence, we show that a MSR under these conditions effectively behaves like a more traditional method of belief aggregation, namely an opinion pool, for agents' true probabilities. In particular, the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents, and as a linear pool for an atypical budget-constrained agent utility with decreasing absolute risk aversion. We also point out the interpretation of a market maker under these conditions as a Bayesian learner even when agent beliefs are static.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2359–2367},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969502,
author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
title = {Generalization in Adaptive Data Analysis and Holdout Reuse},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [7], where we focused on the problem of estimating expectations of adaptively chosen functions.In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment.We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [7] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2350–2358},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969501,
author = {Xie, Bo and Liang, Yingyu and Song, Le},
title = {Scale up Nonlinear Component Analysis with Doubly Stochastic Gradients},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they cannot scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees that it converges at the rate \~{O}(1/t) to the global optimum, even for the top k eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2341–2349},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969500,
author = {Ruozzi, Nicholas},
title = {Exactness of Approximate MAP Inference in Continuous MRFs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Computing the MAP assignment in graphical models is generally intractable. As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations. Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog. In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight. We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models. We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2332–2340},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969499,
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jose Miguel and Turner, Richard E.},
title = {Stochastic Expectation Propagation},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2323–2331},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969498,
author = {Pan, Yunpeng and Theodorou, Evangelos A. and Kontitsis, Michail},
title = {Sample Efficient Path Integral Control under Uncertainty},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a data-driven optimal control framework that is derived using the path integral (PI) control approach. We find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model. The proposed algorithm operates in a forward-backward manner which differentiate it from other PI-related methods that perform forward sampling to find optimal controls. Our method uses significantly less samples to find analytic control laws compared to other approaches within the PI control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion. In addition, the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework. We provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2314–2322},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969497,
author = {Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
title = {Variance Reduced Stochastic Gradient Descent with Neighbors},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2305–2313},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969496,
author = {Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
title = {Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2296–2304},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969495,
author = {Sun, Siqi and Kolar, Mladen and Xu, Jinbo},
title = {Learning Structured Densities via Infinite Dimensional Exponential Families},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2287–2295},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969494,
author = {Chen, Changyou and Ding, Nan and Carin, Lawrence},
title = {On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of L-4/5 at L iterations, compared to L-2/3 for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2278–2286},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969493,
author = {Pehlevan, Cengiz and Chklovskii, Dmitri B.},
title = {A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2269–2277},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969492,
author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
title = {Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection. By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2260–2268},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969491,
author = {Babanezhad, Reza and Ahmed, Mohamed Osama and Virani, Alim and Schmidt, Mark and Kone\v{c}n\'{y}, Jakub and Sallinen, Scott},
title = {Stop Wasting My Gradients: Practical SVRG},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly-used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2251–2259},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969490,
author = {Bekker, Jessa and Davis, Jesse and Choi, Arthur and Darwiche, Adnan and Broeck, Guy Van den},
title = {Tractable Learning for Complex Probability Queries},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient. However, the particular class of queries that is tractable depends on the model and underlying representation. Usually this class is MPE or conditional probabilities Pr(x|y) for joint assignments x, y. We propose a tractable learner that guarantees efficient inference for a broader class of queries. It simultaneously learns a Markov network and its tractable circuit representation, in order to guarantee and measure tractability. Our approach differs from earlier work by using Sentential Decision Diagrams (SDD) as the tractable language instead of Arithmetic Circuits (AC). SDDs have desirable properties, which more general representations such as ACs lack, that enable basic primitives for Boolean circuit compilation. This allows us to support a broader class of complex probability queries, including counting, threshold, and parity, in polytime.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2242–2250},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969489,
author = {Wei, Kai and Iyer, Rishabh and Wang, Shengjie and Bai, Wenruo and Bilmes, Jeff},
title = {Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (SFA) [12] and min-max submodular load balancing (SLB) [25], and also average-case instances, that is the submodular welfare problem (SWP) [26] and submodular multiway partition (SMP) [5]. While the robust versions have been studied in the theory community [11, 12, 16, 25, 26], existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2233–2241},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969488,
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G\'{o}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al\'{a}n and Adams, Ryan P.},
title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2224–2232},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969487,
author = {Shivanna, Rakesh and Chatterjee, Bibaswan and Sankaran, Raman and Bhattacharyya, Chiranjib and Bach, Francis},
title = {Spectral Norm Regularization of Orthonormal Representations for Graph Transduction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction. However, the choice of optimal embedding and an efficient algorithm to compute the same remains open. In this paper, we show that orthonormal representations, a class of unit-sphere graph em-beddings are PAC learnable. Existing PAC-based analysis do not apply as the VC dimension of the function class is infinite. We propose an alternative PAC-based bound, which do not depend on the VC dimension of the underlying function class, but is related to the famous Lov\'{a}sz ϑ function. The main contribution of the paper is SPORE, a SPectral regularized ORthonormal Embedding for graph transduction, derived from the PAC bound. SPORE is posed as a non-smooth convex function over an elliptope. These problems are usually solved as semi-definite programs (SDPs) with time complexity O(n6). We present, Infeasible Inexact proximal (IIP): an Inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible. IIP is more scalable than SDP, has an O(1/√T) convergence, and is generally applicable whenever a suitable approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O(1/√T). The proposed algorithm easily scales to 1000's of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2215–2223},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969486,
author = {Sivakumar, Vidyashankar and Banerjee, Arindam and Ravikumar, Pradeep},
title = {Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of high-dimensional structured estimation with norm-regularized estimators, such as Lasso, when the design matrix and noise are drawn from sub-exponential distributions. Existing results only consider sub-Gaussian designs and noise, and both the sample complexity and non-asymptotic estimation error have been shown to depend on the Gaussian width of suitable sets. In contrast, for the sub-exponential setting, we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets, and the analysis holds for any norm. Further, using generic chaining, we show that the exponential width for any set will be at most √log p times the Gaussian width of the set, yielding Gaussian width based results even for the sub-exponential case. Further, for certain popular estimators, viz Lasso and Group Lasso, using a VC-dimension based analysis, we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2206–2214},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969485,
author = {Abernethy, Jacob and Lee, Chansoo and Tewari, Ambuj},
title = {Fighting Bandits with a New Kind of Smoothness},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a new analysis framework for the adversarial multi-armed bandit problem. Using the notion of convex smoothing, we define a novel family of algorithms with minimax optimal regret guarantees. First, we show that regularization via the Tsallis entropy, which includes EXP3 as a special case, matches the O(√NT) minimax regret with a smaller constant factor. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O(√NT log N), as long as the perturbation distribution has a bounded hazard function. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property and lead to near-optimal algorithms.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2197–2205},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969484,
author = {Rahman, Shafin and Bruce, Neil D. B.},
title = {Saliency, Scale and Information: Towards a Unifying Theory},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we present a definition for visual saliency grounded in information theory. This proposal is shown to relate to a variety of classic research contributions in scale-space theory, interest point detection, bilateral filtering, and to existing models of visual saliency. Based on the proposed definition of visual saliency, we demonstrate results competitive with the state-of-the art for both prediction of human fixations, and segmentation of salient objects. We also characterize different properties of this model including robustness to image transformations, and extension to a wide range of other data types with 3D mesh models serving as an example. Finally, we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2188–2196},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969483,
author = {Vainsencher, Daniel and Liu, Han and Zhang, Tong},
title = {Local Smoothness in Variance Reduced Optimization},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems, our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2179–2187},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969482,
author = {Li, Xiao and Ramchandran, Kannan},
title = {An Active Learning Framework Using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Let f : {-1,1}n → ℝ be an n-variate polynomial consisting of 2n monomials, in which only s ≪ 2n coefficients are non-zero. The goal is to learn the polynomial by querying the values of f. We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding.The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2δn) for any δ ∈ (0,1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2170–2178},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969481,
author = {Lomel\'{\i}, Mar\'{\i}a and Favaro, Stefano and Teh, Yee Whye},
title = {A Hybrid Sampler for Poisson-Kingman Mixture Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2161–2169},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969480,
author = {Wang, Joseph and Trapeznikov, Kirill and Saligrama, Venkatesh},
title = {Efficient Learning by Directed Acyclic Graph for Resource Constrained Prediction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2152–2160},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969479,
author = {Orlitsky, Alon and Suresh, Ananda Theertha},
title = {Competitive Distribution Estimation: Why is Good-Turing Good},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Estimating distributions over large alphabets is a fundamental machine-learning tenet. Yet no method is known to estimate all distributions well. For example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, Jelinek-Mercer, and Good-Turing are not known to be near optimal for essentially any distribution.We describe the first universally near-optimal probability estimators. For every discrete distribution, they are provably nearly the best in the following two competitive ways. First they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation. Second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times.Specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of Good-Turing estimator is always within KL divergence of (3 + on(1))/n1/3 from the best estimator, and that a more involved estimator is within On(min(k/n, 1/√n)). Conversely, we show that any estimator must have a KL divergence at least Ωn(min(k/n, 1/n2/3)) over the best estimator for the first comparison, and at least Ωn(min(k/n, 1/√n)) for the second.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2143–2151},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969478,
author = {Yanardag, Pinar and Vishwanathan, S. V. N.},
title = {A Structural Smoothing Framework for Robust Graph-Comparison},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we propose a general smoothing framework for graph kernels by taking structural similarity into account, and apply it to derive smoothed variants of popular graph kernels. Our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (NLP). However, unlike NLP applications that primarily deal with strings, we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs. Moreover, we discuss extensions of the Pitman-Yor process that can be adapted to smooth structured objects, thereby leading to novel graph kernels. Our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features. Experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants, but also outperform several other graph kernels in the literature. Our kernels are competitive in terms of runtime, and offer a viable option for practitioners.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2134–2142},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969477,
author = {Mohamed, Shakir and Rezende, Danilo J.},
title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm — an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2125–2133},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969476,
author = {Combes, Richard and Talebi, M. Sadegh and Proutiere, Alexandre and Lelarge, Marc},
title = {Combinatorial Bandits Revisited},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose COMBEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2116–2124},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969475,
author = {Rao, Nikhil and Yu, Hsiang-Fu and Ravikumar, Pradeep and Dhillon, Inderjit S.},
title = {Collaborative Filtering with Graph Information: Consistency and Scalable Methods},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2107–2115},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969474,
author = {Awasthi, Pranjal and Risteski, Andrej},
title = {On Some Provably Correct Cases of Variational Inference for Topic Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Variational inference is an efficient, popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. Our initializations are natural, one of them being used in LDA-c, the most popular implementation of variational inference. In addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to use non-standard proof arguments, which we believe might be of general theoretical interest.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2098–2106},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969473,
author = {Goldstein, Thomas and Li, Min and Yuan, Xiaoming},
title = {Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler sub-steps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2089–2097},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969472,
author = {Meeds, Edward and Welling, Max},
title = {Optimization Monte Carlo: Efficient and Embarrassingly Parallel Likelihood-Free Inference},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2080–2088},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969471,
author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
title = {Natural Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2071–2079},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969470,
author = {Slawski, Martin and Li, Ping},
title = {B-Bit Marginal Regression},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the problem of sparse signal recovery from m linear measurements quantized to b bits. b-bit Marginal Regression is proposed as recovery algorithm. We study the question of choosing b in the setting of a given budget of bits B = m · b and derive a single easy-to-compute expression characterizing the trade-off between m and b. The choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive Gaussian noise before quantization as well as for adversarial noise. For b ≥ 2, we show that Lloyd-Max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending [15].},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2062–2070},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969469,
author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
title = {Learning with a Wasserstein Loss},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2053–2061},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969468,
author = {Gao, Yuanjun and Buesing, Lars and Shenoy, Krishna V. and Cunningham, John P.},
title = {High-Dimensional Neural Spike Train Analysis with Generalized Count Linear Dynamical Systems},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Latent factor models have been widely used to analyze simultaneous recordings of spike trains from large, heterogeneous neural populations. These models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time, which is observed in high dimension via noisy point-process observations. These techniques have been well used to capture neural correlations across a population and to provide a smooth, denoised, and concise representation of high-dimensional spiking data. One limitation of many current models is that the observation model is assumed to be Poisson, which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data, thereby introducing bias into estimates of covariance. Here we develop the generalized count linear dynamical system, which relaxes the Poisson assumption by using a more general exponential family for count data. In addition to containing Poisson, Bernoulli, negative binomial, and other common count distributions as special cases, we show that this model can be tractably learned by extending recent advances in variational inference techniques. We apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods, both in capturing the variance structure of the data and in held-out prediction.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2044–2052},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969467,
author = {Lei, Yunwen and Dogan, \"{U}r\"{u}n and Binder, Alexander and Kloft, Marius},
title = {Multi-Class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper studies the generalization performance of multi-class classification algorithms, for which we obtain—for the first time—a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on ℓp-norm regularization, where the parameter p controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2035–2043},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969466,
author = {Seaman, Kevin and Lemonnier, R\'{e}mi and Vayatis, Nicolas},
title = {Anytime Influence Bounds and the Explosive Behavior of Continuous-Time Diffusion Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The paper studies transition phenomena in information cascades observed along a diffusion process over some graph. We introduce the Laplace Hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time. Using this concept, we prove tight non-asymptotic bounds for the influence of a set of nodes, and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical. Our contributions include formal definitions and tight lower bounds of critical explosion time. We illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models. Finally, we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2026–2034},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969465,
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
title = {Spatial Transformer Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2017–2025},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969464,
author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
title = {Secure Multi-Party Differential Privacy},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of interactive function computation by multiple parties, each possessing a bit, in a differential privacy setting (i.e., there remains an uncertainty in any party's bit even when given the transcript of interactions and all the other parties' bits). Each party wants to compute a function, which could differ from party to party, and there could be a central observer interested in computing a separate function. Performance at each party is measured via the accuracy of the function to be computed. We allow for an arbitrary cost metric to measure the distortion between the true and the computed function values. Our main result is the optimality of a simple non-interactive protocol: each party randomizes its bit (sufficiently) and shares the privatized version with the other parties. This optimality result is very general: it holds for all types of functions, heterogeneous privacy conditions on the parties, all types of cost metrics, and both average and worst-case (over the inputs) measures of accuracy.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2008–2016},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969463,
author = {Ulrich, Kyle and Carlson, David E. and Dzirasa, Kafui and Carin, Lawrence},
title = {GP Kernels for Cross-Spectrum Analysis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Multi-output Gaussian processes provide a convenient framework for multi-task problems. An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels. Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework. In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel. This new, flexible kernel represents both the power and phase relationship between multiple observation channels. We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel. Results are presented for measured multi-region electrophysiological data.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1999–2007},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969462,
author = {Pinheiro, Pedro O. and Collobert, Ronan and Doll\'{a}r, Piotr},
title = {Learning to Segment Object Candidates},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1990–1998},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969461,
author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
title = {Fast Two-Sample Testing with Analytic Representations of Probability Measures},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1981–1989},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969460,
author = {Brown, Noam and Sandholm, Tuomas},
title = {Regret-Based Pruning in Extensive-Form Games},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Counterfactual Regret Minimization (CFR) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games. CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set. We introduce an improvement to CFR that prunes any path of play in the tree, and its descendants, that has negative regret. It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration. The new algorithm maintains CFR's convergence guarantees while making iterations significantly faster—even if previously known pruning techniques are used in the comparison. This improvement carries over to CFR+, a recent variant of CFR. Experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1972–1980},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969459,
author = {Hefny, Ahmed and Downey, Carlton and Gordon, Geoffrey J.},
title = {Supervised Learning for Dynamical System Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently there has been substantial interest in spectral methods for learning dynamical systems. These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporate prior information such as sparsity or structure. To address this problem, we present a new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization. Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does; the correctness of these instances follows directly from our general analysis.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1963–1971},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969458,
author = {Farajtabar, Mehrdad and Wang, Yichen and Gomez-Rodriguez, Manuel and Li, Shuang and Zha, Hongyuan and Song, Le},
title = {COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-Evolution},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1954–1962},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969457,
author = {Gotovos, Alkis and Hassani, S. Hamed and Krause, Andreas},
title = {Sampling from Probabilistic Submodular Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Submodular and supermodular functions have found wide applicability in machine learning, capturing notions such as diversity and regularity, respectively. These notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention. However, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference. Prominent, well-studied special cases include Ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied. In this paper, we investigate the use of Markov chain Monte Carlo sampling to perform approximate inference in general log-submodular and log-supermodular models. In particular, we consider a simple Gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (O(n log n)) mixing. We also evaluate the efficiency of the Gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1945–1953},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969456,
author = {Ha, Wooseok and Barber, Rina Foygel},
title = {Robust PCA with Compressed Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The robust principal component analysis (RPCA) problem seeks to separate low-rank trends from sparse outliers within a data matrix, that is, to approximate a n x d matrix D as the sum of a low-rank matrix L and a sparse matrix S. We examine the robust principal component analysis (RPCA) problem under data compression, where the data Y is approximately given by (L+S) ·C, that is, a low-rank + sparse data matrix that has been compressed to size n x m (with m substantially smaller than the original dimension d) via multiplication with a compression matrix C. We give a convex program for recovering the sparse component S along with the compressed low-rank component L · C, along with upper bounds on the error of this reconstruction that scales naturally with the compression dimension m and coincides with existing results for the uncompressed setting m = d. Our results can also handle error introduced through additive noise or through missing data. The scaling of dimension, compression, and signal complexity in our theoretical results is verified empirically through simulations, and we also apply our method to a data set measuring chlorine concentration across a network of sensors to test its performance in practice.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1936–1944},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969455,
author = {Theis, Lucas and Bethge, Matthias},
title = {Generative Image Modeling Using Spatial LSTMs},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1927–1935},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969454,
author = {Vacher, Jonathan and Meso, Andrew Isaac and Perrinet, Laurent and Peyr\'{e}, Gabriel},
title = {Biologically Inspired Dynamic Textures for Probing Motion Perception},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1918–1926},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969453,
author = {Oh, Sewoong and Thekumparampil, Kiran K. and Xu, Jiaming},
title = {Collaboratively Learning Preferences from Ordinal Data},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1909–1917},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969452,
author = {Alistarh, Dan and Iglesias, Jennifer and Vojnovic, Milan},
title = {Streaming Min-Max Hypergraph Partitioning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In many applications, the data is of rich structure that can be represented by a hypergraph, where the data items are represented by vertices and the associations among items are represented by hyperedges. Equivalently, we are given an input bipartite graph with two types of vertices: items, and associations (which we refer to as topics). We consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized. This is a clustering problem with various applications, e.g. partitioning of a set of information objects such as documents, images, and videos, and load balancing in the context of modern computation platforms.In this paper, we focus on the streaming computation model for this problem, in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time. Motivated by scalability requirements, we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components. We show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions. We also report results of an extensive empirical evaluation, which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1900–1908},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969451,
author = {Lee, Juho and Choi, Seungjin},
title = {Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real-world datasets demonstrate the benefit of our method.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1891–1899},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969450,
author = {Sadeghi, Fereshteh and Zitnick, C. Lawrence and Farhadi, Ali},
title = {VISALOGY: Answering Visual Analogy Questions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple siamese architecture. We introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1882–1890},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969449,
author = {Ganti, Ravi and Balzano, Laura and Willett, Rebecca},
title = {Matrix Completion under Monotonic Single Index Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1873–1881},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969448,
author = {Scanagatta, Mauro and Campos, Cassio P. de and Corani, Giorgio and Zaffalon, Marco},
title = {Learning Bayesian Networks with Thousands of Variables},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1864–1872},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969447,
author = {Clevert, Djork-Arn\'{e} and Mayr, Andreas and Unterthiner, Thomas and Hochreiter, Sepp},
title = {Rectified Factor Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.RFN package for GPU/CPU is available at http://www.bioinf.jku.at/software/rfn.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1855–1863},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969446,
author = {Pennington, Jeffrey and Yu, Felix X. and Kumar, Sanjiv},
title = {Spherical Random Features for Polynomial Kernels},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit ℓ2 norm. The question we address in this work is: if we know a priori that data is normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1846–1854},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969445,
author = {Li, Chongxuan and Zhu, Jun and Shi, Tianlin and Zhang, Bo},
title = {Max-Margin Deep Generative Models},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1837–1845},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969444,
author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
title = {Learning to Transduce with Unbounded Memory},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1828–1836},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.5555/2969442.2969443,
author = {Zhang, Huishuai and Zhou, Yi and Liang, Yingbin},
title = {Analysis of Robust PCA via Local Incoherence},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate the robust PCA problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming Principal Component Pursuit (PCP). In contrast to previous studies that assume the support of the error matrix is generated by uniform Bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities. We characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by PCP is guaranteed. Such a refined analysis of robust PCA captures how robust each entry of the low rank matrix combats error corruption. In order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1819–1827},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@proceedings{10.5555/2969442,
title = {NIPS'15: Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
location = {Montreal, Canada}
}

