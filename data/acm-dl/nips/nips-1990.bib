@inproceedings{10.5555/2986766.2986909,
author = {Melton, Matt and Phan, Tan and Reeves, Doug and Van Den Bout, Dave},
title = {VLSI Implementation of TInMANN},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A massively parallel, all-digital, stochastic architecture - TInMANN - is described which performs competitive and Kohonen types of learning. A VLSI design is shown for a TInMANN neuron which fits within a small, inexpensive MOSIS TinyChip frame, yet which can be used to build larger networks of several hundred neurons. The neuron operates at a speed of 15 MHz which allows the network to process 290,000 training examples per second. Use of level sensitive scan logic provides the chip with 100% fault coverage, permitting very reliable neural systems to be built.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1046–1052},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986908,
author = {Chuang, Michael L. and Chiang, Alice M.},
title = {Simulation of the Neocognitron on a CCD Parallel Processing Architecture},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The neocognitron is a neural network for pattern recognition and feature extraction. An analog CCD parallel processing architecture developed at Lincoln Laboratory is particularly well suited to the computational requirements of shared-weight networks such as the neocognitron, and implementation of the neocognitron using the CCD architecture was simulated. A modification to the neocognitron training procedure, which improves network performance under the limited arithmetic precision that would be imposed by the CCD architecture, is presented.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1039–1045},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986907,
author = {Graf, H. P. and Janow, R. and Henderson, D. and Lee, R.},
title = {Reconfigurable Neural Net Chip with 32K Connections},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a CMOS neural net chip with a reconfigurable network architecture. It contains 32,768 binary, programmable connections arranged in 256 'building block' neurons. Several 'building blocks' can be connected to form long neurons with up to 1024 binary connections or to form neurons with analog connections. Single- or multi-layer networks can be implemented with this chip. We have integrated this chip into a board system together with a digital signal processor and fast memory. This system is currently in use for image processing applications in which the chip extracts features such as edges and corners from binary and gray-level images.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1032–1038},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986906,
author = {McCartor, Hal},
title = {Back Propagation Implementation on the Adaptive Solutions CNAPS Neurocomputer Chip},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Adaptive Solutions CNAPS architecture chip is a general purpose neurocomputer chip. It has 64 processors, each with 4 K bytes of local memory, running at 25 megahertz. It is capable of implementing most current neural network algorithms with on chip learning. This paper discusses the implementation of the Back Propagation algorithm on an array of these chips and shows performance figures from a clock accurate hardware simulator. An eight chip configuration on one board can update 2.3 billion connections per second in learning mode and process 9.6 billion connections per second in feed forward mode.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1028–1031},
numpages = {4},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986905,
author = {Miller, W. Thomas and Box, Brian A. and Whitney, Erich C. and Glynn, James M.},
title = {Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A high speed implementation of the CMAC neural network was designed using dedicated CMOS logic. This technology was then used to implement two general purpose CMAC associative memory boards for the VME bus. Each board implements up to 8 independent CMAC networks with a total of one million adjustable weights. Each CMAC network can be configured to have from 1 to 512 integer inputs and from 1 to 8 integer outputs. Response times for typical CMAC networks are well below 1 millisecond, making the networks sufficiently fast for most robot control problems, and many pattern recognition and signal processing problems.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1022–1027},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986904,
author = {Alspector, Joshua and Allen, Robert B. and Jayakumar, Anthony and Zeppenfeld, Torsten and Meir, Ronny},
title = {Relaxation Networks for Large Supervised Learning Problems},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Feedback connections are required so that the teacher signal on the output neurons can modify weights during supervised learning. Relaxation methods are needed for learning static patterns with full-time feedback connections. Feedback network learning techniques have not achieved wide popularity because of the still greater computational efficiency of back-propagation. We show by simulation that relaxation networks of the kind we are implementing in VLSI are capable of learning large problems just like back-propagation networks. A microchip incorporates deterministic mean-field theory learning as well as stochastic Boltzmann learning. A multiple-chip electronic system implementing these networks will make high-speed parallel learning in them feasible in the future.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1015–1021},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986903,
author = {Schwartz, Daniel B. and Samalam, Vijay K.},
title = {An Analog VLSI Splining Network},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have produced a VLSI circuit capable of learning to approximate arbitrary smooth of a single variable using a technique closely related to splines. The circuit effectively has 512 knots space on a uniform grid and has full support for learning. The circuit also can be used to approximate multi-variable functions as sum of splines.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1008–1014},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986902,
author = {Kramer, A. and Sin, C. K. and Chu, R. and Ko, P. K.},
title = {Compact EEPROM-Based Weight Functions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We are focusing on the development of a highly compact neural net weight function based on the use of EEPROM devices. These devices have already proven useful for analog weight storage, but existing designs rely on the use of conventional voltage multiplication as the weight function, requiring additional transistors per synapse. A parasitic capacitance between the floating gate and the drain of the EEPROM structure leads to an unusual I-V characteristic which can be used to advantage in designing a compact synapse. This novel behavior is well characterized by a model we have developed. A single-device circuit results in a 1-quadrant synapse function which is nonlinear, though monotonic. A simple extension employing 2 EEPROMs results in a 2 quadrant function which is much more linear. This approach offers the potential for more than a ten-fold increase in the density of neural net implementations.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {1001–1007},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986901,
author = {Holler, Mark A.},
title = {VLSI Implementations of Learning and Memory Systems: A Review},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A large number of VLSI implementations of neural network models have been reported. The diversity of these implementations is noteworthy. This paper attempts to put a group of representative VLSI implementations in perspective by comparing and contrasting them. Design trade-offs are discussed and some suggestions for the direction of future implementation efforts are made.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {993–1000},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986900,
author = {Snyder, Wesley and Nissman, Daniel and Van Den Bout, David and Bilbro, Grift},
title = {Kohonen Networks and Clustering: Comparative Performance in Color Clustering},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The problem of color clustering is defined and shown to be a problem of assigning a large number (hundreds of thousands) of 3-vectors to a small number (256) of clusters. Finding those clusters in such a way that they best represent a full color image using only 256 distinct colors is a burdensome computational problem. In this paper, the problem is solved using "classical" techniques -- k-means clustering, vector quantization (which turns out to be the same thing in this application), competitive learning, and Kohonen self-organizing feature maps. Quality of the result is judged subjectively by how much the pseudo-color result resembles the true color image, by RMS quantization error, and by run time. The Kohonen map provides the best solution.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {984–990},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986899,
author = {Rohwer, Richard},
title = {Time Trials on Second-Order and Variable-Learning-Rate Algorithms},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The performance of seven minimization algorithms are compared on five neural network problems. These include a variable-step-size algorithm, conjugate gradient, and several methods with explicit analytic or numerical approximations to the Hessian.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {977–983},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986898,
author = {Ng, Kenney and Lippmann, Richard P.},
title = {A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks. Two neural network (radial basis function and high order polynomial GMDH network) and five conventional classifiers (Gaussian mixture, linear tree, K nearest neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. Classifiers were chosen to be representative of different approaches to pattern classification and to complement and extend those evaluated in a previous study (Lee and Lippmann, 1989). This and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum error decision regions, when they are properly tuned, and when sufficient training data is available. Practical characteristics such as training time, classification time, and memory requirements, however, can differ by orders of magnitude. These results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate, but by practical considerations concerning memory usage, computational resources, ease of implementation, and restrictions on training and classification times.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {970–976},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986897,
author = {Tsoi, A. C. and Pearson, R. A.},
title = {Comparison of Three Classification Techniques, CART, C4.5 and Multi-Layer Perceptrons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, after some introductory remarks into the classification problem as considered in various research communities, and some discussions concerning some of the reasons for ascertaining the performances of the three chosen algorithms, viz., CART (Classification and Regression Tree), C4.5 (one of the more recent versions of a popular induction tree technique known as ID3), and a multi-layer perceptron (MLP), it is proposed to compare the performances of these algorithms under two criteria: classification and generalisation. It is found that, in general, the MLP has better classification and generalisation accuracies compared with the other two algorithms.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {963–969},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986896,
author = {Roychowdhury, V. P. and Siu, K. Y. and Orlitsky, A. and Kailath, T.},
title = {On the Circuit Complexity of Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce a geometric approach for investigating the power of threshold circuits. Viewing n-variable boolean functions as vectors in R2n, we invoke tools from linear algebra and linear programming to derive new results on the realizability of boolean functions using threshold gates.Using this approach, one can obtain: (1) upper-bounds on the number of spurious memories in Hopfield networks, and on the number of functions implementable by a depth-d threshold circuit; (2) a lower bound on the number of orthogonal input functions required to implement a threshold function; (3) a necessary condition for an arbitrary set of input functions to implement a threshold function; (4) a lower bound on the error introduced in approximating boolean functions using sparse polynomials; (5) a limit on the effectiveness of the only known lower-bound technique (based on computing correlations among boolean functions) for the depth of threshold circuits implementing boolean functions, and (6) a constructive proof that every boolean function f of n input variables is a threshold function of polynomially many input functions, none of which is significantly correlated with f. Some of these results lead to generalizations of key results concerning threshold circuit complexity, particularly those that are based on the so-called spectral or Harmonic analysis approach. Moreover, our geometric approach yields simple proofs, based on elementary results from linear algebra, for many of these earlier results.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {953–959},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986895,
author = {Williamson, Robert C.},
title = {ε-Entropy and the Complexity of Feedforward Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We develop a new feedforward neural network representation of Lipschitz functions from [0, ρ]n into [0, 1] based on the level sets of the function. We show that nρL/2εr + 1/√2εr + (1+n/√2)(ρL/4εr)n is an upper bound on the number of nodes needed to represent f to within uniform error εr, where L is the Lipschitz constant. We also show that the number of bits needed to represent the weights in the network in order to achieve this approximation is given by O(n2ρL/√2 4nεr (ρL/εr)n). We compare this bound with the ε-entropy of the functional class under consideration.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {946–952},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986894,
author = {Sontag, Eduardo D.},
title = {Remarks on Interpolation and Recognition Using Neural Nets},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We consider different types of single-hidden-layer feedforward nets: with or without direct input to output connections, and using either threshold or sigmoidal activation functions. The main results show that direct connections in threshold nets double the recognition but not the interpolation power, while using sigmoids rather than thresholds allows (at least) doubling both. Various results are also given on VC dimension and other measures of recognition capabilities.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {939–945},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986893,
author = {Snapp, Robert R. and Psaltis, Demetri and Venkatesh, Santosh S.},
title = {Asymptotic Slowing down of the Nearest-Neighbor Classifier},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {If patterns are drawn from an n-dimensional feature space according to a probability distribution that obeys a weak smoothness criterion, we show that the probability that a random input pattern is misclassified by a nearest-neighbor classifier using M random reference patterns asymptotically satisfies PM(error) - P∞(error) + a/M2/n, for sufficiently large values of M. Here, P∞(error) denotes the probability of error in the infinite sample limit, and is at most twice the error of a Bayes classifier. Although the value of the coefficient a depends upon the underlying probability distributions, the exponent of M is largely distribution free. We thus obtain a concise relation between a classifier's ability to generalize from a finite reference sample and the dimensionality of the feature space, as well as an analytic validation of Bellman's well known "curse of dimensionality."},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {932–938},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986892,
author = {Pearlmutter, Barak A. and Rosenfeld, Ronald},
title = {Chaitin-Kolmogorov Complexity and Generalization in Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a unified framework for a number of different ways of failing to generalize properly. During learning, sources of random information contaminate the network, effectively augmenting the training data with random information. The complexity of the function computed is therefore increased, and generalization is degraded. We analyze replicated networks, in which a number of identical networks are independently trained on the same data and their results averaged. We conclude that replication almost always results in a decrease in the expected complexity of the network, and that replication therefore increases expected generalization. Simulations confirming the effect are also presented.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {925–931},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986891,
author = {Le Cun, Yann and Kanter, Ido and Sona, Sara A.},
title = {Second Order Properties of Error Surfaces: Learning Time and Generalization},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {918–924},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986890,
author = {Cohn, David and Tesauro, Gerald},
title = {Can Neural Networks Do Better than the Vapnik-Chervonenkis Bounds?},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a series of careful numerical experiments which measure the average generalization capability of neural networks trained on a variety of simple functions. These experiments are designed to test whether average generalization performance can surpass the worst-case bounds obtained from formal learning theory using the Vapnik-Chervonenkis dimension (Blumer et al., 1989). We indeed find that, in some cases, the average generalization is significantly better than the VC bound: the approach to perfect performance is exponential in the number of examples m, rather than the 1/m result of the bound. In other cases, we do find the 1/m behavior of the VC bound, and in these cases, the numerical prefactor is closely related to prefactor contained in the bound.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {911–917},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986889,
author = {Baum, Eric B. and Lang, Kevin J.},
title = {Constructing Hidden Units Using Examples and Queries},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {While the network loading problem for 2-layer threshold nets is NP-hard when learning from examples alone (as with backpropagation), (Baum, 91) has now proved that a learner can employ queries to evade the hidden unit credit assignment problem and PAC-load nets with up to four hidden units in polynomial time. Empirical tests show that the method can also learn far more complicated functions such as randomly generated networks with 200 hidden units. The algorithm easily approximates Wieland's 2-spirals function using a single layer of 50 hidden units, and requires only 30 minutes of CPU time to learn 200-bit parity to 99.7% accuracy.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {904–910},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986888,
author = {Krogh, Anders and Hertz, John A.},
title = {Dynamics of Generalization in Linear Perceptrons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We study the evolution of the generalization ability of a simple linear perceptron with N inputs which learns to imitate a "teacher perceptron". The system is trained on p = αN binary example inputs and the generalization ability measured by testing for agreement with the teacher on all 2N possible binary input patterns. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at α = 1. Except at this point the generalization ability approaches its asymptotic value exponentially, with critical slowing down near the transition; the relaxation time is ∞ (1 - √α)-2. Right at the critical point, the approach to perfect generalization follows a power law ∞ t-1/2. In the presence of noise, the generalization ability is degraded by an amount ∞ (√α - 1)-1 just above α = 1.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {897–903},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986887,
author = {Chauvin, Yves},
title = {Generalization Dynamics in LMS Trained Linear Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {For a simple linear case, a mathematical analysis of the training and generalization (validation) performance of networks trained by gradient descent on a Least Mean Square cost function is provided as a function of the learning parameters and of the statistics of the training data base. The analysis predicts that generalization error dynamics are very dependent on a priori initial weights. In particular, the generalization error might sometimes weave within a computable range during extended training. In some cases, the analysis provides bounds on the optimal number of training cycles for minimal validation error. For a speech labeling task, predicted weaving effects were qualitatively tested and observed by computer simulations in networks trained by the linear and non-linear back-propagation algorithm.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {890–896},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986886,
author = {Biswas, Sanjay and Venkatesh, Santosh S.},
title = {The Devil and the Network: What Sparsity Implies to Robustness and Memory},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Robustness is a commonly bruited property of neural networks; in particular, a folk theorem in neural computation asserts that neural networks--in contexts with large interconnectivity--continue to function efficiently, albeit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectivity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the "devil" in the network as an agent that produces sparsity by snipping connections.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {883–889},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986885,
author = {Weigend, Andreas S. and Rumelhart, David E. and Huberman, Bernardo A.},
title = {Generalization by Weight-Elimination with Application to Forecasting},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {875–882},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986884,
author = {Rossen, Michael L.},
title = {Closed-Form Inversion of Backpropagation Networks: Theory and Optimization Issues},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a closed-form technique for mapping the output of a trained backpropagation network into input activity space. The mapping is an inverse mapping in the sense that, when the image of the mapping in input activity space is propagated forward through the normal network dynamics, it reproduces the output used to generate that image. When more than one such inverse mappings exist, our inverse mapping is special in that it has no projection onto the nullspace of the activation flow operator for the entire network. An important by-product of our calculation, when more than one inverse mappings exist, is an orthogonal basis set of a significant portion of the activation flow operator nullspace. This basis set can be used to obtain an alternate inverse mapping that is optimized for a particular real-world application.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {868–872},
numpages = {5},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986883,
author = {Kolen, John F. and Pollack, Jordan B.},
title = {Back Propagation is Sensitive to Initial Conditions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result, additional deterministic experiments were performed. The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {860–867},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986882,
author = {Denker, John S. and LeCun, Yann},
title = {Transforming Neural-Net Output Levels to Probability Distributions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments of the probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "softmax" scheme.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {853–859},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986881,
author = {Bilbro, Griff L. and Van Den Bout, David E.},
title = {Learning Theory and Experiments with Competitive Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We apply the theory of Tishby, Levin, and Solla (TLS) to two problems. First we analyze an elementary problem for which we find the predictions consistent with conventional statistical results. Second we numerically examine the more realistic problem of training a competitive net to learn a probability density from samples. We find TLS useful for predicting average training behavior.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {846–852},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986880,
author = {Baras, John S. and La Vigna, Anthony},
title = {Convergence of a Neural Network Classifier},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, we prove that the vectors in the LVQ learning algorithm converge. We do this by showing that the learning algorithm performs stochastic approximation. Convergence is then obtained by identifying the appropriate conditions on the learning rate and on the underlying statistics of the classification problem. We also present a modification to the learning algorithm which we argue results in convergence of the LVQ error to the Bayesian optimal error as the appropriate parameters become large.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {839–845},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986879,
author = {Darken, Christian and Moody, John},
title = {Note on Learning Rate Schedules for Stochastic Optimization},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present and compare learning rate schedules for stochastic gradient descent, a general algorithm which includes LMS, on-line backpropagation and k-means clustering as special cases. We introduce "search-then-converge" type schedules which outperform the classical constant and "running average" (1/t) schedules both in speed of convergence and quality of solution.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {832–838},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986878,
author = {Gupta, Ajay and Maass, Wolfgang},
title = {A Method for the Efficient Design of Boltzmann Machines for Classification Problems},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce a method for the efficient design of a Boltzmann machine (or a Hopfield net) that computes an arbitrary given Boolean function f. This method is based on an efficient simulation of acyclic circuits with threshold gates by Boltzmann machines. As a consequence we can show that various concrete Boolean functions f that are relevant for classification problems can be computed by scalable Boltzmann machines that are guaranteed to converge to their global maximum configuration with high probability after constantly many steps.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {825–831},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986877,
author = {Smyth, Padhraic},
title = {On Stochastic Complexity and Admissible Models for Neural Network Classifiers},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Given some training data how should we choose a particular network classifier from a family of networks of different complexities? In this paper we discuss how the application of stochastic complexity theory to classifier design problems can provide some insights into this problem. In particular we introduce the notion of admissible models whereby the complexity of models under consideration is affected by (among other factors) the class entropy, the amount of training data, and our prior belief. In particular we discuss the implications of these results with respect to neural architectures and demonstrate the approach on real data from a medical diagnosis task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {818–824},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986876,
author = {Fine, Terrence L.},
title = {Designing Linear Threshold Based Neural Network Pattern Classifiers},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The three problems that concern us are identifying a natural domain of pattern classification applications of feed forward neural networks, selecting an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reliability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from Vapnik-Chervonenkis theory, that can narrow, but not close, the chasm between theory and practice.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {811–817},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986875,
author = {Keesing, Ron and Stork, David G.},
title = {Evolution and Learning in Neural Networks: The Number and Distribution of Learning Trials Affect the Rate of Evolution},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Learning can increase the rate of evolution of a population of biological organisms (the Baldwin effect). Our simulations show that in a population of artificial neural networks solving a pattern recognition problem, no learning or too much learning leads to slow evolution of the genes whereas an intermediate amount is optimal. Moreover, for a given total number of training presentations, fastest evoution occurs if different individuals within each generation receive different numbers of presentations, rather than equal numbers. Because genetic algorithms (GAs) help avoid local minima in energy functions, our hybrid learning-GA systems can be applied successfully to complex, high-dimensional pattern recognition problems.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {804–810},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986874,
author = {Chang, Eric I. and Lippmann, Richard P.},
title = {Using Genetic Algorithms to Improve Pattern Classification Performance},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classification tasks. For a complex speech recognition task, genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five (from 153 to 33 features). On a difficult artificial machine-vision task, genetic algorithms were able to create new features (polynomial functions of the original features) which reduced classification error rates from 19% to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were unable to provide such low error rates using only the original features. Genetic algorithms were also used to reduce the number of reference exemplar patterns for a KNN classifier. On a 338 training pattern vowel-recognition problem with 10 classes, genetic algorithms reduced the number of stored exemplars from 338 to 43 without significantly increasing classification error rate. In all applications, genetic algorithms were easy to apply and found good solutions in many fewer trials than would be required by exhaustive search. Run times were long, but not unreasonable. These results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {797–803},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986873,
author = {Mozer, Michael C. and Soukup, Todd},
title = {Connectionist Music Composition Based on Melodic and Stylistic Constraints},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a recurrent connectionist network, called CONCERT, that uses a set of melodies written in a given style to compose new melodies in that style. CONCERT is an extension of a traditional algorithmic composition technique in which transition tables specify the probability of the next note as a function of previous context. A central ingredient of CONCERT is the use of a psychologically-grounded representation of pitch.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {789–796},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986872,
author = {Bottou, L\'{e}on and Gallinari, Patrick},
title = {A Framework for the Cooperation of Learning Algorithms},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce a framework for training architectures composed of several modules. This framework, which uses a statistical formulation of learning systems, provides a unique formalism for describing many classical connectionist algorithms as well as complex systems where several algorithms interact. It allows to design hybrid systems which combine the advantages of connectionist algorithms as well as other learning algorithms.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {781–788},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986871,
author = {Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Evaluation of Adaptive Mixtures of Competing Experts},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {774–780},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986870,
author = {Jacobs, Robert A. and Jordan, Michael I.},
title = {A Competitive Modular Connectionist Architecture},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a multi-network, or modular, connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes. The main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions. A task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns. As a result of the competition, different networks learn different training patterns and, thus, learn to partition the input space. The performance of the architecture on a "what" and "where" vision task and on a multi-payload robotics task are presented.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {767–773},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986869,
author = {Mel, Bartlett W. and Omohundro, Stephen M.},
title = {How Receptive Field Parameters Affect Neural Learning},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We identify the three principle factors affecting the performance of learning by networks with localized units: unit noise, sample density, and the structure of the target function. We then analyze the effect of unit receptive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {757–763},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986868,
author = {Girosi, Federico and Poggio, Tomaso and Caprile, Bruno},
title = {Extensions of a Theory of Networks for Approximation and Learning: Outliers and Negative Examples},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Learning an input-output mapping from a set of examples can be regarded as synthesizing an approximation of a multi-dimensional function. From this point of view, this form of learning is closely related to regularization theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b) the equivalence between reglilarization and a class of three-layer networks that we call regularization networks. In this note, we extend the theory by introducing ways of dealing with two aspects of learning: learning in presence of unreliable examples or outliers, and learning from positive and negative examples.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {750–756},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986867,
author = {Pati, Y. C. and Krishnaprasad, P. S.},
title = {Discrete Affine Wavelet Transforms for Analysis and Synthesis of Feedforward Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper we show that discrete affine wavelet transforms can provide a tool for the analysis and synthesis of standard feedforward neural networks. It is shown that wavelet frames for L2(IR) can be constructed based upon sigmoids. The spatia-spectral localization property of wavelets can be exploited in defining the topology and determining the weights of a feedforward network. Training a network constructed using the synthesis procedure described here involves minimization of a convex cost functional and therefore avoids pitfalls inherent in standard backpropagation algorithms. Extension of these methods to L2(IRN) is also discussed.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {743–749},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986866,
author = {Baldi, Pierre},
title = {Computing with Arrays of Bell-Shaped and Sigmoid Functions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We consider feed-forward neural networks with one non-linear hidden layer and linear output units. The transfer function in the hidden layer are either bell-shaped or sigmoid. In the bell-shaped case, we show how Bernstein polynomials on one hand and the theory of the heat equation on the other are relevant for understanding the properties of the corresponding networks. In particular, these techniques yield simple proofs of universal approximation properties, i.e. of the fact that any reasonable function can be approximated to any degree of precision by a linear combination of bell-shaped functions. In addition, in this framework the problem of learning is equivalent to the problem of reversing the time course of a diffusion process. The results obtained in the bell-shaped case can then be applied to the case of sigmoid transfer functions in the hidden layer, yielding similar universality results. A conjecture related to the problem of generalization is briefly examined.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {735–742},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986865,
author = {Saha, Avijit and Christian, Jim and Tang, D. S. and Wu, Chuan-Lin},
title = {Oriented Non-Radial Basis Functions for Image Coding and Analysis},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce oriented non-radial basis function networks (ONRBF) as a generalization of Radial Basis Function networks (RBF)- wherein the Euclidean distance metric in the exponent of the Gaussian is replaced by a more general polynomial. This permits the definition of more general regions and in particular- hyper-ellipses with orientations. In the case of hyper-surface estimation this scheme requires a smaller number of hidden units and alleviates the "curse of dimensionality" associated kernel type approximators. In the case of an image, the hidden units correspond to features in the image and the parameters associated with each unit correspond to the rotation, scaling and translation properties of that particular "feature". In the context of the ONBF scheme, this means that an image can be represented by a small number of features. Since, transformation of an image by rotation, scaling and translation correspond to identical transformations of the individual features, the ONBF scheme can be used to considerable advantage for the purposes of image recognition and analysis.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {728–734},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986864,
author = {Kadirkamanathan, V. and Niranjan, M. and Fallside, F.},
title = {Sequential Adaptation of Radial Basis Function Neural Networks and Its Application to Time-Series Prediction},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We develop a sequential adaptation algorithm for radial basis function (RBF) neural networks of Gaussian nodes, based on the method of successive F-Projections. This method makes use of each observation efficiently in that the network mapping function so obtained is consistent with that information and is also optimal in the least L2-norm sense. The RBF network with the F-Projections adaptation algorithm was used for predicting a chaotic time-series. We compare its performance to an adaptation scheme based on the method of stochastic approximation, and show that the F-Projections algorithm converges to the underlying model much faster.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {721–727},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986863,
author = {Platt, John C.},
title = {Learning by Combining Memorization and Gradient Descent},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have created a radial basis function network that allocates a new computational unit whenever an unusual pattern is presented to the network. The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated which memorizes the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent. For predicting the Mackey Glass chaotic time series, our network learns much faster than do those using back-propagation and uses a comparable number of synapses.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {714–720},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986862,
author = {Botros, Sherif M. and Atkeson, Christopher G.},
title = {Generalization Properties of Radial Basis Functions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We examine the ability of radial basis functions (RBFs) to generalize. We compare the performance of several types of RBFs. We use the inverse dynamics of an idealized two-joint arm as a test case. We find that without a proper choice of a norm for the inputs, RBFs have poor generalization properties. A simple global scaling of the input variables greatly improves performance. We suggest some efficient methods to approximate this distance metric.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {707–713},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986861,
author = {Sanger, Terence D.},
title = {Basis-Function Trees as a Generalization of Local Variable Selection Methods for Function Approximation},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Local variable selection has proven to be a powerful technique for approximating functions in high-dimensional spaces. It is used in several statistical methods, including CART, ID3, C4, MARS, and others (see the bibliography for references to these algorithms). In this paper I present a tree-structured network which is a generalization of these techniques. The network provides a framework for understanding the behavior of such algorithms and for modifying them to suit particular applications.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {700–706},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986860,
author = {Omohundro, Stephen M.},
title = {Bumptrees for Efficient Function, Constraint and Classification Learning},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A new class of data structures called "bumptrees" is described. These structures are useful for efficiently implementing a number of neural network related operations. An empirical comparison with radial basis functions is presented on a robot arm mapping learning task. Applications to density estimation, classification, and constraint representation and learning are also outlined.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {693–699},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986859,
author = {Lane, Stephen H. and Flax, Marshall G. and Handelman, David A. and Gelfand, Jack J.},
title = {Multi-Layer Perceptrons with B-Spline Receptive Field Functions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Multi-layer perceptrons are often slow to learn nonlinear functions with complex local structure due to the global nature of their function approximations. It is shown that standard multi-layer perceptrons are actually a special case of a more general network formulation that incorporates B-splines into the node computations. This allows novel spline network architectures to be developed that can combine the generalization capabilities and scaling properties of global multi-layer feedforward networks with the computational efficiency and learning speed of local computational paradigms. Simulation results are presented for the well known spiral problem of Weiland and of Lang and Witbrock to show the effectiveness of the Spline Net approach.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {684–692},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986858,
author = {Friedman, Jerome H.},
title = {Adaptive Spline Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A network based on splines is described. It automatically adapts the number of units, unit parameters, and the architecture of the network for each application.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {675–683},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986857,
author = {Shepard, Roger N. and Kannappan, Sheila},
title = {Connectionist Implementation of a Theory of Generalization},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Empirically, generalization between a training and a test stimulus falls off in close approximation to an exponential decay function of distance between the two stimuli in the "stimulus space" obtained by multidimensional scaling. Mathematically, this result is derivable from the assumption that an individual takes the training stimulus to belong to a "consequential" region that includes that stimulus but is otherwise of unknown location, size, and shape in the stimulus space (Shepard, 1987). As the individual gains additional information about the consequential region--by finding other stimuli to be consequential or not--the theory predicts the shape of the generalization function to change toward the function relating actual probability of the consequence to location in the stimulus space. This paper describes a natural connectionist implementation of the theory, and illustrates how implications of the theory for generalization, discrimination, and classification learning can be explored by connectionist simulation.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {665–671},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986856,
author = {Hanson, Stephen Jos\'{e} and Gluck, Mark A.},
title = {Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Spherical Units can be used to construct dynamic reconfigurable consequential regions, the geometric bases for Shepard's (1987) theory of stimulus generalization in animals and humans. We derive from Shepard's (1987) generalization theory a particular multi-layer network with dynamic (centers and radii) spherical regions which possesses a specific mass function (Cauchy). This learning model generalizes the configural-cue network model (Gluck &amp; Bower 1988): (1) configural cues can be learned and do not require pre-wiring the power-set of cues, (2) Consequential regions are continuous rather than discrete and (3) Competition amoungst receptive fields is shown to be increased by the global extent of a particular mass function (Cauchy). We compare other common mass functions (Gaussian; used in models of Moody &amp; Darken; 1989, Krushke, 1990) or just standard backpropogation networks with hyperplane/logistic hidden units showing that neither fare as well as models of human generalization and learning.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {656–664},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986855,
author = {Kruschke, John K.},
title = {ALCOVE: A Connectionist Model of Human Category Learning},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {ALCOVE is a connectionist model of human category learning that fits a broad spectrum of human learning data. Its architecture is based on well-established psychological theory, and is related to networks using radial basis functions. From the perspective of cognitive psychology, ALCOVE can be construed as a combination of exemplar-based representation and error-driven learning. From the perspective of connectionism, it can be seen as incorporating constraints into back-propagation networks appropriate for modelling human learning.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {649–655},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986854,
author = {Ruppin, Eytan and Yeshurun, Yechezkel},
title = {An Attractor Neural Network Model of Recall and Recognition},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This work presents an Attractor Neural Network (ANN) model of Recall and Recognition. It is shown that an ANN model can qualitatively account for a wide range of experimental psychological data pertaining to the these two main aspects of memory access. Certain psychological phenomena are accounted for, including the effects of list-length, word-frequency, presentation time, context shift, and aging. Thereafter, the probabilities of successful Recall and Recognition are estimated, in order to possibly enable further quantitative examination of the model.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {642–648},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986853,
author = {Wiles, Janet and Humphreys, Michael S. and Bain, John D. and Dennis, Simon},
title = {Direct Memory Access Using Two Cues: Finding the Intersection of Sets in a Connectionist Model},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {For lack of alternative models, search and decision processes have provided the dominant paradigm for human memory access using two or more cues, despite evidence against search as an access process (Humphreys, Wiles &amp; Bain, 1990). We present an alternative process to search, based on calculating the intersection of sets of targets activated by two or more cues. Two methods of computing the intersection are presented, one using information about the possible targets, the other constraining the cue-target strengths in the memory matrix. Analysis using orthogonal vectors to represent the cues and targets demonstrates the competence of both processes, and simulations using sparse distributed representations demonstrate the performance of the latter process for tasks involving 2 and 3 cues.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {635–641},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986852,
author = {Mozer, Michael C.},
title = {Discovering Discrete Distributed Representations with Iterative Competitive Learning},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Competitive learning is an unsupervised algorithm that classifies input patterns into mutually exclusive clusters. In a neural net framework, each cluster is represented by a processing unit that competes with others in a winner-take-all pool for an input pattern. I present a simple extension to the algorithm that allows it to construct discrete, distributed representations. Discrete representations are useful because they are relatively easy to analyze and their information content can readily be measured. Distributed representations are useful because they explicitly encode similarity. The basic idea is to apply competitive learning iteratively to an input pattern, and after each stage to subtract from the input pattern the component that was captured in the representation at that stage. This component is simply the weight vector of the winning unit of the competitive pool. The subtraction procedure forces competitive pools at different stages to encode different aspects of the input. The algorithm is essentially the same as a traditional data compression technique known as multistep vector quantization, although the neural net perspective suggests potentially powerful extensions to that approach.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {627–634},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986851,
author = {Pollack, Jordan B.},
title = {Language Induction by Phase Transition in Dynamical Recognizers},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A higher order recurrent neural network architecture learns to recognize and generate languages after being "trained" on categorized exemplars. Studying these networks from the perspective of dynamical systems yields two interesting discoveries: First, a longitudinal examination of the learning process illustrates a new form of mechanical inference: Induction by phase transition. A small weight adjustment causes a "bifurcation" in the limit behavior of the network. This phase transition corresponds to the onset of the network's capacity for generalizing to arbitrary-length strings. Second, a study of the automata resulting from the acquisition of previously published languages indicates that while the architecture is NOT guaranteed to find a minimal finite automata consistent with the given exemplars, which is an NP-Hard problem, the architecture does appear capable of generating nonregular languages by exploiting fractal and chaotic dynamics. I end the paper with a hypothesis relating linguistic generative capacity to the behavioral regimes of non-linear dynamical systems.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {619–626},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986850,
author = {Touretzky, David S. and Wheeler, Deirdre W.},
title = {Exploiting Syllable Structure in a Connectionist Phonology Model},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In a previous paper (Touretzky &amp; Wheeler, 1990a) we showed how adding a clustering operation to a connectionist phonology model produced a parallel processing account of certain "iterative" phenomena. In this paper we show how the addition of a second structuring primitive, syllabification, greatly increases the power of the model. We present examples from a non-Indo-European language that appear to require rule ordering to at least a depth of four. By adding syllabification circuitry to structure the model's perception of the input string, we are able to handle these examples with only two derivational steps. We conclude that in phonology, derivation can be largely replaced by structuring.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {612–618},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986849,
author = {Gasser, Michael and Lee, Chan-Do},
title = {A Short-Term Memory Architecture for the Learning of Morphophonemic Rules},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Despite its successes, Rumelhart and McClelland's (1986) well-known approach to the learning of morphophonemic rules suffers from two deficiencies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and "meanings" as inputs, the model learns to generate verbs in one or another "tense", given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {605–611},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986848,
author = {Munro, Paul W. and Tabasko, Mary},
title = {Translating Locative Prepositions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A network was trained by back propagation to map locative expressions of the form "noun-preposition-noun" to a semantic representation, as in Cosic and Munro (1988). The network's performance was analyzed over several simulations with training sets in both English and German. Translation of prepositions was attempted by presenting a locative expression to a network trained in one language to generate a semantic representation; the semantic representation was then presented to the network trained in the other language to generate the appropriate preposition.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {598–604},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986847,
author = {Legendre, G\'{e}raldine and Miyata, Yoshiro and Smolensky, Paul},
title = {Distributed Recursive Structure Processing},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Harmonic grammar (Legendre, et al., 1990) is a connectionist theory of linguistic well-formedness based on the assumption that the well-formedness of a sentence can be measured by the harmony (negative energy) of the corresponding connectionist state. Assuming a lower-level connectionist network that obeys a few general connectionist principles but is otherwise unspecified, we construct a higher-level network with an equivalent harmony function that captures the most linguistically relevant global aspects of the lower level network. In this paper, we extend the tensor product representation (Smolensky 1990) to fully recursive representations of recursively structured objects like sentences in the lower-level network. We show theoretically and with an example the power of the new technique for parallel distributed structure processing.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {591–597},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986846,
author = {Erlanson, Ruth and Abu-Mostafa, Yaser},
title = {Analog Neural Networks as Decoders},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Analog neural networks with feedback can be used to implement K- (Winner-Take-All (KWTA) networks. In turn, KWTA networks can be used as decoders of a class of nonlinear error-correcting codes. By interconnecting such KWTA networks, we can construct decoders capable of decoding more powerful codes. We consider several families of interconnected KWTA networks, analyze their performance in terms of coding theory metrics, and consider the feasibility of embedding such networks in VLSI technologies.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {585–588},
numpages = {4},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986845,
author = {Hayashi, Yoichi},
title = {A Neural Expert System with Automated Extraction of Fuzzy If-Then Rules and Its Application to Medical Diagnosis},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper proposes a fuzzy neural expert system (FNES) with the following two functions: (1) Generalization of the information derived from the training data and embodiment of knowledge in the form of the fuzzy neural network; (2) Extraction of fuzzy If-Then rules with linguistic relative importance of each proposition in an antecedent (If-part) from a trained neural network. This paper also gives a method to extract automatically fuzzy If-Then rules from the trained neural network. To prove the effectiveness and validity of the proposed fuzzy neural expert system, a fuzzy neural expert system for medical diagnosis has been developed.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {578–584},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986844,
author = {Golomb, B. A. and Lawrence, D. T. and Sejnowski, T. J.},
title = {SEXNET: A Neural Network Identifies Sex from Human Faces},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30\texttimes{}30 were compressed using a 900\texttimes{}40\texttimes{}900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation "SexNet" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {572},
numpages = {1},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986843,
author = {Cottrell, Garrison W. and Metcalfe, Janet},
title = {EMPATH: Face, Emotion, and Gender Recognition Using Holons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The dimensionality of a set of 160 face images of 10 male and 10 female subjects is reduced from 4096 to 40 via an autoencoder network. The extracted features do not correspond to the features used in previous face recognition systems (Kanade, 1973), such as ratios of distances between facial elements. Rather, they are whole-face features we call holons. The holons are given to 1 and 2 layer back propagation networks that are trained to classify the input features for identity, feigned emotional state and gender. The automatically extracted holons provide a sufficient basis for all of the gender discriminations, 99% of the identity discriminations and several of the emotion discriminations among the training set. Network and human judgements of the emotions are compared, and it is found that the networks tend to confuse more distant emotions than humans do.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {564–571},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986842,
author = {Keeler, James D. and Rumelhart, David E. and Leow, Wee-Kheng},
title = {Integrated Segmentation and Recognition of Hand-Printed Numerals},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Neural network algorithms have proven useful for recognition of individual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Conventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recognized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping handprinted numerals.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {557–563},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986841,
author = {Collard, Joseph E.},
title = {A B-P ANN Commodity Trader},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An Artificial Neural Network (ANN) is trained to recognize a buy/sell (long/short) pattern for a particular commodity future contract. The Back-Propagation of errors algorithm was used to encode the relationship between the Long/Short desired output and 18 fundamental variables plus 6 (or 18) technical variables into the ANN. Trained on one year of past data the ANN is able to predict long/short market positions for 9 months in the future that would have made $10,301 profit on an investment of less than $1000.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {551–556},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986840,
author = {Perry, John L. and Baumgardt, Douglas R.},
title = {Lg Depth Estimation and Ripple Fire Characterization Using Artificial Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This srudy has demonstrated how artificial neural networks (ANNs) can be used to characterize seismic sources using high-frequency regional seismic data. We have taken the novel approach of using ANNs as a research tool for obtaining seismic source information, specifically depth of focus for earthquakes and ripple-fire characteristics for economic blasts, rather than as just a feature classifier between earthquake and explosion populations. Overall, we have found that ANNs have potential applications to seismic event characterization and identification, beyond just as a feature classifier. In future studies, these techniques should be applied to actual data of regional seismic events recorded at the new regional seismic arrays. The results of this study indicates that an ANN should be evaluated as part of an operational seismic event identification system.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {544–550},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986839,
author = {Marko, Kenneth A.},
title = {Neural Network Application to Diagnostics and Control of Vehicle Control Systems},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Diagnosis of faults in complex, real-time control systems is a complicated task that has resisted solution by traditional methods. We have shown that neural networks can be successfully employed to diagnose faults in digitally controlled powertrain systems. This paper discusses the means we use to develop the appropriate databases for training and testing in order to select the optimum network architectures and to provide reasonable estimates of the classification accuracy of these networks on new samples of data. Recent work applying neural nets to adaptive control of an active suspension system is presented.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {537–543},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986838,
author = {Noordewier, Michiel O. and Towell, Geoffrey G. and Shavlik, Jude W.},
title = {Training Knowledge-Based Neural Networks to Recognize Genes in DNA Sequences},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences. The symbolic portion of the KBANN system utilizes inference rules that provide a roughly-correct method for recognizing a class of DNA sequences known as eukaryotic splice-junctions. We then map this "domain theory" into a neural network and provide training examples. Using the samples, the neural network's learning algorithm adjusts the domain theory so that it properly classifies these DNA sequences. Our procedure constitutes a general method for incorporating preexisting knowledge into artificial neural networks. We present an experiment in molecular genetics that demonstrates the value of doing so.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {530–536},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986837,
author = {Fredholm, Henrik and Bohr, Henrik and Bohr, Jakob and Brunak, S\o{}ren and Cotteril, Rodney M. J. and Lautrup, Benny and Petersen, Steffen B.},
title = {A Novel Approach to Prediction of the 3-Dimensional Structures of Protein Backbones by Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Three-dimensional (3D) structures of protein backbones have been predicted using neural networks. A feed forward neural network was trained on a class of functionally, but not structurally, homologous proteins, using backpropagation learning. The network generated tertiary structure information in the form of binary distance constraints for the Cα atoms in the protein backbone. The binary distance between two Cα atoms was 0 if the distance between them was less than a certain threshold distance, and 1 otherwise. The distance constraints predicted by the trained neural network were utilized to generate a folded conformation of the protein backbone, using a steepest descent minimization approach.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {523–529},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986836,
author = {Faller, William E. and Luttges, Marvin W.},
title = {Flight Control in the Dragonfly: A Neurobiological Simulation},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Neural network simulations of the dragonfly flight neurocontrol system have been developed to understand how this insect uses complex, unsteady aerodynamics. The simulation networks account for the ganglionic spatial distribution of cells as well as the physiologic operating range and the stochastic cellular firing history of each neuron. In addition the motor neuron firing patterns, "flight command sequences", were utilized. Simulation training was targeted against both the cellular and flight motor neuron firing patterns. The trained networks accurately resynthesized the intraganglionic cellular firing patterns. These in turn controlled the motor neuron firing patterns that drive wing musculature during flight. Such networks provide both neurobiological analysis tools and first generation controls for the use of "unsteady" aerodynamics.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {514–520},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986835,
author = {Beer, R. D. and Kacmarcik, G. J. and Ritzmann, R. E. and Chie, H. J.},
title = {A Model of Distributed Sensorimotor Control in the Cockroach Escape Turn},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In response to a puff of wind, the American cockroach turns away and runs. The circuit underlying the initial turn of this escape response consists of three populations of individually identifiable nerve cells and appears to employ distributed representations in its operation. We have reconstructed several neuronal and behavioral properties of this system using simplified neural network models and the backpropagation learning algorithm constrained by known structural characteristics of the circuitry. In order to test and refine the model, we have also compared the model's responses to various lesions with the insect's responses to similar lesions.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {507–513},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986834,
author = {Schmidhuber, J\"{u}rgen},
title = {Reinforcement Learning in Markovian and Non-Markovian Environments},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This work addresses three problems with reinforcement learning and adaptive neuro-control: 1. Non-Markovian interfaces between learner and environment. 2. On-line learning based on system realization. 3. Vector-valued adaptive critics. An algorithm is described which is based on system realization and on two interacting fully recurrent continually running networks which may learn in parallel. Problems with parallel learning are attacked by 'adaptive randomness'. It is also described how interacting model/controller systems can be combined with vector-valued 'adaptive critics' (previous critics have been scalar).},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {500–506},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986833,
author = {Milito, Rodolfo A. and Guyon, Isabelle and Solla, Sara A.},
title = {Neural Network Implementation of Admission Control},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A feedforward layered network implements a mapping required to control an unknown stochastic nonlinear dynamical system. Training is based on a novel approach that combines stochastic approximation ideas with backpropagation. The method is applied to control admission into a queueing system operating in a time-varying environment.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {493–499},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986832,
author = {Rosen, Bruce E. and Goodwin, James M. and Vidal, Jacques J.},
title = {Adaptive Range Coding},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper examines a class of neuron based learning systems for dynamic control that rely on adaptive range coding of sensor inputs. Sensors are assumed to provide binary coded range vectors that coarsely describe the system state. These vectors are input to neuron-like processing elements. Output decisions generated by these "neurons" in turn affect the system state, subsequently producing new inputs. Reinforcement signals from the environment are received at various intervals and evaluated. The neural weights as well as the range boundaries determining the output decisions are then altered with the goal of maximizing future reinforcement from the environment. Preliminary experiments show the promise of adapting "neural receptive fields" when learning dynamical control. The observed performance with this method exceeds that of earlier approaches.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {486–492},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986831,
author = {Guha, Aloke},
title = {A Reinforcement Learning Variant for Control Scheduling},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present an algorithm based on reinforcement and state recurrence learning techniques to solve control scheduling problems. In particular, we have devised a simple learning scheme called "handicapped learning", in which the weights of the associative search element are reinforced, either positively or negatively, such that the system is forced to move towards the desired setpoint in the shortest possible trajectory. To improve the learning rate, a variable reinforcement scheme is employed: negative reinforcement values are varied depending on whether the failure occurs in handicapped or normal mode of operation. Furthermore, to realize a simulated annealing scheme for accelerated learning, if the system visits the same failed state successively, the negative reinforcement value is increased. In examples studied, these learning schemes have demonstrated high learning rates, and therefore may prove useful for in-situ learning.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {479–485},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986830,
author = {Sutton, Richard S.},
title = {Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This is a summary of results with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned forward model of the world. We describe and show results for two Dyna architectures, Dyna-AHC and Dyna-Q. Using a navigation task, results are shown for a simple Dyna-AHC system which simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. We show that Dyna-Q architectures (based on Watkins's Q-learning) are easy to adapt for use in changing environments.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {471–478},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986829,
author = {Dayan, Peter},
title = {Navigating through Temporal Difference},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Barto, Sutton and Watkins [2] introduced a grid task as a didactic example of temporal difference planning and asynchronous dynamical programming. This paper considers the effects of changing the coding of the input stimulus, and demonstrates that the self-supervised learning of a particular form of hidden unit representation improves performance.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {464–470},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986828,
author = {Bachrach, Jonathan R.},
title = {A Connectionist Learning Control Architecture for Navigation},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A novel learning control architecture is used for navigation. A sophisticated test-bed is used to simulate a cylindrical robot with a sonar belt in a planar environment. The task is short-range homing in the presence of obstacles. The robot receives no global information and assumes no comprehensive world model. Instead the robot receives only sensory information which is inherently limited. A connectionist architecture is presented which incorporates a large amount of a priori knowledge in the form of hard-wired networks, architectural constraints, and initial weights. Instead of hard-wiring static potential fields from object models, my architecture learns sensor-based potential fields, automatically adjusting them to avoid local minima and to produce efficient homing trajectories. It does this without object models using only sensory information. This research demonstrates the use of a large modular architecture on a difficult task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {457–463},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986827,
author = {Thrun, Sebastian B. and M\"{o}ller, Knut and Linden, Alexander},
title = {Planning with an Adaptive World Model},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a new connectionist planning method [TML90]. By interaction with an unknown environment, a world model is progressively constructed using gradient descent. For deriving optimal actions with respect to future reinforcement, planning is applied in two steps: an experience network proposes a plan which is subsequently optimized by gradient descent with a chain of world models, so that an optimal reinforcement may be obtained when it is actually run. The appropriateness of this method is demonstrated by a robotics application and a pole balancing task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {450–456},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986826,
author = {Frye, Robert C. and Cummings, Kevin D. and Rietman, Edward A.},
title = {Proximity Effect Corrections in Electron Beam Lithography Using a Neural Network},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have used a neural network to compute corrections for images written by electron beams to eliminate the proximity effects caused by electron scattering. Iterative methods are effective, but require prohibitively computation time. We have instead trained a neural network to perform equivalent corrections, resulting in a significant speed-up. We have examined hardware implementations using both analog and digital electronic networks. Both had an acceptably small error of 0.5% compared to the iterative results. Additionally, we verified that the neural network correctly generalized the solution of the problem to include patterns not contained in its training set. We have experimentally verified this approach on a Cambridge Instruments EBMF 10.5 exposure system.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {443–449},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986825,
author = {Katayama, Masazumi and Kawato, Mitsuo},
title = {Learning Trajectory and Force Control of an Artificial Muscle Arm by Parallel-Hierarchical Neural Network Model},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We propose a new parallel-hierarchical neural network model to enable motor learning for simultaneous control of both trajectory and force, by integrating Hogan's control method and our previous neural network control model using a feedback-error-learning scheme. Furthermore, two hierarchical control laws which apply to the model, are derived by using the Moore-Penrose pseudo-inverse matrix. One is related to the minimum muscle-tension-change trajectory and the other is related to the minimum motor-command-change trajectory. The human arm is redundant at the dynamics level since joint torque is generated by agonist and antagonist muscles. Therefore, acquisition of the inverse model is an ill-posed problem. However, the combination of these control laws and feedback-error-learning resolve the ill-posed problem. Finally, the efficiency of the parallel-hierarchical neural network model is shown by learning experiments using an artificial muscle arm and computer simulations.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {436–442},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986824,
author = {Pomerleau, Dean A.},
title = {Rapidly Adapting Artificial Neural Networks for Autonomous Navigation},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a back-propagation network that uses inputs from a video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified Chevy van. This paper describes training techniques which allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching a human driver's response to new situations. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, multilane lined and unlined roads, and obstacle-ridden on- and off-road environments, at speeds of up to 20 miles per hour.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {429–435},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986823,
author = {Tarassenko, Lionel and Brownlow, Michael and Marshall, Gillian and Tombs, Jon and Murray, Alan},
title = {Real-Time Autonomous Robot Navigation Using VLSI Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a real time robot navigation system based on three VLSI neural network modules. These are a resistive grid for path planning, a nearest-neighbour classifier for localization using range data from a time-of-flight infra-red sensor and a sensory-motor associative network for dynamic obstacle avoidance.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {422–428},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986822,
author = {Schley, Charles and Chauvin, Yves and Henkle, Van and Golden, Richard},
title = {Neural Networks Structured for Control Application to Aircraft Landing},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a generic neural network architecture capable of controlling non-linear plants. The network is composed of dynamic, parallel, linear maps gated by non-linear switches. Using a recurrent form of the back-propagation algorithm, control is achieved by optimizing the control gains and task-adapted switch parameters. A mean quadratic cost function computed across a nominal plant trajectory is minimized along with performance constraint penalties. The approach is demonstrated for a control task consisting of landing a commercial aircraft in difficult wind conditions. We show that the network yields excellent performance while remaining within acceptable damping response constraints.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {415–421},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986821,
author = {Horiuchi, Tim and Lazzaro, John and Moore, Andrew and Koch, Christof},
title = {A Delay-Line Based Motion Detection Chip},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl, we have designed a chip that employs a correlation model to report the one-dimensional field motion of a scene in real time. Using subthreshold analog VLSI techniques, we have fabricated and successfully tested a 8000 transistor chip using a standard MOSIS process.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {406–412},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986820,
author = {Bair, Wyeth and Koch, Christof},
title = {An Analog VLSI Chip for Finding Edges from Zero-Crossings},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have designed and tested a one-dimensional 64 pixel, analog CMOS VLSI chip which localizes intensity edges in real-time. This device exploits on-chip photoreceptors and the natural filtering properties of resistive networks to implement a scheme similar to and motivated by the Difference of Gaussians (DOG) operator proposed by Marr and Hildreth (1980). Our chip computes the zero-crossings associated with the difference of two exponential weighting functions. If the derivative across this zero-crossing is above a threshold, an edge is reported. Simulations indicate that this technique will extend well to two dimensions.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {399–405},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986819,
author = {Skrzypek, Josef},
title = {Feedback Synapse to Cone and Light Adaptation},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Light adaptation (LA) allows cone vision to remain functional between twilight and the brightest time of day even though, at anyone time, their intensity-response (I-R) characteristic is limited to 3 log units of the stimulating light. One mechanism underlying LA, was localized in the outer segment of an isolated cone (1,2). We found that by adding annular illumination, an I-R characteristic of a cone can be shifted along the intensity domain. Neural network involving feedback synapse from horizontal cells to cones is involved to be in register with ambient light level of the periphery. An equivalent electrical circuit with three different transmembrane channels leakage, photocurrent and feedback was used to model static behavior of a cone. SPICE simulation showed that interactions between feedback synapse and the light sensitive conductance in the outer segment can shift the IR curves along the intensity domain, provided that phototransduction mechanism is not saturated during maximally hyperpolarized light response.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {391–398},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986818,
author = {Teeters, Jeffrey L. and Eeckman, Frank H. and Werblin, Frank S.},
title = {A Four Neuron Circuit Accounts for Change Sensitive Inhibition in Salamander Retina},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In salamander retina, the response of On-Off ganglion cells to a central flash is reduced by movement in the receptive field surround. Through computer simulation of a 2-D model which takes into account their anatomical and physiological properties, we show that interactions between four neuron types (two bipolar and two amacrine) may be responsible for the generation and lateral conductance of this change sensitive inhibition. The model shows that the four neuron circuit can account for previously observed movement sensitive reductions in ganglion cell sensitivity and allows visualization and prediction of the spatio-temporal pattern of activity in change sensitive retinal cells.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {384–390},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986817,
author = {Rieke, Fred and Owen, W. Geoffrey and Bialek, William},
title = {Optimal Filtering in the Salamander Retina},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The dark-adapted visual system can count photons with a reliability limited by thermal noise in the rod photoreceptors - the processing circuitry between the rod cells and the brain is essentially noiseless and in fact may be close to optimal. Here we design an optimal signal processor which estimates the time-varying light intensity at the retina based on the rod signals. We show that the first stage of optimal signal processing involves passing the rod cell output through a linear filter with characteristics determined entirely by the rod signal and noise spectra. This filter is very general; in fact it is the first stage in any visual signal processing task at low photon flux. We identify the output of this first-stage filter with the intracellular voltage response of the bipolar cell, the first anatomical stage in retinal signal processing. From recent data on tiger salamander photoreceptors we extract the relevant spectra and make parameter-free, quantitative predictions of the bipolar cell response to a dim, diffuse flash. Agreement with experiment is essentially perfect. As far as we know this is the first successful predictive theory for neural dynamics.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {377–383},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986816,
author = {Moore, Andrew and Fox, Geoffrey and Allman, John and Goodman, Rodney},
title = {A VLSI Neural Network for Color Constancy},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A system for color correction has been designed, built, and tested successfully; the essential components are three custom chips built using subthreshold analog CMOS VLSI. The system, based on Land's Retinex theory of color constancy, produces colors similar in many respects to those produced by the visual system. Resistive grids implemented in analog VLSI perform the smoothing operation central to the algorithm at video rates. With the electronic system, the strengths and weaknesses of the algorithm are explored.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {370–376},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986815,
author = {Bialek, William and Ruderman, Daniel L. and Zee, A.},
title = {Optimal Sampling of Natural Images: A Design Principle for the Visual System?},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We formulate the problem of optimizing the sampling of natural images using an array of linear filters. Optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long-range interconnections in the array. At low signal-to-noise ratios the optimal filter characteristics correspond to bound states of a Schr\"{o}dinger equation in which the signal spectrum plays the role of the potential. The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates. The observed scale invariance of natural images plays an essential role in this construction.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {363–369},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986814,
author = {Weinshall, Daphna},
title = {Qualitative Structure from Motion},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Exact structure from motion is an ill-posed computation and therefore very sensitive to noise. In this work I describe how a qualitative shape representation, based on the sign of the Gaussian curvature, can be computed directly from motion disparities, without the computation of an exact depth map or the directions of surface normals. I show that humans can judge the curvature sense of three points undergoing 3D motion from two, three and four views with success rate significantly above chance. A simple RBF net has been trained to perform the same task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {356–362},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986813,
author = {Wang, H. Taichi and Mathur, Bimal and Koch, Christof},
title = {A Multiscale Adaptive Network Model of Motion Computation in Primates},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We demonstrate a multiscale adaptive network model of motion computation in primate area MT. The model consists of two stages: (1) local velocities are measured across multiple spatio-temporal channels, and (2) the optical flow field is computed by a network of direction-selective neurons at multiple spatial resolutions. This model embeds the computational efficiency of Multigrid algorithms within a parallel network as well as adaptively computes the most reliable estimate of the flow field across different spatial scales. Our model neurons show the same nonclassical receptive field properties as Allman's type I MT neurons. Since local velocities are measured across multiple channels, various channels often provide conflicting measurements to the network. We have incorporated a veto scheme for conflict resolution. This mechanism provides a novel explanation for the spatial frequency dependency of the psychophysical phenomenon called Motion Capture.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {349–355},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986812,
author = {Mingolla, Ennio},
title = {Neural Dynamics of Motion Segmentation and Grouping},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A neural network model of motion segmentation by visual cortex is described. The model clarifies how preprocessing of motion signals by a Motion Oriented Contrast Filter (MOC Filter) is joined to long-range cooperative motion mechanisms in a motion Cooperative Competitive Loop (CC Loop) to control phenomena such as as induced motion, motion capture, and motion aftereffects. The total model system is a motion Boundary Contour System (BCS) that is computed in parallel with a static BCS before both systems cooperate to generate a boundary representation for three dimensional visual form perception. The present investigations clarify how the static BCS can be modified for use in motion segmentation problems, notably for analyzing how ambiguous local movements (the aperture problem) on a complex moving shape are suppressed and actively reorganized into a coherent global motion signal.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {342–348},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986811,
author = {Shashua, Amnon and Ullman, Shimon},
title = {Grouping Contours by Iterated Pairing Network},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe in this paper a network that performs grouping of image contours. The input to the net are fragments of image contours, and the output is the partitioning of the fragments into groups, together with a saliency measure for each group. The grouping is based on a measure of overall length and curvature. The network decomposes the overall optimization problem into independent optimal pairing problems performed at each node. The resulting computation maps into a uniform locally connected network of simple computing elements.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {335–341},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986810,
author = {Khotanzad, Alireza and Lee, Ying-Wung},
title = {Stereopsis by a Neural Network Which Learns the Constraints},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper presents a neural network (NN) approach to the problem of stereopsis. The correspondence problem (finding the correct matches between the pixels of the epipolar lines of the stereo pair from amongst all the possible matches) is posed as a non-iterative many-to-one mapping. A two-layer feed forward NN architecture is developed to learn and code this nonlinear and complex mapping using the back-propagation learning rule and a training set. The important aspect of this technique is that none of the typical constraints such as uniqueness and continuity are explicitly imposed. All the applicable constraints are learned and internally coded by the NN enabling it to be more flexible and more accurate than the existing methods. The approach is successfully tested on several random-dot stereograms. It is shown that the net can generalize its learned mapping to cases outside its training set. Advantages over the Marr-Poggio Algorithm are discussed and it is shown that the NN performance is superior.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {327–334},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986809,
author = {Sereno, Martin I. and Sereno, Margaret E.},
title = {Learning to See Rotation and Dilation with a Hebb Rule},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a feedforward network with area V1-like input-layer units and a Hebb rule can develop area MT-like second layer units that solve the aperture problem for pattern motion. The present study extends this earlier work to more complex motions. Saito et al. (1986) showed that neurons with large receptive fields in macaque visual area MST are sensitive to different senses of rotation and dilation, irrespective of the receptive field location of the movement singularity. A network with an MT-like second layer was trained and tested on combinations of rotating, dilating, and translating patterns. Third-layer units learn to detect specific senses of rotation or dilation in a position-independent fashion, despite having position-dependent direction selectivity within their receptive fields.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {320–326},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986808,
author = {Goggin, Shelly D. D. and Johnson, Kristina M. and Gustafson, Karl E.},
title = {A Second-Order Translation, Rotation and Scale Invariant Neural Network},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A second-order architecture is presented here for translation, rotation and scale invariant processing of 2-D images mapped to n input units. This new architecture has a complexity of O(n) weights as opposed to the O(n3) weights usually required for a third-order, rotation invariant architecture. The reduction in complexity is due to the use of discrete frequency information. Simulations show favorable comparisons to other neural network architectures.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {313–319},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986807,
author = {Tresp, Volker},
title = {A Neural Network Approach for Three-Dimensional Object Recognition},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The model-based neural vision system presented here determines the position and identity of three-dimensional objects. Two stereo images of a scene are described in terms of shape primitives (line segments derived from edges in the scenes) and their relational structure. A recurrent neural matching network solves the correspondence problem by assigning corresponding line segments in right and left stereo images. A 3-D relational scene description it then generated and matched by a second neural network against models in a model base. The quality of the solutions and the convergence speed were both improved by using mean field approximations.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {306–312},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986806,
author = {Zemel, Richard S. and Hinton, Geoffrey E.},
title = {Discovering Viewpoint-Invariant Relationships That Characterize Objects},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network "sees" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {299–305},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986805,
author = {Pearson, John C. and Spence, Clay D. and Sverdlove, Ronald},
title = {Applications of Neural Networks in Video Signal Processing},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Although color TV is an established technology, there are a number of longstanding problems for which neural networks may be suited. Impulse noise is such a problem, and a modular neural network approach is presented in this paper. The training and analysis was done on conventional computers, while real-time simulations were performed on a massively parallel computer called the Princeton Engine. The network approach was compared to a conventional alternative, a median filter. Real-time simulations and quantitative analysis demonstrated the technical superiority of the neural system. Ongoing work is investigating the complexity and cost of implementing this system in hardware.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {289–295},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986804,
author = {Tam, David C.},
title = {Signal Processing by Multiplexing and Demultiplexing in Neurons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Signal processing capabilities of biological neurons are investigated. Temporally coded signals in neurons can be multiplexed to increase the transmission capacity. Multiplexing of signal is suggested in bi-threshold neurons with "high-threshold" and "low-threshold" for switching firing modes. To extract the signal embedded in the interspike-intervals of firing, the encoded signal are demultiplexed and multiplexed by a network of neurons with delayed-line circuitry for signal processing. The temporally coded input signal is transformed spatially by mapping the firing intervals topographically to the output of the network, thus decoding the specific firing interspike-intervals. The network also provides a band-pass filtering capability where the variability of the timing of the original signal can be decoded.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {282–288},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986803,
author = {Roitblat, Herbert L. and Moore, Patrick W. B. and Nachtigall, Paul E. and Penner, Ralph H.},
title = {Natural Dolphin Echo Recognition Using an Integrator Gateway Network},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have been studying the performance of a bottlenosed dolphin on a delayed matching-to-sample task to gain insight into the processes and mechanisms that the animal uses during echolocation. The dolphin recognizes targets by emitting natural sonar signals and listening to the echoes that return. This paper describes a novel neural network architecture, called an integrator gateway network, that we have developed to account for this performance. The integrator gateway network combines information from multiple echoes to classify targets with about 90% accuracy. In contrast, a standard backpropagation network performed with only about 63% accuracy.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {273–281},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986802,
author = {Choukri, Khalid},
title = {Speech Recognition Using Connectionist Approaches},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper is a summary of SPRINT project aims and results. The project focus on the use of neuro-computing techniques to tackle various problems that remain unsolved in speech recognition. First results concern the use of feedforward nets for phonetic units classification, isolated word recognition, and speaker adaptation.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {262–269},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986801,
author = {Zue, Victor and Glass, James and Goodine, David and Hirschman, Lynette and Leung, Hong and Phillips, Michael and Polifroni, Joseph and Seneff, Stephanie},
title = {From Speech Recognition to Spoken Language Understanding: The Development of the MIT SUMMIT and VOYAGER Systems},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Spoken language is one of the most natural, efficient, flexible, and economical means of communication among humans. As computers play an ever increasing role in our lives, it is important that we address the issue of providing a graceful human-machine interface through spoken language. In this paper, we will describe our recent efforts in moving beyond the scope of speech recognition into the realm of spoken-language understanding. Specifically, we report on the development of an urban navigation and exploration system called VOYAGER, an application which we have used as a basis for performing research in spoken-language understanding.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {255–261},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986800,
author = {Leung, Hong C. and Glass, James R. and Phillips, Michael S. and Zue, Victor W.},
title = {Phonetic Classification and Recognition Using the Multi-Layer Perceptron},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, we will describe several extensions to our earlier work, utilizing a segment-based approach. We will formulate our segmental framework and report our study on the use of multi-layer perceptrons for detection and classification of phonemes. We will also examine the outputs of the network, and compare the network performance with other classifiers. Our investigation is performed within a set of experiments that attempts to recognize 38 vowels and consonants in American English independent of speaker. When evaluated on the TIMIT database, our system achieves an accuracy of 56%.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {248–254},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986799,
author = {Intrator, Nathan},
title = {Exploratory Feature Extraction in Speech Signals},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed. This leads to a new statistical insight to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982).The importance of a dimensionality reduction principle based solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using back-propagation network.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {241–247},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986798,
author = {Bridle, John S. and Cox, Stephen J.},
title = {RecNorm: Simultaneous Normalisation and Classification Applied to Speech Recognition},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A particular form of neural network is described, which has terminals for acoustic patterns, class labels and speaker parameters. A method of training this network to "tune in" the speaker parameters to a particular speaker is outlined, based on a trick for converting a supervised network to an unsupervised mode. We describe experiments using this approach in isolated word recognition based on whole-word hidden Markov models. The results indicate an improvement over speaker-independent performance and, for unlabelled data, a performance close to that achieved on labelled data.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {234–240},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986797,
author = {Iso, Ken-Ichi and Watanabe, Takao},
title = {Speech Recognition Using Demi-Syllable Neural Prediction Model},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Neural Prediction Model is the speech recognition model based on pattern prediction by multilayer perceptrons. Its effectiveness was confirmed by the speaker-independent digit recognition experiments. This paper presents an improvement in the model and its application to large vocabulary speech recognition, based on subword units. The improvement involves an introduction of "backward prediction," which further improves the prediction accuracy of the original model with only "forward prediction". In application of the model to speaker-dependent large vocabulary speech recognition, the demi-syllable unit is used as a subword recognition unit. Experimental results indicated a 95.2% recognition accuracy for a 5000 word test set and the effectiveness was confirmed for the proposed model improvement and the demi-syllable subword units.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {227–233},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986796,
author = {Fanty, Mark and Cole, Ronald},
title = {Spoken Letter Recognition},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Through the use of neural network classifiers and careful feature selection, we have achieved high-accuracy speaker-independent spoken letter recognition. For isolated letters, a broad-category segmentation is performed Location of segment boundaries allows us to measure features at specific locations in the signal such as vowel onset, where important information resides. Letter classification is performed with a feed-forward neural network. Recognition accuracy on a test set of 30 speakers was 96%. Neural network classifiers are also used for pitch tracking and broad-category segmentation of letter strings. Our research has been extended to recognition of names spelled with pauses between the letters. When searching a database of 50,000 names, we achieved 95% first choice name retrieval. Work has begun on a continuous letter classifier which does frame-by-frame phonetic classification of spoken letters.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {220–226},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986795,
author = {Bourlard, Herv\'{e} and Morgan, Nelson and Wooters, Chuck},
title = {Connectionist Approaches to the Use of Markov Models for Speech Recognition},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recognition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take account of time correlation between successive observations, without any restrictive assumptions about the driving noise.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {213–219},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986794,
author = {Allen, Robert B. and Kamm, Candace A.},
title = {A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A neural network architecture was designed for locating word boundaries and identifying words from phoneme sequences. This architecture was tested in three sets of studies. First, a highly redundant corpus with a restricted vocabulary was generated and the network was trained with a limited number of phonemic variations for the words in the corpus. Tests of network performance on a transfer set yielded a very low error rate. In a second study, a network was trained to identify words from expert transcriptions of speech. On a transfer test, error rate for correct simultaneous identification of words and word boundaries was 18%. The third study used the output of a phoneme classifier as the input to the word and word boundary identification network. The error rate on a transfer test set was 49% for this task. Overall, these studies provide a first step at identifying words in connected discourse with a neural network.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {206–212},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986793,
author = {Tebelskis, Joe and Waibel, Alex and Petek, Bojan and Schmidbauer, Otto},
title = {Continuous Speech Recognition by Linked Predictive Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a large vocabulary, continuous speech recognition system based on Linked Predictive Neural Networks (LPNN's). The system uses neural networks as predictors of speech frames, yielding distortion measures which are used by the One Stage DTW algorithm to perform continuous speech recognition. The system, already deployed in a Speech to Speech Translation system, currently achieves 95%, 58%, and 39% word accuracy on tasks with perplexity 5, 111, and 402 respectively, outperforming several simple HMMs that we tested. We also found that the accuracy and speed of the LPNN can be slightly improved by the judicious use of hidden control inputs. We conclude by discussing the strengths and weaknesses of the predictive approach.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {199–205},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986792,
author = {Fahlman, Scott E.},
title = {The Recurrent Cascade-Correlation Architecture},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {190–196},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986791,
author = {Kuh, Anthony and Petsche, Thomas and Rivest, Ronald L.},
title = {Learning Time-Varying Concepts},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This work extends computational learning theory to situations in which concepts vary over time, e.g., system identification of a time-varying plant. We have extended formal definitions of concepts and learning to provide a framework in which an algorithm can track a concept as it evolves over time. Given this framework and focusing on memory-based algorithms, we have derived some PAC-style sample complexity results that determine, for example, when tracking is feasible. We have also used a similar framework and focused on incremental tracking algorithms for which we have derived some bounds on the mistake or error rates for some specific concept classes.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {183–189},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986790,
author = {Herz, Andreas V. M. and Li, Zhaoping and Van Hemmen, J. Leo},
title = {Statistical Mechanics of Temporal Association in Neural Networks with Delayed Interactions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We study the representation of static patterns and temporal associations in neural networks with a broad distribution of signal delays. For a certain class of such systems, a simple intuitive understanding of the spatio-temporal computation becomes possible with the help of a novel Lyapunov functional. It allows a quantitative study of the asymptotic network behavior through a statistical mechanical analysis. We present analytic calculations of both retrieval quality and storage capacity and compare them with simulation results.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {176–182},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986789,
author = {S\o{}rheim, Einar},
title = {ART2/BP Architecture for Adaptive Estimation of Dynamic Processes},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The goal has been to construct a supervised artificial neural network that learns incrementally an unknown mapping. As a result a network consisting of a combination of ART2 and backpropagation is proposed and is called an "ART2/BP" network. The ART2 network is used to build and focus a supervised backpropagation network. The ART2/BP network has the advantage of being able to dynamically expand itself in response to input patterns containing new information. Simulation results show that the ART2/BP network outperforms a classical maximum likelihood method for the estimation of a discrete dynamic and nonlinear transfer function.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {169–175},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986788,
author = {De Vries, Bert and Principe, Jose C.},
title = {A Theory for Neural Networks with Time Delays},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a new neural network model for processing of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {162–168},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986787,
author = {Bodenhausen, Ulrich and Waibel, Alex},
title = {The Tempo 2 Algorithm: Adjusting Time-Delays by Supervised Learning},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this work we describe a new method that adjusts time-delays and the widths of time-windows in artificial neural networks automatically. The input of the units are weighted by a gaussian input-window over time which allows the learning rules for the delays and widths to be derived in the same way as it is used for the weights. Our results on a phoneme classification task compare well with results obtained with the TDNN by Waibel et al., which was manually optimized for the same task.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {155–161},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986786,
author = {Levin, Esther},
title = {Modeling Time Varying Systems Using Hidden Control Neural Architecture},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Multi-layered neural networks have recently been proposed for nonlinear prediction and system modeling. Although proven successful for modeling time invariant nonlinear systems, the inability of neural networks to characterize temporal variability has so far been an obstacle in applying them to complicated non stationary signals, such as speech. In this paper we present a network architecture, called "Hidden Control Neural Network" (HCNN), for modeling signals generated by nonlinear dynamical systems with restricted time variability. The approach taken here is to allow the mapping that is implemented by a multi layered neural network to change with time as a function of an additional control input signal. This network is trained using an algorithm that is based on "back-propagation" and segmentation algorithms for estimating the unknown control together with the network's parameters. The HCNN approach was applied to several tasks including modeling of time-varying nonlinear systems and speaker-independent recognition of connected digits, yielding a word accuracy of 99.1%.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {147–154},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986785,
author = {Kruglyak, Leonid and Bialek, William},
title = {Analog Computation at a Critical Point: A Novel Function for Neuronal Oscillations?},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We show that a simple spin system biased at its critical point can encode spatial characteristics of external signals, such as the dimensions of "objects" in the visual field, in the temporal correlation functions of individual spins. Qualitative arguments suggest that regularly firing neurons should be described by a planar spin of unit length, and such XY models exhibit critical dynamics over a broad range of parameters. We show how to extract these spins from spike trains and then measure the interaction Hamiltonian using simulations of small dusters of cells. Static correlations among spike trains obtained from simulations of large arrays of cells are in agreement with the predictions from these Hamiltonians, and dynamic correlations display the predicted encoding of spatial information. We suggest that this novel representation of object dimensions in temporal correlations may be relevant to recent experiments on oscillatory neural firing in the visual cortex.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {137–143},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986784,
author = {Longtin, Andr\'{e}},
title = {Oscillation Onset in Neural Delayed Feedback},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper studies dynamical aspects of neural systems with delayed negative feedback modelled by nonlinear delay-differential equations. These systems undergo a Hopf bifurcation from a stable fixed point to a stable limit cycle oscillation as certain parameters are varied. It is shown that their frequency of oscillation is robust to parameter variations and noisy fluctuations, a property that makes these systems good candidates for pacemakers. The onset of oscillation is postponed by both additive and parametric noise in the sense that the state variable spends more time near the fixed point than it would in the absence of noise. This is also the case when noise affects the delayed variable, i.e. when the system has a faulty memory. Finally, it is shown that a distribution of delays (rather than a fixed delay) also stabilizes the fixed point solution.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {130–136},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986783,
author = {Niebur, Ernst and Kammen, Daniel M. and Koch, Christof and Ruderman, Daniel and Schuster, Heinz G.},
title = {Phase-Coupling in Two-Dimensional Networks of Interacting Oscillators},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Coherent oscillatory activity in large networks of biological or artificial neural units may be a useful mechanism for coding information pertaining to a single perceptual object or for detailing regularities within a data set. We consider the dynamics of a large array of simple coupled oscillators under a variety of connection schemes. Of particular interest is the rapid and robust phase-locking that results from a "sparse" scheme where each oscillator is strongly coupled to a tiny, randomly selected, subset of its neighbors.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {123–129},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986782,
author = {Toomarian, N. and Barhen, J.},
title = {Adjoint-Functions and Temporal Learning Algorithms in Neural Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The development of learning algorithms is generally based upon the minimization of an energy function. It is a fundamental requirement to compute the gradient of this energy function with respect to the various parameters of the neural architecture, e.g., synaptic weights, neural gain, etc. In principle, this requires solving a system of nonlinear equations for each parameter of the model, which is computationally very expensive. A new methodology for neural learning of time-dependent nonlinear mappings is presented. It exploits the concept of adjoint operators to enable a fast global computation of the network's response to perturbations in all the systems parameters. The importance of the time boundary conditions of the adjoint functions is discussed. An algorithm is presented in which the adjoint sensitivity equations are solved simultaneously (i.e., forward in time) along with the nonlinear dynamics of the neural networks. This methodology makes real-time applications and hardware implementation of temporal learning feasible.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {113–120},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986781,
author = {Simard, Patrice Y. and Raysz, Jean Pierre and Victorri, Bernard},
title = {Shaping the State Space Landscape in Recurrent Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Fully recurrent (asymmetrical) networks can be thought of as dynamic systems. The dynamics can be shaped to perform content addressable memories, recognize sequences, or generate trajectories. Unfortunately several problems can arise: First, the convergence in the state space is not guaranteed. Second, the learned fixed points or trajectories are not necessarily stable. Finally, there might exist spurious fixed points and/or spurious "attracting" trajectories that do not correspond to any patterns. In this paper, we introduce a new energy function that presents solutions to all of these problems. We present an efficient gradient descent algorithm which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction. The results are illustrated by the simulation of a small content addressable memory.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {105–112},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986780,
author = {Marcus, C. M. and Waugh, F. R. and Westervelt, R. M.},
title = {Connection Topology and Dynamics in Lateral Inhibition Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We show analytically how the stability of two-dimensional lateral inhibition neural networks depends on the local connection topology. For various network topologies, we calculate the critical time delay for the onset of oscillation in continuous-time networks and present analytic phase diagrams characterizing the dynamics of discrete-time networks.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {98–104},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986779,
author = {Baird, Bill and Eeckman, Frank},
title = {CAM Storage of Analog Patterns and Continuous Sequences with 3N<sup>2</sup> Weights},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A simple architecture and algorithm for analytically guaranteed associative memory storage of analog patterns, continuous sequences, and chaotic attractors in the same network is described. A matrix inversion determines network weights, given prototype patterns to be stored. There are N units of capacity in an N node network with 3N2 weights. It costs one unit per static attractor, two per Fourier component of each sequence, and four per chaotic attractor. There are no spurious attractors, and there is a Liapunov function in a special coordinate system which governs the approach of transient states to stored trajectories. Unsupervised or supervised incremental learning algorithms for pattern classification, such as competitive learning or bootstrap Widrow-Hoff can easily be implemented. The architecture can be "folded" into a recurrent network with higher order weights that can be used as a model of cortex that stores oscillatory and chaotic attractors by a Hebb rule. Hierarchical sensory-motor control networks may be constructed of interconnected "cortical patches" of these network modules. Network performance is being investigated by application to the problem of real time handwritten digit recognition.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {91–97},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986778,
author = {Gerstner, Wulfram},
title = {Associative Memory in a Network of 'biological' Neurons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Hopfield network (Hopfield, 1982, 1984) provides a simple model of an associative memory in a neuronal structure. This model, however, is based on highly artificial assumptions, especially the use of formal-two state neurons (Hopfield, 1982) or graded-response neurons (Hopfield, 1984). What happens if we replace the formal neurons by 'real' biological neurons? We address this question in two steps. First, we show that a simple model of a neuron can capture all relevant features of neuron spiking, i.e., a wide range of spiking frequencies and a realistic distribution of interspike intervals. Second, we construct an associative memory by linking these neurons together. The analytical solution for a large and fully connected network shows that the Hopfield solution is valid only for neurons with a short refractory period. If the refractory period is longer than a critical duration γc, the solutions are qualitatively different. The associative character of the solutions, however, is preserved.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {84–90},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986777,
author = {Mjolsness, Eric and Miranker, Willard L.},
title = {A Lagrangian Approach to Fixed Points},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a new way to derive dissipative, optimizing dynamics from the Lagrangian formulation of mechanics. It can be used to obtain both standard and novel neural net dynamics for optimization problems. To demonstrate this we derive standard descent dynamics as well as nonstandard variants that introduce a computational attention mechanism.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {77–83},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986776,
author = {Leen, Todd K.},
title = {Dynamics of Learning in Recurrent Feature-Discovery Networks},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The self-organization of recurrent feature-discovery networks is studied from the perspective of dynamical systems. Bifurcation theory reveals parameter regimes in which multiple equilibria or limit cycles coexist with the equilibrium at which the networks perform principal component analysis.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {70–76},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986775,
author = {Cowan, J. D.},
title = {Stochastic Neurodynamics},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The main point of this paper is that stochastic neural networks have a mathematical structure that corresponds quite closely with that of quantum field theory. Neural network Liouvillians and Lagrangians can be derived, just as can spin Hamiltonians and Lagrangians in QFT. It remains to show the efficacy of such a description.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {62–69},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986774,
author = {Kepler, Thomas B. and Abbott, L. F. and Marder, Eve},
title = {Order Reduction for Dynamical Systems Describing the Behavior of Complex Neurons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have devised a scheme to reduce the complexity of dynamical systems belonging to a class that includes most biophysically realistic neural models. The reduction is based on transformations of variables and perturbation expansions and it preserves a high level of fidelity to the original system. The techniques are illustrated by reductions of the Hodgkin-Huxley system and an augmented Hodgkin-Huxley system.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {55–61},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986773,
author = {Hasselmo, Michael E. and Anderson, Brooke P. and Bower, James M.},
title = {Cholinergic Modulation May Enhance Cortical Associative Memory Function},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Combining neuropharmacological experiments with computational modeling, we have shown that cholinergic modulation may enhance associative memory function in piriform (olfactory) cortex. We have shown that the acetylcholine analogue carbachol selectively suppresses synaptic transmission between cells within piriform cortex, while leaving input connections unaffected. When tested in a computational model of piriform cortex, this selective suppression, applied during learning, enhances associative memory performance.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {46–52},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986772,
author = {Brown, Thomas H. and Mainen, Zachary F. and Zador, Anthony M. and Claiborne, Brenda J.},
title = {Self-Organization of Hebbian Synapses in Hippocampal Neurons},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We are exploring the significance of biological complexity for neuronal computation. Here we demonstrate that Hebbian synapses in realistically-modeled hippocampal pyramidal cells may give rise to two novel forms of self-organization in response to structured synaptic input. First, on the basis of the electrotonic relationships between synaptic contacts, a cell may become tuned to a small subset of its input space. Second, the same mechanisms may produce clusters of potentiated synapses across the space of the dendrites. The latter type of self-organization may be functionally significant in the presence of nonlinear dendritic conductances.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {39–45},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986771,
author = {Anastasio, Thomas J.},
title = {A Recurrent Neural Network Model of Velocity Storage in the Vestibulo-Ocular Reflex},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A three-layered neural network model was used to explore the organization of the vestibulo-ocular reflex (VOR). The dynamic model was trained using recurrent back-propagation to produce compensatory, long duration eye muscle motoneuron outputs in response to short duration vestibular afferent head velocity inputs. The network learned to produce this response prolongation, known as velocity storage, by developing complex, lateral inhibitory interactions among the interneurons. These had the low baseline, long time constant, rectified and skewed responses that are characteristic of real VOR interneurons. The model suggests that all of these features are interrelated and result from lateral inhibition.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {32–38},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986770,
author = {Cowan, J. D. and Friedman, A. E.},
title = {Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models, in which long-range interactions occur in a pattern given by the Mexican Hat operator, can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {26–31},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986769,
author = {Tanaka, Shigeru},
title = {Interaction among Ocularity, Retinotopy and on-Center/off-Center Pathways during Development},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The development of projections from the retinas to the cortex is mathematically analyzed according to the previously proposed thermodynamic formulation of the self-organization of neural networks. Three types of submodality included in the visual afferent pathways are assumed in two models: model (A), in which the ocularity and retinotopy are considered separately, and model (B), in which on-center/off-center pathways are considered in addition to ocularity and retinotopy. Model (A) shows striped ocular dominance spatial patterns and, in ocular dominance histograms, reveals a dip in the binocular bin. Model (B) displays spatially modulated irregular patterns and shows single-peak behavior in the histograms. When we compare the simulated results with the observed results, it is evident that the ocular dominance spatial patterns and histograms for models (A) and (B) agree very closely with those seen in monkeys and cats.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {18–25},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986768,
author = {Obermayer, K. and Ritter, H. and Schulten, K.},
title = {Development and Spatial Structure of Cortical Feature Maps: A Model Study},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Feature selective cells in the primary visual cortex of several species are organized in hierarchical topographic maps of stimulus features like "position in visual space", "orientation" and" ocular dominance". In order to understand and describe their spatial structure and their development, we investigate a self-organizing neural network model based on the feature map algorithm. The model explains map formation as a dimension-reducing mapping from a high-dimensional feature space onto a two-dimensional lattice, such that "similarity" between features (or feature combinations) is translated into "spatial proximity" between the corresponding feature selective cells. The model is able to reproduce several aspects of the spatial structure of cortical maps in the visual cortex.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {11–17},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}

@inproceedings{10.5555/2986766.2986767,
author = {Cowan, J. D. and Friedman, A. E.},
title = {Further Studies of a Model for the Development and Regeneration of Eye-Brain Maps},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a computational model of the development and regeneration of specific eye-brain circuits. The model comprises a self-organizing map-forming network which uses local Hebb rules, constrained by (genetically determined) molecular markers. Various simulations of the development and regeneration of eye-brain maps in fish and frogs are described, in particular successful simulations of experiments by Schmidt-Cicerone-Easter; Meyer; and Yoon.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {3–10},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'90}
}

@proceedings{10.5555/2986766,
title = {NIPS'90: Proceedings of the 3rd International Conference on Neural Information Processing Systems},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
location = {Denver, Colorado}
}

