@inproceedings{10.5555/2987061.2987188,
author = {Skaggs, William E. and McNaughton, Bruce L. and Gothard, Katalin M. and Markus, Etan J.},
title = {An Information-Theoretic Approach to Deciphering the Hippocampal Code},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Information theory is used to derive a simple formula for the amount of information conveyed by the firing rate of a neuron about any experimentally measured variable or combination of variables (e.g. running speed, head direction, location of the animal, etc.). The derivation treats the cell as a communication channel whose input is the measured variable and whose output is the cell's spike train. Applying the formula, we find systematic differences in the information content of hippocampal "place cells" in different experimental conditions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {1030–1037},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987187,
author = {Linster, Christiane and Marsan, David and Masson, Claudine and Kerszberg, Michel and Dreyfus, G\'{e}rard and Personnaz, L\'{e}on},
title = {A Formal Model of the Insect Olfactory Macroglomerulus: Simulations and Analytic Results},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {It is known from biological data that the response patterns of interneurons in the olfactory macroglomerulus (MGC) of insects are of central importance for the coding of the olfactory signal. We propose an analytically tractable model of the MGC which allows us to relate the distribution of response patterns to the architecture of the network.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {1022–1029},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987186,
author = {Massone, Lina L. E.},
title = {A Recurrent Neural Network for Generation of Occular Saccades},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper presents a neural network able to control saccadic movements. The input to the network is a specification of a stimulation site on the collicular motor map. The output is the time course of the eye position in the orbit (horizontal and vertical angles). The units in the network exhibit a one-to-one correspondance with neurons in the intermediate layer of the superior colliculus (collicular motor map), in the brainstem and with oculomotor neurons. Simulations carried out with this network demonstrate its ability to reproduce in a straightforward fashion many experimental observations.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {1014–1021},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987185,
author = {Walton, Lance C. and Bisset, David L.},
title = {Parameterising Feature Sensitive Cell Formation in Linsker Networks in the Auditory System},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper examines and extends the work of Linsker (1986) on self organising feature detectors. Linsker concentrates on the visual processing system, but infers that the weak assumptions made will allow the model to be used in the processing of other sensory information. This claim is examined here, with special attention paid to the auditory system, where there is much lower connectivity and therefore more statistical variability. On-line training is utilised, to obtain an idea of training times. These are then compared to the time available to pre-natal mammals for the formation of feature sensitive cells.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {1007–1013},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987184,
author = {Milton, John G. and Chu, Po Hsiang and Cowan, Jack D.},
title = {Spiral Waves in Integrate-and-Fire Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The formation of propagating spiral waves is studied in a randomly connected neural network composed of integrate-and-fire neurons with recovery period and excitatory connections using computer simulations. Network activity is initiated by periodic stimulation at a single point. The results suggest that spiral waves can arise in such a network via a sub-critical Hopf bifurcation.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {1001–1006},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987183,
author = {Douglass, John K. and Moss, Frank and Longtin, Andre},
title = {Statistical and Dynamical Interpretation of ISIH Data from Periodically Stimulated Sensory Neurons},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We interpret the time interval data obtained from periodically stimulated sensory neurons in terms of two simple dynamical systems driven by noise with an embedded weak periodic function called the signal: 1) a bistable system defined by two potential wells separated by a barrier, and 2) a FitzHugh-Nagumo system. The implementation is by analog simulation: electronic circuits which mimic the dynamics. For a given signal frequency, our simulators have only two adjustable parameters, the signal and noise intensities. We show that experimental data obtained from the periodically stimulated mechanoreceptor in the crayfish tailfan can be accurately approximated by these simulations. Finally, we discuss stochastic resonance in the two models.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {993–1000},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987182,
author = {Goodhill, Geoffrey J.},
title = {Topography and Ocular Dominance with Positive Correlations},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A new computational model that addresses the formation of both topography and ocular dominance is presented. This is motivated by experimental evidence that these phenomena may be subserved by the same mechanisms. An important aspect of this model is that ocular dominance segregation can occur when input activity is both distributed, and positively correlated between the eyes. This allows investigation of the dependence of the pattern of ocular dominance stripes on the degree of correlation between the eyes: it is found that increasing correlation leads to narrower stripes. Experiments are suggested to test whether such behaviour occurs in the natural system.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {985–992},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987181,
author = {Pawelzik, K. and Bauer, H.-U. and Deppisch, J. and Geisel, T.},
title = {How Oscillatory Neuronal Responses Reflect Bistability and Switching of the Hidden Assembly Dynamics},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A switching between apparently coherent (oscillatory) and stochastic episodes of activity has been observed in responses from cat and monkey visual cortex. We describe the dynamics of these phenomena in two parallel approaches, a phenomenological and a rather microscopic one. On the one hand we analyze neuronal responses in terms of a hidden state model (HSM). The parameters of this model are extracted directly from experimental spike trains. They characterize the underlying dynamics as well as the coupling of individual neurons to the network. This phenomenological model thus provides a new framework for the experimental analysis of network dynamics. The application of this method to multi unit activities from the visual cortex of the cat substantiates the existence of oscillatory and stochastic states and quantifies the switching behaviour in the assembly dynamics. On the other hand we start from the single spiking neuron and derive a master equation for the time evolution of the assembly state which we represent by a phase density. This phase density dynamics (PDD) exhibits costability of two attractors, a limit cycle, and a fixed point when synaptic interaction is nonlinear. External fluctuations can switch the bistable system from one state to the other. Finally we show, that the two approaches are mutually consistent and therefore both explain the detailed time structure in the data.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {977–984},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987180,
author = {Montague, P. R. and Dayan, P. and Nowlan, S. J. and Pouget, A. and Sejnowski, T. J.},
title = {Using Aperiodic Reinforcement for Directed Self-Organization during Development},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal. We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target. Three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {969–976},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987179,
author = {Coenen, Olivier and Sejnowski, Terrence J. and Lisberger, Stephen G.},
title = {Biologically Plausible Local Learning Rules for the Adaptation of the Vestibulo-Ocular Reflex},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The vestibulo-ocular reflex (VOR) is a compensatory eye movement that stabilizes images on the retina during head turns. Its magnitude, or gain, can be modified by visual experience during head movements. Possible learning mechanisms for this adaptation have been explored in a model of the oculomotor system based on anatomical and physiological constraints. The local correlational learning rules in our model reproduce the adaptation and behavior of the VOR under certain parameter conditions. From these conditions, predictions for the time course of adaptation at the learning sites are made.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {961–968},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987178,
author = {Linsker, Ralph},
title = {Deriving Receptive Fields Using an Optimal Encoding Criterion},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An information-theoretic optimization principle ('infomax') has previously been used for unsupervised learning of statistical regularities in an input ensemble. The principle states that the input-output mapping implemented by a processing stage should be chosen so as to maximize the average mutual information between input and output patterns, subject to constraints and in the presence of processing noise. In the present work I show how infomax, when applied to a class of nonlinear input-output mappings, can under certain conditions generate optimal filters that have additional useful properties: (1) Output activity (for each input pattern) tends to be concentrated among a relatively small number of nodes. (2) The filters are sensitive to higher-order statistical structure (beyond pairwise correlations). If the input features are localized, the filters' receptive fields tend to be localized as well. (3) Multiresolution sets of filters with subsampling at low spatial frequencies - related to pyramid coding and wavelet representations - emerge as favored solutions for certain types of input ensembles.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {953–960},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987177,
author = {Gat, Itay and Tishby, Naftali},
title = {Statistical Modeling of Cell Assemblies Activities in Associative Cortex of Behaving Monkeys},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {So far there has been no general method for relating extracellular electrophysiological measured activity of neurons in the associative cortex to underlying network or "cognitive" states. We propose to model such data using a multivariate Poisson Hidden Markov Model. We demonstrate the application of this approach for temporal segmentation of the firing patterns, and for characterization of the cortical responses to external stimuli. Using such a statistical model we can significantly discriminate two behavioral modes of the monkey, and characterize them by the different firing patterns, as well as by the level of coherency of their multi-unit firing activity.Our study utilized measurements carried out on behaving Rhesus monkeys by M. Abeles, E. Vaadia, and H. Bergman, of the Hadassa Medical School of the Hebrew University.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {945–952},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987176,
author = {Gluck, Mark A. and Myers, Catherine E.},
title = {Adaptive Stimulus Representations: A Computational Theory of Hippocampal-Region Function},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a theory of cortico-hippocampal interaction in discrimination learning. The hippocampal region is presumed to form new stimulus representations which facilitate learning by enhancing the discriminability of predictive stimuli and compressing stimulus-stimulus redundancies. The cortical and cerebellar regions, which are the sites of long-term memory. may acquire these new representations but are not assumed to be capable of forming new representations themselves. Instantiated as a connectionist model. this theory accounts for a wide range of trial-level classical conditioning phenomena in normal (intact) and hippocampal-lesioned animals. It also makes several novel predictions which remain to be investigated empirically. The theory implies that the hippocampal region is involved in even the simplest learning tasks; although hippocampal-lesioned animals may be able to use other strategies to learn these tasks. the theory predicts that they will show consistently different patterns of transfer and generalization when the task demands change.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {937–944},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987175,
author = {Burgess, Neil and O'Keefe, John and Recce, Michael},
title = {Using Hippocampal 'place Cells' for Navigation, Exploiting Phase Coding},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A model of the hippocampus as a central element in rat navigation is presented. Simulations show both the behaviour of single cells and the resultant navigation of the rat. These are compared with single unit recordings and behavioural data. The firing of CA1 place cells is simulated as the (artificial) rat moves in an environment. This is the input for a neuronal network whose output, at each theta (θ) cycle, is the next direction of travel for the rat. Cells are characterised by the number of spikes fired and the time of firing with respect to hippocampal θ rhythm. 'Learning' occurs in 'on-off' synapses that are switched on by simultaneous pre- and post-synaptic activity. The simulated rat navigates successfully to goals encountered one or more times during exploration in open fields. One minute of random exploration of a 1m2 environment allows navigation to a newly-presented goal from novel starting positions. A limited number of obstacles can be successfully avoided.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {929–936},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987174,
author = {Nelson, Mark E.},
title = {A Neural Model of Descending Gain Control in the Electrosensory System},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In the electrosensory system of weakly electric fish, descending pathways to a first-order sensory nucleus have been shown to influence the gain of its output neurons. The underlying neural mechanisms that subserve this descending gain control capability are not yet fully understood. We suggest that one possible gain control mechanism could involve the regulation of total membrane conductance of the output neurons. In this paper, a neural model based on this idea is used to demonstrate how activity levels on descending pathways could control both the gain and baseline excitation of a target neuron.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {921–928},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987173,
author = {Doya, Kenji and Boyle, Mary E. T. and Selverston, Allen I.},
title = {Mapping between Neural and Physical Activities of the Lobster Gastric Mill},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A computer model of the musculoskeletal system of the lobster gastric mill was constructed in order to provide a behavioral interpretation of the rhythmic patterns obtained from isolated stomatogastric ganglion. The model was based on Hill's muscle model and quasi-static approximation of the skeletal dynamics and could simulate the change of chewing patterns by the effect of neuromodulators.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {913–920},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987172,
author = {Goebel, Rainer},
title = {Perceiving Complex Visual Scenes: An Oscillator Neural Network Model That Integrates Selective Attention, Perceptual Organisation, and Invariant Recognition},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Which processes underly our ability to quickly recognize familiar objects within a complex visual input scene? In this paper an implemented neural network model is described that attempts to specify how selective visual attention, perceptual organisation, and invariance transformations might work together in order to segment, select, and recognize objects out of complex input scenes containing multiple, possibly overlapping objects. Retinotopically organized feature maps serve as input for two main processing routes: the 'where-pathway' dealing with location information and the 'what-pathway' computing the shape and attributes of objects. A location-based attention mechanism operates on an early stage of visual processing selecting a contigous region of the visual field for preferential processing. Additionally, location-based attention plays an important role for invariant object recognition controling appropriate normalization processes within the what-pathway. Object recognition is supported through the segmentation of the visual field into distinct entities. In order to represent different segmented entities at the same time, the model uses an oscillatory binding mechanism. Connections between the where-pathway and the what-pathway lead to a flexible cooperation between different functional subsystems producing an overall behavior which is consistent with a variety of psychophysical data.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {903–910},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987171,
author = {Sch\"{u}tze, Hinrich},
title = {Word Space},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {895–902},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987170,
author = {Towell, Geoffrey and Lehrer, Richard},
title = {A Knowledge-Based Model of Geometry Learning},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We propose a model of the development of geometric reasoning in children that explicitly involves learning. The model uses a neural network that is initialized with an understanding of geometry similar to that of second-grade children. Through the presentation of a series of examples, the model is shown to develop an understanding of geometry similar to that of fifth-grade children who were trained using similar materials.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {887–894},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987169,
author = {Bavelier, Daphne and Jordan, Michael I.},
title = {A Dynamical Model of Priming and Repetition Blindness},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a model of visual word recognition that accounts for several aspects of the temporal processing of sequences of briefly presented words. The model utilizes a new representation for written words, based on dynamic time warping and multidimensional scaling. The visual input passes through cascaded perceptual, comparison, and detection stages. We describe how these dynamical processes can account for several aspects of word recognition, including repetition priming and repetition blindness.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {879–886},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987168,
author = {Tresp, Volker and Hollatz, J\"{u}rgen and Ahmad, Subutai},
title = {Network Structuring and Training Using Rule-Based Knowledge},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We demonstrate in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture. We describe several ways to assure that rule-based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the number of rules and the number of conjuncts. After training the refined rules are extracted and analyzed.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {871–878},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987167,
author = {Mozer, Michael C. and Das, Sreerupa},
title = {A Connectionist Symbol Manipulator That Discovers the Structure of Context-Free Languages},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a neural net architecture that can discover hierarchical and recursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(O) context-free grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a straightforward interpretation of its behavior.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {863–870},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987166,
author = {Gentner, Dedre and Markman, Arthur B.},
title = {Analogy-Watershed or Waterloo? Structural Alignment and the Development of Connectionist Models of Analogy},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {855–862},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987165,
author = {Smolensky, Paul},
title = {Harmonic Grammars for Formal Languages},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Basic connectionist principles imply that grammars should take the form of systems of parallel soft constraints defining an optimization problem the solutions to which are the well-formed structures in the language. Such Harmonic Grammars have been successfully applied to a number of problems in the theory of natural languages. Here it is shown that formal languages too can be specified by Harmonic Grammars, rather than by conventional serial rewrite rule systems.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {847–854},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987164,
author = {Alspector, J. and Meir, R. and Yuhas, B. and Jayakumar, A. and Lippe, D.},
title = {A Parallel Gradient Descent Method for Learning in Analog VLSI Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Typical methods for gradient descent in neural network learning involve calculation of derivatives based on a detailed knowledge of the network model. This requires extensive, time consuming calculations for each pattern presentation and high precision that makes it difficult to implement in VLSI. We present here a perturbation technique that measures, not calculates, the gradient. Since the technique uses the actual network as a measuring device, errors in modeling neuron activation and synaptic weights do not cause errors in gradient descent. The method is parallel in nature and easy to implement in VLSI. We describe the theory of such an algorithm, an analysis of its domain of applicability, some simulations using it and an outline of a hardware implementation.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {836–844},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987163,
author = {Koch, Christof and Mathur, Binnal and Liu, Shih-Chii and Harris, John G. and Luo, Jin and Sivilotti, Massimo},
title = {Object-Based Analog VLSI Vision Circuits},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe two successfully working, analog VLSI vision circuits that move beyond pixel-based early vision algorithms. One circuit, implementing the dynamic wires model, provides for dedicated lines of communication among groups of pixels that share a common property. The chip uses the dynamic wires model to compute the arclength of visual contours. Another circuit labels all points inside a given contour with one voltage and all other with another voltage. Its behavior is very robust, since small breaks in contours are automatically sealed, providing for Figure-Ground segregation in a noisy environment. Both chips are implemented using networks of resistors and switches and represent a step towards object level processing since a single voltage value encodes the property of an ensemble of pixels.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {828–835},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987162,
author = {Lazzaro, John and Wawrzynek, John and Mahowald, M. and Sivilotti, Massimo and Gillespie, Dave},
title = {Silicon Auditory Processors as Computer Peripherals},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Several research groups are implementing analog integrated circuit models of biological auditory processing. The outputs of these circuit models have taken several forms, including video format for monitor display, simple scanned output for oscilloscope display and parallel analog outputs suitable for data-acquisition systems. In this paper, we describe an alternative output method for silicon auditory models, suitable for direct interface to digital computers.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {820–827},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987161,
author = {Renaud-Le Masson, Sylvie and Le Masson, Gwendal and Marder, Eve and Abbott, L. F.},
title = {Hybrid Circuits of Interacting Computer Model and Biological Neurons},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We demonstrate the use of a digital signal processing board to construct hybrid networks consisting of computer model neurons connected to a biological neural network. This system operates in real time, and the synaptic connections are realistic effective conductances. Therefore, the synapses made from the computer model neuron are integrated correctly by the postsynaptic biological neuron. This method provides us with the ability to add additional, completely known elements to a biological network and study their effect on network activity. Moreover, by changing the parameters of the model neuron, it is possible to assess the role of individual conductances in the activity of the neuron, and in the network in which it participates.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {813–819},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987160,
author = {Pasero, E. and Zecchina, R.},
title = {Attractor Neural Networks with Local Inhibition: From Statistical Physics to a Digitial Programmable Integrated Circuit},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Networks with local inhibition are shown to have enhanced computational performance with respect to the classical Hopfield-like networks. In particular the critical capacity of the network is increased as well as its capability to store correlated patterns. Chaotic dynamic behaviour (exponentially long transients) of the devices indicates the overloading of the associative memory. An implementation based on a programmable logic device is here presented. A 16 neurons circuit is implemented whit a XILINK 4020 device. The peculiarity of this solution is the possibility to change parts of the project (weights, transfer function or the whole architecture) with a simple software download of the configuration into the XILINK chip.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {805–812},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987159,
author = {Linden, A. and Sudbrak, Th. and Tietz, Ch. and Weber, F.},
title = {An Object-Oriented Framework for the Simulation of Neural Nets},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The field of software simulators for neural networks has been expanding very rapidly in the last years but their importance is still being underestimated. They must provide increasing levels of assistance for the design, simulation and analysis of neural networks. With our object-oriented framework (SESAME) we intend to show that very high degrees of transparency, manageability and flexibility for complex experiments can be obtained. SESAME's basic design philosophy is inspired by the natural way in which researchers explain their computational models. Experiments are performed with networks of building blocks, which can be extended very easily. Mechanisms have been integrated to facilitate the construction and analysis of very complex architectures. Among these mechanisms are the automatic configuration of building blocks for an experiment and multiple inheritance at run-time.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {797–804},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987158,
author = {Kirk, David B. and Kerns, Douglas and Fleischer, Kurt and Barr, Alan H.},
title = {Analog VLSI Implementation of Multi-Dimensional Gradient Descent},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe an analog VLSI implementation of a multi-dimensional gradient estimation and descent technique for minimizing an onchip scalar function f(). The implementation uses noise injection and multiplicative correlation to estimate derivatives, as in [Anderson, Kerns 92]. One intended application of this technique is setting circuit parameters on-chip automatically, rather than manually [Kirk 91]. Gradient descent optimization may be used to adjust synapse weights for a backpropagation or other on-chip learning implementation. The approach combines the features of continuous multi-dimensional gradient descent and the potential for an annealing style of optimization. We present data measured from our analog VLSI implementation.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {789–796},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987157,
author = {Sarpeshkar, Rahul and Bair, Wyeth and Koch, Christof},
title = {Visual Motion Computation in Analog VLSI Using Pulses},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The real time computation of motion from real images using a single chip with integrated sensors is a hard problem. We present two analog VLSI schemes that use pulse domain neuromorphic circuits to compute motion. Pulses of variable width, rather than graded potentials, represent a natural medium for evaluating temporal relationships. Both algorithms measure speed by timing a moving edge in the image. Our first model is inspired by Reichardt's algorithm in the fly and yields a non-monotonic response vs. velocity curve. We present data from a chip that implements this model. Our second algorithm yields a monotonic response vs. velocity curve and is currently being translated into silicon.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {781–788},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987156,
author = {Churcher, Stephen and Baxter, Donald J. and Hamilton, Alister and Murray, Alan F. and Reekie, H. Martin},
title = {Generic Analog Neural Computation: The EPSILON Chip},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An analog CMOS VLSI neural processing chip has been designed and fabricated. The device employs "pulse-stream" neural state signalling, and is capable of computing some 360 million synaptic connections per second. In addition to basic characterisation results, the performance of the chip in solving "real-world" problems is also demonstrated.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {773–780},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987155,
author = {Anderson, Janeen and Platt, John C. and Kirk, David B.},
title = {An Analog VLSI Chip for Radial Basis Functions},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have designed, fabricated, and tested an analog VLSI chip which computes radial basis functions in parallel. We have developed a synapse circuit that approximates a quadratic function. We aggregate these circuits to form radial basis functions. These radial basis functions are then averaged together using a follower aggregator.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {765–772},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987154,
author = {Rosenberg, Charles and Erel, Jacob and Atlan, Henri},
title = {A Neural Network That Learns to Interpret Myocardial Planar Thallium Scintigrams},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The planar thallium-201 myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease. Neural networks learned to interpret 100 thallium scintigrams as determined by individual expert ratings. Standard error backpropagation was compared to standard LMS, and LMS combined with one layer of RBF units. Using the "leave-one-out" method, generalization was tested on all 100 cases. Training time was determined automatically from cross-validation performance. Best performance was attained by the RBF/LMS network with three hidden units per view and compares favorably with human experts.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {755–762},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987153,
author = {Baldi, Pierre and Chauvin, Yves and Hunkapiller, Tim and McClure, Marcella A.},
title = {Hidden Markov Models in Molecular Biology: New Algorithms and Applications},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Hidden Markov Models (HMMs) can be applied to several important problems in molecular biology. We introduce a new convergent learning algorithm for HMMs that, unlike the classical Baum-Welch algorithm is smooth and can be applied on-line or in batch mode, with or without the usual Viterbi most likely path approximation. Left-right HMMs with insertion and deletion states are then trained to represent several protein families including immunoglobulins and kinases. In all cases, the models derived capture all the important statistical properties of the families and can be used efficiently in a number of important tasks such as multiple alignment, motif detection, and classification.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {747–754},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987152,
author = {Yuan, Jen-Lun and Fine, Terrence L.},
title = {Forecasting Demand for Electric Power},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We are developing a forecaster for daily extremes of demand for electric power encountered in the service area of a large midwestern utility and using this application as a testbed for approaches to input dimension reduction and decomposition of network training. Projection pursuit regression representations and the ability of algorithms like SIR to quickly find reasonable weighting vectors enable us to confront the vexing architecture selection problem by reducing high-dimensional gradient searchs to fitting single-input single-output (SISO) subnets. We introduce dimension reduction algorithms, to select features or relevant subsets of a set of many variables, based on minimizing an index of level-set dispersions (closely related to a projection index and to SIR), and combine them with backfitting to implement a neural network version of projection pursuit. The performance achieved by our approach, when trained on 1989, 1990 data and tested on 1991 data, is comparable to that achieved in our earlier study of backpropagation trained networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {739–746},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987151,
author = {Levin, Esther and Pieraccini, Roberto},
title = {Planar Hidden Markov Modeling: From Speech to Optical Character Recognition},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We propose in this paper a statistical model (planar hidden Markov model - PHMM) describing statistical properties of images. The model generalizes the single-dimensional HMM, used for speech processing, to the planar case. For this model to be useful an efficient segmentation algorithm, similar to the Viterbi algorithm for HMM, must exist. We present conditions in terms of the PHMM parameters that are sufficient to guarantee that the planar segmentation problem can be solved in polynomial time, and describe an algorithm for that. This algorithm aligns optimally the image with the model, and therefore is insensitive to elastic distortions of images. Using this algorithm a joint optimal segmentation and recognition of the image can be performed, thus overcoming the weakness of traditional OCR systems where segmentation is performed independently before the recognition leading to unrecoverable recognition errors.The PHMM approach was evaluated using a set of isolated handwritten digits. An overall digit recognition accuracy of 95% was achieved. An analysis of the results showed that even in the simple case of recognition of isolated characters, the elimination of elastic distortions enhances the performance Significantly. We expect that the advantage of this approach will be even more significant for tasks such as connected writing recognition/spotting, for which there is no known high accuracy method of recognition.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {731–738},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987150,
author = {Schenkel, M. and Weissman, H. and Guyon, I. and Nohl, C. and Henderson, D.},
title = {Recognition-Based Segmentation of on-Line Hand-Printed Words},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper reports on the performance of two methods for recognition-based segmentation of strings of on-line handprinted capital Latin characters. The input strings consist of a time-ordered sequence of X-Y coordinates, punctuated by pen-lifts. The methods were designed to work in "run-on mode" where there is no constraint on the spacing between characters. While both methods use a neural network recognition engine and a graph-algorithmic post-processor, their approaches to segmentation are quite different. The first method, which we call INSEG (for input segmentation), uses a combination of heuristics to identify particular penlifts as tentative segmentation points. The second method, which we call OUTSEG (for output segmentation), relies on the empirically trained recognition engine for both recognizing characters and identifying relevant segmentation points.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {723–730},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987149,
author = {Hild, Hermann and Waibel, Alex},
title = {Connected Letter Recognition with a Multi-State Time Delay Neural Network},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Multi-State Time Delay Neural Network (MS-TDNN) integrates a nonlinear time alignment procedure (DTW) and the high-accuracy phoneme spotting capabilities of a TDNN into a connectionist speech recognition system with word-level classification and error backpropagation. We present an MS-TDNN for recognizing continuously spelled letters, a task characterized by a small but highly confusable vocabulary. Our MS-TDNN achieves 98.5/92.0% word accuracy on speaker dependent/independent tasks, outperforming previously reported results on the same databases. We propose training techniques aimed at improving sentence level performance, including free alignment across word boundaries, word duration modeling and error backpropagation on the sentence rather than the word level. Architectures integrating submodules specialized on a subset of speakers achieved further improvements.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {712–719},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987148,
author = {Zavaliagkos, G. and Zhao, Y. and Schwartz, R. and Makhoul, J.},
title = {A Hybrid Neural Net System for State-of-the-Art Continuous Speech Recognition},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Untill recently, state-of-the-art, large-vocabulary, continuous speech recognition (CSR) has employed Hidden Markov Modeling (HMM) to model speech sounds. In an attempt to improve over HMM we developed a hybrid system that integrates HMM technology with neural networks. We present the concept of a "Segmental Neural Net" (SNN) for phonetic modeling in CSR. By taking into account all the frames of a phonetic segment simultaneously, the SNN overcomes the well-known conditional-independence limitation of HMMs. In several speaker-independent experiments with the DARPA Resource Management corpus, the hybrid system showed a consistent improvement in performance over the baseline HMM system.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {704–711},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987147,
author = {Tebelskis, Joe and Waibel, Alex},
title = {Performance through Consistency: MS-TDNN's for Large Vocabulary Continuous Speech Recognition},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Connectionist speech recognition systems are often handicapped by an inconsistency between training and testing criteria. This problem is addressed by the Multi-State Time Delay Neural Network (MS-TDNN), a hierarchical phoneme and word classifier which uses DTW to modulate its connectivity pattern, and which is directly trained on word-level targets. The consistent use of word accuracy as a criterion during both training and testing leads to very high system performance, even with limited training data. Until now, the MS-TDNN has been applied primarily to small vocabulary recognition and word spotting tasks. In this paper we apply the architecture to large vocabulary continuous speech recognition, and demonstrate that our MS-TDNN outperforms all other systems that have been tested on the CMU Conference Registration database.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {696–703},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987146,
author = {Principe, Jose C. and Zahalka, Abir},
title = {Transient Signal Detection with Neural Networks: The Search for the Desired Signal},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (0/1 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {688–695},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987145,
author = {Konig, Yochai and Morgan, Nelson and Wooters, Chuck and Abrash, Victor and Cohen, Michael and Franco, Horacio},
title = {Modeling Consistency in a Speaker Independent Continuous Speech Recognition System},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We would like to incorporate speaker-dependent consistencies, such as gender, in an otherwise speaker-independent speech recognition system. In this paper we discuss a Gender Dependent Neural Network (GDNN) which can be tuned for each gender, while sharing most of the speaker independent parameters. We use a classification network to help generate gender-dependent phonetic probabilities for a statistical (HMM) recognition system. The gender classification net predicts the gender with high accuracy, 98.3% on a Resource Management test set. However, the integration of the GDNN into our hybrid HMM-neural network recognizer provided an improvement in the recognition score that is not statistically significant on a Resource Management test set.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {682–687},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987144,
author = {Lee, Wei-Tsih and Pearson, John},
title = {A Hybrid Linear/Nonlinear Approach to Channel Equalization Problems},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Channel equalization problem is an important problem in high-speed communications. The sequences of symbols transmitted are distorted by neighboring symbols. Traditionally, the channel equalization problem is considered as a channel-inversion operation. One problem of this approach is that there is no direct correspondence between error probability and residual error produced by the channel inversion operation. In this paper, the optimal equalizer design is formulated as a classification problem. The optimal classifier can be constructed by Bayes decision rule. In general it is nonlinear. An efficient hybrid linear/nonlinear equalizer approach has been proposed to train the equalizer. The error probability of new linear/nonlinear equalizer has been shown to be better than a linear equalizer in an experimental channel.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {674–681},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987143,
author = {Liu, Weimin and Andreou, Andreas G. and Goldstein, Moise H.},
title = {Analog Cochlear Model for Multiresolution Speech Analysis},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper discusses the parameterization of speech by an analog cochlear model. The tradeoff between time and frequency resolution is viewed as the fundamental difference between conventional spectrographic analysis and cochlear signal processing for broadband, rapid-changing signals. The model's response exhibits a wavelet-like analysis in the scale domain that preserves good temporal resolution; the frequency of each spectral component in a broadband signal can be accurately determined from the interpeak intervals in the instantaneous firing rates of auditory fibers. Such properties of the cochlear model are demonstrated with natural speech and synthetic complex signals.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {666–673},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987142,
author = {Hirayama, Makoto and Vatikiotis-Bateson, Eric and Honda, Kiyoshi and Koike, Yasuharu and Kawato, Mitsuo},
title = {Physiologically Based Speech Synthesis},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This study demonstrates a paradigm for modeling speech production based on neural networks. Using physiological data from speech utterances, a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior that allows articulator trajectories to be generated from motor commands constrained by phoneme input strings and global performance parameters. From these movement trajectories, a second neural network generates PARCOR parameters that are then used to synthesize the speech acoustics.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {658–665},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987141,
author = {Cohen, Michael and Franco, Horacio and Morgan, Nelson and Rumelhart, David and Abrash, Victor},
title = {Context-Dependent Multiple Distribution Phonetic Modeling with MLPs},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A number of hybrid multilayer perceptron (MLP)/hidden Markov model (HMM) speech recognition systems have been developed in recent years (Morgan and Bourlard, 1990). In this paper, we present a new MLP architecture and training algorithm which allows the modeling of context-dependent phonetic classes in a hybrid MLP/HMM framework. The new training procedure smooths MLPs trained at different degrees of context dependence in order to obtain a robust estimate of the context-dependent probabilities. Tests with the DARPA Resource Management database have shown substantial advantages of the context-dependent MLPs over earlier context-independent MLPs, and have shown substantial advantages of this hybrid approach over a pure HMM approach.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {649–657},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987140,
author = {Kowalczyk, Adam},
title = {Some Estimates of Necessary Number of Connections and Hidden Units for Feed-Forward Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The feed-forward networks with fixed hidden units (FHU-networks) are compared against the category of remaining feed-forward networks with variable hidden units (VHU-networks). Two broad classes of tasks on a finite domain X ⊂ Rn are considered: approximation of every function from an open subset of functions on X and representation of every dichotomy of X. For the first task it is found that both network categories require the same minimal number of synaptic weights. For the second task and X in general position it is shown that VHU-networks with threshold logic hidden units can have approximately 1/n times fewer hidden units than any FHU-network must have.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {639–646},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987139,
author = {Wulff, N. H. and Hertz, J. A.},
title = {Learning Cellular Automaton Dynamics with Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have trained networks of Σ - II units with short-range connections to simulate simple cellular automata that exhibit complex or chaotic behaviour. Three levels of learning are possible (in decreasing order of difficulty): learning the underlying automaton rule, learning asymptotic dynamical behaviour, and learning to extrapolate the training history. The levels of learning achieved with and without weight sharing for different automata provide new insight into their dynamics.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {631–638},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987138,
author = {Helmke, Uwe and Williamson, Robert C.},
title = {Rational Parametrizations of Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A connection is drawn between rational functions, the realization theory of dynamical systems, and feedforward neural networks. This allows us to parametrize single hidden layer scalar neural networks with (almost) arbitrary analytic activation functions in terms of strictly proper rational functions. Hence, we can solve the uniqueness of parametrization problem for such networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {623–630},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987137,
author = {DasGupta, Bhaskar and Schnitger, Georg},
title = {The Power of Approximating: A Comparison of Activation Functions},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We compare activation functions in terms of the approximation power of their feedforward nets. We consider the case of analog as well as boolean input.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {615–622},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987136,
author = {Murata, Noboru and Yoshizawa, Shuji and Amari, Shun-Ichi},
title = {Learning Curves, Model Selection and Complexity of Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Learning curves show how a neural network is improved as the number of training examples increases and how it is related to the network complexity. The present paper clarifies asymptotic properties and their relation of two learning curves, one concerning the predictive loss or generalization loss and the other the training loss. The result gives a natural definition of the complexity of a neural network. Moreover, it provides a new criterion of model selection.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {607–614},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987135,
author = {Liu, Yong},
title = {Neural Network Model Selection Using Asymptotic Jackknife Estimator and Cross-Validation Method},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Two theorems and a lemma are presented about the use of jackknife estimator and the cross-validation method for model selection. Theorem 1 gives the asymptotic form for the jackknife estimator. Combined with the model selection criterion, this asymptotic form can be used to obtain the fit of a model. The model selection criterion we used is the negative of the average predictive likehood, the choice of which is based on the idea of the cross-validation method. Lemma 1 provides a formula for further exploration of the asymptotics of the model selection criterion. Theorem 2 gives an asymptotic form of the model selection criterion for the regression case, when the parameters optimization criterion has a penalty term. Theorem 2 also proves the asymptotic equivalence of Moody's model selection criterion (Moody, 1992) and the cross-validation method, when the distance measure between response y and regression function takes the form of a squared difference.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {599–606},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987134,
author = {Golea, Mostefa and Marchand, Mario and Hancock, Thomas R.},
title = {On Learning µ-Perceptron Networks with Binary Weights},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Neural networks with binary weights are very important from both the theoretical and practical points of view. In this paper, we investigate the learnability of single binary perceptrons and unions of µ-binary-perceptron networks, i.e. an "OR" of binary perceptrons where each input unit is connected to one and only one perceptron. We give a polynomial time algorithm that PAC learns these networks under the uniform distribution. The algorithm is able to identify both the network connectivity and the weight values necessary to represent the target function. These results suggest that, under reasonable distributions, µ-perceptron networks may be easier to learn than fully connected networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {591–598},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987133,
author = {DeMers, David and Cottrell, Garrison},
title = {Non-Linear Dimensionality Reduction},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A method for creating a non-linear encoder-decoder for multidimensional data with compact representations is presented. The commonly used technique of autoassociation is extended to allow non-linear representations, and an objective function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {580–587},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987132,
author = {Meilijson, Isaac and Ruppin, Eytan},
title = {History-Dependent Attractor Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a methodological framework enabling a detailed description of the performance of Hopfield-like attractor neural networks (ANN) in the first two iterations. Using the Bayesian approach, we find that performance is improved when a history-based term is included in the neuron's dynamics. A further enhancement of the network's performance is achieved by judiciously choosing the censored neurons (those which become active in a given iteration) on the basis of the magnitude of their post-synaptic potentials. The contribution of biologically plausible, censored, history-dependent dynamics is especially marked in conditions of low firing activity and sparse connectivity, two important characteristics of the mammalian cortex. In such networks, the performance attained is higher than the performance of two 'independent' iterations, which represents an upper bound on the performance of history-independent networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {572–579},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987131,
author = {Meilijson, Isaac and Ruppin, Eytan and Sipper, Moshe},
title = {Single-Iteration Threshold Hamming Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We analyze in detail the performance of a Hamming network classifying inputs that are distorted versions of one of its m stored memory patterns. The activation function of the memory neurons in the original Hamming network is replaced by a simple threshold function. The resulting Threshold Hamming Network (THN) correctly classifies the input pattern, with probability approaching 1, using only O(m ln m) connections, in a single iteration. The THN drastically reduces the time and space complexity of Hamming Network classifiers.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {564–571},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987130,
author = {Minai, Ali A. and Levy, William B.},
title = {Predicting Complex Behavior in Sparse Asymmetric Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recurrent networks of threshold elements have been studied intensively as associative memories and pattern-recognition devices. While most research has concentrated on fully-connected symmetric networks, which relax to stable fixed points, asymmetric networks show richer dynamical behavior, and can be used as sequence generators or flexible pattern-recognition devices. In this paper, we approach the problem of predicting the complex global behavior of a class of random asymmetric networks in terms of network parameters. These networks can show fixed-point, cyclical or effectively aperiodic behavior, depending on parameter values, and our approach can be used to set parameters, as necessary, to obtain a desired complexity of dynamics. The approach also provides qualitative insight into why the system behaves as it does and suggests possible applications.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {556–563},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987129,
author = {Doyon, Bernard and Cessac, Bruno and Quoy, Mathias and Samuelides, Manuel},
title = {Destabilization and Route to Chaos in Neural Networks with Random Connectivity},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The occurence of chaos in recurrent neural networks is supposed to depend on the architecture and on the synaptic coupling strength. It is studied here for a randomly diluted architecture. By normalizing the variance of synaptic weights, we produce a bifurcation parameter, dependent on this variance and on the slope of the transfer function but independent of the connectivity, that allows a sustained activity and the occurence of chaos when reaching a critical value. Even for weak connectivity and small size, we find numerical results in accordance with the theoretical ones previously established for fully connected infinite sized networks. Moreover the route towards chaos is numerically checked to be a quasi-periodic one, whatever the type of the first bifurcation is (Hopf bifurcation, pitchfork or flip).},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {549–555},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987128,
author = {Wolpert, David H.},
title = {On the Use of Evidence in Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Bayesian "evidence" approximation has recently been employed to determine the noise and weight-penalty terms used in back-propagation. This paper shows that for neural nets it is far easier to use the exact result than it is to use the evidence approximation. Moreover, unlike the evidence approximation, the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code (the exact result is closed form). In addition, it turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error. Another advantage of the exact analysis is that it does not lead one to incorrect intuition, like the claim that using evidence one can "evaluate different priors in light of the data". This paper also discusses sufficiency conditions for the evidence approximation to hold, why it can sometimes give "reasonable" results, etc.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {539–546},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987127,
author = {Miller, John W. and Goodman, Rodney M.},
title = {Probability Estimation from a Database Using a Gibbs Energy Model},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present an algorithm for creating a neural network which produces accurate probability estimates as outputs. The network implements a Gibbs probability distribution model of the training database. This model is created by a new transformation relating the joint probabilities of attributes in the database to the weights (Gibbs potentials) of the distributed network model. The theory of this transformation is presented together with experimental results. One advantage of this approach is the network weights are prescribed without iterative gradient descent. Used as a classifier the network tied or outperformed published results on a variety of databases.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {531–538},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987126,
author = {Schwarze, Holm and Hertz, John A.},
title = {Statistical Mechanics of Learning in a Large Committee Machine},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We use statistical mechanics to study generalization in large committee machines. For an architecture with nonoverlapping receptive fields a replica calculation yields the generalization error in the limit of a large number of hidden units. For continuous weights the generalization error falls off asymptotically inversely proportional to α, the number of training examples per weight. For binary weights we find a discontinuous transition from poor to perfect generalization followed by a wide region of metastability. Broken replica symmetry is found within this region at low temperatures. For a fully connected architecture the generalization error is calculated within the annealed approximation. For both binary and continuous weights we find transitions from a symmetric state to one with specialized hidden units, accompanied by discontinuous drops in the generalization error.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {523–530},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987125,
author = {Shiono, Satoru and Yamada, Satoshi and Nakashima, Michio and Matsumoto, Kenji},
title = {Information Theoretic Analysis of Connection Structure from Spike Trains},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have attempted to use information theoretic quantities for analyzing neuronal connection structure from spike trains. Two point mutual information and its maximum value, channel capacity, between a pair of neurons were found to be useful for sensitive detection of crosscorrelation and for estimation of synaptic strength, respectively. Three point mutual information among three neurons could give their interconnection structure. Therefore, our information theoretic analysis was shown to be a very powerful technique for deducing neuronal connection structure. Some concrete examples of its application to simulated spike trains are presented.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {515–522},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987124,
author = {Orr, Genevieve B. and Leen, Todd K.},
title = {Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function P(w, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {507–514},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987123,
author = {Schraudolph, Nicol N. and Sejnowski, Terrence J.},
title = {Unsupervised Discrimination of Clustered Data via Optimization of Binary Information Gain},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present the information-theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants. In contrast to methods that try to preserve information about the input patterns, we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes. We derive a local weight adaptation rule via gradient ascent in this objective, demonstrate its dynamics on some simple data sets, relate our approach to previous work and suggest directions in which it may be extended.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {499–506},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987122,
author = {Murray, Alan F. and Edwards, Peter J.},
title = {Synaptic Weight Noise during MLP Learning Enhances Fault-Tolerance, Generalization and Learning Trajectory},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We analyse the effects of analog noise on the synaptic arithmetic during MultiLayer Perceptron training, by expanding the cost function to include noise-mediated penalty terms. Predictions are made in the light of these calculations which suggest that fault tolerance, generalisation ability and learning trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving "inaccurate" analog neural VLSI.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {491–498},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987121,
author = {Freund, Yoav and Seung, H. Sebastian and Shamir, Eli and Tishby, Naftali},
title = {Information, Prediction, and Query by Committee},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We analyze the "query by committee" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of thresholded smooth functions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {483–490},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987120,
author = {Neal, Radford M.},
title = {Bayesian Learning via Stochastic Dynamics},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The attempt to find a single "optimal" weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {475–482},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987119,
author = {Xu, Lei and Yuille, Alan},
title = {Self-Organizing Rules for Robust Principal Component Analysis},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In the presence of outliers, the existing self-organizing rules for Principal Component Analysis (PCA) perform poorly. Using statistical physics techniques including the Gibbs distribution, binary decision fields and effective energies, we propose self-organizing PCA rules which are capable of resisting outliers while fulfilling various PCA-related tasks such as obtaining the first principal component vector, the first k principal component vectors, and directly finding the subspace spanned by the first k vector principal component vectors without solving for each vector individually. Comparative experiments have shown that the proposed robust rules improve the performances of the existing PCA algorithms significantly when outliers are present.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {467–474},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987118,
author = {Finnoff, William},
title = {Diffusion Approximations for the Constant Learning Rate Backpropagation Algorithm and Resistence to Local Minima},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper we discuss the asymptotic properties of the most commonly used variant of the backpropagation algorithm in which network weights are trained by means of a local gradient descent on examples drawn randomly from a fixed training set, and the learning rate η of the gradient updates is held constant (simple backpropagation). Using stochastic approximation results, we show that for η → 0 this training process approaches a batch training and provide results on the rate of convergence. Further, we show that for small η one can approximate simple back propagation by the sum of a batch training process and a Gaussian diffusion which is the unique solution to a linear stochastic differential equation. Using this approximation we indicate the reasons why simple backpropagation is less likely to get stuck in local minima than the batch training process and demonstrate this empirically on a number of examples.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {459–466},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987117,
author = {Leen, Todd K. and Moody, John E.},
title = {Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The ensemble dynamics of stochastic learning algorithms can be studied using theoretical techniques from statistical physics. We develop the equations of motion for the weight space probability densities for stochastic learning algorithms. We discuss equilibria in the diffusion approximation and provide expressions for special cases of the LMS algorithm. The equilibrium densities are not in general thermal (Gibbs) distributions in the objective function being minimized, but rather depend upon an effective potential that includes diffusion effects. Finally we present an exact analytical expression for the time evolution of the density for a learning algorithm with weight updates proportional to the sign of the gradient.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {451–458},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987116,
author = {Martin, Gale and Rashid, Mosfeq and Chapman, David and Pittman, James A.},
title = {Learning to See Where and What: Training a Net to Make Saccades and Recognize Handwritten Characters},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes an approach to integrated segmentation and recognition of hand-printed characters. The approach, called Saccade, integrates ballistic and corrective saccades (eye movements) with character recognition. A single backpropagation net is trained to make a classification decision on a character centered in its input window, as well as to estimate the distance of the current and next character from the center of the input window. The net learns to accurately estimate these distances regardless of variations in character width, spacing between characters, writing style and other factors. During testing, the system uses the net-extracted classification and distance information, along with a set of jumping rules, to jump from character to character.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {441–447},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987115,
author = {Lappe, Markus and Rauschecker, Josef P.},
title = {Computation of Heading Direction from Optic Flow in Visual Cortex},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have designed a neural network which detects the direction of egomotion from optic flow in the presence of eye movements (Lappe and Rauschecker, 1993). The performance of the network is consistent with human psychophysical data, and its output neurons show great similarity to "triple component" cells in area MSTd of monkey visual cortex. We now show that by using assumptions about the kind of eye movements that the observer is likely to perform, our model can generate various other cell types found in MSTd as well.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {433–440},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987114,
author = {Greenspan, Hayit K. and Goodman, Rodney},
title = {Remote Sensing Image Analysis via a Texture Classification Neural Network},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this work we apply a texture classification network to remote sensing image analysis. The goal is to extract the characteristics of the area depicted in the input image, thus achieving a segmented map of the region. We have recently proposed a combined neural network and rule-based framework for texture recognition. The framework uses unsupervised and supervised learning, and provides probability estimates for the output classes. We describe the texture classification network and extend it to demonstrate its application to the Landsat and Aerial image analysis domain.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {425–432},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987113,
author = {Martin, Kevin E. and Marshall, Jonathan A.},
title = {Unsmearing Visual Motion: Development of Long-Range Horizontal Intrinsic Connections},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Human vision systems integrate information nonlocally, across long spatial ranges. For example, a moving stimulus appears smeared when viewed briefly (30 ms), yet sharp when viewed for a longer exposure (100 ms) (Burr, 1980). This suggests that visual systems combine information along a trajectory that matches the motion of the stimulus. Our self-organizing neural network model shows how developmental exposure to moving stimuli can direct the formation of horizontal trajectory-specific motion integration pathways that unsmear representations of moving stimuli. These results account for Burr's data and can potentially also model other phenomena, such as visual inertia.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {417–424},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987112,
author = {Brody, Carlos D.},
title = {A Model of Feedback to the Lateral Geniculate Nucleus},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Simplified models of the lateral geniculate nucles (LGN) and striate cortex illustrate the possibility that feedback to the LGN may be used for robust, low-level pattern analysis. The information fed back to the LGN is rebroadcast to cortex using the LGN's full fan-out, so the cortex→LGN→cortex pathway mediates extensive cortico-cortical communication while keeping the number of necessary connections small.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {409–416},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987111,
author = {Utans, Joachim and Gindi, Gene},
title = {Improving Convergence in Hierarchical Matching Networks for Object Recognition},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We are interested in the use of analog neural networks for recognizing visual objects. Objects are described by the set of parts they are composed of and their structural relationship. Structural models are stored in a database and the recognition problem reduces to matching data to models in a structurally consistent way. The object recognition problem is in general very difficult in that it involves coupled problems of grouping, segmentation and matching. We limit the problem here to the simultaneous labelling of the parts of a single object and the determination of analog parameters. This coupled problem reduces to a weighted match problem in which an optimizing neural network must minimize E(M, p) = Σαi Mαi Wαi(p), where the {Mαi} are binary match variables for data parts i to model parts α and {Wαi(P)} are weights dependent on parameters p. In this work we show that by first solving for estimates p without solving for Mαi, we may obtain good initial parameter estimates that yield better solutions for M and p.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {401–408},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987110,
author = {Ahmad, Subutai and Tresp, Volker},
title = {Some Solutions to the Missing Feature Problem in Vision},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In visual processing the ability to deal with missing and noisy information is crucial. Occlusions and unreliable feature detectors often lead to situations where little or no direct information about features is available. However the available information is usually sufficient to highly constrain the outputs. We discuss Bayesian techniques for extracting class probabilities given partial data. The optimal solution involves integrating over the missing dimensions weighted by the local probability densities. We show how to obtain closed-form approximations to the Bayesian solution using Gaussian basis function networks. The framework extends naturally to the case of noisy features. Simulations on a complex task (3D hand gesture recognition) validate the theory. When both integration and weighting by input densities are used, performance decreases gracefully with the number of missing or noisy features. Performance is substantially degraded if either step is omitted.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {393–400},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987109,
author = {Madarasmi, Suthep and Kersten, Daniel and Pong, Ting-Chuen},
title = {The Computation of Stereo Disparity for Transparent and for Opaque Surfaces},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The classical computational model for stereo vision incorporates a uniqueness inhibition constraint to enforce a one-to-one feature match, thereby sacrificing the ability to handle transparency. Critics of the model disregard the uniqueness constraint and argue that the smoothness constraint can provide the excitation support required for transparency computation. However, this modification fails in neighborhoods with sparse features. We propose a Bayesian approach to stereo vision with priors favoring cohesive over transparent surfaces. The disparity and its segmentation into a multi-layer "depth planes" representation are simultaneously computed. The smoothness constraint propagates support within each layer, providing mutual excitation for non-neighboring transparent or partially occluded regions. Test results for various random-dot and other stereograms are presented.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {385–390},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987108,
author = {Stern, Edward and Aertsen, Ad and Vaadia, Eilon and Hochstein, Shaul},
title = {Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations in V1 of Awake Monkey},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Multiple single neuron responses were recorded from a single electrode in V1 of alert, behaving monkeys. Drifting sinusoidal gratings were presented in the cells' overlapping receptive fields, and the stimulus was varied along several visual dimensions. The degree of dimensional separability was calculated for a large population of neurons, and found to be a continuum. Several cells showed different temporal response dependencies to variation of different stimulus dimensions, i.e. the tuning of the modulated firing was not necessarily the same as that of the mean firing rate. We describe a multidimensional receptive field, and use simultaneously recorded responses to compute a multi-neuron receptive field, describing the information processing capabilities of a group of cells. Using dynamic correlation analysis, we propose several computational schemes for multidimensional spatiotemporal tuning for groups of cells. The implications for neuronal coding of stimuli are discussed.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {377–384},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987107,
author = {Nowlan, Steven J. and Sejnowski, Terrence J.},
title = {Filter Selection Model for Generating Visual Motion Signals},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Neurons in area MT of primate visual cortex encode the velocity of moving objects. We present a model of how MT cells aggregate responses from VI to form such a velocity representation. Two different sets of units, with local receptive fields, receive inputs from motion energy filters. One set of units forms estimates of local motion, while the second set computes the utility of these estimates. Outputs from this second set of units "gate" the outputs from the first set through a gain control mechanism. This active process for selecting only a subset of local motion responses to integrate into more global responses distinguishes our model from previous models of velocity estimation. The model yields accurate velocity estimates in synthetic images containing multiple moving targets of varying size, luminance, and spatial frequency profile and deals well with a number of transparency phenomena.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {369–376},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987106,
author = {Becker, Suzanna},
title = {Learning to Categorize Objects Using Temporal Coherence},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The invariance of an objects' identity as it transformed over time provides a powerful cue for perceptual learning. We present an unsupervised learning procedure which maximizes the mutual information between the representations adopted by a feed-forward network at consecutive time steps. We demonstrate that the network can learn, entirely unsupervised, to classify an ensemble of several patterns by observing pattern trajectories, even though there are abrupt transitions from one object to another between trajectories. The same learning procedure should be widely applicable to a variety of perceptual learning tasks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {361–368},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987105,
author = {Higgins, Charles M. and Goodman, Rodney M.},
title = {Learning Fuzzy Rule-Based Neural Networks for Control},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A three-step method for function approximation with a fuzzy system is proposed. First, the membership functions and an initial rule representation are learned; second, the rules are compressed as much as possible using information theory; and finally, a computational network is constructed to compute the function value. This system is applied to two control examples: learning the truck and trailer backer-upper control system, and learning a cruise control system for a radio-controlled model car.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {350–357},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987104,
author = {Fahner, Gerald and Eckmiller, Rolf},
title = {Learning Spatio-Temporal Planning from a Dynamic Programming Teacher: Feed-Forward Neurocontrol for Moving Obstacle Avoidance},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Within a simple test-bed, application of feed-forward neurocontrol for short-term planning of robot trajectories in a dynamic environment is studied. The action network is embedded in a sensory-motoric system architecture that contains a separate world model. It is continuously fed with short-term predicted spatio-temporal obstacle trajectories, and receives robot state feedback. The action net allows for external switching between alternative planning tasks. It generates goal-directed motor actions - subject to the robot's kinematic and dynamic constraints - such that collisions with moving obstacles are avoided. Using supervised learning, we distribute examples of the optimal planner mapping over a structure-level adapted parsimonious higher order network. The training database is generated by a Dynamic Programming algorithm. Extensive simulations reveal, that the local planner mapping is highly nonlinear, but can be effectively and sparsely represented by the chosen powerful net model. Excellent generalization occurs for unseen obstacle configurations. We also discuss the limitations of feed-forward neurocontrol for growing planning horizons.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {342–349},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987103,
author = {Sanger, Terence D.},
title = {A Practice Strategy for Robot Learning Control},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {"Trajectory Extension Learning" is a new technique for Learning Control in Robots which assumes that there exists some parameter of the desired trajectory that can be smoothly varied from a region of easy solvability of the dynamics to a region of desired behavior which may have more difficult dynamics. By gradually varying the parameter, practice movements remain near the desired path while a Neural Network learns to approximate the inverse dynamics. For example, the average speed of motion might be varied, and the inverse dynamics can be "bootstrapped" from slow movements with simpler dynamics to fast movements. This provides an example of the more general concept of a "Practice Strategy" in which a sequence of intermediate tasks is used to simplify learning a complex task. I show an example of the application of this idea to a real 2-joint direct drive robot arm.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {335–341},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987102,
author = {Gullapalli, Vijaykumar},
title = {Learning Control under Extreme Uncertainty},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise. Task complexity due to the use of an unchamfered hole and a clearance of less than 0.2mm is compounded by the presence of positional uncertainty of magnitude exceeding 10 to 50 times the clearance. Despite this extreme degree of uncertainty, our results indicate that direct reinforcement learning can be used to learn a robust reactive control strategy that results in skillful peg-in-hole insertions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {327–334},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987101,
author = {Peterson, James K.},
title = {On-Line Estimation of the Optimal Value Function: HJB-Estimators},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, we discuss on-line estimation strategies that model the optimal value function of a typical optimal control problem. We present a general strategy that uses local corridor solutions obtained via dynamic programming to provide local optimal control sequence training data for a neural architecture model of the optimal value function.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {319–326},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987100,
author = {Uno, Yoji and Fukumura, Naohiro and Suzuki, Ryoji and Kawato, Mitsuo},
title = {Integration of Visual and Somatosensory Information for Preshaping Hand in Grasping Movements},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The primate brain must solve two important problems in grasping movements. The first problem concerns the recognition of grasped objects: specifically, how does the brain integrate visual and motor information on a grasped object? The second problem concerns hand shape planning: specifically, how does the brain design the hand configuration suited to the shape of the object and the manipulation task? A neural network model that solves these problems has been developed. The operations of the network are divided into a learning phase and an optimization phase. In the learning phase, internal representations, which depend on the grasped objects and the task, are acquired by integrating visual and somatosensory information. In the optimization phase, the most suitable hand shape for grasping an object is determined by using a relaxation computation of the network.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {311–318},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987099,
author = {Bowman, Christopher},
title = {Neural Network On-Line Learning Control of Spacecraft Smart Structures},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The overall goal is to reduce spacecraft weight, volume, and cost by online adaptive non-linear control of flexible structural components. The objective of this effort is to develop an adaptive Neural Network (NN) controller for the Ball C-Side 1m \texttimes{} 3m antenna with embedded actuators and the RAMS sensor system. A traditional optimal controller for the major modes is provided perturbations by the NN to compensate for unknown residual modes. On-line training of recurrent and feed-forward NN architectures have achieved adaptive vibration control with unknown modal variations and noisy measurements. On-line training feedback to each actuator NN output is computed via Newton's method to reduce the difference between desired and achieved antenna positions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {303–310},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987098,
author = {Bradtke, Steven J.},
title = {Reinforcement Learning Applied to Linear Quadratic Regulation},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recent research on reinforcement learning has focused on algorithms based on the principles of Dynamic Programming (DP). One of the most promising areas of application for these algorithms is the control of dynamical systems, and some impressive results have been achieved. However, there are significant gaps between practice and theory. In particular, there are no convergence proofs for problems with continuous state and action spaces, or for systems involving non-linear function approximators (such as multilayer perceptrons). This paper presents research applying DP-based reinforcement learning theory to Linear Quadratic Regulation (LQR), an important class of control problems involving continuous state and action spaces and requiring a simple type of non-linear function approximator. We describe an algorithm based on Q-learning that is proven to converge to the optimal controller for a large class of LQR problems. We also describe a slightly different algorithm that is only locally convergent to the optimal Q-function, demonstrating one of the possible pitfalls of using a non-linear function approximator with DP-based learning.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {295–302},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987097,
author = {Mitchell, Tom M. and Thrun, Sebastian B.},
title = {Explanation-Based Neural Network Learning for Robot Control},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {287–294},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987096,
author = {Pomerleau, Dean A.},
title = {Input Reconstruction Reliability Estimation},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes a technique called Input Reconstruction Reliability Estimation (IRRE) for determining the response reliability of a restricted class of multi-layer perceptrons (MLPs). The technique uses a network's ability to accurately encode the input pattern in its internal representation as a measure of its reliability. The more accurately a network is able to reconstruct the input pattern from its internal representation, the more reliable the network is considered to be. IRRE is provides a good estimate of the reliability of MLPs trained for autonomous driving. Results are presented in which the reliability estimates provided by IRRE are used to select between networks trained for different driving situations.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {279–286},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987095,
author = {Dayan, Peter and Hinton, Geoffrey E.},
title = {Feudal Reinforcement Learning},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command.We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {271–278},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987094,
author = {Moore, Andrew W. and Atkeson, Christopher G.},
title = {Memory-Based Reinforcement Learning: Efficient Computation with Prioritized Sweeping},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Differencing and Q-learning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have difficulty.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {263–270},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987093,
author = {DeMers, David and Kreutz-Delgado, Kenneth},
title = {Global Regularization of Inverse Kinematics for Redundant Manipulators},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The inverse kinematics problem for redundant manipulators is ill-posed and nonlinear. There are two fundamentally different issues which result in the need for some form of regularization; the existence of multiple solution branches (global ill-posedness) and the existence of excess degrees of freedom (local ill-posedness). For certain classes of manipulators, learning methods applied to input-output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the forward mapping into a finite set of regions over which the inverse problem is well-posed. Local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region. As a result, the ill-posed problem can be transformed into a finite set of well-posed problems. Each can then be solved separately to construct approximate direct inverse functions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {255–262},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987092,
author = {Cauwenberghs, Gert},
title = {A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A parallel stochastic algorithm is investigated for error-descent learning and optimization in deterministic networks of arbitrary topology. No explicit information about internal network structure is needed. The method is based on the model-free distributed learning mechanism of Dembo and Kailath. A modified parameter update rule is proposed by which each individual parameter vector perturbation contributes a decrease in error. A substantially faster learning speed is hence allowed. Furthermore, the modified algorithm supports learning time-varying features in dynamical networks. We analyze the convergence and scaling properties of the algorithm, and present simulation results for dynamic trajectory learning in recurrent networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {244–251},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987091,
author = {Baird, Bill and Troyer, Todd and Eeckman, Frank},
title = {Synchronization and Grammatical Inference in an Oscillating Elman Net},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We have designed an architecture to span the gap between biophysics and cognitive science to address and explore issues of how a discrete symbol processing system can arise from the continuum, and how complex dynamics like oscillation and synchronization can then be employed in its operation and affect its learning. We show how a discrete-time recurrent "Elman" network architecture can be constructed from recurrently connected oscillatory associative memory modules described by continuous nonlinear ordinary differential equations. The modules can learn connection weights between themselves which will cause the system to evolve under a clocked "machine cycle" by a sequence of transitions of attractors within the modules, much as a digital computer evolves by transitions of its binary flip-flop attractors. The architecture thus employs the principle of "computing with attractors" used by macroscopic systems for reliable computation in the presence of noise. We have specifically constructed a system which functions as a finite state automaton that recognizes or generates the infinite set of six symbol strings that are defined by a Reber grammar. It is a symbol processing system, but with analog input and oscillatory subsymbolic representations. The time steps (machine cycles) of the system are implemented by rhythmic variation (clocking) of a bifurcation parameter. This holds input and "context" modules clamped at their attractors while 'hidden and output modules change state, then clamps hidden and output states while context modules are released to load those states as the new context for the next cycle of input. Superior noise immunity has been demonstrated for systems with dynamic attractors over systems with static attractors, and synchronization ("binding") between coupled oscillatory attractors in different modules has been shown to be important for effecting reliable transitions.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {236–243},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987090,
author = {Finnoff, W. and Hergert, F. and Zimmermann, H. G.},
title = {Extended Regularization Methods for Nonconvergent Model Selection},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Many techniques for model selection in the field of neural networks correspond to well established statistical methods. The method of 'stopped training', on the other hand, in which an oversized network is trained until the error on a further validation set of examples deteriorates, then training is stopped, is a true innovation, since model selection doesn't require convergence of the training process.In this paper we show that this performance can be significantly enhanced by extending the 'non convergent model selection method' of stopped training to include dynamic topology modifications (dynamic weight pruning) and modified complexity penalty term methods in which the weighting of the penalty term is adjusted during the training process.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {228–235},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987089,
author = {De Sa, Virginia R. and Ballard, Dana H.},
title = {A Note on Learning Vector Quantization},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Vector Quantization is useful for data compression. Competitive Learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data. Vector quantization of labelled data for classification has a different objective, to minimize the number of misclassifications, and a different algorithm is appropriate. We show that a variant of Kohonen's LVQ2.1 algorithm can be seen as a multiclass extension of an algorithm which in a restricted 2 class case can be proven to converge to the Bayes optimal classification boundary. We compare the performance of the LVQ2.1 algorithm to that of a modified version having a decreasing window and normalized step size, on a ten class vowel classification problem.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {220–227},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987088,
author = {Flower, Barry and Jabri, Marwan},
title = {Summed Weight Neuron Perturbation: An O(N) Improvement over Weight Perturbation},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The algorithm presented performs gradient descent on the weight space of an Artificial Neural Network (ANN), using a finite difference to approximate the gradient. The method is novel in that it achieves a computational complexity similar to that of Node Perturbation, O(N3), but does not require access to the activity of hidden or internal neurons. This is possible due to a stochastic relation between perturbations at the weights and the neurons of an ANN. The algorithm is also similar to Weight Perturbation in that it is optimal in terms of hardware requirements when used for the training of VLSI implementations of ANN's.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {212–219},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987087,
author = {Pratt, L. Y.},
title = {Discriminability-Based Transfer between Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm, called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {204–211},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987086,
author = {Paass, Gerhard},
title = {Assessing and Improving Neural Network Predictions by the Bootstrap Algorithm},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The bootstrap algorithm is a computational intensive procedure to derive nonparametric confidence intervals of statistical estimators in situations where an analytic solution is intractable. It is applied to neural networks to estimate the predictive distribution for unseen inputs. The consistency of different bootstrap procedures and their convergence speed is discussed. A small scale simulation experiment shows the applicability of the bootstrap to practical problems and its potential use.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {196–203},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987085,
author = {Littmann, E. and Ritter, H.},
title = {Generalization Abilities of Cascade Network Architecture},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In [5], a new incremental cascade network architecture has been presented. This paper discusses the properties of such cascade networks and investigates their generalization abilities under the particular constraint of small data sets. The evaluation is done for cascade networks consisting of local linear maps using the Mackey-Glass time series prediction task as a benchmark. Our results indicate that to bring the potential of large networks to bear on the problem of extracting information from small data sets without running the risk of overfitting, deeply cascaded network architectures are more favorable than shallow broad architectures that contain the same number of nodes.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {188–195},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987084,
author = {Sun, Guo-Zheng and Chen, Hsing-Hen and Lee, Yee-Chun},
title = {Time Warping Invariant Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We proposed a model of Time Warping Invariant Neural Networks (TWINN) to handle the time warped continuous signals. Although TWINN is a simple modification of well known recurrent neural network, analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem. It is also shown that TWINN has certain advantages over the current available sequential processing schemes: Dynamic Programming(DP)[1], Hidden Markov Model(HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite Automata(NNFA)[4].We also analyzed the time continuity employed in TWINN and pointed out that this kind of structure can memorize longer input history compared with Neural Network Finite Automata (NNFA). This may help to understand the well accepted fact that for learning grammatical reference with NNFA one had to start with very short strings in training set.The numerical example we used is a trajectory classification problem. This problem, making a feature of variable sampling rates, having internal states, continuous dynamics, heavily time-warped data and deformed phase space trajectories, is shown to be difficult to other schemes. With TWINN this problem has been learned in 100 iterations. For benchmark we also trained the exact same problem with TDNN and completely failed as expected.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {180–187},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987083,
author = {Zemel, Richard S. and Williams, Christopher K. I. and Mozer, Michael C.},
title = {Directional-Unit Boltzmann Machines},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a general formulation for a network of stochastic directional units. This formulation is an extension of the Boltzmann machine in which the units are not binary, but take on values in a cyclic range, between 0 and 2π radians. The state of each unit in a Directional-Unit Boltzmann Machine (DUBM) is described by a complex variable, where the phase component specifies a direction; the weights are also complex variables. We associate a quadratic energy function, and corresponding probability, with each DUBM configuration. The conditional distribution of a unit's stochastic state is a circular version of the Gaussian probability distribution, known as the von Mises distribution. In a mean-field approximation to a stochastic DUBM, the phase component of a unit's state represents its mean direction, and the magnitude component specifies the degree of certainty associated with this direction. This combination of a value and a certainty provides additional representational power in a unit. We describe a learning algorithm and simulations that demonstrate a mean-field DUBM'S ability to learn interesting mappings.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {172–179},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987082,
author = {Hassibi, Babak and Stork, David G.},
title = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {164–171},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987081,
author = {LeCun, Yann and Simard, Patrice Y. and Pearlmutter, Barak},
title = {Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We propose a very simple, and well principled way of computing the optimal step size in gradient descent algorithms. The on-line version is very efficient computationally, and is applicable to large backpropagation networks trained on large data sets. The main ingredient is a technique for estimating the principal eigenvalue(s) and eigenvector(s) of the objective function's second derivative matrix (Hessian), which does not require to even calculate the Hessian. Several other applications of this technique are proposed for speeding up learning, or for eliminating useless parameters.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {156–163},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987080,
author = {Guyon, I. and Boser, B. and Vapnik, V.},
title = {Automatic Capacity Tuning of Very Large VC-Dimension Classifiers},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {147–155},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987079,
author = {Chang, Eric I. and Lippmann, Richard P.},
title = {A Boundary Hunting Radial Basis Function Classifier Which Allocates Centers Constructively},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A new boundary hunting radial basis function (BH-RBF) classifier which allocates RBF centers constructively near class boundaries is described. This classifier creates complex decision boundaries only in regions where confusions occur and corresponding RBF outputs are similar. A predicted square error measure is used to determine how many centers to add and to determine when to stop adding centers. Two experiments are presented which demonstrate the advantages of the BH-RBF classifier. One uses artificial data with two classes and two input features where each class contains four clusters but only one cluster is near a decision region boundary. The other uses a large seismic database with seven classes and 14 input features. In both experiments the BH-RBF classifier provides a lower error rate with fewer centers than are required by more conventional RBF, Gaussian mixture, or MLP classifiers.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {139–146},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987078,
author = {Bonnlander, Brian V. and Mozer, Michael C.},
title = {Metamorphosis Networks: An Alternative to Constructive Models},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms--called metamorphosis algorithms--in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {131–138},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987077,
author = {Fritzke, Bernd},
title = {Kohonen Feature Maps and Growing Cell Structures: A Performance Comparison},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A performance comparison of two self-organizing networks, the Kohonen Feature Map and the recently proposed Growing Cell Structures is made. For this purpose several performance criteria for self-organizing networks are proposed and motivated. The models are tested with three example problems of increasing difficulty. The Kohonen Feature Map demonstrates slightly superior results only for the simplest problem. For the other more difficult and also more realistic problems the Growing Cell Structures exhibit significantly better performance by every criterion. Additional advantages of the new model are that all parameters are constant over time and that size as well as structure of the network are determined automatically.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {123–130},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987076,
author = {Ring, Mark},
title = {Learning Sequential Tasks by Incrementally Adding Higher Orders},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higher-order connections and incremental introduction of new units. The network adds higher orders when needed by adding new units that dynamically modify connection weights. Since the new units modify the weights at the next time-step with information from the previous step, temporal tasks can be learned without the use of feedback, thereby greatly simplifying training. Furthermore, a theoretically unlimited number of units can be added to reach into the arbitrarily distant past. Experiments with the Reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {115–122},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987075,
author = {Mahoney, J. Jeffrey and Mooney, Raymond J.},
title = {Combining Neural and Symbolic Learning to Revise Probabilistic Rule Bases},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes RAPTURE - a system for revising probabilistic knowledge bases that combines neural and symbolic learning methods. RAPTURE uses a modified version of backpropagation to refine the certainty factors of a MYCIN-style rule base and uses ID3's information gain heuristic to add new rules. Results on refining two actual expert knowledge bases demonstrate that this combined approach performs better than previous methods.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {107–114},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987074,
author = {Belew, Richard K.},
title = {Interposing an Ontogenetic Model between Genetic Algorithms and Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The relationships between learning, development and evolution in Nature is taken seriously, to suggest a model of the developmental process whereby the genotypes manipulated by the Genetic Algorithm (GA) might be expressed to form phenotypic neural networks (NNet) that then go on to learn. ONTOL is a grammar for generating polynomial NNets for time-series prediction. Genomes correspond to an ordered sequence of ONTOL productions and define a grammar that is expressed to generate a NNet. The NNet's weights are then modified by learning, and the individual's prediction error is used to determine GA fitness. A new gene doubling operator appears critical to the formation of new genetic alternatives in the preliminary but encouraging results presented.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {99–106},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987073,
author = {Judd, Stephen and Munro, Paul W.},
title = {Nets with Unreliable Hidden Nodes Learn Error-Correcting Codes},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In a multi-layered neural network, any one of the hidden layers can be viewed as computing a distributed representation of the input. Several "encoder" experiments have shown that when the representation space is small it can be fully used. But computing with such a representation requires completely dependable nodes. In the case where the hidden nodes are noisy and unreliable, we find that error correcting schemes emerge simply by using noisy units during training; random errors injected during backpropagation result in spreading representations apart. Average and minimum distances increase with misfire probability, as predicted by coding-theoretic considerations. Furthermore, the effect of this noise is to protect the machine against permanent node failure, thereby potentially extending the useful lifetime of the machine.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {89–96},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987072,
author = {Anderson, Charles W.},
title = {Q-Learning with Hidden-Unit Restarting},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Platt's resource-allocation network (RAN) (Platt, 1991a, 1991b) is modified for a reinforcement-learning paradigm and to "restart" existing hidden units rather than adding new units. After restarting, units continue to learn via back-propagation. The resulting restart algorithm is tested in a Q-learning network that learns to solve an inverted pendulum problem. Solutions are found faster on average with the restart algorithm than without it.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {81–88},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987071,
author = {Abu-Mostafa, Yaser S.},
title = {A Method for Learning from Hints},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We address the problem of learning an unknown function by putting together several pieces of information (hints) that we know about the function. We introduce a method that generalizes learning from examples to learning from hints. A canonical representation of hints is defined and illustrated for new types of hints. All the hints are represented to the learning process by examples, and examples of the function are treated on equal footing with the rest of the hints. During learning, examples from different hints are selected for processing according to a given schedule. We present two types of schedules; fixed schedules that specify the relative emphasis of each hint, and adaptive schedules that are based on how well each hint has been learned so far. Our learning method is compatible with any descent technique that we may choose to use.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {73–80},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987070,
author = {Das, Sreerupa and Giles, C. Lee and Sun, Guo-Zheng},
title = {Using Prior Knowledge in a NNPDA to Learn Context-Free Languages},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Although considerable interest has been shown in language inference and automata induction using recurrent neural networks, success of these models has mostly been limited to regular languages. We have previously demonstrated that Neural Network Pushdown Automaton (NNPDA) model is capable of learning deterministic context-free languages (e.g., anbn and parenthesis languages) from examples. However, the learning task is computationally intensive. In this paper we discus some ways in which a priori knowledge about the task and data could be used for efficient learning. We also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages (eg. anbncbmam).},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {65–72},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987069,
author = {Siu, Kai-Yeung and Roychowdhury, Vwani},
title = {Optimal Depth Neural Networks for Multiplication and Related Problems},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An artificial neural network (ANN) is commonly modeled by a threshold circuit, a network of interconnected processing units called linear threshold gates. The depth of a network represents the number of unit delays or the time for parallel computation. The Size of a circuit is the number of gates and measures the amount of hardware. It was known that traditional logic circuits consisting of only unbounded fan-in AND, OR, NOT gates would require at least Ω(log n/log log n) depth to compute common arithmetic functions such as the product or the quotient of two n-bit numbers, unless we allow the size (and fan-in) to increase exponentially (in n). We show in this paper that ANNs can be much more powerful than traditional logic circuits. In particular, we prove that that iterated addition can be computed by depth-2 ANN, and multiplication and division can be computed by depth-3 ANNs with polynomial size and polynomially bounded integer weights, respectively. Moreover, it follows from known lower bound results that these ANNs are optimal in depth. We also indicate that these techniques can be applied to construct polynomial-size depth-3 ANN for powering, and depth-4 ANN for multiple product.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {59–64},
numpages = {6},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987068,
author = {Simard, Patrice and Le Cun, Yann and Denker, John},
title = {Efficient Pattern Recognition Using a New Transformation Distance},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {50–58},
numpages = {9},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987067,
author = {Drucker, Harris and Schapire, Robert and Simard, Patrice},
title = {Improving Performance in Neural Networks Using a Boosting Algorithm},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {42–49},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987066,
author = {Plate, Tony A.},
title = {Holographic Recurrent Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Holographic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing sequential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs is found to be superior to that of ordinary recurrent networks on these sequence generation tasks.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {34–41},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987065,
author = {Wiles, Janet and Ollila, Mark},
title = {Intersecting Regions: The Key to Combinatorial Structure in Hidden Unit Space},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Hidden units in multi-layer networks form a representation space in which each region can be identified with a class of equivalent outputs (Elman, 1989) or a logical state in a finite state machine (Cleeremans, Servan-Schreiber &amp; McClelland, 1989; Giles, Sun, Chen, Lee, &amp; Chen, 1990). We extend the analysis of the spatial structure of hidden unit space to a combinatorial task, based on binding features together in a visual scene. The logical structure requires a combinatorial number of states to represent all valid scenes. On analysing our networks, we find that the high dimensionality of hidden unit space is exploited by using the intersection of neighboring regions to represent conjunctions of features. These results show how combinatorial structure can be based on the spatial nature of networks, and not just on their emulation of logical structure.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {27–33},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987064,
author = {Siu, Kai-Yeung and Roychowdhury, Vwani and Kailath, Thomas},
title = {Computing with Almost Optimal Size Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Artificial neural networks are comprised of an interconnected collection of certain nonlinear devices; examples of commonly used devices include linear threshold elements, sigmoidal elements and radial-basis elements. We employ results from harmonic analysis and the theory of rational approximation to obtain almost tight lower bounds on the size (i.e. number of elements) of neural networks. The class of neural networks to which our techniques can be applied is quite general; it includes any feedforward network in which each element can be piecewise approximated by a low degree rational function. For example, we prove that any depth-(d + 1) network of sigmoidal units or linear threshold elements computing the parity function of n variables must have Ω(dn1/d-≈) size, for any fixed ≈ &gt; 0. In addition, we prove that this lower bound is almost tight by showing that the parity function can be computed with O(dn1/d) sigmoidal units or linear threshold elements in a depth-(d + 1) network. These almost tight bounds are the first known complexity results on the size of neural networks with depth more than two. Our lower bound techniques yield a unified approach to the complexity analysis of various models of neural networks with feedforward structures. Moreover, our results indicate that in the context of computing highly oscillating symmetric Boolean functions, networks of continuous-output units such as sigmoidal elements do not offer significant reduction in size compared with networks of linear threshold elements of binary outputs.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {19–26},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987063,
author = {Stolcke, Andreas and Omohundro, Stephen},
title = {Hidden Markov Model Induction by Bayesian Model Merging},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {11–18},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@inproceedings{10.5555/2987061.2987062,
author = {Intrator, Nathan},
title = {On the Use of Projection Pursuit Constraints for Training Neural Networks},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a novel classification and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression (supervised training), to yield a new family of cost/complexity penalty terms. Some improved generalization properties are demonstrated on real world problems.},
booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
pages = {3–10},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'92}
}

@proceedings{10.5555/2987061,
title = {NIPS'92: Proceedings of the 5th International Conference on Neural Information Processing Systems},
year = {1992},
isbn = {1558602747},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
location = {Denver, Colorado}
}

