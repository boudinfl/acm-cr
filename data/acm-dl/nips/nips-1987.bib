@inproceedings{10.5555/2969644.2969734,
author = {Wyatt, J. L. and Standley, D. L.},
title = {A Method for the Design of Stable Lateral Inhibition Networks That is Robust in the Presence of Circuit Parasitics},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the analog VLSI implementation of neural systems, it is sometimes convenient to build lateral inhibition networks by using a locally connected on-chip resistive grid. A serious problem of unwanted spontaneous oscillation often arises with these circuits and renders them unusable in practice. This paper reports a design approach that guarantees such a system will be stable, even though the values of designed elements and parasitic elements in the resistive grid may be unknown. The method is based on a rigorous, somewhat novel mathematical analysis using Tellegen's theorem and the idea of Popov multipliers from control theory. It is thoroughly practical because the criteria are local in the sense that no overall analysis of the interconnected system is required, empirical in the sense that they involve only measurable frequency response data on the individual cells, and robust in the sense that unmodelled parasitic resistances and capacitances in the interconnection network cannot affect the analysis.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {860–867},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969733,
author = {Wittner, Ben S. and Denker, John S.},
title = {Strategies for Teaching Layered Networks Classification Tasks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {There is a widespread misconception that the delta-rule is in some sense guaranteed to work on networks without hidden units. As previous authors have mentioned, there is no such guarantee for classification tasks. We will begin by presenting explicit counterexamples illustrating two different interesting ways in which the delta rule can fail. We go on to provide conditions which do guarantee that gradient descent will successfully train networks without hidden units to perform two-category classification tasks. We discuss the generalization of our ideas to networks with hidden units and to multicategory classification tasks.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {850–859},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969732,
author = {Windecker, Richard C.},
title = {Learning in Networks of Nondeterministic Adaptive Logic Elements},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a model of nondeterministic adaptive automata that are constructed from simpler nondeterministic adaptive information processing elements. The first half of the paper describes the model. The second half discusses some of its significant adaptive properties using computer simulation examples. Chief among these properties is that network aggregates of the model elements can adapt appropriately when a single reinforcement channel provides the same positive or negative reinforcement signal to all adaptive elements of the network at the same time. This holds for multiple-input, multiple-output, multiple-layered, combinational and sequential networks. It also holds when some network elements are "hidden" in that their outputs are not directly seen by the external environment.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {840–849},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969731,
author = {Wechsler, Harry and Zimmerman, George Lee},
title = {Invariant Object Recognition Using a Distributed Associative Memory},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes an approach to 2-dimensional object recognition. Complex-log conformal mapping is combined with a distributed associative memory to create a system which recognizes objects regardless of changes in rotation or scale. Recalled information from the memorized database is used to classify an object, reconstruct the memorized version of the object, and estimate the magnitude of changes in scale or rotation. The system response is resistant to moderate amounts of noise and occlusion. Several experiments, using real, gray scale images, are presented to show the feasibility of our approach.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {830–839},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969730,
author = {Vidal, Jacques J. and Haggerty, John},
title = {Synchronization in Neural Nets},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The paper presents an artificial neural network concept (the Synchronizable Oscillator Networks) where the instants of individual firings in the form of point processes constitute the only form of information transmitted between joining neurons. This type of communication contrasts with that which is assumed in most other models which typically are continuous or discrete value-passing networks. Limiting the messages received by each processing unit to time markers that signal the firing of other units presents significant implementation advantages.In our model, neurons fire spontaneously and regularly in the absence of perturbation. When interaction is present, the scheduled firings are advanced or delayed by the firing of neighboring neurons. Networks of such neurons become global oscillators which exhibit multiple synchronizing attractors. From arbitrary initial states, energy minimization learning procedures can make the network converge to oscillatory modes that satisfy multi-dimensional constraints Such networks can directly represent routing and scheduling problems that consist of ordering sequences of events.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {824–829},
numpages = {6},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969729,
author = {Vecchi, Mario P. and Salehi, Jawad A.},
title = {Neuromorphic Networks Based on Sparse Optical Orthogonal Codes},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A family of neuromorphic networks specifically designed for communications and optical signal processing applications is presented. The information is encoded utilizing sparse Optical Orthogonal Code sequences on the basis of unipolar, binary (0,1) signals. The generalized synaptic connectivity matrix is also unipolar, and clipped to binary (0,1) values. In addition to high-capacity associative memory, the resulting neural networks can be used to implement general functions, such as code filtering, code mapping, code joining, code shifting and code projecting.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {814–823},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969728,
author = {Tomboulian, Sherryl},
title = {Introduction to a System for Implementing Neural Net Connections on Simd Architectures},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Neural networks have attracted much interest recently, and using parallel architectures to simulate neural networks is a natural and necessary application. The SIMD model of parallel computation is chosen, because systems of this type can be built with large numbers of processing elements. However, such systems are not naturally suited to generalized communication. A method is proposed that allows an implementation of neural network connections on massively parallel SIMD architectures. The key to this system is an algorithm that allows the formation of arbitrary connections between the "neurons". A feature is the ability to add new connections quickly. It also has error recovery ability and is robust over a variety of network topologies. Simulations of the general connection system, and its implementation on the Connection Machine, indicate that the time and space requirements are proportional to the product of the average number of connections per neuron and the diameter of the interconnection network.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {804–813},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969727,
author = {Tesauro, G. and Sejnowski, T. J.},
title = {A 'neural' Network That Learns to Play Backgammon},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a class of connectionist networks that have learned to play backgammon at an intermediate-to-advanced level. The networks were trained by a supervised learning procedure on a large set of sample positions evaluated by a human expert. In actual match play against humans and conventional computer programs, the networks demonstrate substantial ability to generalize on the basis of expert knowledge. Our study touches on some of the most important issues in network learning theory, including the development of efficient coding schemes and training procedures, scaling, generalization, the use of real-valued inputs and outputs, and techniques for escaping from local minima. Practical applications in games and other domains are also discussed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {794–803},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969726,
author = {Tenorio, Manoel F.},
title = {Using Neural Networks to Improve Cochlear Implant Speech Perception},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An increasing number of profoundly deaf patients suffering from sensorineural deafness are using cochlear implants as prostheses. After the implant, sound can be detected through the electrical stimulation of the remaining peripheral auditory nervous system. Although great progress has been achieved in this area, no useful speech recognition has been attained with either single or multiple channel cochlear implants.Coding evidence suggests that it is necessary for any implant which would effectively couple with the natural speech perception system to simulate the temporal dispersion and other phenomena found in the natural receptors, and currently not implemented in any cochlear implants. To this end, it is presented here a computational model using artificial neural networks (ANN) to incorporate the natural phenomena in the artificial cochlear.The ANN model presents a series of advantages to the implementation of such systems. First, the hardware requirements, with constraints on power, size, and processing speeds, can be taken into account together with the development of the underlining software, before the actual neural structures are totally defined. Second, the ANN model, since it is an abstraction of natural neurons, carries the necessary ingredients and is a close mapping for implementing the necessary functions. Third, some of the processing, like sorting and majority functions, could be implemented more efficiently, requiring only local decisions. Fourth, the ANN model allows function modifications through parametric modification (no software recoding), which permits a variety of fine-tuning experiments, with the opinion of the patients, to be conceived. Some of those will permit the user some freedom in system modification at real-time, allowing finer and more subjective adjustments to fit differences on the condition and operation of individual's remaining peripheral auditory system.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {783–793},
numpages = {11},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969725,
author = {Tagliarini, Gene A. and Page, Edward W.},
title = {A Neural-Network Solution to the Concentrator Assignment Problem},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Networks of simple analog processors having neuron-like properties have been employed to compute good solutions to a variety of optimization problems. This paper presents a neural-net solution to a resource allocation problem that arises in providing local access to the backbone of a wide-area communication network. The problem is described in terms of an energy function that can be mapped onto an analog computational network. Simulation results characterizing the performance of the neural computation are also presented.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {775–782},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969724,
author = {Suzuki, Hisashi and Arimoto, Suguru},
title = {Self-Organization of Associative Database and Its Applications},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An efficient method of self-organizing associative databases is proposed together with applications to robot eyesight systems. The proposed databases can associate any input with some output. In the first half part of discussion, an algorithm of self-organization is proposed. From an aspect of hardware, it produces a new style of neural network. In the latter half part, an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {767–774},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969723,
author = {Sun, G. Z. and Lee, Y. C. and Chen, H. H.},
title = {A Novel Net That Learns Sequential Decision Process},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new scheme to construct neural networks to classify patterns. The new scheme has several novel features : 1. We focus attention on the important attributes of patterns in ranking order. Extract the most important ones first and the less important ones later. 2. In training we use the information as a measure instead of the error function. 3. A multi-perceptron-like architecture is formed auomatically. Decision is made according to the tree structure of learned attributes. This new scheme is expected to self-organize and perform well in large scale problems.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {760–766},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969722,
author = {Stornetta, W. Scott and Hogg, Tad and Huberman, B. A.},
title = {A Dynamical Approach to Temporal Pattern Processing},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recognizing patterns with temporal context is important for such tasks as speech recognition, motion detection and signature verification. We propose an architecture in which time serves as its own representation, and temporal context is encoded in the state of the nodes. We contrast this with the approach of replicating portions of the architecture to represent time.As one example of these ideas, we demonstrate an architecture with capacitive inputs serving as temporal feature detectors in an otherwise standard back propagation model. Experiments involving motion detection and word discrimination serve to illustrate novel features of the system. Finally, we discuss possible extensions of the architecture.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {750–759},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969721,
author = {Stafylopatis, A. and Dikaiakos, M. and Kontoravdis, D.},
title = {Spatial Organization of Neural Networks: A Probabilistic Modeling Approach},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The aim of this paper is to explore the spatial organization of neural networks under Markovian assumptions, in what concerns the behaviour of individual cells and the interconnection mechanism. Space-organizational properties of neural nets are very relevant in image modeling and pattern analysis, where spatial computations on stochastic two-dimensional image fields are involved. As a first approach we develop a random neural network model, based upon simple probabilistic assumptions, whose organization is studied by means of discrete-event simulation. We then investigate the possibility of approximating the random network's behaviour by using an analytical approach originating from the theory of general product-form queueing networks. The neural network is described by an open network of nodes, in which customers moving from node to node represent stimulations and connections between nodes are expressed in terms of suitably selected routing probabilities. We obtain the solution of the model under different disciplines affecting the time spent by a stimulation at each node visited. Results concerning the distribution of excitation in the network as a function of network topology and external stimulation arrival pattern are compared with measures obtained from the simulation and validate the approach followed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {740–749},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969720,
author = {Smolensky, Paul},
title = {Analysis of Distributed Representation of Constituent Structure in Connectionist Systems},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A general method, the tensor product representation, is described for the distributed representation of value/variable bindings. The method allows the fully distributed representation of symbolic structures: the roles in the structures, as well as the fillers for those roles, can be arbitrarily non-local. Fully and partially localized special cases reduce to existing cases of connectionist representations of structured data; the tensor product representation generalizes these and the few existing examples of fully distributed representations of structures. The representation saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; it enables analysis of the interference of symbolic structures stored in associative memories; and it leads to characterization of optimal distributed representations of roles and a recirculation algorithm for learning them.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {730–739},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969719,
author = {Singer, Alexander and Donoghue, John P.},
title = {A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A synthetic neural network simulation of cerebral neocortex was developed based on detailed anatomy and physiology. Processing elements possess temporal nonlinearities and connection patterns similar to those of cortical neurons. The network was able to replicate spatial and temporal integration properties found experimentally in neocortex. A certain level of randomness was found to be crucial for the robustness of at least some of the network's computational capabilities. Emphasis was placed on how synthetic simulations can be of use to the study of both artificial and biological neural networks.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {715–729},
numpages = {15},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969718,
author = {Silverman, Ronald H. and Noetzel, Andrew S.},
title = {Time-Sequential Self-Organization of Hierarchical Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-organization of multi-layered networks can be realized by time-sequential organization of successive neural layers. Lateral inhibition operating in the surround of firing cells in each layer provides for unsupervised capture of excitation patterns presented by the previous layer. By presenting patterns of increasing complexity, in co-ordination with network self-organization, higher levels of the hierarchy capture concepts implicit in the pattern set.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {709–714},
numpages = {6},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969717,
author = {Siegel, Ralph M.},
title = {Discovering Structure from Motion in Monkey, Man and Machine},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The ability to obtain three-dimensional structure from visual motion is important for survival of human and non-human primates. Using a parallel processing model, the current work explores how the biological visual system might solve this problem and how the neurophysiologist might go about understanding the solution.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {701–708},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969716,
author = {Shepanski, J. F. and Macy, S. A.},
title = {Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have developed a methodology for manually training autonomous control systems based on artificial neural systems (ANS). In applications where the rule set governing an expert's decisions is difficult to formulate, ANS can be used to extract rules by associating the information an expert receives with the actions he takes. Properly constructed networks imitate rules of behavior that permits them to function autonomously when they are trained on the spanning set of possible situations. This training can be provided manually, either under the direct supervision of a system trainer, or indirectly using a background mode where the network assimilates training data as the expert performs his day-to-day tasks. To demonstrate these methods we have trained an ANS network to drive a vehicle through simulated freeway traffic.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {693–700},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969715,
author = {Scofield, Christopher L.},
title = {A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A single cell theory for the development of selectivity and ocular dominance in visual cortex has been presented previously by Bienenstock, Cooper and Munro. This has been extended to a network applicable to layer IV of visual cortex. In this paper we present a mean field approximation that captures in a fairly transparent manner the qualitative, and many of the quantitative, results of the network theory. Finally, we consider the application of this theory to artificial neural networks and show that a significant reduction in architectural complexity is possible.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {683–692},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969714,
author = {Scofield, Christopher L. and Reilly, Douglas L. and Elbaum, Charles and Cooper, Leon N.},
title = {Pattern Class Degeneracy in an Unrestricted Storage Density Memory},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The study of distributed memory systems has produced a number of models which work well in limited domains. However, until recently, the application of such systems to real-world problems has been difficult because of storage limitations, and their inherent architectural (and for serial simulation, computational) complexity. Recent development of memories with unrestricted storage capacity and economical feedforward architectures has opened the way to the application of such systems to complex pattern recognition problems. However, such problems are sometimes underspecified by the features which describe the environment, and thus a significant portion of the pattern environment is often non-separable. We will review current work on high density memory systems and their network implementations. We will discuss a general learning algorithm for such high density memories and review its application to separable point sets. Finally, we will introduce an extension of this method for learning the probability distributions of non-separable point sets.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {674–682},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969713,
author = {Schuling, F. H. and Mastebroek, H. A. K. and Zaagman, W. H.},
title = {An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recent experimental work on the stimulus velocity dependent time resolving power of the neural units, situated in the highest order optic ganglion of the blowfly, revealed the at first sight amazing phenomenon that at this high level of the fly visual system, the time constants of these units which are involved in the processing of neural activity evoked by moving objects, are -roughly spoken-inverse proportional to the velocity of those objects over an extremely wide range. In this paper we will discuss the implementation of a two dimensional heterodyne adaptive filter construction into a computer simulation model. The features of this simulation model include the ability to account for the experimentally observed stimulus-tuned adaptive temporal behaviour of time constants in the fly visual system. The simulation results obtained, clearly show that the application of such an adaptive processing procedure delivers an improved imaging technique of moving patterns in the high velocity range.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {662–673},
numpages = {12},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969712,
author = {Rosenfeld, Ronald and Touretzky, David S.},
title = {Scaling Properties of Coarse-Coded Symbol Memories},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Coarse-coded symbol memories have appeared in several neural network symbol processing models. In order to determine how these models would scale, one must first have some understanding of the mathematics of coarse-coded representations. We define the general structure of coarse-coded symbol memories and derive mathematical relationships among their essential parameters: memory size, symbol-set size and capacity. The computed capacity of one of the schemes agrees well with actual measurements of the coarse-coded working memory of DCPS, Touretzky and Hinton's distributed connectionist production system.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {652–661},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969711,
author = {Rosen, Bruce E. and Goodwin, James M. and Vidal, Jacques J.},
title = {Learning by State Recurrence Detection},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This research investigates a new technique for unsupervised learning of nonlinear control problems. The approach is applied both to Michie and Chambers BOXES algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and has significantly improved the convergence rate of stochastically based learning automata.Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits information found during learning trials to reinforce decisions resulting in the recurrence of nonfailing states. Recurrence learning applies positive reinforcement during the exploration of the search space, whereas in the BOXES or ASE algorithms, only negative weight reinforcement is applied, and then only on failure. Simulation results show that the added information from recurrence learning increases the learning rate.Our empirical results show that recurrence learning is faster than both basic failure driven learning and failure prediction methods. Although recurrence learning has only been tested in failure driven experiments, there are goal directed learning applications where detection of recurring oscillations may provide useful information that reduces the learning time by applying negative, instead of positive reinforcement.Detection of cycles provides a heuristic to improve the balance between evidence gathering and goal directed search.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {642–651},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969710,
author = {Robinson, A. J. and Fallside, F.},
title = {Static and Dynamic Error Propagation Networks with Application to Speech Coding},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns, and three possible architectures are explored. As an example, dynamic nets are applied to the problem of speech coding, in which a time sequence of speech data are coded by one net and decoded by another. The use of dynamic nets gives a better signal to noise ratio than that achieved using static nets.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {632–641},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969709,
author = {Hurlbert, Anya C. and Poggio, Tomaso A.},
title = {Learning a Color Algorithm from Examples},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A lightness algorithm that separates surface reflectance from illumination in a Mondrian world is synthesized automatically from a set of examples, pairs of input (image irradiance) and desired output (surface reflectance). The algorithm, which resembles a new lightness algorithm recently proposed by Land, is approximately equivalent to filtering the image through a center-surround receptive field in individual chromatic channels. The synthesizing technique, optimal linear estimation, requires only one assumption, that the operator that transforms input into output is linear. This assumption is true for a certain class of early vision algorithms that may therefore be synthesized in a similar way from examples. Other methods of synthesizing algorithms from examples, or "learning", such as backpropagation, do not yield a significantly different or better lightness algorithm in the Mondrian world. The linear estimation and backpropagation techniques both produce simultaneous brightness contrast effects.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {622–631},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969708,
author = {Platt, John C. and Barr, Alan H.},
title = {Constrained Differential Optimization},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many optimization models of neural networks need constraints to restrict the space of outputs to a subspace which satisfies external criteria. Optimizations using energy methods yield "forces" which act upon the state of the neural network. The penalty method, in which quadratic energy constraints are added to an existing optimization energy, has become popular recently, but is not guaranteed to satisfy the constraint conditions when there are other forces on the neural model or when there are multiple constraints. In this paper, we present the basic differential multiplier method (BDMM), which satisfies constraints exactly; we create forces which gradually apply the constraints over time, using "neurons" that estimate Lagrange multipliers.The basic differential multiplier method is a differential version of the method of multipliers from Numerical Analysis. We prove that the differential equations locally converge to a constrained minimum.Examples of applications of the differential method of multipliers include enforcing permutation codewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {612–621},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969707,
author = {Pineda, Fernando J.},
title = {Generalization of Backpropagation to Recurrent and Higher Order Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A general method for deriving backpropagation algorithms for networks with recurrent and higher order networks is introduced. The propagation of activation in these networks is determined by dissipative differential equations. The error signal is backpropagated by integrating an associated differential equation. The method is introduced by applying it to the recurrent generalization of the feedforward backpropagation network. The method is extended to the case of higher order networks and to a constrained dynamical system for training a content addressable memory. The essential feature of the adaptive algorithms is that adaptive equation has a simple outer product form.Preliminary experiments suggest that learning can occur very rapidly in networks with recurrent connections. The continuous formalism makes the new approach more suitable for implementation in VLSI.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {602–611},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969706,
author = {Petsche, Thomas and Dickinson, Bradley W.},
title = {A Trellis-Structured Neural Network},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have developed a neural network which consists of cooperatively interconnected Grossberg on-center off-surround subnets and which can be used to optimize a function related to the log likelihood function for decoding convolutional codes or more general FIR signal deconvolution problems. Connections in the network are confined to neighboring subnets, and it is representative of the types of networks which lend themselves to VLSI implementation. Analytical and experimental results for convergence and stability of the network have been found. The structure of the network can be used for distributed representation of data items while allowing for fault tolerance and replacement of faulty units.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {592–601},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969705,
author = {Noest, Andr\'{e} J.},
title = {Phasor Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A novel network type is introduced which uses unit-length 2-vectors for local variables. As an example of its applications, associative memory nets are defined and their performance analyzed. Real systems corresponding to such 'phasor' models can be e.g. (neuro)biological networks of limit-cycle oscillators or optical resonators that have a hologram in their feedback path.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {584–591},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969704,
author = {Murray, Alan F. and Smith, Anthony V. W. and Butler, Zoe F.},
title = {BIT: Serial Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A bit - serial VLSI neural network is described from an initial architecture for a synapse array through to silicon layout and board design. The issues surrounding bit - serial computation, and analog/digital arithmetic are discussed and the parallel development of a hybrid analog/digital neural network is outlined. Learning and recall capabilities are reported for the bit - serial network along with a projected specification for a 64 - neuron, bit - serial board operating at 20 MHz. This technique is extended to a 256 (2562 synapses) network with an update time of 3ms, using a "paging" technique to time - multiplex calculations through the synapse array.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {573–583},
numpages = {11},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969703,
author = {Moopenn, A. and Langenbacher, H. and Thakoor, A. P. and Khanna, S. K.},
title = {Programmable Synaptic Chip for Electronic Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A binary synaptic matrix chip has been developed for electronic neural networks. The matrix chip contains a programmable 32\texttimes{}32 array of "long channel" NMOSFET binary connection elements implemented in a 3-um bulk CMOS process. Since the neurons are kept off-chip, the synaptic chip serves as a "cascadable" building block for a multi-chip synaptic network as large as 512\texttimes{}512 in size. As an alternative to the programmable NMOSFET (long channel) connection elements, tailored thin film resistors are deposited, in series with FET switches, on some CMOS test chips, to obtain the weak synaptic connections. Although deposition and patterning of the resistors require additional processing steps, they promise substantial savings in silcon area. The performance of a synaptic chip in a 32- neuron breadboard system in an associative memory test application is discussed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {564–572},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969702,
author = {Michel, A. N. and Farrell, J. A. and Porod, W.},
title = {Stability Results for Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the present paper we survey and utilize results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative theory for the Hopfield model of neural networks. In our approach we view such networks as an interconnection of many single neurons. Our results are phrased in terms of the qualitative properties of the individual neurons and in terms of the properties of the interconnecting structure of the neural networks. Aspects of neural networks which we address include asymptotic stability, exponential stability, and instability of an equilibrium; estimates of trajectory bounds; estimates of the domain of attraction of an asymptotically stable equilibrium; and stability of neural networks under structural perturbations.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {554–563},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969701,
author = {Mel, Bartlett W.},
title = {Murphy: A Robot That Learns by Doing},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {MURPHY consists of a camera looking at a robot arm, with a connectionist network architecture situated in between. By moving its arm through a small, representative sample of the 1 billion possible joint configurations, MURPHY learns the relationships, backwards and forwards, between the positions of its joints and the state of its visual field. MURPHY can use its internal model in the forward direction to "envision" sequences of actions for planning purposes, such as in grabbing a visually presented object, or in the reverse direction to "imitate", with its arm, autonomous activity in its visual field. Furthermore, by taking explicit advantage of continuity in the mappings between visual space and joint space, MURPHY is able to learn non-linear mappings with only a single layer of modifiable weights.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {544–553},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969700,
author = {Marks, Robert J. and Atlas, Les E. and Oh, Seho and Ritcey, James A.},
title = {The Performance of Convex Set Projection Based Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider a class of neural networks whose performance can be analyzed and geometrically visualized in a signal space environment. Alternating projection neural networks (APNN's) perform by alternately projecting between two or more constraint sets. Criteria for desired and unique convergence are easily established. The network can be configured in either a homogeneous or layered form. The number of patterns that can be stored in the network is on the order of the number of input and hidden neurons. If the output neurons can take on only one of two states, then the trained layered APNN can be easily configured to converge in one iteration. More generally, convergence is at an exponential rate. Convergence can be improved by the use of sigmoid type nonlinearities, network relaxation and/or increasing the number of neurons in the hidden layer. The manner in which the network responds to data for which it was not specifically trained (i.e. how it generalizes) can be directly evaluated analytically.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {534–543},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969699,
author = {Marcus, C. M. and Westervelt, R. M.},
title = {Basins of Attraction for Electronic Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We have studied the basins of attraction for fixed point and oscillatory attractors in an electronic analog neural network. Basin measurement circuitry periodically opens the network feedback loop, loads raster-scanned initial conditions and examines the resulting attractor. Plotting the basins for fixed points (memories), we show that overloading an associative memory network leads to irregular basin shapes. The network also includes analog time delay circuitry, and we have shown that delay in symmetric networks can introduce basins for oscillatory attractors. Conditions leading to oscillation are related to the presence of frustration; reducing frustration by diluting the connections can stabilize a delay network.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {524–533},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969698,
author = {Mackie, Stuart and Graf, Hans P. and Schwartz, Daniel B. and Denker, John S.},
title = {Microelectronic Implementations of Connectionist Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we discuss why special purpose chips are needed for useful implementations of connectionist neural networks in such applications as pattern recognition and classification. Three chip designs are described: a hybrid digital/analog programmable connection matrix, an analog connection matrix with adjustable connection strengths, and a digital pipelined best-match chip. The common feature of the designs is the distribution of arithmetic processing power amongst the data storage to minimize data movement.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {515–523},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969697,
author = {MacDonald, Bruce A.},
title = {Connecting to the Past},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently there has been renewed interest in neural-like processing systems, evidenced for example in the two volumes Parallel Distributed Processing edited by Rumelhart and McClelland, and discussed as parallel distributed systems, connectionist models, neural nets, value passing systems and multiple context systems. Dissatisfaction with symbolic manipulation paradigms for artificial intelligence seems partly responsible for this attention, encouraged by the promise of massively parallel systems implemented in hardware. This paper relates simple neural-like systems based on multiple context to some other well-known formalisms-namely production systems, k-length sequence prediction, finite-state machines and Turing machines-and presents earlier sequence prediction results in a new light.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {505–514},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969696,
author = {Loos, Hendricus G.},
title = {Reflexive Associative Memories},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the synchronous discrete model, the average memory capacity of bidirectional associative memories (BAMs) is compared with that of Hopfield memories, by means of a calculation of the percentage of good recall for 100 random BAMs of dimension 64\texttimes{}64, for different numbers of stored vectors. The memory capacity is found to be much smaller than the Kosko upper bound, which is the lesser of the two dimensions of the BAM. On the average, a 64\texttimes{}64 BAM has about 68 % of the capacity of the corresponding Hopfield memory with the same number of neurons. Orthonormal coding of the BAM Increases the effective storage capacity by only 25 %. The memory capacity limitations are due to spurious stable states, which arise in BAMs in much the same way as in Hopfleld memories. Occurrence of spurious stable states can be avoided by replacing the thresholding in the backlayer of the BAM by another nonlinear process, here called "Dominant Label Selection" (DLS). The simplest DLS is the winner-take-all net, which gives a fault-sensitive memory. Fault tolerance can be improved by the use of an orthogonal or unitary transformation. An optical application of the latter is a Fourier transform, which is implemented simply by a lens.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {495–504},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969695,
author = {Linsker, Ralph},
title = {Towards an Organizing Principle for a Layered Perceptual Network},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An information-theoretic optimization principle is proposed for the development of each processing stage of a multilayered perceptual network. This principle of "maximum information preservation" states that the signal transformation that is to be realized at each stage is one that maximizes the information that the output signal values (from that stage) convey about the input signals values (to that stage), subject to certain constraints and in the presence of processing noise. The quantity being maximized is a Shannon information rate. I provide motivation for this principle and -- for some simple model cases -- derive some of its consequences, discuss an algorithmic implementation, and show how the principle may lead to biologically relevant neural architectural features such as topographic maps, map distortions, orientation selectivity, and extraction of spatial and temporal signal correlations. A possible connection between this information-theoretic principle and a principle of minimum entropy production in nonequilibrium thermodynamics is suggested.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {485–494},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969694,
author = {Leong, Harrison MonFook},
title = {Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {General formulae for mapping optimization problems into systems of ordinary differential equations associated with artificial neural networks are presented. A comparison is made to optimization using gradient-search methods. The performance measure is the settling time from an initial state to a target state. A simple analytical example illustrates a situation where dynamical systems representing artificial neural network methods would settle faster than those representing gradient-search. Settling time was investigated for a more complicated optimization problem using computer simulations. The problem was a simplified version of a problem in medical imaging: determining loci of cerebral activity from electromagnetic measurements at the scalp. The simulations showed that gradient based systems typically settled 50 to 100 times faster than systems based on current neural network optimization methods.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {474–484},
numpages = {11},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969693,
author = {Lehmann, D. and Brandeis, D. and Horst, A. and Ozaki, H. and Pal, I.},
title = {Spontaneous and Information-Triggered Segments of Series of Human Brain Electric Field Maps},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The brain works in a state-dependent manner: processing strategies and access to stored information depends on the momentary functional state which is continuously re-adjusted. The state is manifest as spatial configuration of the brain electric field. Spontaneous and information-triggered brain electric activity is a series of momentary field maps. Adaptive segmentation of spontaneous series into spatially stable epochs (states) exhibited 210 msec mean segments, discontinuous changes. Different maps imply different active neural populations, hence expectedly different effects on information processing: Reaction time differred between map classes at stimulus arrival. Segments might be units of brain information processing (content/mode/step), possibly operationalizing consciousness time. Related units (e.g. triggered by stimuli during figure perception and voluntary attention) might specify brain submechanisms of information treatment.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {467–473},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969692,
author = {Lau, Clifford and Honrubia, Vicente},
title = {Distributed Neural Information Processing in the Vestibulo-Ocular System},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A new distributed neural information-processing model is proposed to explain the response characteristics of the vestibulo-ocular system and to reflect more accurately the latest anatomical and neurophysiological data on the vestibular afferent fibers and vestibular nuclei. In this model, head motion is sensed topographically by hair cells in the semicircular canals. Hair cell signals are then processed by multiple synapses in the primary afferent neurons which exhibit a continuum of varying dynamics. The model is an application of the concept of "multilayered" neural networks to the description of findings in the bullfrog vestibular nerve, and allows us to formulate mathematically the behavior of an assembly of neurons whose physiological characteristics vary according to their anatomical properties.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {457–466},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969691,
author = {Lapedes, Alan and Farber, Robert},
title = {How Neural Nets Work},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {There is presently great interest in the abilities of neural networks to mimic "qualitative reasoning" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for "fuzzy" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net "number crunching" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn → Rm may be accurately approximated by a backpropagation neural net with just two "hidden" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {442–456},
numpages = {15},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969690,
author = {Kuh, Anthony},
title = {Performance Measures for Associative Memories That Learn and Forget},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, many modifications to the McCulloch/Pitts model have been proposed where both learning and forgetting occur. Given that the network never saturates (ceases to function effectively due to an overload of information), the learning updates can continue indefinitely. For these networks, we need to introduce performance measures in addition to the information capacity to evaluate the different networks. We mathematically define quantities such as the plasticity of a network, the efficacy of an information vector, and the probability of network saturation. From these quantities we analytically compare different networks.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {432–441},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969689,
author = {Koch, Christof and Luo, Jin and Mead, Carver and Hutchinson, James},
title = {Computing Motion Using Resistive Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {422–431},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969688,
author = {Keeler, James D.},
title = {Capacity for Patterns and Sequences in Kanerva's SDM as Compared to Other Associative Memory Models},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The information capacity of Kanerva's Sparse, Distributed Memory (SDM) and Hopfield-type neural networks is investigated. Under the approximations used here, it is shown that the total information stored in these systems is proportional to the number connections in the network. The proportionality constant is the same for the SDM and Hopfield-type models independent of the particular model, or the order of the model. The approximations are checked numerically. This same analysis can be used to show that the SDM can store sequences of spatiotemporal patterns, and the addition of time-delayed connections allows the retrieval of context dependent temporal patterns. A minor modification of the SDM can be used to store correlated patterns.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {412–421},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969687,
author = {Kanwal, Jagmeet S.},
title = {How the Processing Catfish Tracks Its Prey: An Interactive "Pipelined" System May Direct Foraging via Reticulospinal Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Ictalurid catfish use a highly developed gustatory system to localize, track and acquire food from their aquatic environment. The neural organization of the gustatory system illustrates well the importance of the four fundamental ingredients (representation, architecture, search and knowledge) of an "intelligent" system. In addition, the "pipelined" design of architecture illustrates how a goal-directed system effectively utilizes interactive feedback from its environment. Anatomical analysis of neural networks involved in target-tracking indicated that reticular neurons within the medullary region of the brainstem, mediate connections between the gustatory (sensory) inputs and the motor outputs of the spinal cord. Electrophysiological analysis suggested that these neurons integrate selective spatio-temporal patterns of sensory input transduced through a rapidly adapting-type peripheral filter (responding tonically only to a continuously increasing stimulus concentration). The connectivity and response patterns of reticular cells and the nature of the peripheral taste response suggest a unique "gustation-seeking" fUnction of reticulospinal cells, which may enable a catfish to continuously track a stimulus source once its directionality has been computed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {402–411},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969686,
author = {Jang, Ju-Seog and Lee, Soo-Young and Shin, Sang-Yung},
title = {An Optimization Network for Matrix Inversion},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Inverse matrix calculation can be considered as an optimization. We have demonstrated that this problem can be rapidly solved by highly interconnected simple neuron-like analog processors. A network for matrix inversion based on the concept of Hopfield's neural network was designed, and implemented with electronic hardware. With slight modifications, the network is readily applicable to solving a linear simultaneous equation efficiently. Notable features of this circuit are potential speed due to parallel processing, and robustness against variations of device parameters.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {397–401},
numpages = {5},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969685,
author = {Huang, William Y. and Lippmann, Richard P.},
title = {Neural Net and Traditional Classifiers},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Previous work on nets with continuous-valued inputs led to generative procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third "feature map classifier" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available. The architecture of this classifier can be used to implement a neural net k-nearest neighbor classifier.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {387–396},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969684,
author = {Hsu, Ken and Brady, David and Psaltis, Demetri},
title = {Experimental Demonstrations of Optical Neural Computers},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe two expriments in optical neural computing. In the first a closed optical feedback loop is used to implement auto-associative image recall. In the second a perceptron-like learning algorithm is implemented with photorefractive holography.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {377–386},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969683,
author = {Houk, James C.},
title = {Schema for Motor Control Utilizing a Network Model of the Cerebellum},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper outlines a schema for movement control based on two stages of signal processing. The higher stage is a neural network model that treats the cerebellum as an array of adjustable motor pattern generators. This network uses sensory input to preset and to trigger elemental pattern generators and to evaluate their performance. The actual patterned outputs, however, are produced by intrinsic circuitry that includes recurrent loops and is thus capable of self-sustained activity. These patterned outputs are sent as motor commands to local feedback systems called motor servos. The latter control the forces and lengths of individual muscles. Overall control is thus achieved in two stages: (1) an adaptive cerebellar network generates an array of feedforward motor commands and (2) a set of local feedback systems translates these commands into actual movements.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {367–376},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969682,
author = {Hinton, Geoffrey E. and McClelland, James L.},
title = {Learning Representations by Recirculation},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a "visible" group to be represented by activity vectors in a "hidden" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error, and the learning procedure aims to minimize this error. The learning procedure has two passes. On the first pass, the original visible vector is passed around the loop, and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the "presynaptic" activity and the difference in the post-synaptic activity on the two passes. This procedure is much simpler to implement than methods like back-propagation. Simulations in simple networks show that it usually converges rapidly on a good set of codes, and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {358–366},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969681,
author = {Hanson, Stephen Jos\'{e} and Burr, David J.},
title = {Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Many connectionist learning models are implemented using a gradient descent in a least squares error function of the output and teacher signal. The present model generalizes, in particular, back-propagation [1] by using Minkowski-r power metrics. For small r's a "city-block" error metric is approximated and for large r's the "maximum" or "supremum" metric is approached. while for r=2 the standard back-propagation model results. An implementation of Minkowski-r back-propagation is described, and several experiments are done which show that different values of r may be desirable for various purposes. Different r values may be appropriate for the reduction of the effects of outliers (noise), modeling the input space with more compact clusters, or modeling the statistics of a particular domain more naturally or in a way that may be more perceptually or psychologically meaningful (e.g. speech or vision).},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {348–357},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969680,
author = {Hammerstrom, Dan},
title = {The Connectivity Analysis of Simple Association},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The efficient realization, using current silicon technology, of Very Large Connection Networks (VLCN) with more than a billion connections requires that these networks exhibit a high degree of communication locality. Real neural networks exhibit significant locality, yet most connectionist/neural network models have little. In this paper, the connectivity requirements of a simple associative network are analyzed using communication theory. Several techniques based on communication theory are presented that improve the robustness of the network in the face of sparse, local interconnect structures. Also discussed are some potential problems when information is distributed too widely.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {338–347},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969679,
author = {Granger, Richard and Ambros-Ingerson, Jos\'{e} and Henry, Howard and Lynch, Gary},
title = {Partitioning of Sensory Data by a Cortical Network},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {To process sensory data, sensory brain areas must preserve information about both the similarities and differences among learned cues: without the latter, acuity would be lost, whereas without the former, degraded versions of a cue would be erroneously thought to be distinct cues, and would not be recognized. We have constructed a model of piriform cortex incorporating a large number of biophysical, anatomical and physiological parameters, such as two-step excitatory firing thresholds, necessary and sufficient conditions for long-term potentiation (LTP) of synapses, three distinct types of inhibitory currents (short IPSPs, long hyperpolarizing currents (LHP) and long cellspecific afterhyperpolarization (AHP)), sparse connectivity between bulb and layer-II cortex, caudally-flowing excitatory collateral fibers, nonlinear dendritic summation, etc. We have tested the model for its ability to learn similarity- and difference-preserving encodings of incoming sensory cue; the biological characteristics of the model enable it to produce multiple encodings of each input cue in such a way that different readouts of the cell firing activity of the model preserve both similarity and difference information.In particular, probabilistic quantal transmitter-release properties of piriform synapses give rise to probabilistic postsynaptic voltage levels which, in combination with the activity of local patches of inhibitory interneurons in layer II, differentially select bursting vs. single-pulsing layer-II cells. Time-locked firing to the theta rhythm (Larson and Lynch, 1986) enables distinct spatial patterns to be read out against a relatively quiescent background firing rate. Training trials using the physiological rules for induction of LTP yield stable layer-II-cell spatial firing patterns for learned cues. Multiple simulated olfactory input patterns (i.e., those that share many chemical features) will give rise to strongly-overlapping bulb firing patterns, activating many shared lateral olfactory tract (LOT) axons innervating layer Ia of piriform cortex, which in turn yields highly overlapping layer-II-cell excitatory potentials, enabling this spatial layer-II-cell encoding to preserve the overlap (similarity) among similar inputs. At the same time, those synapses that are enhanced by the learning process cause stronger cell firing, yielding strong, cell-specific afterhyperpolarizing (AHP) currents. Local inhibitory intemeurons effectively select alternate cells to fire once strongly-firing cells have undergone AHP. These alternate cells then activate their caudally-flowing recurrent collaterals, activating distinct populations of synapses in caudal layer lb. Potentiation of these synapses in combination with those of still-active LOT axons selectively enhance the response of caudal cells that tend to accentuate the differences among even very-similar cues.Empirical tests of the computer simulation have shown that, after training, the initial spatial layer II cell firing responses to similar cues enhance the similarity of the cues, such that the overlap in response is equal to or greater than the overlap in input cell firing (in the bulb): e.g., two cues that overlap by 65% give rise to response patterns that overlap by 80% or more. Reciprocally, later cell firing patterns (after AHP), increasingly enhance the differences among even very-similar patterns, so that cues with 90% input overlap give rise to output responses that overlap by less than 10%. This difference-enhancing response can be measured with respect to its acuity; since 90% input overlaps are reduced to near zero response overlaps, it enables the structure to distinguish between even very-similar cues. On the other hand, the similarity-enhancing response is properly viewed as a partitioning mechanism, mapping quite-distinct input cues onto nearly-identical response patterns (or category indicators). We therefore use a statistical metric for the information value of categorizations to measure the value of partitionings produced by the piriform simulation network.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {317–337},
numpages = {21},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969678,
author = {Golden, Richard M.},
title = {Probabilistic Characterization of Neural Model Computations},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Information retrieval in a neural network is viewed as a procedure in which the network computes a "most probable" or MAP estimate of the unknown information. This viewpoint allows the class of probability distributions, P, the neural network can acquire to be explicitly specified. Learning algorithms for the neural network which search for the "most probable" member of P can then be designed. Statistical tests which decide if the "true" or environmental probability distribution is in P can also be developed. Example applications of the theory to the highly nonlinear back-propagation learning algorithm, and the networks of Hopfield and Anderson are discussed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {310–316},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969677,
author = {Giles, C. L. and Griffin, R. D. and Maxwell, T.},
title = {Encoding Geometric Invariances in Higher-Order Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a method of constructing higher-order neural networks that respond invariantly under geometric transformations on the input space. By requiring each unit to satisfy a set of constraints on the interconnection weights, a particular structure is imposed on the network. A network built using such an architecture maintains its invariant performance independent of the values the weights assume, of the learning rules used, and of the form of the nonlinearities in the network. The invariance exhibited by a first-order network is usually of a trivial sort, e.g., responding only to the average input in the case of translation invariance, whereas higher-order networks can perform useful functions and still exhibit the invariance. We derive the weight constraints for translation, rotation, scale, and several combinations of these transformations, and report results of simulation studies.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {301–309},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969676,
author = {Gaudiano, Paolo},
title = {Temporal Patterns of Activity in Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Patterns of activity over real neural structures are known to exhibit time-dependent behavior. It would seem that the brain may be capable of utilizing temporal behavior of activity in neural networks as a way of performing functions which cannot otherwise be easily implemented. These might include the origination of sequential behavior and the recognition of time-dependent stimuli. A model is presented here which uses neuronal populations with recurrent feedback connections in an attempt to observe and describe the resulting time-dependent behavior. Shortcomings and problems inherent to this model are discussed. Current models by other researchers are reviewed and their similarities and differences discussed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {297–300},
numpages = {4},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969675,
author = {Gately, Michael T.},
title = {CYCLES: A Simulation Tool for Studying Cyclic Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A computer program has been designed and implemented to allow a researcher to analyze the oscillatory behavior of simulated neural networks with cyclic connectivity. The computer program, implemented on the Texas Instruments Explorer /Odyssey system, and the results of numerous experiments are discussed.The program, CYCLES, allows a user to construct, operate, and inspect neural networks containing cyclic connection paths with the aid of a powerful graphics-based interface. Numerous cycles have been studied, including cycles with one or more activation points, non-interruptible cycles, cycles with variable path lengths, and interacting cycles. The final class, interacting cycles, is important due to its ability to implement time-dependent goal processing in neural networks.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {290–296},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969674,
author = {Fleisher, Michael},
title = {The Hopfield Model with Multi-Level Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The Hopfield neural network model for associative memory is generalized. The generalization replaces two state neurons by neurons taking a richer set of values. Two classes of neuron input output relations are developed guaranteeing convergence to stable states. The first is a class of "continuous" relations and the second is a class of allowed quantization rules for the neurons. The information capacity for networks from the second class is found to be of order N3 bits for a network with N neurons.A generalization of the sum of outer products learning rule is developed and investigated as well.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {278–289},
numpages = {12},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969673,
author = {Fetz, Eberhard E.},
title = {Correlational Strength and Computational Algebra of Synaptic Connections between Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Intracellular recordings in spinal cord motoneurons and cerebral cortex neurons have provided new evidence on the correlational strength of monosynaptic connections, and the relation between the shapes of postsynaptic potentials and the associated increased firing probability. In these cells, excitatory postsynaptic potentials (EPSPs) produce cross-correlogram peaks which resemble in large part the derivative of the EPSP. Additional synaptic noise broadens the peak, but the peak area -- i.e., the number of above-chance firings triggered per EPSP -- remains proportional to the EPSP amplitude. A typical EPSP of 100 µv triggers about .01 firings per EPSP. The consequences of these data for information processing by polysynaptic connections is discussed. The effects of sequential polysynaptic links can be calculated by convolving the effects of the underlying monosynaptic connections. The net effect of parallel pathways is the sum of the individual contributions.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {270–277},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969672,
author = {Fern\'{a}ndez, Manuel F.},
title = {On Tropistic Processing and Its Applications},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The interaction of a set of tropisms is sufficient in many cases to explain the seemingly complex behavioral responses exhibited by varied classes of biological systems to combinations of stimuli. It can be shown that a straightforward generalization of the tropism phenomenon allows the efficient implementation of effective algorithms which appear to respond "intelligently" to changing environmental conditions. Examples of the utilization of tropistic processing techniques will be presented in this paper in applications entailing simulated behavior synthesis, path-planning, pattern analysis (clustering), and engineering design optimization.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {262–269},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969671,
author = {Ers\"{u}, E. and Tolle, H.},
title = {Hierarchical Learning Control: An Approach with Neuron-like Associative Memories},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Advances in brain theory need two complementary approaches: Analytical investigations by in situ measurements and as well synthetic modelling supported by computer simulations to generate suggestive hypothesis on purposeful structures in the neural tissue. In this paper research of the second line is described: Starting from a neurophysiologically inspired model of stimulus-response (S-R) and/or associative memorization and a psychologically motivated ministructure for basic control tasks, preconditions and conditions are studied for cooperation of such units in a hierarchical organisation, as can be assumed to be the general layout of macrostructures in the brain.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {249–261},
numpages = {13},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969670,
author = {Eeckman, Frank H.},
title = {The Sigmoid Nonlinearity in Prepyriform Cortex},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We report a study on the relationship between EEG amplitude values and unit spike output in the prepyriform cortex of awake and motivated rats. This relationship takes the form of a sigmoid curve, that describes normalized pulse-output for normalized wave input. The curve is fitted using nonlinear regression and is described by its slope and maximum value.Measurements were made for both excitatory and inhibitory neurons in the cortex. These neurons are known to form a monosynaptic negative feedback loop. Both classes of cells can be described by the same parameters.The sigmoid curve is asymmetric in that the region of maximal slope is displaced toward the excitatory side. The data are compatible with Freeman's model of prepyriform burst generation. Other analogies with existing neural nets are being discussed, and the implications for signal processing are reviewed. In particular the relationship of sigmoid slope to efficiency of neural computation is examined.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {242–248},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969669,
author = {Guyon, I. and Personnaz, L. and Nadal, J. P. and Dreyfus, G.},
title = {High Order Neural Networks for Efficient Associative Memory Design},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose learning rules for recurrent neural networks with high-order interactions between some or all neurons. The designed networks exhibit the desired associative memory function: perfect storage and retrieval of pieces of information and/or sequences of information of any complexity.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {233–241},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969668,
author = {Derthick, Mark and Tebelskis, Joe},
title = { 'Ensemble' Boltzmann Units Have Collective Computational Properties like Those of Hopfield and Tank Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {223–232},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969667,
author = {Denker, John S. and Wittner, Ben S.},
title = {Network Generality, Training Required, and Precision Required},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show how to estimate (1) the number of functions that can be implemented by a particular network architecture, (2) how much analog precision is needed in the connections in the network, and (3) the number of training examples the network must see before it can be expected to form reliable generalizations.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {219–222},
numpages = {4},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969666,
author = {Dembo, Amir and Zeitouni, Ofer},
title = {High Density Associative Memories},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A class of high density associative memories is constructed, starting from a description of desired properties those should exhibit. These properties include high capacity, controllable basins of attraction and fast speed of convergence. Fortunately enough, the resulting memory is implementable by an artificial Neural Net.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {211–218},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969665,
author = {Coon, D. D. and Perera, A. G. U.},
title = {New Hardware for Massive Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Transient phenomena associated with forward biased silicon p+ - n - n+ structures at 4.2K show remarkable similarities with biological neurons. The devices play a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent circuit diagrams. The devices provide simpler and more realistic neuron emulation than transistors or op-amps. They have such low power and current requirements that they could be used in massive neural networks. Some observed properties of simple circuits containing the devices include action potentials, refractory periods, threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic weights, temporal integration, memory, network connectivity modification based on experience, pacemaker activity, firing thresholds, coupling to sensors with graded signal outputs and the dependence of firing rate on input current. Transfer functions for simple artificial neurons with spiketrain inputs and spiketrain outputs have been measured and correlated with input coupling.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {201–210},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969664,
author = {Chover, Joshua},
title = {Phase Transitions in Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Various simulations of cortical subnetworks have evidenced something like phase transitions with respect to key parameters. We demonstrate that such transitions must indeed exist in analogous infinite array models. For related finite array models classical phase transitions (which describe steady-state behavior) may not exist, but there can be distinct qualitative changes in ("metastable") transient behavior as key system parameters pass through critical values.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {192–200},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969663,
author = {Chou, P. A.},
title = {The Capacity of the Kanerva Associative Memory is Exponential},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The capacity of an associative memory is defined as the maximum number of vords that can be stored and retrieved reliably by an address within a given sphere of attraction. It is shown by sphere packing arguments that as the address length increases, the capacity of any associative memory is limited to an exponential growth rate of 1 - h2(δ), where h2(δ) is the binary entropy function in bits, and δ is the radius of the sphere of attraction. This exponential growth in capacity can actually be achieved by the Kanerva associative memory, if its parameters are optimally set. Formulas for these optimal values are provided. The exponential growth in capacity for the Kanerva associative memory contrasts sharply with the sub-linear growth in capacity for the Hopfield associative memory.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {184–191},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969662,
author = {Chiueh, Tzi-Dar and Goodman, Rodney},
title = {A Neural Network Classifier Based on Coding Theory},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The new neural network classifier we propose transforms the classification problem into the coding theory problem of decoding a noisy codeword. An input vector in the feature space is transformed into an internal representation which is a codeword in the code space, and then error correction decoded in this space to classify the input feature vector to its class. Two classes of codes which give high performance are the Hadamard matrix code and the maximal length sequence code. We show that the number of classes stored in an N-neuron system is linear in N and significantly more than that obtainable by using the Hopfield type memory as a classifier.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {174–183},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969661,
author = {Cheung, John Y. and Omidvar, Massoud},
title = {Mathematical Analysis of Learning Behavior of Neuronal Models},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we wish to analyze the convergence behavior of a number of neuronal plasticity models. Recent neurophysiological research suggests that the neuronal behavior is adaptive. In particular, memory stored within a neuron is associated with the synaptic weights which are varied or adjusted to achieve learning. A number of adaptive neuronal models have been proposed in the literature. Three specific models will be analyzed in this paper, specifically the Hebb model, the Sutton-Barto model, and the most recent trace model. In this paper we will examine the conditions for convergence, the position of convergence and the rate at convergence, of these models as they applied to classical conditioning. Simulation results are also presented to verify the analysis.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {164–173},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969660,
author = {Carley, L. R.},
title = {Presynaptic Neural Information Processing},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The potential for presynaptic information processing within the arbor of a single axon will be discussed in this paper. Current knowledge about the activity dependence of the firing threshold, the conditions required for conduction failure, and the similarity of nodes along a single axon will be reviewed. An electronic circuit model for a site of low conduction safety in an axon will be presented. In response to single frequency stimulation the electronic circuit acts as a lowpass filter.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {154–163},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969659,
author = {Burr, D. J.},
title = {Speech Recognition Experiments with Perceptrons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Artificial neural networks (ANNs) are capable of accurate recognition of simple speech vocabularies such as isolated digits [1]. This paper looks at two more difficult vocabularies, the alphabetic E-set and a set of polysyllabic words. The E-set is difficult because it contains weak discriminants and polysyllables are difficult because of timing variation. Polysyllabic word recognition is aided by a time pre-alignment technique based on dynamic programming and E-set recognition is improved by focusing attention. Recognition accuracies are better than 98% for both vocabularies when implemented with a single layer perceptron.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {144–153},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969658,
author = {Bruck, Jehoshua and Goodman, Joseph W.},
title = {On the Power of Neural Networks for Solving Hard Problems},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper deals with a neural network model in which each neuron performs a threshold logic function. An important property of the model is that it always converges to a stable state when operating in a serial mode [2,5]. This property is the basis of the potential applications of the model such as associative memory devices and combinatorial optimization [3,6].One of the motivations for use of the model for solving hard combinatorial problems is the fact that it can be implemented by optical devices and thus operate at a higher speed than conventional electronics.The main theme in this work is to investigate the power of the model for solving NP-hard problems [4,8], and to understand the relation between speed of operation and the size of a neural network. In particular, it will be shown that for any NP-hard problem the existence of a polynomial size network that solves it implies that NP=co-NP. Also, for Traveling Salesman Problem (TSP), even a polynomial size network that gets an ε-approximate solution does not exist unless P=NP.The above results are of great practical interest, because right now it is possible to build neural networks which will operate fast but are limited in the number of neurons.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {137–143},
numpages = {7},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969657,
author = {Brown, Nathan H.},
title = {Neural Network Implementation Approaches for the Connection Machine},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The SIMD parallelism of the Connection Machine (eM) allows the construction of neural network simulations by the use of simple data and control structures. Two approaches are described which allow parallel computation of a model's nonlinear functions, parallel modification of a model's weights, and parallel propagation of a model's activation and error. Each approach also allows a model's interconnect structure to be physically dynamic. A Hopfield model is implemented with each approach at six sizes over the same number of CM processors to provide a performance comparison.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {127–136},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969656,
author = {Wilson, Matthew A. and Bower, James M.},
title = {A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Based on anatomical and physiological data, we have developed a computer simulation of piri-form (olfactory) cortex which is capable of reproducing spatial and temporal patterns of actual cortical activity under a variety of conditions. Using a simple Hebb-type learning rule in conjunction with the cortical dynamics which emerge from the anatomical and physiological organization of the model, the simulations are capable of establishing cortical representations for different input patterns. The basis of these representations lies in the interaction of sparsely distributed, highly divergent/convergent interconnections between modeled neurons. We have shown that different representations can be stored with minimal interference. and that following learning these representations are resistant to input degradation, allowing reconstruction of a representation following only a partial presentation of an original training stimulus. Further, we have demonstrated that the degree of overlap of cortical representations for different stimuli can also be modulated. For instance similar input patterns can be induced to generate distinct cortical representations (discrimination). while dissimilar inputs can be induced to generate overlapping representations (accommodation). Both features are presumably important in classifying olfactory stimuli.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {114–126},
numpages = {13},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969655,
author = {Wong, Yiu-Fai and Banik, Jashojiban and Bower, James M.},
title = {Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Much experimental study of real neural networks relies on the proper classification of extracellulary sampled neural signals (i.e. action potentials) recorded from the brains of experimental animals. In most neurophysiology laboratories this classification task is simplified by limiting investigations to single, electrically well-isolated neurons recorded one at a time. However, for those interested in sampling the activities of many single neurons simultaneously, waveform classification becomes a serious concern. In this paper we describe and constrast three approaches to this problem each designed not only to recognize isolated neural events, but also to separately classify temporally overlapping events in real time. First we present two formulations of waveform classification using a neural network template matching approach. These two formulations are then compared to a simple template matching implementation. Analysis with real neural signals reveals that simple template matching is a better solution to this problem than either neural network approach.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {103–113},
numpages = {11},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969654,
author = {Atiya, Amir F. and Bower, James M.},
title = {Optimal Neural Spike Classification},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Being able to record the electrical activities of a number of neurons simultaneously is likely to be important in the study of the functional organization of networks of real neurons. Using one extracellular microelectrode to record from several neurons is one approach to studying the response properties of sets of adjacent and therefore likely related neurons. However, to do this, it is necessary to correctly classify the signals generated by these different neurons. This paper considers this problem of classifying the signals in such an extracellular recording, based upon their shapes, and specifically considers the classification of signals in the case when spikes overlap temporally.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {95–102},
numpages = {8},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969653,
author = {Borg-Graham, Lyle J.},
title = {Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A computer model of the hippocampal pyramidal cell (HPC) is described which integrates data from a variety of sources in order to develop a consistent description for this cell type. The model presently includes descriptions of eleven non-linear somatic currents of the HPC, and the electrotonic structure of the neuron is modelled with a soma/short-cable approximation. Model simulations qualitatively or quantitatively reproduce a wide range of somatic electrical behavior in HPCs, and demonstrate possible roles for the various currents in information processing.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {82–94},
numpages = {13},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969652,
author = {Bernasconi, J.},
title = {Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate the behavior of different learning algorithms for networks of neuron-like units. As test cases we use simple pattern association problems, such as the XOR-problem and symmetry detection problems. The algorithms considered are either versions of the Boltzmann machine learning rule or based on the backpropagation of errors. We also propose and analyze a generalized delta rule for linear threshold units. We find that the performance of a given learning algorithm depends strongly on the type of units used. In particular, we observe that networks with ±1 units quite generally exhibit a significantly better learning behavior than the corresponding 0,1 versions. We also demonstrate that an adaption of the weight-structure to the symmetries of the problem can lead to a drastic increase in learning speed.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {72–81},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969651,
author = {Baxter, William and Dow, Bruce},
title = {Centric Models of the Orientation Map in Primary Visual Cortex},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In the visual cortex of the monkey the horizontal organization of the preferred orientations of orientation-selective cells follows two opposing rules: 1) neighbors tend to have similar orientation preferences, and 2) many different orientations are observed in a local region. Several orientation models which satisfy these constraints are found to differ in the spacing and the topological index of their singularities. Using the rate of orientation change as a measure, the models are compared to published experimental results.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {62–71},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969650,
author = {Baum, Eric B. and Wilczek, Frank},
title = {Supervised Learning of Probability Distributions by Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose that the back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {52–61},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969649,
author = {Baldi, Pierre and Venkatesh, Santosh S.},
title = {On Properties of Networks of Neuron-like Elements},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The complexity and computational capacity of multi-layered, feedforward neural networks is examined. Neural networks for special purpose (structured) functions are examined from the perspective of circuit complexity. Known results in complexity theory are applied to the special instance of neural network circuits, and in particular, classes of functions that can be implemented in shallow circuits characterised. Some conclusions are also drawn about learning complexity, and some open problems raised. The dual problem of determining the computational capacity of a class of multi-layered networks with dynamics regulated by an algebraic Hamiltonian is considered. Formal results are presented on the storage capacities of programmed higher-order structures, and a tradeoff between ease of programming and capacity is shown. A precise determination is made of the static fixed point structure of random higher-order constructs, and phase-transitions (0-1 laws) are shown.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {41–51},
numpages = {11},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969648,
author = {Homma, Toshiteru and Atlas, Les E. and Marks, Robert J.},
title = {An Artificial Neural Network for Spatiotemporal: Application to Phoneme Classification},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {An artificial neural network is developed to recognize spatio-temporal bipolar patterns associatively. The function of a formal neuron is generalized by replacing multiplication with convolution, weights with transfer functions, and thresholding with nonlinear transform following adaptation. The Hebbian learning rule and the delta learning rule are generalized accordingly, resulting in the learning of weights and delays. The neural network which was first developed for spatial patterns was thus generalized for spatio-temporal patterns. It was tested using a set of bipolar input patterns derived from speech signals, showing robust classification of 30 model phonemes.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {31–40},
numpages = {10},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969647,
author = {Atiya, Amir F.},
title = {Learning on a General Network},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper generalizes the backpropagation method to a general network containing feedback connections. The network model considered consists of interconnected groups of neurons, where each group could be fully interconnected (it could have feedback connections, with possibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent algorithm is applied, under a certain inequality constraint on each intra-group weight matrix which ensures for the network to possess a unique equilibrium state for every input.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {22–30},
numpages = {9},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969646,
author = {Alspector, Joshua and Allen, Robert B. and Hu, Victor and Satyanarayana, Srinagesh},
title = {Stochastic Learning Networks and Their Electronic Implementation},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a family of learning algorithms that operate on a recurrent, symmetrically connected, neuromorphic network that, like the Boltzmann machine, settles in the presence of noise. These networks learn by modifying synaptic connection strengths on the basis of correlations seen locally by each synapse. We describe a version of the supervised learning algorithm for a network with analog activation functions. We also demonstrate unsupervised competitive learning with this approach, where weight saturation and decay play an important role, and describe preliminary experiments in reinforcement learning, where noise is used in the search procedure. We identify the above described phenomena as elements that can unify learning techniques at a physical microscopic level.These algorithms were chosen for ease of implementation in vlsi. We have designed a CMOS test chip in 2 micron rules that can speed up the learning about a millionfold over an equivalent simulation on a VAX 11/780. The speedup is due to parallel analog computation for summing and multiplying weights and activations, and the use of physical processes for generating random noise. The components of the test chip are a noise amplifier, a neuron amplifier, and a 300 transistor adaptive synapse, each of which is separately testable. These components are also integrated into a 6 neuron and 15 synapse network. Finally, we point out techniques for reducing the area of the electronic correlational synapse both in technology and design and show how the algorithms we study can be implemented naturally in electronic systems.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {9–21},
numpages = {13},
series = {NIPS'87}
}

@inproceedings{10.5555/2969644.2969645,
author = {Abu-Mostafa, Yaser S.},
title = {Connectivity versus Entropy},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {How does the connectivity of a neural network (number of synapses per neuron) relate to the complexity of the problems it can handle (measured by the entropy)? Switching theory would suggest no relation at all, since all Boolean functions can be implemented using a circuit with very low connectivity (e.g., using two-input NAND gates). However, for a network that learns a problem from examples using a local learning rule, we prove that the entropy of the problem becomes a lower bound for the connectivity of the network.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {1–8},
numpages = {8},
series = {NIPS'87}
}

@proceedings{10.5555/2969644,
title = {NIPS'87: Proceedings of the 1987 International Conference on Neural Information Processing Systems},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

