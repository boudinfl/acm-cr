@inproceedings{10.1145/1995966.1995968,
author = {Contractor, Noshir},
title = {From Disasters to WOW: Using Web Science to Understand and Enable 21st Century Multidimensional Networks},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995968},
doi = {10.1145/1995966.1995968},
abstract = {Recent advances in Web Science provide comprehensive digital traces of social actions, interactions, and transactions. These data provide an unprecedented exploratorium to model the socio-technical motivations for creating, maintaining, dissolving, and reconstituting multidimensional social networks. Multidimensional networks include multiple types of nodes (people, documents, datasets, tags, etc.) and multiple types of relationships (co-authorship, citation, web links, etc). Using examples from research in a wide range of activities such as disaster response, science and engineering communities, public health and massively multiplayer online games (WoW - the World of Warcraft), Contractor will argue that Web Science serves as the foundation for the development of social network theories and methods to help advance our ability to understand and enable multidimensional networks.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {1–2},
numpages = {2},
keywords = {social networks, web science},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995969,
author = {Hall, Wendy},
title = {From Hypertext to Linked Data: The Ever Evolving Web},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995969},
doi = {10.1145/1995966.1995969},
abstract = {In this talk, we will reflect on the evolution of the Web. We will do this by analyzing the reasons why it became the first truly ubiquitous hypertext system against all competitors, and then by looking both at the way it has evolved from a network of linked documents to a system that facilitates social networking on a scale previously unimaginable, and at how it will evolve in the future as a network of linked data and beyond. The study of the Web - its evolution and its impact on society, on business, and on government - is referred to as Web science. We consider some of the major challenges of Web science and discuss possible Web worlds of the future.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {3–4},
numpages = {2},
keywords = {web science, ubiquitous hypertext, linked data},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995970,
author = {Hearst, Marti A.},
title = {Emerging Trends in Search User Interfaces},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995970},
doi = {10.1145/1995966.1995970},
abstract = {What does the future hold for search user interfaces? Following on a recently completed book on this topic, this talk identifies some important trends in the use of information technology and suggest how these may affect search in future. This includes is a notable trend towards more "natural" user interfaces, a trend towards social rather than solo usage of information technology, and a trend in technology advancing the integration of massive quantities of user behavior and large-scale knowledge bases. These trends are, or will be, interweaving in various ways, which will have some interesting ramifications for search interfaces, and should suggest promising directions for research.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {5–6},
numpages = {2},
keywords = {speech, search user interfaces, forecast, trends, social search, video, natural interfaces},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995972,
author = {Ashman, Helen and Antunovic, Michael and Chaprasit, Satit and Smith, Gavin and Truran, Mark},
title = {Implicit Association via Crowd-Sourced Coselection},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995972},
doi = {10.1145/1995966.1995972},
abstract = {The interaction of vast numbers of search engine users with sets of search results sets is a potential source of significant quantities of resource classification data. In this paper we discuss work which uses coselection data (i.e. multiple click-through events generated by the same user on a single search engine result page) as an indicator of mutual relevance between web resources and a means for the automatic clustering of sense-singular resources. The results indicate that coselection can be used in this way. We ground-truthed unambiguous query clustering, forming a foundation for work on automatic ambiguity detection based on the resulting number of generated clusters. Using the cluster overlap by population principle, the extension of previous work allowed determination of synonyms or lingual translations where overlapping clusters indicated the mutual relevance in coselection and subsequently the irrelevance of the actual label inherited from the user query.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {7–16},
numpages = {10},
keywords = {implicit relevance feedback, implicit association evaluation, web search analysis},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995973,
author = {Dai, Na and Qi, Xiaoguang and Davison, Brian D.},
title = {Bridging Link and Query Intent to Enhance Web Search},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995973},
doi = {10.1145/1995966.1995973},
abstract = {Understanding query intent is essential to generating appropriate rankings for users. Existing methods have provided customized rankings to answer queries with different intent. While previous methods have shown improvement over their non-discriminating counterparts, the web authors' intent when creating a hyperlink is seldom taken into consideration. To mitigate this gap, we categorize hyperlinks into two types that are reasonably comparable to query intent, i.e., links describing the target page's identity and links describing the target page's content. We argue that emphasis on one type of link when ranking documents can benefit the retrieval for that type of query. We start by presenting a link intent classification approach based on the link context representations that captures evidence from anchors, target pages, and their associated links, and then introduce our enhanced retrieval model that incorporates link intent into the estimation of anchor text importance. Comparative experiments on two large scale web corpora demonstrate the efficacy of our approaches.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {17–26},
numpages = {10},
keywords = {anchor text, query intent, link intent, Kronecker product, term weighting},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995974,
author = {Kawase, Ricardo and Papadakis, George and Herder, Eelco and Nejdl, Wolfgang},
title = {Beyond the Usual Suspects: Context-Aware Revisitation Support},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995974},
doi = {10.1145/1995966.1995974},
abstract = {A considerable amount of our activities on the Web involves revisits to pages or sites. Reasons for revisiting include active monitoring of content, verification of information, regular use of online services, and reoccurring tasks. Browsers support for revisitation is mainly focused on frequently and recently visited pages. In this paper we present a dynamic browser toolbar that provides recommendations beyond these usual suspects, balancing diversity and relevance. The recommendation method used is a combination of ranking and propagation methods. Experimental outcomes show that this algorithm performs significantly better than the baseline method. Further experiments address the question whether it is more appropriate to recommend specific pages or rather (portal pages of) Web sites. We conducted two user studies with a dynamic toolbar that relies on our recommendation algorithm. In this context, the outcomes confirm that users appreciate and use the contextual recommendations provided by the toolbar.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {27–36},
numpages = {10},
keywords = {contextual support, revisitation prediction, web behavior},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995975,
author = {Sah, Melike and Wade, Vincent},
title = {Automatic Mining of Cognitive Metadata Using Fuzzy Inference},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995975},
doi = {10.1145/1995966.1995975},
abstract = {Personalized search and browsing is increasingly vital especially for enterprises to able to reach their customers. Key challenge in supporting personalization is the need for rich metadata such as cognitive metadata about documents. As we consider size of large knowledge bases, manual annotation is not scalable and feasible. On the other hand, automatic mining of cognitive metadata is challenging since it is very difficult to understand underlying intellectual knowledge about documents automatically. To alleviate this problem, we introduce a novel metadata extraction framework, which is based on fuzzy information granulation and fuzzy inference system for automatic cognitive metadata mining. The user evaluation study shows that our approach provides reasonable precision rates for difficulty, interactivity type, and interactivity level on the examined 100 documents. In addition, proposed fuzzy inference system achieves improved results compared to a rule-based reasoner for document difficulty metadata extraction (11% improvement).},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {37–46},
numpages = {10},
keywords = {cognitive metadata, automatic metadata extraction, IEEE LOM, fuzzy inference, personalization},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995976,
author = {Seroussi, Yanir and Bohnert, Fabian and Zukerman, Ingrid},
title = {Personalised Rating Prediction for New Users Using Latent Factor Models},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995976},
doi = {10.1145/1995966.1995976},
abstract = {In recent years, personalised recommendations have gained importance in helping users deal with the abundance of information available online. Personalised recommendations are often based on rating predictions, and thus accurate rating prediction is essential for the generation of useful recommendations. Recently, rating prediction algorithms that are based on matrix factorisation have become increasingly popular, due to their high accuracy and scalability. However, these algorithms still deliver inaccurate rating predictions for new users, who submitted only a few ratings.In this paper, we address the new user problem by introducing several extensions to the basic matrix factorisation algorithm, which take user attributes into account when generating rating predictions. We consider both demographic attributes, explicitly supplied by users, and attributes inferred from user-generated texts. Our results show that employing our text-based user attributes yields personalised rating predictions that are more accurate than our baselines, while not requiring users to explicitly supply any information about themselves and their preferences.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {47–56},
numpages = {10},
keywords = {latent dirichlet allocation, new users, latent factor models, recommender systems, matrix factorization, rating prediction},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995977,
author = {Simko, Jakub and Tvarozek, Michal and Bielikova, Maria},
title = {Little Search Game: Term Network Acquisition via a Human Computation Game},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995977},
doi = {10.1145/1995966.1995977},
abstract = {Semantic structures, ranging from ontologies to flat folksonomies, are widely used on the Web despite the fact that their creation in sufficient quality is often a costly task. We propose a new approach for acquiring a lightweight network of related terms via the Little Search Game - a competitive browser game in search query formulation. The format of game queries forces players to express their perception of term relatedness. The term network is aggregated using "votes" from multiple players playing the same problem instance. We show that nearly 91% of the relationships produced by Little Search Game are correct and also elaborate on the game's unique ability to discover term relations, that are otherwise hidden to typical corpora mining methods.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {57–62},
numpages = {6},
keywords = {games with a purpose, human computing, term network},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995978,
author = {Smits, David and De Bra, Paul},
title = {GALE: A Highly Extensible Adaptive Hypermedia Engine},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995978},
doi = {10.1145/1995966.1995978},
abstract = {This paper presents GALE, the GRAPPLE Adaptive Learning Environment, which (contrary to what the word suggests) is a truly generic and general purpose adaptive hypermedia engine. Five years have passed since "The Design of AHA!" was published at ACM Hypertext (2006). GALE takes the notion of general-purpose a whole lot further. We solve shortcomings of existing adaptive systems in terms of genericity, extensibility and usability and show how GALE improves on the state of the art in all these aspects. We illustrate different authoring styles for GALE, including the use of template pages, and show how adaptation can be defined in a completely decentralized way by using the open corpus adaptation facility of GALE. GALE has been used in a number of adaptive hypermedia workshops and assignments to test whether authors can actually make use of the extensive functionality that GALE offers. Adaptation has been added to wiki sites, existing material e.g. from w3schools, and of course also to locally authored hypertext. Soon GALE will be used in cross-course adaptation at the TU/e in a pilot project to improve the success rate of university students.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {63–72},
numpages = {10},
keywords = {authoring, adaptive hypermedia, adaptation engine},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995979,
author = {Steichen, Ben and O'Connor, Alexander and Wade, Vincent},
title = {Personalisation in the Wild: Providing Personalisation across Semantic, Social and Open-Web Resources},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995979},
doi = {10.1145/1995966.1995979},
abstract = {One of the key motivating factors for information providers to use personalization is to maximise the benefit to the user in accessing their content. However, traditionally such systems have focussed on mainly corporate or professionally authored content and have not been able to leverage the benefits of other material already on the web, written about that subject by other authors. Such information includes open-web information as well as user-generated content such as forums, blogs, tags, etc. By providing personalized compositions and presentations across these heterogeneous information sources, a potentially richer user experience can be created, leveraging the mutual benefits of professionally authored content as well as open-web information and active user communities. This paper presents novel techniques and architectures that extend the personalization reserved for corporate or professionally developed content with that of user generated content and pages in the wild. Complementary affordances of Personalized Information Retrieval and Adaptive Hypermedia are leveraged in order to provide Adaptive Retrieval and Composition of Heterogeneous INformation sources for personalized hypertext Generation (ARCHING). The approach enables adaptive selection and navigation according to multiple adaptation dimensions and across a variety of heterogeneous data sources. The architectures have been applied in a real-life personalized customer care scenario and a user study evaluation involving authentic information needs has been conducted. The evidence clearly shows that the system successfully blends a user's search experience with adaptive selection and navigation techniques and that the user experience is improved in terms of both task assistance and user satisfaction.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {73–82},
numpages = {10},
keywords = {adaptive hypertext composition, adaptive result presentation, hypertext generation, personalized search},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995980,
author = {Takahashi, Yuku and Ohshima, Hiroaki and Yamamoto, Mitsuo and Iwasaki, Hirotoshi and Oyama, Satoshi and Tanaka, Katsumi},
title = {Evaluating Significance of Historical Entities Based on Tempo-Spatial Impacts Analysis Using Wikipedia Link Structure},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995980},
doi = {10.1145/1995966.1995980},
abstract = {We propose a method to evaluate the significance of historical entities (people, events, and so on.). Here, the significance of a historical entity means how it affected other historical entities. Our proposed method first calculates the tempo-spacial impact of historical entities. The impact of a historical entity varies according to time and location. Historical entities are collected from Wikipedia. We assume that a Wikipedia link between historical entities represents an impact propagation. That is, when an entity has a link to another entity, we regard the former is influenced by the latter. Historical entities in Wikipedia usually have the date and location of their occurrence. Our proposed iteration algorithm propagates such initial tempo-spacial information through links in the similar manner as PageRank, so the tempo-spacial impact scores of all the historical entities can be calculated. We assume that a historical entity is significant if it influences many other entities that are far from it temporally or geographically. We demonstrate a prototype system and show the results of experiments that prove the effectiveness of our method.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {83–92},
numpages = {10},
keywords = {pagerank, historical entities, historical entity importance, wikipedia structure analysis},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995981,
author = {Zubiaga, Arkaitz and K\"{o}rner, Christian and Strohmaier, Markus},
title = {Tags vs Shelves: From Social Tagging to Social Classification},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995981},
doi = {10.1145/1995966.1995981},
abstract = {Recent research has shown that different tagging motivation and user behavior can effect the overall usefulness of social tagging systems for certain tasks. In this paper, we provide further evidence for this observation by demonstrating that tagging data obtained from certain types of users - so-called Categorizers - outperforms data from other users on a social classification task. We show that segmenting users based on their tagging behavior has significant impact on the performance of automated classification of tagged data by using (i) tagging data from two different social tagging systems, (ii) a Support Vector Machine as a classification mechanism and (iii) existing classification systems such as the Library of Congress Classification System as ground truth. Our results are relevant for scientists studying pragmatics and semantics of social tagging systems as well as for engineers interested in influencing emerging properties of deployed social tagging systems.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {93–102},
numpages = {10},
keywords = {folksonomies, classification, libraries, tagging},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@dataset{10.1145/review-1995966.1995981_R47099,
author = {Sheng, Xinxin},
title = {Review ID:R47099 for DOI: 10.1145/1995966.1995981},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1995966.1995981_R47099}
}

@inproceedings{10.1145/1995966.1995983,
author = {Bernstein, Mark},
title = {Can We Talk about Spatial Hypertext},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995983},
doi = {10.1145/1995966.1995983},
abstract = {Spatial hypertexts are difficult to explain and to share because we have so little vocabulary with which to discuss them. From examination of actual spatial hypertexts drawn from a variety of domains and created in a variety of systems, we may identify and name several common patterns.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {103–112},
numpages = {10},
keywords = {patterns, diagrams, visualization, hypertext, knowledge representation, spatial hypertext, graphs},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995984,
author = {Jones, William and Anderson, Kenneth M.},
title = {Many Views, Many Modes, Many Tools ... One Structure: Towards a Non-Disruptive Integration of Personal Information},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995984},
doi = {10.1145/1995966.1995984},
abstract = {People yearn for more integration of their information. But tools meant to help often do the opposite-pulling people and their information in different directions. Fragmentation is potentially worsened as personal information moves onto the Web and into a myriad of special-purpose, mobile-enabled applications. How can tool developers innovate "non-disruptively" in ways that do not force people to re-organize or re-locate their information? This paper makes two arguments: 1. An integration of personal information is not likely to happen through some new release of a desktop operating system or via a Web-based "super tool." 2. Instead, integration is best supported through the development of a standards-based infrastructure that makes provision for the shared manipulation of common structure by any number of tools, each in its own way. To illustrate this approach, the paper describes an XML-based schema, considerations in its design and its current use in three separate tools. The schema in its design and use builds on the lessons learned by the open hypermedia and structural computing communities while moving forward with new techniques that address some of the changes introduced by the evolution of the term "application" to move beyond desktop apps to mobile apps, cloud-based apps and various hybrid architectures.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {113–122},
numpages = {10},
keywords = {PIM, application integration, structural computing., XML schemas, personal information management, open hypermedia},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995985,
author = {Petersen, Rasmus Rosenqvist and Wiil, Uffe Kock},
title = {Hypertext Structures for Investigative Teams},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995985},
doi = {10.1145/1995966.1995985},
abstract = {Investigations such as police investigations, intelligence analysis, and investigative journalism involves a number of complex knowledge management tasks. Investigative teams collect, process, and analyze information related to a specific target to create products that can be disseminated to their customers. This paper presents a novel hypertext-based tool that supports a human-centered, target-centric model for investigative teams. The model divides investigative tasks into five overall processes: acquisition, synthesis, sense-making, dissemination, and cooperation. The developed tool provides more comprehensive support for synthesis and sense-making tasks than existing tools.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {123–132},
numpages = {10},
keywords = {synthesis, cooperation, sense-making, acquisition, policing, dissemination, counterterrorism, investigative journalism},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995986,
author = {Solis, Carlos and Ali, Nour},
title = {An Experience Using a Spatial Hypertext Wiki},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995986},
doi = {10.1145/1995966.1995986},
abstract = {Most wikis do not allow users to collaboratively organize relations among wiki pages, nor ways to visualize them because such relations are hard to express using hyperlinks. The Spatial Hypertext Wiki (ShyWiki) is a wiki that uses Spatial Hypertext to represent visual and spatial implicit relations. This paper reports an experience about the use of ShyWiki features and its spatial hypertext model. Four groups, consisting of 3 members each, were asked to use ShyWiki for creating, sharing and brainstorming knowledge during the design and documentation of a software architecture. We present the evaluation of a questionnaire that users answered about their perceived usefulness and easiness of use of the spatial and visual properties of ShyWiki, and several of its features. We have also asked the users if they would find the visual and spatial properties useful in a wiki such as Wikipedia. In addition, we have analyzed the visual and spatial structures used in the wiki pages, and which features have been used.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {133–142},
numpages = {10},
keywords = {knowledge management, ShyWiki, Wiki, spatial hypertext},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995987,
author = {Van Woensel, William and Casteleyn, Sven and De Troyer, Olga},
title = {A Generic Approach for On-the-Fly Adding of Context-Aware Features to Existing Websites},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995987},
doi = {10.1145/1995966.1995987},
abstract = {More and more, mobile devices act as personal information managers and are able to obtain rich contextual information on the user's environment. Mobile, context-aware web applications can exploit this information to better address the needs of mobile users. Currently, such websites are either developed separately from their associated desktop-oriented version, or both versions are created simultaneously by employing methodologies that support multi-platform context-aware websites, requiring an extensive engineering effort. While these approaches provide a solution for developing new websites, they go past the plethora of existing websites. To address this issue, we present an approach for enhancing existing websites on-the-fly with context-aware features. We first discuss the requirements for such an adaptation process, and identify applicable adaptation methods to realize context-aware features. Next, we explain our generic approach, which is grounded in the use of semantic information extracted from existing websites. Finally, we present a concrete application of our approach that is based on the SCOUT framework for mobile and context-aware application development.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {143–152},
numpages = {10},
keywords = {client-side adaptation, context-aware, mobile web, semantic web},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995989,
author = {Chiabrando, Elisa and Likavec, Silvia and Lombardi, Ilaria and Picardi, Claudia and Theseider Dupr\'{e}, Daniele},
title = {Semantic Similarity in Heterogeneous Ontologies},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995989},
doi = {10.1145/1995966.1995989},
abstract = {Recent extensive usage of ontologies as knowledge bases that enable rigorous representation and reasoning over heterogenous data poses certain challenges in their construction and maintenance. Many of these ontologies are incomplete, containing many dense sub-ontologies. A need arises for a measure that would help calculate the similarity between the concepts in these kinds of ontologies. In this work, we introduce a new similarity measure for ontological concepts that takes these issues into account. It is based on conceptual specificity, which measures how much a certain concept is relevant in a given context, and on conceptual distance, which introduces different edge lengths in the ontology graph. We also address the problem of computing similarity between concepts in the presence of implicit classes in ontologies. The evaluation of our approach shows an improvement over Leacock and Chodorow's distance based measure. Finally, we provide two application domains which can benefit when this similarity measure is used.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {153–160},
numpages = {8},
keywords = {social semantic web, ontology, semantic similarity},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995990,
author = {De Choudhury, Munmun and Counts, Scott and Czerwinski, Mary},
title = {Identifying Relevant Social Media Content: Leveraging Information Diversity and User Cognition},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995990},
doi = {10.1145/1995966.1995990},
abstract = {As users turn to large scale social media systems like Twitter for topic-based content exploration, they quickly face the issue that there may be hundreds of thousands of items matching any given topic they might query. Given the scale of the potential result sets, how does one identify the 'best' or 'right' set of items? We explore a solution that aligns characteristics of the information space, including specific content attributes and the information diversity of the results set, with measurements of human information processing, including engagement and recognition memory. Using Twitter as a test bed, we propose a greedy iterative clustering technique for selecting a set of items on a given topic that matches a specified level of diversity.In a user study, we show that our proposed method yields sets of items that were, on balance, more engaging, better remembered, and rated as more interesting and informative compared to baseline techniques. Additionally, diversity indeed seemed to be important to participants in the study in the consumption of content. However as a rather surprising result, we also observe that content was perceived to be more relevant when it was highly homogeneous or highly heterogeneous. In this light, implications for the selection and evaluation of topic-centric item sets in social media contexts are discussed.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {161–170},
numpages = {10},
keywords = {twitter, user interfaces, information selection, information seeking, cognition, real-time search, social media},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995991,
author = {Gayo Avello, Daniel},
title = {All Liaisons Are Dangerous When All Your Friends Are Known to Us},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995991},
doi = {10.1145/1995966.1995991},
abstract = {Abstract Online Social Networks (OSNs) are used by millions of users worldwide. Academically speaking, there is little doubt about the usefulness of demographic studies conducted on OSNs and, hence, methods to label unknown users from small labeled samples are very useful. However, from the general public point of view, this can be a serious privacy concern. Thus, both topics are tackled in this paper: First, a new algorithm to perform user profiling in social networks is described, and its performance is reported and discussed. Secondly, the experiments --conducted on information usually considered sensitive-- reveal that by just publicizing one's contacts privacy is at risk and, thus, measures to minimize privacy leaks due to social graph data mining are outlined.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {171–180},
numpages = {10},
keywords = {privacy, online social networks, graph labeling, twitter},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995992,
author = {G\'{o}mez, Vicen\c{c} and Kappen, Hilbert J. and Kaltenbrunner, Andreas},
title = {Modeling the Structure and Evolution of Discussion Cascades},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995992},
doi = {10.1145/1995966.1995992},
abstract = {We analyze the structure and evolution of discussion cascades in four popular websites: Slashdot, Barrapunto, Meneame and Wikipedia. Despite the big heterogeneities between these sites, a preferential attachment (PA) model with bias to the root can capture the temporal evolution of the observed trees and many of their statistical properties, namely, probability distributions of the branching factors (degrees), subtree sizes and certain correlations. The parameters of the model are learned efficiently using a novel maximum likelihood estimation scheme for PA and provide a figurative interpretation about the communication habits and the resulting discussion cascades on the four different websites.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {181–190},
numpages = {10},
keywords = {threads, discussion cascades, conversations, slashdot, preferential attachment, maximum likelihood, wikipedia},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995993,
author = {Iturrioz, Jon and D\'{\i}az, Oscar and Azpeitia, Iker},
title = {Reactive Tags: Associating Behaviour to Prescriptive Tags},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995993},
doi = {10.1145/1995966.1995993},
abstract = {Social tagging is one of the hallmarks of Web2.0. The most common role of tags is descriptive. However, tags are being used for other purposes such as to indicate some actions to be conducted on the resource (e.g. 'toread'). This work focuses on 'prescriptive tags' that have associated some implicit behaviour in the user's mind. So far, little support is given for the automation of this "implicit behaviour", more to the point, if this behaviour is outside the tagging site. This paper introduces the notion of 'reactive tags' as a means for tagging to impact sites other than the tagging site itself. The operational semantics of reactive tags is defined through event-condition-action rules. Events are the action of tagging. Conditions check for additional data. Finally, rule's actions might impact someone else's account in a different website. The specification of this behaviour semantics is hidden through a graphical interface that permits users with no programming background to easily associate 'reactions' to the act of tagging. A working system, TABASCO, is presented as proof of concept.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {191–200},
numpages = {10},
keywords = {eca rules, interoperability, tagging, rule-ml, sioc},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995994,
author = {Laniado, David and Tasso, Riccardo},
title = {Co-Authorship 2.0: Patterns of Collaboration in Wikipedia},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995994},
doi = {10.1145/1995966.1995994},
abstract = {The study of collaboration patterns in wikis can help shed light on the process of content creation by online communities. To turn a wiki's revision history into a collaboration network, we propose an algorithm that identifies as authors of a page the users who provided the most of its relevant content, measured in terms of quantity and of acceptance by the community. The scalability of this approach allows us to study the English Wikipedia community as a co-authorship network. We find evidence of the presence of a nucleus of very active contributors, who seem to spread over the whole wiki, and to interact preferentially with inexperienced users. The fundamental role played by this elite is witnessed by the growing centrality of sociometric stars in the network. Isolating the community active around a category, it is possible to study its specific dynamics and most influential authors.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {201–210},
numpages = {10},
keywords = {wikipedia, social network analysis, online production, collaboration network},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995995,
author = {Liu, Xin and Murata, Tsuyoshi},
title = {Extracting the Mesoscopic Structure from Heterogeneous Systems},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995995},
doi = {10.1145/1995966.1995995},
abstract = {Heterogeneous systems in nature are often characterized by the mesoscopic structure known as communities. In this paper, we propose a framework to address the problem of community detection in bipartite networks and tripartite hypernetworks, which are appropriate models for many heterogeneous systems. The most important advantage of our method is that it is competent for detecting both communities of one-to-one correspondence and communities of many-to-many correspondence, while state of the art techniques can only handle the former. We demonstrate this advantage and show other desired properties of our method through extensive experiments in both synthetic and real-world datasets.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {211–220},
numpages = {10},
keywords = {bipartite network, community detection, tripartite hypernetwork, social tagging},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995996,
author = {Massa, Paolo},
title = {Social Networks of Wikipedia},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995996},
doi = {10.1145/1995966.1995996},
abstract = {Wikipedia, the free online encyclopedia anyone can edit, is a live social experiment: millions of individuals volunteer their knowledge and time to collective create it. It is hence interesting trying to understand how they do it. While most of the scholar attention focused on article pages, a less investigated share of activities happen on user talk pages, Wikipedia pages where a message can be left for the specific user. This public conversations can be studied from a Social Network Analysis perspective in order to highlight the structure of the "talk" network. In this paper we focus on this preliminary extraction step by proposing different algorithms. We then empirically validate the differences in the networks they generate on the Venetian Wikipedia with the real network of conversations extracted manually by coding every message left on all user talk pages. The comparisons show that both the algorithms and the manual process contain inaccuracies that are intrinsic in the freedom and unpredictability of Wikipedia syntax and practices. Nevertheless, a precise description of the involved issues allows to make informed decisions and to base empirical findings on reproducible evidence. Our goal is to lay the foundation for a solid computational sociology of wikis. For this reason we release the scripts encoding our algorithms as open source and also some datasets extracted out of Wikipedia conversations, in order to let other researchers replicate and improve our initial effort.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {221–230},
numpages = {10},
keywords = {wiki, social network, empirical analysis, open source, wikipedia},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995997,
author = {Nemoto, Keiichi and Gloor, Peter and Laubacher, Robert},
title = {Social Capital Increases Efficiency of Collaboration among Wikipedia Editors},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995997},
doi = {10.1145/1995966.1995997},
abstract = {In this study we measure the impact of pre-existing social capital on the efficiency of collaboration among Wikipedia editors. To construct a social network among Wikipedians we look to mutual interaction on the user talk pages of Wikipedia editors. As our data set, we analyze the communication networks associated with 3085 featured articles - the articles of highest quality in the English Wikipedia, comparing it to the networks of 80154 articles of lower quality. As the metric to assess the quality of collaboration, we measure the time of quality promotion from when an article is started until it is promoted to featured article. The study finds that the higher pre-existing social capital of editors working on an article is, the faster the articles they work on reach higher quality status, such as featured articles. The more cohesive and more centralized the collaboration network, and the more network members were already collaborating before starting to work together on an article, the faster the article they work on will be promoted or featured.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {231–240},
numpages = {10},
keywords = {time-to-market, social networks, collaboration, social capital, open source projects, Wikipedia, social network analysis, social media, community governance},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995998,
author = {Papagelis, Manos and Murdock, Vanessa and van Zwol, Roelof},
title = {Individual Behavior and Social Influence in Online Social Systems},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995998},
doi = {10.1145/1995966.1995998},
abstract = {The capacity to collect and analyze the actions of individuals in online social systems at minute-by-minute time granularity offers new perspectives on collective human behavior research. Macroscopic analysis of massive datasets raises interesting observations of patterns in online social processes. But working at a large scale has its own limitations, since it typically doesn't allow for interpretations on a microscopic level. We examine how different types of individual behavior affect the decisions of friends in a network. We begin with the problem of detecting social influence in a social system. Then we investigate the causality between individual behavior and social influence by observing the diffusion of an innovation among social peers. Are more active users more influential? Are more credible users more influential? Bridging this gap and finding points where the macroscopic and microscopic worlds converge contributes to better interpretations of the mechanisms of spreading of ideas and behaviors in networks and offer design opportunities for online social systems.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {241–250},
numpages = {10},
keywords = {behavior, geotagging, diffusion, social networks, influence},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1995999,
author = {Pera, Maria Soledad and Ng, Yiu-Kai},
title = {A Community Question-Answering Refinement System},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1995999},
doi = {10.1145/1995966.1995999},
abstract = {Community Question Answering (CQA) websites, which archive millions of questions and answers created by CQA users to provide a rich resource of information that is missing at web search engines and QA websites, have become increasingly popular. Web users who search for answers to their questions at CQA websites, however, are often required to either (i) wait for days until other CQA users post answers to their questions which might even be incorrect, offensive, or spam, or (ii) deal with restricted answer sets created by CQA websites due to the exact-match constraint that is employed and imposed between archived questions and user-formulated questions. To automate and enhance the process of locating high-quality answers to a user's question Q at a CQA website, we introduce a CQA refinement system, called QAR. Given Q, QAR first retrieves a set of CQA questions QS that are the same as, or similar to, Q in terms of its specified information need. Thereafter, QAR selects as answers to Q the top-ranked answers (among the ones to the questions in QS) based on various similarity scores and the length of the answers. Empirical studies, which were conducted using questions provided by the Text Retrieval Conference (TREC) and Text Analysis Conference (TAC), in addition to more than four millions questions (and their corresponding answers) extracted from Yahoo! Answers, show that QAR is effective in locating archived answers, if they exist, that satisfy the information need specified in Q. We have further assessed the performance of QAR by comparing its question-matching and answer-ranking strategies with their Yahoo! Answers' counterparts and verified that QAR outperforms Yahoo! Answers in (i) locating the set of questions QS that have the highest degrees of similarity with Q and (ii) ranking archived answers to QS as answers to Q.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {251–260},
numpages = {10},
keywords = {answer ranking, question matching, community question answering, word similarity measure},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996000,
author = {Squicciarini, Anna Cinzia and Sundareswaran, Smitha and Lin, Dan and Wede, Josh},
title = {A3P: Adaptive Policy Prediction for Shared Images over Popular Content Sharing Sites},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996000},
doi = {10.1145/1995966.1996000},
abstract = {More and more people go online today and share their personal images using popular web services like Picasa. While enjoying the convenience brought by advanced technology, people also become aware of the privacy issues of data being shared. Recent studies have highlighted that people expect more tools to allow them to regain control over their privacy. In this work, we propose an Adaptive Privacy Policy Prediction (A3P) system to help users compose privacy settings for their images. In particular, we examine the role of image content and metadata as possible indicators of users' privacy preferences. We propose a two-level image classification framework to obtain image categories which may be associated with similar policies. Then, we develop a policy prediction algorithm to automatically generate a policy for each newly uploaded image. Most importantly, the generated policy will follow the trend of the user's privacy concerns evolved with time. We have conducted an extensive user study and the results demonstrate effectiveness of our system with the prediction accuracy around 90%.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {261–270},
numpages = {10},
keywords = {images, privacy},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996001,
author = {Stewart, Avar\'{e} and Smith, Matthew and Nejdl, Wolfgang},
title = {A Transfer Approach to Detecting Disease Reporting Events in Blog Social Media},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996001},
doi = {10.1145/1995966.1996001},
abstract = {Event-Based Epidemic Intelligence (e-EI) has arisen as a body of work which relies upon different forms of pattern recognition in order to detect the disease reporting events from unstructured text that is present on the Web. Current supervised approaches to e-EI suffer both from high initial and high maintenance costs, due to the need to manually label examples to train and update a classifier for detecting disease reporting events in dynamic information sources, such as blogs.In this paper, we propose a new method for the supervised detection of disease reporting events. We tackle the burden of manually labelling data and address the problems associated with building a supervised learner to classify frequently evolving, and variable blog content. We automatically classify outbreak reports to train a supervised learner, and the knowledge acquired from the learning process is then transferred to the task of classifying blogs. Our experiments show that with the automatic classification of training data, and the transfer approach, we achieve an overall precision of 92% and an accuracy of 78.20%.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {271–280},
numpages = {10},
keywords = {transfer learning, epidemic intelligence, automatic labeling},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996002,
author = {Zhang, Lei and Liu, Bing},
title = {Entity Set Expansion in Opinion Documents},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996002},
doi = {10.1145/1995966.1996002},
abstract = {Opinion mining has been an active research area in recent years. The task is to extract opinions expressed on entities and their attributes. For example, the sentence, "I love the picture quality of Sony cameras," expresses a positive opinion on the picture quality attribute of Sony cameras. Sony is the entity. This paper focuses on mining entities (e.g., Sony). This is an important problem because without knowing the entity, the extracted opinion is of little use. The problem is similar to the classic named entity recognition problem. However, there is a major difference. In a typical opinion mining application, the user wants to find opinions on some competing entities, e.g., competing or relevant products. However, he/she often can only provide a few names as there are too many of them. The system has to find the rest from a corpus. This implies that the discovered entities must be of the same type/class. This is the set expansion problem. Classic methods for solving the problem are based on distributional similarity. However, we found this method is inaccurate. We then employ a learning-based method called Bayesian Sets. However, directly applying Bayesian Sets produces poor results. We then propose a more sophisticated way to use Bayesian Sets. This method, however, causes two major problems: entity ranking and feature sparseness. For entity ranking, we propose a re-ranking method to solve the problem. For feature sparseness, we propose two methods to re-weight features and to determine the quality of features. These methods help improve the mining results substantially. Additionally, like any learning algorithm, Bayesian Sets requires the user to engineer a set of features. We design some generic features based on part-of-speech tags of words for learning, which thus does not need to engineer features for each specific domain. Experimental results using 10 real-life datasets from diverse domains demonstrated the effectiveness of the proposed technique.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {281–290},
numpages = {10},
keywords = {text mining, set expansion, entity extraction},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996004,
author = {Chilukuri, Vinay and Indurkhya, Bipin},
title = {An Algorithm to Generate Engaging Narratives through Non-Linearity},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996004},
doi = {10.1145/1995966.1996004},
abstract = {The order in which the events of a story are presented plays an important role in story-telling. In this paper, we present an algorithm that generates narratives of different presentation orders for a story by taking its plan representation and the desired amount of non-linearity as input. We use the principles of event-indexing model, a cognitive model of narrative comprehension, to generate narratives without affecting the ease of comprehension. We hypothesize that a narrative deviated from its chronological order and presented without affecting the ease of comprehension might lead to cognitive engagement. Empirical evaluation of the system was conducted to test this hypothesis along with the amount of non-linearity that could be introduced in a story without affecting the ease of comprehension.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {291–298},
numpages = {8},
keywords = {event-indexing model, cognitive model, non-linear narrative, processing load, narrative comprehension, narrative generation, engagement},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996005,
author = {de Goede, Bart and Marx, Maarten and Nusselder, Arjan and van Wees, Justin},
title = {Succinct Summaries of Narrative Events Using Social Networks},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996005},
doi = {10.1145/1995966.1996005},
abstract = {This paper addresses the following research aim: provide a useful but succinct summary of long narrative events involving the interaction of several speakers. The summary should enable users to navigate to specific parts of the event using hyperlinks.Our solution is based on a representation of the main actors of the event and their interactions as a social network. The solution is applicable to events in which these interactions are more or less formally structured and detectable. This includes theatre and radio plays, recordings of a scientific workshop, proceedings of parliament and meetings notes in general.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {299–304},
numpages = {6},
keywords = {summarization, xml},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996006,
author = {Landow, George P.},
title = {The Victorian Web and the Victorian Course Wiki: Comparing the Educational Effectiveness of Identical Assignments in Web 1.0 and Web 2.0},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996006},
doi = {10.1145/1995966.1996006},
abstract = {In September 2008, the author delivered a keynote at WikiSym2008, in Porto, Portugal entitled "When a Wiki is not a Wiki: Twenty Years of the Victorian Web" in which he argued that the 45,000 documents that then made up www.victorianweb.org function as a moderated wiki and that, therefore, Web 1.0 can function for educational purposes much as Web 2.0 - and has done so for many years. Challenged to employ an actual wiki, Landow taught the same course with the same weekly student assignments in successive years (2009, 2010), the first using the website, the second a closed, password-protected wiki. After briefly describing the composition, history, and authorship of the Victorian Web, key parts of which have existed in multiple hypermedia environments since their creation in 1988 for the Brown University Intermedia project, it presents the assignment, explains its goals, and then sets forth the results of this experience, listing advantages and disadvantages of using the wiki for instructors, students, and the related website.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {305–312},
numpages = {8},
keywords = {wiki, education, connectivity, expository writing, evaluation, hypermedia, student-led discussion, student-centered discussion, web 2.0, assignments, the victorian web, hypertext},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996007,
author = {Pisarski, Mariusz},
title = {New Plots for Hypertext? Towards Poetics of a Hypertext Node},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996007},
doi = {10.1145/1995966.1996007},
abstract = {While the significance of hypertext links for the new ways of telling stories has been widely discussed, there has been not many debates about the very elements that are being connected: hypertext nodes. Apart from few exceptions, poetics of the link overshadows poetics of the node. My goal is to re-focus on a single node, or lexia, by introducing the concept of contextual regulation as the major force that shapes hypertext narrative units. Because many lexias must be capable of occurring in different contexts and at different stages of the unfolding story, several compromises have to be made on the level of language, style, plot and discourse. Each node, depending on its position and importance, has a varying level of connectivity and autonomy, which affects the global coherence of text.After focusing on relations between the notion of lexia (as a coherent and flexible unit) and the notion of kernel in narrative theory, an explanation of rules behind contextual regulation is presented, along with the basic typology of nodes. Then an attempt to enhance existing plot pools for hypertext fiction is undertaken. Several suggestions for the new plots, offered by the node-centered approach, are introduced.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {313–318},
numpages = {6},
keywords = {coherence, kernel, poetics, fiction, lexia, narrative, node, hypertext},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996008,
author = {Rowberry, Simon},
title = {Vladimir Nabokov's Pale Fire: The Lost 'Father of All Hypertext Demos'?},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996008},
doi = {10.1145/1995966.1996008},
abstract = {In the mid-sixties, Ted Nelson worked at Brown University on an early hypertext system. In 1969, IBM wanted to show the system at a conference, and Nelson gained permission to use Vladimir Nabokov's highly unconventional and hypertextual novel, Pale Fire (1962) as a technical demonstration of hypertext's potential. Unfortunately, the idea was dismissed in favor of a more technical-looking presentation, and thus was never demonstrated publicly. This paper re-considers Pale Fire's position in hypertext history, and posits that if it was used in this early hypertext demonstration, it would have been the 'father of all hypertext demonstrations' to complement Douglas Engelbart's 'Mother of All Demos' in 1968. In order to demonstrate the significance of Pale Fire's hypertextuality and Nelson's ambitions to use it, this paper will explore its hypertextual structure, the implication thereof for the novel and evaluate its success as a hypertext compared to electronic systems.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {319–324},
numpages = {6},
keywords = {narrative, literature, Ted Nelson, hypertext, structure, Vladimir Nabokov, networks, intertextuality},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1995966.1996009,
author = {Zsombori, Vilmos and Frantzis, Michael and Guimaraes, Rodrigo Laiola and Ursu, Marian Florin and Cesar, Pablo and Kegel, Ian and Craigie, Roland and Bulterman, Dick C.A.},
title = {Automatic Generation of Video Narratives from Shared UGC},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996009},
doi = {10.1145/1995966.1996009},
abstract = {This paper introduces an evaluated approach to the automatic generation of video narratives from user generated content gathered in a shared repository. In the context of social events, end-users record video material with their personal cameras and upload the content to a common repository. Video narrative techniques, implemented using Narrative Structure Language (NSL) and ShapeShifting Media, are employed to automatically generate movies recounting the event. Such movies are personalized according to the preferences expressed by each individual end-user, for each individual viewing. This paper describes our prototype narrative system, MyVideos, deployed as a web application, and reports on its evaluation for one specific use case: assembling stories of a school concert by parents, relatives and friends. The evaluations carried out through focus groups, interviews and field trials, in the Netherlands and UK, provided validating results and further insights into this approach.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {325–334},
numpages = {10},
keywords = {computational narrativity, interactive narrative, video, shapeshifting media, smil, media share, nsl, user generated content, interactive storytelling, interactive television, digital storytelling},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

