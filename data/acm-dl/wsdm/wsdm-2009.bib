@inproceedings{10.1145/1498759.1498761,
author = {Dean, Jeffrey},
title = {Challenges in Building Large-Scale Information Retrieval Systems: Invited Talk},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498761},
doi = {10.1145/1498759.1498761},
abstract = {Building and operating large-scale information retrieval systems used by hundreds of millions of people around the world provides a number of interesting challenges. Designing such systems requires making complex design tradeoffs in a number of dimensions, including (a) the number of user queries that must be handled per second and the response latency to these requests, (b) the number and size of various corpora that are searched, (c) the latency and frequency with which documents are updated or added to the corpora, and (d) the quality and cost of the ranking algorithms that are used for retrieval. In this talk I will discuss the evolution of Google's hardware infrastructure and information retrieval systems and some of the design challenges that arise from ever-increasing demands in all of these dimensions. I will also describe how we use various pieces of distributed systems infrastructure when building these retrieval systems. Finally, I will describe some future challenges and open research problems in this area.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {1},
numpages = {1},
keywords = {scalability, search engines},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498762,
author = {Kumar, Ravi},
title = {Online Social Networks: Modeling and Mining: Invited Talk},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498762},
doi = {10.1145/1498759.1498762},
abstract = {Online social networks have become major and driving phenomena on the Web. In this talk, we will address key modeling and algorithmic questions related to large online social networks. From the modeling perspective, we raise the question of whether there is a generative model for network evolution. The availability of time-stamped data makes it possible to study this question at an extremely fine granularity. We exhibit a simple, natural model that leads to synthetic networks with properties similar to the online ones. From an algorithmic viewpoint, we focus on data mining challenges posed by the magnitude of data in these networks. In particular, we examine topics related to influence and correlation in user activities and compressibility of such networks.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {2},
numpages = {1},
keywords = {social networks, evolution, influence, graph models},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498763,
author = {Weikum, Gerhard},
title = {Harvesting, Searching, and Ranking Knowledge on the Web: Invited Talk},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498763},
doi = {10.1145/1498759.1498763},
abstract = {There are major trends to advance the functionality of search engines to a more expressive semantic level (e.g., [2, 4, 6, 7, 8, 9, 13, 14, 18]). This is enabled by employing large-scale information extraction [1, 11, 20] of entities and relationships from semistructured as well as natural-language Web sources. In addition, harnessing Semantic-Web-style ontologies [22] and reaching into Deep-Web sources [16] can contribute towards a grand vision of turning the Web into a comprehensive knowledge base that can be efficiently searched with high precision.This talk presents ongoing research towards this objective, with emphasis on our work on the YAGO knowledge base [23, 24] and the NAGA search engine [14] but also covering related projects. YAGO is a large collection of entities and relational facts that are harvested from Wikipedia and WordNet with high accuracy and reconciled into a consistent RDF-style "semantic" graph. For further growing YAGO from Web sources while retaining its high quality, pattern-based extraction is combined with logic-based consistency checking in a unified framework [25]. NAGA provides graph-template-based search over this data, with powerful ranking capabilities based on a statistical language model for graphs. Advanced queries and the need for ranking approximate matches pose efficiency and scalability challenges that are addressed by algorithmic and indexing techniques [15, 17].YAGO is publicly available and has been imported into various other knowledge-management projects including DB-pedia. YAGO shares many of its goals and methodologies with parallel projects along related lines. These include Avatar [19], Cimple/DBlife [10, 21], DBpedia [3], Know-ItAll/TextRunner [12, 5], Kylin/KOG [26, 27], and the Libra technology [18, 28] (and more). Together they form an exciting trend towards providing comprehensive knowledge bases with semantic search capabilities.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {3–4},
numpages = {2},
keywords = {information retrieval, Information extraction, knowledge management, scalability},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498766,
author = {Agrawal, Rakesh and Gollapudi, Sreenivas and Halverson, Alan and Ieong, Samuel},
title = {Diversifying Search Results},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498766},
doi = {10.1145/1498759.1498766},
abstract = {We study the problem of answering ambiguous web queries in a setting where there exists a taxonomy of information, and that both queries and documents may belong to more than one category according to this taxonomy. We present a systematic approach to diversifying results that aims to minimize the risk of dissatisfaction of the average user. We propose an algorithm that well approximates this objective in general, and is provably optimal for a natural special case. Furthermore, we generalize several classical IR metrics, including NDCG, MRR, and MAP, to explicitly account for the value of diversification. We demonstrate empirically that our algorithm scores higher in these generalized metrics compared to results produced by commercial search engines.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {5–14},
numpages = {10},
keywords = {marginal utility, relevance, result diversification},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498786,
author = {Teevan, Jaime and Morris, Meredith Ringel and Bush, Steve},
title = {Discovering and Using Groups to Improve Personalized Search},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498786},
doi = {10.1145/1498759.1498786},
abstract = {Personalized Web search takes advantage of information about an individual to identify the most relevant results for that person. A challenge for personalization lies in collecting user profiles that are rich enough to do this successfully. One way an individual's profile can be augmented is by using data from other people. To better understand whether groups of people can be used to benefit personalized search, we explore the similarity of query selection, desktop information, and explicit relevance judgments across people grouped in different ways. The groupings we explore fall along two dimensions: the longevity of the group members' relationship, and how explicitly the group is formed. We find that some groupings provide valuable insight into what members consider relevant to queries related to the group focus, but that it can be difficult to identify valuable groups implicitly. Building on these findings, we explore an algorithm to "groupize" (versus "personalize") Web search results that leads to a significant improvement in result ranking on group-relevant queries.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {15–24},
numpages = {10},
keywords = {collaborative search, collaborative filtering, personalization},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498805,
author = {Seki, Kazuhiro and Uehara, Kuniaki},
title = {Adaptive Subjective Triggers for Opinionated Document Retrieval},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498805},
doi = {10.1145/1498759.1498805},
abstract = {This paper proposes a novel application of a statistical language model to opinionated document retrieval targeting weblogs (blogs). In particular, we explore the use of the trigger model---originally developed for incorporating distant word dependencies---in order to model the characteristics of personal opinions that cannot be properly modeled by standard n-grams. Our primary assumption is that there are two constituents to form a subjective opinion. One is the subject of the opinion or the object that the opinion is about, and the other is a subjective expression; the former is regarded as a triggering word and the latter as a triggered word. We automatically identify those subjective trigger patterns to build a language model from a corpus of product customer reviews. Experimental results on the TREC Blog Track test collections show that, when used for reranking initial search results, our proposed model significantly improves opinionated document retrieval by over 20% in MAP. In addition, we report on an experiment on dynamic adaptation of the model to a given query, which is found effective for most of difficult queries categorized under politics and organizations.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {25–33},
numpages = {9},
keywords = {trigger language models, weblog, opinion retrieval},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498806,
author = {Yang, Yin and Bansal, Nilesh and Dakka, Wisam and Ipeirotis, Panagiotis and Koudas, Nick and Papadias, Dimitris},
title = {Query by Document},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498806},
doi = {10.1145/1498759.1498806},
abstract = {We are experiencing an unprecedented increase of content contributed by users in forums such as blogs, social networking sites and microblogging services. Such abundance of content complements content on web sites and traditional media forums such as news papers, news and financial streams, and so on. Given such plethora of information there is a pressing need to cross reference information across textual services. For example, commonly we read a news item and we wonder if there are any blogs reporting related content or vice versa.In this paper, we present techniques to automate the process of cross referencing online information content. We introduce methodologies to extract phrases from a given "query document" to be used as queries to search interfaces with the goal to retrieve content related to the query document. In particular, we consider two techniques to extract and score key phrases. We also consider techniques to complement extracted phrases with information present in external sources such as Wikipedia and introduce an algorithm called RelevanceRank for this purpose.We discuss both these techniques in detail and provide an experimental study utilizing a large number of human judges from Amazons's Mechanical Turk service. Detailed experiments demonstrate the effectiveness and efficiency of the proposed techniques for the task of automating retrieval of documents related to a query document.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {34–43},
numpages = {10},
keywords = {web 2.0, similarity matching, Wikipedia, blog},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498807,
author = {Koolen, Marijn and Kazai, Gabriella and Craswell, Nick},
title = {Wikipedia Pages as Entry Points for Book Search},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498807},
doi = {10.1145/1498759.1498807},
abstract = {A lot of the world's knowledge is stored in books, which, as a result of recent mass-digitisation efforts, are increasingly available online. Search engines, such as Google Books, provide mechanisms for searchers to enter this vast knowledge space using queries as entry points. In this paper, we view Wikipedia as a summary of this world knowledge and aim to use this resource to guide users to relevant books. Thus, we investigate possible ways of using Wikipedia as an intermediary between the user's query and a collection of books being searched. We experiment with traditional query expansion techniques, exploiting Wikipedia articles as rich sources of information that can augment the user's query. We then propose a novel approach based on link distance in an extended Wikipedia graph: we associate books with Wikipedia pages that cite these books and use the link distance between these nodes and the pages that match the user query as an estimation of a book's relevance to the query. Our results show that a) classical query expansion using terms extracted from query pages leads to increased precision, and b) link distance between query and book pages in Wikipedia provides a good indicator of relevance that can boost the retrieval score of relevant books in the result ranking of a book search engine.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {44–53},
numpages = {10},
keywords = {Wikipedia, domain specific, link graph, query expansion},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498809,
author = {Ramage, Daniel and Heymann, Paul and Manning, Christopher D. and Garcia-Molina, Hector},
title = {Clustering the Tagged Web},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498809},
doi = {10.1145/1498759.1498809},
abstract = {Automatically clustering web pages into semantic groups promises improved search and browsing on the web. In this paper, we demonstrate how user-generated tags from large-scale social bookmarking websites such as del.icio.us can be used as a complementary data source to page text and anchor text for improving automatic clustering of web pages. This paper explores the use of tags in 1) K-means clustering in an extended vector space model that includes tags as well as page text and 2) a novel generative clustering algorithm based on latent Dirichlet allocation that jointly models text and tags. We evaluate the models by comparing their output to an established web directory. We find that the naive inclusion of tagging data improves cluster quality versus page text alone, but a more principled inclusion can substantially improve the quality of all models with a statistically significant absolute F-score increase of 4%. The generative model outperforms K-means with another 8% F-score increase.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {54–63},
numpages = {10},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498810,
author = {Overell, Simon and Sigurbj\"{o}rnsson, B\"{o}rkur and van Zwol, Roelof},
title = {Classifying Tags Using Open Content Resources},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498810},
doi = {10.1145/1498759.1498810},
abstract = {Tagging has emerged as a popular means to annotate on-line objects such as bookmarks, photos and videos. Tags vary in semantic meaning and can describe different aspects of a media object. Tags describe the content of the media as well as locations, dates, people and other associated meta-data. Being able to automatically classify tags into semantic categories allows us to understand better the way users annotate media objects and to build tools for viewing and browsing the media objects. In this paper we present a generic method for classifying tags using third party open content resources, such as Wikipedia and the Open Directory. Our method uses structural patterns that can be extracted from resource meta-data. We describe the implementation of our method on Wikipedia using WordNet categories as our classification schema and ground truth. Two structural patterns found in Wikipedia are used for training and classification: categories and templates. We apply our system to classifying Flickr tags. Compared to a WordNet baseline our method increases the coverage of the Flickr vocabulary by 115%. We can classify many important entities that are not covered by WordNet, such as, London Eye, Big Island, Ronaldinho, geocaching and wii.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {64–73},
numpages = {10},
keywords = {wikipedia, Flickr, categorization, multimedia annotation, user-generated content},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498811,
author = {Wang, Xuerui and Broder, Andrei and Gabrilovich, Evgeniy and Josifovski, Vanja and Pang, Bo},
title = {Cross-Language Query Classification Using Web Search for Exogenous Knowledge},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498811},
doi = {10.1145/1498759.1498811},
abstract = {The non-English Web is growing at phenomenal speed, but available language processing tools and resources are predominantly English-based. Taxonomies are a case in point: while there are plenty of commercial and non-commercial taxonomies for the English Web, taxonomies for other languages are either not available or of arguable quality. Given that building comprehensive taxonomies for each language is prohibitively expensive, it is natural to ask whether existing English taxonomies can be leveraged, possibly via machine translation, to enable text processing tasks in other languages. Our experimental results confirm that the answer is affirmative with respect to at least one task. In this study we focus on query classification, which is essential for understanding the user intent both in Web search and in online advertising. We propose a robust method for classifying non-English queries into an English taxonomy, using an existing English text classifier and off-the-shelf machine translation systems. In particular, we show that by considering the Web search results in the query's original language as additional sources of information, we can alleviate the effect of erroneous machine translation. Empirical evaluation on query sets in languages as diverse as Chinese and Russian yields very encouraging results; consequently, we believe that our approach is also applicable to many additional languages.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {74–83},
numpages = {10},
keywords = {relevance feedback, web search, machine translation, query classification},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498812,
author = {Chen, Ling and Wright, Phillip and Nejdl, Wolfgang},
title = {Improving Music Genre Classification Using Collaborative Tagging Data},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498812},
doi = {10.1145/1498759.1498812},
abstract = {As a fundamental and critical component of music information retrieval (MIR) systems, music genre classification has attracted considerable research attention. Automatically classifying music by genre is, however, a challenging problem due to the fact that music is an evolving art. While most of the existing work categorizes music using features extracted from music audio signals, in this paper, we propose to exploit the semantic information embedded in tags supplied by users of social networking websites. Particularly, we consider the tag information by creating a graph of tracks so that tracks are neighbors if they are similar in terms of their associated tags. Two classification methods based on the track graph are developed. The first one employs a classification scheme which simultaneously considers the audio content and neighborhood of tracks. In contrast, the second one is a two-level classifier which initializes genre label for unknown tracks using their audio content, and then iteratively updates the genres considering the influence from their neighbors. A set of optimizing strategies are designed for the purpose of further enhancing the quality of the two-level classifier. Extensive experiments are conducted on real-world data collected from Last.fm. Promising experimental results demonstrate the benefit of using tags for accurate music genre classification.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {84–93},
numpages = {10},
keywords = {relaxation labeling, exploiting tag semantics, music genre classification},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498813,
author = {Adar, Eytan and Skinner, Michael and Weld, Daniel S.},
title = {Information Arbitrage across Multi-Lingual Wikipedia},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498813},
doi = {10.1145/1498759.1498813},
abstract = {The rapid globalization of Wikipedia is generating a parallel, multi-lingual corpus of unprecedented scale. Pages for the same topic in many different languages emerge both as a result of manual translation and independent development. Unfortunately, these pages may appear at different times, vary in size, scope, and quality. Furthermore, differential growth rates cause the conceptual mapping between articles in different languages to be both complex and dynamic. These disparities provide the opportunity for a powerful form of information arbitrage--leveraging articles in one or more languages to improve the content in another. Analyzing four large language domains (English, Spanish, French, and German), we present Ziggurat, an automated system for aligning Wikipedia infoboxes, creating new infoboxes as necessary, filling in missing information, and detecting discrepancies between parallel pages. Our method uses self-supervised learning and our experiments demonstrate the method's feasibility, even in the absence of dictionaries.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {94–103},
numpages = {10},
keywords = {multi-lingual, Wikipedia, translation, information arbitrage},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498815,
author = {Bollegala, Danushka and Matsuo, Yutaka and Ishizuka, Mitsuru},
title = {Measuring the Similarity between Implicit Semantic Relations Using Web Search Engines},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498815},
doi = {10.1145/1498759.1498815},
abstract = {Measuring the similarity between implicit semantic relations is an important task in information retrieval and natural language processing. For example, consider the situation where you know an entity-pair (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition), and you are interested in retrieving other entity-pairs for which the same relation holds (e.g. Yahoo, Inktomi). Existing keyword-based search engines cannot be directly applied in this case because in keyword-based search, the goal is to retrieve documents that are relevant to the words used in the query -- not necessarily to the relations implied by a pair of words. Accurate measurement of relational similarity is an important step in numerous natural language processing tasks such as identification of word analogies, and classification of noun-modifier pairs. We propose a method that uses Web search engines to efficiently compute the relational similarity between two pairs of words. Our method consists of three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different semantic relations implied by them, and measuring the similarity between different semantic relations using an inter-cluster correlation matrix. We propose a pattern extraction algorithm to extract a large number of lexical patterns that express numerous semantic relations. We then present an efficient clustering algorithm to cluster the extracted lexical patterns. Finally, we measure the relational similarity between word-pairs using inter-cluster correlation. We evaluate the proposed method in a relation classification task. Experimental results on a dataset covering multiple relation types show a statistically significant improvement over the current state-of-the-art relational similarity measures.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {104–113},
numpages = {10},
keywords = {web mining, relational similarity measures},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498816,
author = {Pereira, \'{A}lvaro and Baeza-Yates, Ricardo and Ziviani, Nivio and Bisbal, Jes\'{u}s},
title = {A Model for Fast Web Mining Prototyping},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498816},
doi = {10.1145/1498759.1498816},
abstract = {Web mining is a computation intensive task, even after the mining tool itself has been developed. Most mining software are developed ad-hoc and usually are not scalable nor reused for other mining tasks. The objective of this paper is to present a model for fast Web mining prototyping, referred to as WIM -- Web Information Mining. The underlying conceptual model of WIM provides its users with a level of abstraction appropriate for prototyping and experimentation throughout the Web data mining task. Abstracting from the idiosyncrasies of raw Web data representations facilitates the inherently iterative mining process. We present the WIM conceptual model, its associated algebra, and the WIM tool software architecture, which implements the WIM model. We also illustrate how the model can be applied to real Web data mining tasks. The experimentation of WIM in real use cases has shown to significantly facilitate Web mining prototyping.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {114–123},
numpages = {10},
keywords = {web mining, model, prototyping, web mining applications},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498818,
author = {Guo, Fan and Liu, Chao and Wang, Yi Min},
title = {Efficient Multiple-Click Models in Web Search},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498818},
doi = {10.1145/1498759.1498818},
abstract = {Many tasks that leverage web search users' implicit feedback rely on a proper and unbiased interpretation of user clicks. Previous eye-tracking experiments and studies on explaining position-bias of user clicks provide a spectrum of hypotheses and models on how an average user examines and possibly clicks web documents returned by a search engine with respect to the submitted query. In this paper, we attempt to close the gap between previous work, which studied how to model a single click, and the reality that multiple clicks on web documents in a single result page are not uncommon. Specifically, we present two multiple-click models: the independent click model (ICM) which is reformulated from previous work, and the dependent click model (DCM) which takes into consideration dependencies between multiple clicks. Both models can be efficiently learned with linear time and space complexities. More importantly, they can be incrementally updated as new click logs flow in. These are well-demanded properties in reality.We systematically evaluate the two models on click logs obtained in July 2008 from a major commercial search engine. The data set, after preprocessing, contains over 110 thousand distinct queries and 8.8 million query sessions. Extensive experimental studies demonstrate the gain of modeling multiple clicks and their dependencies. Finally, we note that since our experimental setup does not rely on tweaking search result rankings, it can be easily adopted by future studies.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {124–131},
numpages = {8},
keywords = {click log analysis, statistical models, web search},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498819,
author = {White, Ryen W. and Dumais, Susan T. and Teevan, Jaime},
title = {Characterizing the Influence of Domain Expertise on Web Search Behavior},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498819},
doi = {10.1145/1498759.1498819},
abstract = {Domain experts search differently than people with little or no domain knowledge. Previous research suggests that domain experts employ different search strategies and are more successful in finding what they are looking for than non-experts. In this paper we present a large-scale, longitudinal, log-based analysis of the effect of domain expertise on web search behavior in four different domains (medicine, finance, law, and computer science). We characterize the nature of the queries, search sessions, web sites visited, and search success for users identified as experts and non-experts within these domains. Large-scale analysis of real-world interactions allows us to understand how expertise relates to vocabulary, resource use, and search task under more realistic search conditions than has been possible in previous small-scale studies. Building upon our analysis we develop a model to predict expertise based on search behavior, and describe how knowledge about domain expertise can be used to present better results and query suggestions to users and to help non-experts gain expertise.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {132–141},
numpages = {10},
keywords = {web search, domain expertise},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498820,
author = {Suryanto, Maggy Anastasia and Lim, Ee Peng and Sun, Aixin and Chiang, Roger H. L.},
title = {Quality-Aware Collaborative Question Answering: Methods and Evaluation},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498820},
doi = {10.1145/1498759.1498820},
abstract = {Community Question Answering (QA) portals contain questions and answers contributed by hundreds of millions of users. These databases of questions and answers are of great value if they can be used directly to answer questions from any user. In this research, we address this collaborative QA task by drawing knowledge from the crowds in community QA portals such as Yahoo! Answers. Despite their popularity, it is well known that answers in community QA portals have unequal quality. We therefore propose a quality-aware framework to design methods that select answers from a community QA portal considering answer quality in addition to answer relevance. Besides using answer features for determining answer quality, we introduce several other quality-aware QA methods using answer quality derived from the expertise of answerers. Such expertise can be question independent or question dependent. We evaluate our proposed methods using a database of 95K questions and 537K answers obtained from Yahoo! Answers. Our experiments have shown that answer quality can improve QA performance significantly. Furthermore, question dependent expertise based methods are shown to outperform methods using answer features only. It is also found that there are also good answers not among the best answers identified by Yahoo! Answers users.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {142–151},
numpages = {10},
keywords = {expertise, answer quality, question answering},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498821,
author = {Xu, Songhua and Jin, Tao and Lau, Francis C. M.},
title = {A New Visual Search Interface for Web Browsing},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498821},
doi = {10.1145/1498759.1498821},
abstract = {We introduce a new visual search interface for search engines. The interface is a user-friendly and informative graphical front-end for organizing and presenting search results in the form of topic groups. Such a semantics-oriented search result presentation is in contrast with conventional search interfaces which present search results according to the physical structures of the information. Given a user query, our interface first retrieves relevant online materials via a third-party search engine. And then we analyze the semantics of search results to detect latent topics in the result set. Once the topics are detected, we map the search result pages into topic clusters. According to the topic clustering result, we divide the available screen space for our visual interface into multiple topic displaying regions, one for each topic. For each topic's displaying region, we summarize the information contained in the search results under the corresponding topic so that only key messages will be displayed. With this new visual search interface, users are conveyed the key information in the search results expediently. With the key information, users can navigate to the final, desired results with less effort and time than conventional searching. Supplementary materials for this paper are available at http://www.cs.hku.hk/~songhua/visualsearch/.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {152–161},
numpages = {10},
keywords = {online browsing and navigation, web search, document summarization, visual search interface},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@dataset{10.1145/review-1498759.1498821_R45071,
author = {Moody, Scott Arthur},
title = {Review ID:R45071 for DOI: 10.1145/1498759.1498821},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1498759.1498821_R45071}
}

@inproceedings{10.1145/1498759.1498823,
author = {Piwowarski, Benjamin and Dupret, Georges and Jones, Rosie},
title = {Mining User Web Search Activity with Layered Bayesian Networks or How to Capture a Click in Its Context},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498823},
doi = {10.1145/1498759.1498823},
abstract = {Mining user web search activity potentially has a broad range of applications including web result pre-fetching, automatic search query reformulation, click spam detection, estimation of document relevance and prediction of user satisfaction. This analysis is difficult because the data recorded by search engines while users interact with them, although abundant, is very noisy. In this work, we explore the utility of mining search behavior of users, represented by observed variables including the time the user spends on the page, and whether the user reformulated his or her query. As a case study, we examine the contribution this data makes to predicting the relevance of a document in the absence of document content models. To this end, we first propose a method for grouping the interactions of a particular user according to the different tasks he or she undertakes. With each task corresponding to a distinct information need, we then propose a Bayesian Network to holistically model these interactions. The aim is to identify distinct patterns of search behaviors. Finally, we join these patterns to a list of custom features and we use gradient boosted decision trees to predict the relevance of a set of query document pairs for which we have relevance assessments. The experimental results confirm the potential of our model, with significant improvements in precision for predicting the relevance of documents based on a model of the user's search and click behavior, over a baseline model using only click and query features, with no Bayesian Network input.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {162–171},
numpages = {10},
keywords = {relevance prediction, user modelling, click-through data, web retrieval, query log analysis},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498824,
author = {Agrawal, R. and Halverson, A. and Kenthapadi, K. and Mishra, N. and Tsaparas, P.},
title = {Generating Labels from Clicks},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498824},
doi = {10.1145/1498759.1498824},
abstract = {The ranking function used by search engines to order results is learned from labeled training data. Each training point is a (query, URL) pair that is labeled by a human judge who assigns a score of Perfect, Excellent, etc., depending on how well the URL matches the query. In this paper, we study whether clicks can be used to automatically generate good labels. Intuitively, documents that are clicked (resp., skipped) in aggregate can indicate relevance (resp., lack of relevance). We give a novel way of transforming clicks into weighted, directed graphs inspired by eye-tracking studies and then devise an objective function for finding cuts in these graphs that induce a good labeling. In its full generality, the problem is NP-hard, but we show that, in the case of two labels, an optimum labeling can be found in linear time. For the more general case, we propose heuristic solutions. Experiments on real click logs show that click-based labels align with the opinion of a panel of judges, especially as the consensus of the panel grows stronger.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {172–181},
numpages = {10},
keywords = {graph partitioning, generating training data},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498825,
author = {Diaz, Fernando},
title = {Integration of News Content into Web Results},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498825},
doi = {10.1145/1498759.1498825},
abstract = {Aggregated search refers to the integration of content from specialized corpora or verticals into web search results. Aggregation improves search when the user has vertical intent but may not be aware of or desire vertical search. In this paper, we address the issue of integrating search results from a news vertical into web search results. News is particularly challenging because, given a query, the appropriate decision---to integrate news content or not---changes with time. Our system adapts to news intent in two ways. First, by inspecting the dynamics of the news collection and query volume, we can track development of and interest in topics. Second, by using click feedback, we can quickly recover from system errors. We define several click-based metrics which allow a system to be monitored and tuned without annotator effort.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {182–191},
numpages = {10},
keywords = {distributed information retrieval, query similarity, click prediction, news search},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498826,
author = {Wang, Xiang and Zhang, Kai and Jin, Xiaoming and Shen, Dou},
title = {Mining Common Topics from Multiple Asynchronous Text Streams},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498826},
doi = {10.1145/1498759.1498826},
abstract = {Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {192–201},
numpages = {10},
keywords = {topic model, temporal text mining, asynchronous streams},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498827,
author = {Kanungo, Tapas and Orr, David},
title = {Predicting the Readability of Short Web Summaries},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498827},
doi = {10.1145/1498759.1498827},
abstract = {Readability is a crucial presentation attribute that web summarization algorithms consider while generating a querybaised web summary. Readability quality also forms an important component in real-time monitoring of commercial search-engine results since readability of web summaries impacts clickthrough behavior, as shown in recent studies, and thus impacts user satisfaction and advertising revenue.The standard approach to computing the readability is to first collect a corpus of random queries and their corresponding search result summaries, and then each summary is then judged by a human for its readabilty quality. An average readability score is then reported. This process is time consuming and expensive. Besides, the manual evaluation process can not be used in the real-time summary generation process. In this paper we propose a machine learning approach to the problem. We use the corpus as described above and extract summary features that we think may characterize readability. We then estimate a model (gradient boosted decision tree) that predicts human judgments given the features. This model can then be used in real time to estimate the readability of new (unseen) web search summaries and also be used in the summary generation process.We present results on approximately 5000 editorial judgments collected over the course of a year and show examples where the model predicts the quality well and where it disagrees with human judgments. We compare the results of the model to previous models of readability, most notably Collins-Thompson-Callan, Fog and Flesch-Kincaid, and see that our model shows substantially better correlation with editorial judgments as measured by Pearson's correlation coefficient. The learning algorithm also provides us with the relative importance of the features used.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {202–211},
numpages = {10},
keywords = {gradient boosted decision trees, summarization, readability, realtime},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498829,
author = {Deng, Hongbo and Lyu, Michael R. and King, Irwin},
title = {Effective Latent Space Graph-Based Re-Ranking Model with Global Consistency},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498829},
doi = {10.1145/1498759.1498829},
abstract = {Recently the re-ranking algorithms have been quite popular for web search and data mining. However, one of the issues is that those algorithms treat the content and link information individually. Inspired by graph-based machine learning algorithms, we propose a novel and general framework to model the re-ranking algorithm, by regularizing the smoothness of ranking scores over the graph, along with a regularizer on the initial ranking scores (which are obtained by the base ranker). The intuition behind the model is the global consistency over the graph: similar entities are likely to have the same ranking scores with respect to a query. Our approach simultaneously incorporates the content with other explicit or implicit link information in a latent space graph. Then an effective unified re-ranking algorithm is performed on the graph with respect to the query. To illustrate our methodology, we apply the framework to literature retrieval and expert finding applications on DBLP bibliography data. We compare the proposed method with the initial language model method and another PageRank-style re-ranking method. Also, we evaluate the proposed method with varying graphs and settings. Experimental results show that the improvement in our proposed method is consistent and promising.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {212–221},
numpages = {10},
keywords = {latent space, graph-based re-ranking model, DBLP, expert finding, regularization},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498830,
author = {Kumar, Ravi and Punera, Kunal and Suel, Torsten and Vassilvitskii, Sergei},
title = {Top-<i>k</i> Aggregation Using Intersections of Ranked Inputs},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498830},
doi = {10.1145/1498759.1498830},
abstract = {There has been considerable past work on efficiently computing top k objects by aggregating information from multiple ranked lists of these objects. An important instance of this problem is query processing in search engines: One has to combine information from several different posting lists (rankings) of web pages (objects) to obtain the top k web pages to answer user queries. Two particularly well-studied approaches to achieve efficiency in top-k aggregation include early-termination algorithms (e.g., TA and NRA) and preaggregation of some of the input lists. However, there has been little work on a rigorous treatment of combining these approaches.We generalize the TA and NRA algorithms to the case when preaggregated intersection lists are available in addition to the original lists. We show that our versions of TA and NRA continue to remain "instance optimal," a very strong optimality notion that is a highlight of the original TA and NRA algorithms. Using an index of millions of web pages and real-world search engine queries, we empirically characterize the performance gains offered by our new algorithms. We show that the practical benefits of intersection lists can be fully realized only with an early-termination algorithm.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {222–231},
numpages = {10},
keywords = {intersections, early-termination, TA, NRA},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498831,
author = {Kamps, Jaap and Koolen, Marijn},
title = {Is Wikipedia Link Structure Different?},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498831},
doi = {10.1145/1498759.1498831},
abstract = {In this paper, we investigate the difference between Wikipedia and Web link structure with respect to their value as indicators of the relevance of a page for a given topic of request. Our experimental evidence is from two IR test-collections: the .GOV collection used at the TREC Web tracks and the Wikipedia XML Corpus used at INEX. We first perform a comparative analysis of Wikipedia and .GOV link structure and then investigate the value of link evidence for improving search on Wikipedia and on the .GOV domain. Our main findings are: First, Wikipedia link structure is similar to the Web, but more densely linked. Second, Wikipedia's outlinks behave similar to inlinks and both are good indicators of relevance, whereas on the Web the inlinks are more important. Third, when incorporating link evidence in the retrieval model, for Wikipedia the global link evidence fails and we have to take the local context into account.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {232–241},
numpages = {10},
keywords = {Wikipedia, link evidence, web information retrieval},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@dataset{10.1145/review-1498759.1498831_R45873,
author = {Ramaswamy, Srini},
title = {Review ID:R45873 for DOI: 10.1145/1498759.1498831},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1498759.1498831_R45873}
}

@inproceedings{10.1145/1498759.1498832,
author = {Najork, Marc and Gollapudi, Sreenivas and Panigrahy, Rina},
title = {Less is More: Sampling the Neighborhood Graph Makes SALSA Better and Faster},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498832},
doi = {10.1145/1498759.1498832},
abstract = {In this paper, we attempt to improve the effectiveness and the efficiency of query-dependent link-based ranking algorithms such as HITS, MAX and SALSA. All these ranking algorithms view the results of a query as nodes in the web graph, expand the result set to include neighboring nodes, and compute scores on the induced neighborhood graph. In previous work it was shown that SALSA in particular is substantially more effective than query-independent link-based ranking algorithms such as PageRank. In this work, we show that whittling down the neighborhood graph through consistent sampling of nodes and edges makes SALSA and its cousins both faster (more efficient) and better (more effective). We offer a hypothesis as to why "less is more", i.e. why using a reduced graph improves performance.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {242–251},
numpages = {10},
keywords = {link-based ranking, retrieval performance, MAX, SALSA, web search, HITS},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498834,
author = {Singla, Adish and Weber, Ingmar},
title = {Camera Brand Congruence in the Flickr Social Graph},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498834},
doi = {10.1145/1498759.1498834},
abstract = {Given that my friends on Flickr use cameras of brand X, am I more likely to also use a camera of brand X? Given that one of these friends changes her brand, am I likely to do the same? These are the kind of questions addressed in this work. Direct applications involve personalized advertising in social networks.For our study we crawled a complete connected component of the Flickr friendship graph with a total of 67M edges and 3.9M users. Camera brands and models were assigned to users and time slots according to the model specific meta data pertaining to their images taken during these time slots. Similarly, we used, where provided in a user's profile, information about a user's geographic location and the groups joined on Flickr.Our main findings are the following. First, a pair of friends on Flickr has a significantly higher probability of being congruent, i.e., using the same brand, compared to two random users (27% vs. 19%). Second, the degree of congruence goes up for pairs of friends (i) in the same country (29%), (ii) who both only have very few friends (30%), and (iii) with a very high cliqueness (38%). Third, given that a user changes her camera model between March-May 2007 and March-May 2008, high cliqueness friends are more likely than random users to do the same (54% vs. 48%). Fourth, users using high-end cameras are far more loyal to their brand than users using point-and-shoot cameras, with a probability of staying with the same brand of 60% vs 33%, given that a new camera is bought. Fifth, these "expert" users' brand congruence reaches 66% (!) for high cliqueness friends.To the best of our knowledge this is the first time that the phenomenon of brand congruence is studied for hundreds of thousands of users and over a period of two years.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {252–261},
numpages = {10},
keywords = {social network, brand loyalty, brand congruence, Flickr},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498835,
author = {Bendersky, Michael and Croft, W. Bruce},
title = {Finding Text Reuse on the Web},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498835},
doi = {10.1145/1498759.1498835},
abstract = {With the overwhelming number of reports on similar events originating from different sources on the web, it is often hard, using existing web search paradigms, to find the original source of "facts", statements, rumors, and opinions, and to track their development. Several techniques have been previously proposed for detecting such text reuse between different sources, however these techniques have been tested against relatively small and homogeneous TREC collections. In this work, we test the feasibility of text reuse detection techniques in the setting of web search. In addition to text reuse detection, we develop a novel technique that addresses the unique challenges of finding original sources on the web, such as defining a timeline. We also explore the use of link analysis for identifying reliable and relevant reports. Our experimental results show that the proposed techniques can operate on the scale of the web, are significantly more accurate than standard web search for finding text reuse, and provide a richer representation for tracking the information flow.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {262–271},
numpages = {10},
keywords = {web search, information flow, text reuse},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498836,
author = {Karande, Chinmay and Chellapilla, Kumar and Andersen, Reid},
title = {Speeding up Algorithms on Compressed Web Graphs},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498836},
doi = {10.1145/1498759.1498836},
abstract = {A variety of lossless compression schemes have been proposed to reduce the storage requirements of web graphs. One successful approach is virtual node compression [7], in which often-used patterns of links are replaced by links to virtual nodes, creating a compressed graph that succinctly represents the original. In this paper, we show that several important classes of web graph algorithms can be extended to run directly on virtual node compressed graphs, such that their running times depend on the size of the compressed graph rather than the original. These include algorithms for link analysis, estimating the size of vertex neighborhoods, and a variety of algorithms based on matrix-vector products and random walks. Similar speed-ups have been obtained previously for classical graph algorithms like shortest paths and maximum bipartite matching. We measure the performance of our modified algorithms on several publicly available web graph datasets, and demonstrate significant empirical speedups that nearly match the compression ratios.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {272–281},
numpages = {10},
keywords = {graph compression, stochastic processes, algorithms, web graph analysis},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@inproceedings{10.1145/1498759.1498837,
author = {Adar, Eytan and Teevan, Jaime and Dumais, Susan T. and Elsas, Jonathan L.},
title = {The Web Changes Everything: Understanding the Dynamics of Web Content},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498837},
doi = {10.1145/1498759.1498837},
abstract = {The Web is a dynamic, ever changing collection of information. This paper explores changes in Web content by analyzing a crawl of 55,000 Web pages, selected to represent different user visitation patterns. Although change over long intervals has been explored on random (and potentially unvisited) samples of Web pages, little is known about the nature of finer grained changes to pages that are actively consumed by users, such as those in our sample. We describe algorithms, analyses, and models for characterizing changes in Web content, focusing on both time (by using hourly and sub-hourly crawls) and structure (by looking at page-, DOM-, and term-level changes). Change rates are higher in our behavior-based sample than found in previous work on randomly sampled pages, with a large portion of pages changing more than hourly. Detailed content and structure analyses identify stable and dynamic content within each page. The understanding of Web change we develop in this paper has implications for tools designed to help people interact with dynamic Web content, such as search engines, advertising, and Web browsers.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
pages = {282–291},
numpages = {10},
keywords = {web page dynamics, re-finding, change},
location = {Barcelona, Spain},
series = {WSDM '09}
}

